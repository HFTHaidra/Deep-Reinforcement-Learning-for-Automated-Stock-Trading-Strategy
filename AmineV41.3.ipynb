{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AmineV41.3.ipynb","provenance":[],"collapsed_sections":["V8_namfrWOg9","E4pj1K8_WOiU"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V8_namfrWOg9"},"source":["> 1. > # HyperParameter\n","***DRL for stock trading*:**\n","Deep Reinforcement Learning for Automated Stock Trading\n","Using reinforcement learning to trade multiple stocks through Python and OpenAI Gym"]},{"cell_type":"code","metadata":{"id":"VbRtbKLjWOhc"},"source":["#input_shape=(Memoryin_window_length,ob_space_Length,window)\n","Memoryin_window_length      = 10\n","window                      = 360\n","ob_space_Length             = 5\n","actions                     = 3\n","MemoryLimit                 = 1000000\n","#----------------------------------------------------------- \n","fee                         = 3.0\n","SL                          = 25.0\n","TP                          = 25.0\n","#------------------------------------------------------------\n","steps = 1000\n","Looping = 2\n","sIndex                        = 4000\n","eIndex                        = 3200000    # MAX = 3.490.000 [ 290.000 Traning 200 Evalation]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzyorWIRWOhe"},"source":["from google.colab import drive ; drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7SB58R8WOhf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619354502497,"user_tz":0,"elapsed":688,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"87af8350-def4-467d-d1bf-a9588b7c024f"},"source":["!pip install gym keras keras-rl2 tensorflow==2.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Successfully installed gast-0.2.2 keras-applications-1.0.8 keras-rl2-1.0.4 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ltZWqNRRWOhl"},"source":["import os\n","import pandas as pd \n","from datetime import datetime as dt\n","from gym import Env\n","from gym.spaces import Discrete, Box\n","import numpy as np\n","from numpy import loadtxt\n","from datetime import datetime\n","from numpy import savetxt\n","import random ; from random import randint\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, Dense , Flatten \n","from tensorflow.keras.optimizers import Adam\n","from rl.policy import * ; from rl.memory import * ; from rl.agents import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E4pj1K8_WOiU"},"source":["# Reloud AI"]},{"cell_type":"code","metadata":{"id":"R-BBF-RoYjW9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619354557183,"user_tz":0,"elapsed":15114,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"0ade39f2-377d-42a4-d2ad-0fb22c92c39b"},"source":["#os.chdir(\"/content/drive/MyDrive\") \n","#/content/2015_2021_M1_US30All.csv\n","#data = loadtxt('DATA2/data.csv', delimiter=',') \n","#info = loadtxt('DATA2/info.csv', delimiter=',')  \n","\n","data = loadtxt('/content/2015_2021_M1_US30All.csv', delimiter=',' , encoding='utf-16') \n","#data = loadtxt('DATA2/US30_from2021.csv', delimiter=',' , encoding='utf-16')\n","prices = data[:,1]\n","dates = data[:,0]\n","dates = dates.astype(int)\n","len(prices)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1940371"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"GsSA8uBthgH7"},"source":["def GrapheV15(stats):\n","    plt.figure(figsize=(16, 9)) \n","    x = stats[:,0]\n","    y = np.arange(0,len(x) ) \n","    plt.plot(  y ,   x   , label=\"US30 Line\" )\n","    plt.plot(  y ,  stats[:,1]    , label=\"US30 Line\" )\n","    plt.plot(  y ,  stats[:,2]    , label=\"US30 Line\" )\n","    plt.plot(  y ,  stats[:,3]    , label=\"US30 Line\" )\n","    plt.plot(  y ,  stats[:,4]    , label=\"US30 Line\" )\n","    plt.show()\n","def GrapheV16(price):\n","    plt.figure(figsize=(16, 9)) \n","    x = price\n","    y = np.arange(0,len(x) ) \n","    plt.plot(  y ,   x   , label=\"US30 Line\" )\n","    plt.show()    \n","def showV409(nb):\n","  x = []\n","  y = []\n","  p = 0\n","  s = 0\n","  for i in range(0, len(env.R)  ):\n","    for j in range( 0 , len(env.R[i])   ):\n","      s = s + env.R[i][j]\n","      if ( env.R[i][j] != 0 ):\n","        x.append(s)\n","        y.append(p)\n","        p = p + 1\n","  k = nb\n","  l = 0\n","  print( ' All Rewards : {} \\n'.format(  x[ len(x)-1 ]  ) )\n","  for i in range(0,k):\n","    m = int (len(x)/k )\n","    x1 = x[ l:(m + m*i) ]\n","    y1 = y[ l:(m + m*i) ]\n","    plt.figure(figsize=(16, 9))\n","    plt.plot( y1, x1, label=\"US30 Line\") \n","    plt.xlabel(\"x axis\")\n","    plt.ylabel(\"y axis\")\n","    plt.title(\"Line Graph Example\")\n","    plt.show()  # 200\n","    l = (m + m*i)    \n","def ShowReward(nb,AllReward):\n","  x = []\n","  y = []\n","  p = 0\n","  s = 0\n","  for i in range(0, len(AllReward)  ):\n","    if ( AllReward[i] != 0 ):\n","      s+=AllReward[i]\n","      x.append(s)\n","      y.append(p)\n","      p = p + 1\n","  k = nb\n","  l = 0\n","  if(len(x)>2):\n","    print( ' All Rewards : {} \\n'.format(  x[ len(x)-1 ]  ) )\n","  for i in range(0,k):\n","    m = int (len(x)/k )\n","    x1 = x[ l:(m + m*i) ]\n","    y1 = y[ l:(m + m*i) ]\n","    plt.figure(figsize=(16, 9))\n","    plt.plot( y1, x1, label=\"US30 Line\") \n","    plt.xlabel(\"x axis\")\n","    plt.ylabel(\"y axis\")\n","    plt.title(\"Line Graph Example\")\n","    plt.show()  # 200\n","    l = (m + m*i)         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4wUU7NdYjqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619354575230,"user_tz":0,"elapsed":1068,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"f6ee0f39-7df5-4af3-8360-ab1b8ac8769c"},"source":["def build_model(ob_space_Length, window, actions,Memoryin_window_length):\n","    model = Sequential()\n","    model.add(Dense(128,activation='relu', input_shape=(Memoryin_window_length,ob_space_Length,window) ) )\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(actions))\n","    return model    \n","def build_agent(model, actions,Memoryin_window_length):\n","    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n","    memory = SequentialMemory(limit= MemoryLimit , window_length=Memoryin_window_length)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n","                  enable_dueling_network=True, dueling_type='avg', \n","                   nb_actions=actions, nb_steps_warmup=1000 ,\n","                   gamma=0.95 \n","                  )\n","    return dqn  \n","model = build_model(ob_space_Length, window, actions,Memoryin_window_length)\n","model.summary()\n","dqn = build_agent(model, actions,Memoryin_window_length)\n","dqn.compile( Adam(lr=1e-4))    \n","#name  = 'W/MULTITRANING/WSaveddqn_15-21-22.h5f'\n","#dqn.load_weights(name)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 10, 5, 128)        46208     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10, 5, 256)        33024     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10, 5, 256)        65792     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10, 5, 128)        32896     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 6400)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 3)                 19203     \n","=================================================================\n","Total params: 197,123\n","Trainable params: 197,123\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j1dyCqVDWx7t"},"source":["def isShort( indexStart ):\n","  global prices , TP , SL , fee\n","  res = -1\n","  index = 1 #min + random.randint(0, 3)\n","  min = indexStart\n","  max = min+ 4000\n","  for i in range(min,max):\n","    if( prices[i] <= (prices[min]-TP) ):\n","      res = TP - fee\n","      index = i - min  \n","      break\n","    if( prices[i] >= (prices[min]+SL) ):\n","      res = -SL \n","      index = i - min    \n","      break\n","  if( res == -1 ):\n","    print(\"XLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLX\")  \n","    res = -SL -fee\n","    index = max - min\n","  return [res , index]\n","def isLong( indexStart ):\n","  global prices , TP , SL , fee\n","  res = -1\n","  index = 1 #min + random.randint(1, 3)\n","  min = indexStart\n","  max = min+ 4000\n","  for i in range(min,max):\n","    if( prices[i] >= (prices[min]+TP) ):\n","      res = TP - fee\n","      index = i - min   \n","      break\n","    if( prices[i] <= (prices[min]-SL) ):\n","      res = -SL \n","      index = i - min   \n","      break\n","  if( res == -1 ):\n","    print(\"XLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLXLX\")  \n","    res = -SL -fee\n","    index = max - min\n","  return [res , index]  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiBG_h2TRmaw"},"source":["def SMAOLD(Data,Periodes):\n","    n = len(Data)-1 \n","    res = [Data[0]]\n","    for i in range(0,n):\n","      iSMA = 0\n","      for j in range(i,i+Periodes):\n","        if(j>=n):\n","          iSMA = iSMA +Data[n]\n","        else:\n","          iSMA = iSMA + Data[j]\n","      iSMA = iSMA/Periodes  \n","      res.append(iSMA)\n","    return res "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJKqaQ_UURkx"},"source":["def getStats(price_):\n","  res  = []\n","  sma5 =  SMAOLD( price_, 5  )\n","  sma10 = SMAOLD( price_, 10 )\n","  sma20 = SMAOLD( price_, 20 )\n","  sma50 = SMAOLD( price_, 50 )\n","  for i in range(0,len(price_)):\n","    res.append( [ price_[i] ,sma5[i],sma10[i],sma20[i],sma50[i]  ] )\n","  res     = np.array(res)  \n","  ma = res.max()\n","  mi = res.min()\n","  res = ( res - mi )/(ma - mi) \n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flZRsFfStD_H","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1619354882836,"user_tz":0,"elapsed":788,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"dfcaef8a-490f-4de8-9dc7-2783e786fe6e"},"source":["class env1(Env ):\n","    def __init__(self,  prices  ,tp_,sl_, ifTesting):\n","         #------------  observation_space  observation_space --------------------------------------------\n","        global ob_space_Length , window  , fee , sIndex , eIndex \n","        self.windowM1 = window   # 360Min = 6H (Best View) \n","        self.prices = prices\n","        self.observation_space = Box(low= 0.0, high= 1.0, shape=( ob_space_Length ,self.windowM1), dtype=np.float64 )\n","        self.action_space = Discrete(3)\n"," \n","        #------------  Parameters --------------------------------------------\n","        self.DayStep =  1300 #1380\n","        self.startIndex = sIndex\n","        self.iwin = 0               # iWindow [ 0 - (length_data - window ) ]\n","        self.SL = sl_\n","        self.TP = tp_\n","        self.ifTesting = ifTesting\n","        self.nbtrade0 = 0\n","        self.nbtrade1 = 0\n"," \n","        self.R      = []\n","        self.Rmin   = []\n"," \n","    def step(self, action):\n","        reward = 0\n","        \n","        iaccPrice = self.startIndex  + self.windowM1 + self.iwin\n","        \n","        accPrice = self.prices[iaccPrice]\n","        newprice = 0\n","        \n","        if(action==0):# sell\n","          reward , index = isShort( iaccPrice  )\n","          self.iwin+=index\n","          if( reward>0 ):\n","            self.nbtrade1+=1\n","          else:\n","            self.nbtrade0+=1\n","          \n","        \n","        if(action==1):# Buy\n","          reward , index = isLong( iaccPrice  )\n","          self.iwin+=index\n","          if( reward>0 ):\n","            self.nbtrade1+=1\n","          else:\n","            self.nbtrade0+=1\n","           \n","        if(action==2):# none\n","          reward = 0\n","          self.iwin = self.iwin + 1\n"," \n","        \n","        \n","        # Check if shower is done\n","        if self.iwin >= ( self.DayStep - self.windowM1 -1  ) :\n","            done = True\n","            if( self.ifTesting==1 ):\n","              for i in range( len(self.Rmin) , 100 ):\n","                self.Rmin.append(0)\n","              self.R.append(self.Rmin)  \n","              self.Rmin = []\n","           \n","        else:\n","            done = False\n","        info_ = {}\n","        i2 = iaccPrice - self.startIndex\n","        i1 = iaccPrice - self.windowM1 - self.startIndex\n","        #print( ' Action : {} __  i1 : {} '.format(action,i1) )\n","        self.state = np.transpose( self.DayStats[i1:i2] )\n","        #GrapheV15(   np.transpose( self.state) )\n","        if(self.ifTesting==1):\n","          self.Rmin.append( reward )\n","        return self.state, reward, done, info_\n"," \n","    def render(self):\n","       r=0\n","         \n","    def reset(self):\n","        self.iwin = 0\n","        self.startIndex += self.DayStep + random.randint(1, 10) \n","        d1 = self.startIndex\n","        d2 = d1 + self.DayStep +  random.randint(5, 10) \n","        self.DayStats  = getStats( prices[d1:d2] )\n","\n","        i1 = self.iwin\n","        i2 = self.windowM1 + self.iwin\n","        \n","        self.state = np.transpose( self.DayStats[i1:i2]  )\n","\n","        if( d2 > eIndex  ):\n","          self.startIndex = sIndex\n","        #GrapheV15(self.DayStats)\n","        #GrapheV16(self.prices[d1:d2])\n","        # Reset shower time\n","        print( ' 1 : {}  '.format(self.nbtrade1+self.nbtrade0) )\n","        print('--->:{}:<---'.format( self.startIndex  ))\n","        self.nbtrade0 = 0\n","        self.nbtrade1 = 0\n","        return self.state\n","#-----------------------------------------------------------------\n"," \n","sIndex                        = 3300000\n","eIndex                        = 3400000\n","env = env1(    prices , TP, SL,1)\n","_ = dqn.test(env, nb_episodes=10  , visualize=False)\n"," \n","showV409(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing for 10 episodes ...\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-d1efce221597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0meIndex\u001b[0m                        \u001b[0;34m=\u001b[0m \u001b[0;36m3400000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv1\u001b[0m\u001b[0;34m(\u001b[0m    \u001b[0mprices\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mshowV409\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-d1efce221597>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDayStep\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDayStats\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgetStats\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miwin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-0feb37fc98d7>\u001b[0m in \u001b[0;36mgetStats\u001b[0;34m(price_)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mres\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msma5\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mSMAOLD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprice_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0msma10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMAOLD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprice_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msma20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMAOLD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mprice_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-89dd67d2f25d>\u001b[0m in \u001b[0;36mSMAOLD\u001b[0;34m(Data, Periodes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mSMAOLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPeriodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0miSMA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"]}]},{"cell_type":"code","metadata":{"id":"gsV3M__m26K-"},"source":[" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2beYwoC3Wy78"},"source":[" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyLs1bLPYGKg"},"source":[" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-Qa_kU4b_9g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619385238235,"user_tz":0,"elapsed":78939,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"980c5019-cf34-4520-86ed-2842645ca5fd"},"source":["# hjgh\n","sIndex                        = 2000\n","eIndex                        = 1600000\n","env = env1(    prices , TP, SL,0)\n","FileVersion = \"WSaved/Aminev7/\"\n","time = datetime.now()\n","time = time.strftime(\"%H-%M-%S\")\n","render = 0\n","steps = 500000\n","Looping = 1\n","NameSaving = \"testing\"\n","for i in range(0,Looping):\n","  dqn.fit(env, nb_steps=steps, visualize=False, verbose=2)\n","  time_ = datetime.now()\n","  time_ = time_.strftime(\"%H-%M-%S\")\n","  NameSaving = FileVersion+time_+'/' +\"WSaveddqn_\"+str(time_)+\".h5f\"\n","  print('IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII') \n","  print(NameSaving)\n","  print('IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n","  dqn.save_weights(NameSaving, overwrite=True)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","--->:89407:<---\n","   2126/500000: episode: 67, duration: 1.782s, episode steps:  19, steps per second:  11, episode reward: -87.000, mean reward: -4.579 [-25.000, 22.000], mean action: 1.158 [0.000, 2.000],  loss: 228.808106, mean_q: 81.961996, mean_eps: 0.809560\n"," 1 : 11  \n","--->:90713:<---\n","   2154/500000: episode: 68, duration: 2.503s, episode steps:  28, steps per second:  11, episode reward: -121.000, mean reward: -4.321 [-25.000, 22.000], mean action: 1.071 [0.000, 2.000],  loss: 239.300863, mean_q: 83.126048, mean_eps: 0.807445\n"," 1 : 18  \n","--->:92022:<---\n","   2178/500000: episode: 69, duration: 2.180s, episode steps:  24, steps per second:  11, episode reward: -162.000, mean reward: -6.750 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 225.797602, mean_q: 83.004098, mean_eps: 0.805105\n"," 1 : 14  \n","--->:93323:<---\n","   2204/500000: episode: 70, duration: 2.389s, episode steps:  26, steps per second:  11, episode reward: 26.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.115 [0.000, 2.000],  loss: 238.448606, mean_q: 82.059228, mean_eps: 0.802855\n"," 1 : 14  \n","--->:94628:<---\n","   2238/500000: episode: 71, duration: 3.030s, episode steps:  34, steps per second:  11, episode reward: 70.000, mean reward:  2.059 [-25.000, 22.000], mean action: 1.206 [0.000, 2.000],  loss: 239.674009, mean_q: 82.432747, mean_eps: 0.800155\n"," 1 : 16  \n","--->:95933:<---\n","   2262/500000: episode: 72, duration: 2.210s, episode steps:  24, steps per second:  11, episode reward: 51.000, mean reward:  2.125 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 247.554878, mean_q: 81.381268, mean_eps: 0.797545\n"," 1 : 13  \n","--->:97234:<---\n","   2279/500000: episode: 73, duration: 1.583s, episode steps:  17, steps per second:  11, episode reward: -43.000, mean reward: -2.529 [-25.000, 22.000], mean action: 0.765 [0.000, 2.000],  loss: 281.257182, mean_q: 85.567491, mean_eps: 0.795700\n"," 1 : 13  \n","--->:98542:<---\n","   2310/500000: episode: 74, duration: 2.846s, episode steps:  31, steps per second:  11, episode reward: 299.000, mean reward:  9.645 [-25.000, 22.000], mean action: 0.968 [0.000, 2.000],  loss: 236.951249, mean_q: 82.744081, mean_eps: 0.793540\n"," 1 : 20  \n","--->:99844:<---\n","   2356/500000: episode: 75, duration: 4.037s, episode steps:  46, steps per second:  11, episode reward: 71.000, mean reward:  1.543 [-25.000, 22.000], mean action: 0.913 [0.000, 2.000],  loss: 249.086985, mean_q: 83.058858, mean_eps: 0.790075\n"," 1 : 31  \n","--->:101148:<---\n","   2407/500000: episode: 76, duration: 4.368s, episode steps:  51, steps per second:  12, episode reward: -4.000, mean reward: -0.078 [-25.000, 22.000], mean action: 0.980 [0.000, 2.000],  loss: 219.667991, mean_q: 83.525489, mean_eps: 0.785710\n"," 1 : 34  \n","--->:102454:<---\n","   2431/500000: episode: 77, duration: 2.210s, episode steps:  24, steps per second:  11, episode reward: -40.000, mean reward: -1.667 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 210.207132, mean_q: 83.455522, mean_eps: 0.782335\n"," 1 : 11  \n","--->:103761:<---\n","   2446/500000: episode: 78, duration: 1.419s, episode steps:  15, steps per second:  11, episode reward: -34.000, mean reward: -2.267 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 212.457893, mean_q: 82.755628, mean_eps: 0.780580\n"," 1 : 7  \n","--->:105068:<---\n","   2488/500000: episode: 79, duration: 3.703s, episode steps:  42, steps per second:  11, episode reward: 221.000, mean reward:  5.262 [-25.000, 22.000], mean action: 1.119 [0.000, 2.000],  loss: 248.475375, mean_q: 81.476307, mean_eps: 0.778015\n"," 1 : 25  \n","--->:106378:<---\n","   2519/500000: episode: 80, duration: 2.710s, episode steps:  31, steps per second:  11, episode reward: 155.000, mean reward:  5.000 [-25.000, 22.000], mean action: 1.032 [0.000, 2.000],  loss: 227.016495, mean_q: 82.615305, mean_eps: 0.774730\n"," 1 : 22  \n","--->:107686:<---\n","   2532/500000: episode: 81, duration: 1.202s, episode steps:  13, steps per second:  11, episode reward: -56.000, mean reward: -4.308 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 220.858212, mean_q: 81.426778, mean_eps: 0.772750\n"," 1 : 6  \n","--->:108994:<---\n","   2537/500000: episode: 82, duration: 0.567s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.400 [0.000, 2.000],  loss: 301.789084, mean_q: 79.532344, mean_eps: 0.771940\n"," 1 : 4  \n","--->:110300:<---\n","   2570/500000: episode: 83, duration: 2.861s, episode steps:  33, steps per second:  12, episode reward: 230.000, mean reward:  6.970 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 230.447618, mean_q: 82.766399, mean_eps: 0.770230\n"," 1 : 19  \n","--->:111602:<---\n","   2601/500000: episode: 84, duration: 2.701s, episode steps:  31, steps per second:  11, episode reward: -74.000, mean reward: -2.387 [-25.000, 22.000], mean action: 1.129 [0.000, 2.000],  loss: 242.776564, mean_q: 80.709718, mean_eps: 0.767350\n"," 1 : 18  \n","--->:112906:<---\n","   2615/500000: episode: 85, duration: 1.317s, episode steps:  14, steps per second:  11, episode reward: 54.000, mean reward:  3.857 [-25.000, 22.000], mean action: 1.071 [0.000, 2.000],  loss: 303.207590, mean_q: 81.447352, mean_eps: 0.765325\n"," 1 : 11  \n","--->:114215:<---\n","   2629/500000: episode: 86, duration: 1.276s, episode steps:  14, steps per second:  11, episode reward: 57.000, mean reward:  4.071 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 251.172104, mean_q: 82.364814, mean_eps: 0.764065\n"," 1 : 9  \n","--->:115518:<---\n","   2636/500000: episode: 87, duration: 0.720s, episode steps:   7, steps per second:  10, episode reward: -9.000, mean reward: -1.286 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 254.002138, mean_q: 80.061510, mean_eps: 0.763120\n"," 1 : 6  \n","--->:116821:<---\n","   2642/500000: episode: 88, duration: 0.626s, episode steps:   6, steps per second:  10, episode reward: -56.000, mean reward: -9.333 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 250.958165, mean_q: 77.615283, mean_eps: 0.762535\n"," 1 : 6  \n","--->:118123:<---\n","   2669/500000: episode: 89, duration: 2.317s, episode steps:  27, steps per second:  12, episode reward: -46.000, mean reward: -1.704 [-25.000, 22.000], mean action: 1.148 [0.000, 2.000],  loss: 244.940719, mean_q: 83.556847, mean_eps: 0.761050\n"," 1 : 15  \n","--->:119424:<---\n","   2670/500000: episode: 90, duration: 0.214s, episode steps:   1, steps per second:   5, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 1.000 [1.000, 1.000],  loss: 390.639404, mean_q: 83.766594, mean_eps: 0.759790\n"," 1 : 1  \n","--->:120728:<---\n","   2683/500000: episode: 91, duration: 1.197s, episode steps:  13, steps per second:  11, episode reward: 57.000, mean reward:  4.385 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 232.955735, mean_q: 83.110201, mean_eps: 0.759160\n"," 1 : 9  \n","--->:122032:<---\n","   2722/500000: episode: 92, duration: 3.314s, episode steps:  39, steps per second:  12, episode reward: -171.000, mean reward: -4.385 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 237.896412, mean_q: 83.773295, mean_eps: 0.756820\n"," 1 : 20  \n","--->:123333:<---\n","   2734/500000: episode: 93, duration: 1.171s, episode steps:  12, steps per second:  10, episode reward: -12.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 237.116925, mean_q: 82.826347, mean_eps: 0.754525\n"," 1 : 8  \n","--->:124642:<---\n","   2759/500000: episode: 94, duration: 2.265s, episode steps:  25, steps per second:  11, episode reward: -93.000, mean reward: -3.720 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 234.630508, mean_q: 83.145242, mean_eps: 0.752860\n"," 1 : 15  \n","--->:125946:<---\n","   2787/500000: episode: 95, duration: 2.466s, episode steps:  28, steps per second:  11, episode reward: 23.000, mean reward:  0.821 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 249.494118, mean_q: 81.781581, mean_eps: 0.750475\n"," 1 : 16  \n","--->:127253:<---\n","   2807/500000: episode: 96, duration: 1.953s, episode steps:  20, steps per second:  10, episode reward: 26.000, mean reward:  1.300 [-25.000, 22.000], mean action: 0.950 [0.000, 2.000],  loss: 253.746445, mean_q: 80.936496, mean_eps: 0.748315\n"," 1 : 14  \n","--->:128554:<---\n","   2816/500000: episode: 97, duration: 0.985s, episode steps:   9, steps per second:   9, episode reward: 38.000, mean reward:  4.222 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 251.231583, mean_q: 85.028452, mean_eps: 0.747010\n"," 1 : 6  \n","--->:129864:<---\n","   2841/500000: episode: 98, duration: 2.444s, episode steps:  25, steps per second:  10, episode reward: 48.000, mean reward:  1.920 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 233.936734, mean_q: 81.632310, mean_eps: 0.745480\n"," 1 : 15  \n","--->:131167:<---\n","   2868/500000: episode: 99, duration: 2.688s, episode steps:  27, steps per second:  10, episode reward: -46.000, mean reward: -1.704 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 228.876034, mean_q: 84.636829, mean_eps: 0.743140\n"," 1 : 15  \n","--->:132473:<---\n","   2886/500000: episode: 100, duration: 1.835s, episode steps:  18, steps per second:  10, episode reward: -12.000, mean reward: -0.667 [-25.000, 22.000], mean action: 1.278 [0.000, 2.000],  loss: 203.944988, mean_q: 82.325542, mean_eps: 0.741115\n"," 1 : 8  \n","--->:133775:<---\n","   2915/500000: episode: 101, duration: 2.909s, episode steps:  29, steps per second:  10, episode reward: -52.000, mean reward: -1.793 [-25.000, 22.000], mean action: 1.138 [0.000, 2.000],  loss: 221.398972, mean_q: 81.173814, mean_eps: 0.739000\n"," 1 : 19  \n","--->:135081:<---\n","   2932/500000: episode: 102, duration: 1.690s, episode steps:  17, steps per second:  10, episode reward: -15.000, mean reward: -0.882 [-25.000, 22.000], mean action: 1.235 [0.000, 2.000],  loss: 221.661624, mean_q: 81.684280, mean_eps: 0.736930\n"," 1 : 10  \n","--->:136383:<---\n","   2948/500000: episode: 103, duration: 1.568s, episode steps:  16, steps per second:  10, episode reward: -15.000, mean reward: -0.938 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 244.453043, mean_q: 79.429352, mean_eps: 0.735445\n"," 1 : 10  \n","--->:137687:<---\n","   2982/500000: episode: 104, duration: 3.299s, episode steps:  34, steps per second:  10, episode reward: -30.000, mean reward: -0.882 [-25.000, 22.000], mean action: 1.059 [0.000, 2.000],  loss: 234.984541, mean_q: 83.836858, mean_eps: 0.733195\n"," 1 : 20  \n","--->:138988:<---\n","   3007/500000: episode: 105, duration: 2.369s, episode steps:  25, steps per second:  11, episode reward: 186.000, mean reward:  7.440 [-25.000, 22.000], mean action: 0.880 [0.000, 2.000],  loss: 240.425179, mean_q: 84.562789, mean_eps: 0.730540\n"," 1 : 17  \n","--->:140296:<---\n","   3031/500000: episode: 106, duration: 2.144s, episode steps:  24, steps per second:  11, episode reward:  4.000, mean reward:  0.167 [-25.000, 22.000], mean action: 1.208 [0.000, 2.000],  loss: 282.112910, mean_q: 79.841260, mean_eps: 0.728335\n"," 1 : 13  \n","--->:141603:<---\n","   3070/500000: episode: 107, duration: 3.362s, episode steps:  39, steps per second:  12, episode reward: 271.000, mean reward:  6.949 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 243.554494, mean_q: 82.070384, mean_eps: 0.725500\n"," 1 : 23  \n","--->:142913:<---\n","   3076/500000: episode: 108, duration: 0.669s, episode steps:   6, steps per second:   9, episode reward: -6.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 222.963783, mean_q: 81.979786, mean_eps: 0.723475\n"," 1 : 4  \n","--->:144222:<---\n","   3093/500000: episode: 109, duration: 1.564s, episode steps:  17, steps per second:  11, episode reward: -18.000, mean reward: -1.059 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 265.048175, mean_q: 80.484677, mean_eps: 0.722440\n"," 1 : 12  \n","--->:145530:<---\n","   3107/500000: episode: 110, duration: 1.300s, episode steps:  14, steps per second:  11, episode reward: 57.000, mean reward:  4.071 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 241.045944, mean_q: 83.925098, mean_eps: 0.721045\n"," 1 : 9  \n","--->:146836:<---\n","   3124/500000: episode: 111, duration: 1.543s, episode steps:  17, steps per second:  11, episode reward: 148.000, mean reward:  8.706 [-25.000, 22.000], mean action: 0.882 [0.000, 2.000],  loss: 229.136979, mean_q: 81.699935, mean_eps: 0.719650\n"," 1 : 11  \n","--->:148144:<---\n","   3150/500000: episode: 112, duration: 2.270s, episode steps:  26, steps per second:  11, episode reward: -93.000, mean reward: -3.577 [-25.000, 22.000], mean action: 1.269 [0.000, 2.000],  loss: 243.657100, mean_q: 81.574571, mean_eps: 0.717715\n"," 1 : 15  \n","--->:149452:<---\n","   3184/500000: episode: 113, duration: 2.933s, episode steps:  34, steps per second:  12, episode reward: -11.000, mean reward: -0.324 [-25.000, 22.000], mean action: 0.882 [0.000, 2.000],  loss: 247.832115, mean_q: 81.811304, mean_eps: 0.715015\n"," 1 : 23  \n","--->:150754:<---\n","   3221/500000: episode: 114, duration: 3.246s, episode steps:  37, steps per second:  11, episode reward: -64.000, mean reward: -1.730 [-25.000, 22.000], mean action: 0.892 [0.000, 2.000],  loss: 229.296882, mean_q: 84.617083, mean_eps: 0.711820\n"," 1 : 27  \n","--->:152064:<---\n","   3273/500000: episode: 115, duration: 4.416s, episode steps:  52, steps per second:  12, episode reward: -117.000, mean reward: -2.250 [-25.000, 22.000], mean action: 1.019 [0.000, 2.000],  loss: 239.526576, mean_q: 82.426861, mean_eps: 0.707815\n"," 1 : 31  \n","--->:153367:<---\n","   3294/500000: episode: 116, duration: 1.893s, episode steps:  21, steps per second:  11, episode reward: -65.000, mean reward: -3.095 [-25.000, 22.000], mean action: 1.238 [0.000, 2.000],  loss: 281.961370, mean_q: 82.881494, mean_eps: 0.704530\n"," 1 : 12  \n","--->:154675:<---\n","   3304/500000: episode: 117, duration: 0.973s, episode steps:  10, steps per second:  10, episode reward: 57.000, mean reward:  5.700 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 277.028575, mean_q: 81.940489, mean_eps: 0.703135\n"," 1 : 9  \n","--->:155977:<---\n","   3334/500000: episode: 118, duration: 2.588s, episode steps:  30, steps per second:  12, episode reward: -74.000, mean reward: -2.467 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 240.819860, mean_q: 82.611891, mean_eps: 0.701335\n"," 1 : 18  \n","--->:157283:<---\n","   3352/500000: episode: 119, duration: 1.620s, episode steps:  18, steps per second:  11, episode reward: -68.000, mean reward: -3.778 [-25.000, 22.000], mean action: 0.944 [0.000, 2.000],  loss: 228.054132, mean_q: 82.970119, mean_eps: 0.699175\n"," 1 : 14  \n","--->:158593:<---\n","   3392/500000: episode: 120, duration: 3.428s, episode steps:  40, steps per second:  12, episode reward: 58.000, mean reward:  1.450 [-25.000, 22.000], mean action: 1.175 [0.000, 2.000],  loss: 257.054042, mean_q: 83.084688, mean_eps: 0.696565\n"," 1 : 24  \n","--->:159901:<---\n","   3412/500000: episode: 121, duration: 1.805s, episode steps:  20, steps per second:  11, episode reward: -21.000, mean reward: -1.050 [-25.000, 22.000], mean action: 0.900 [0.000, 2.000],  loss: 219.099457, mean_q: 81.618007, mean_eps: 0.693865\n"," 1 : 14  \n","--->:161203:<---\n","   3440/500000: episode: 122, duration: 2.462s, episode steps:  28, steps per second:  11, episode reward: 258.000, mean reward:  9.214 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 227.397339, mean_q: 83.577117, mean_eps: 0.691705\n"," 1 : 16  \n","--->:162505:<---\n","   3475/500000: episode: 123, duration: 2.962s, episode steps:  35, steps per second:  12, episode reward: -102.000, mean reward: -2.914 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 227.325844, mean_q: 83.486247, mean_eps: 0.688870\n"," 1 : 21  \n","--->:163814:<---\n","   3480/500000: episode: 124, duration: 0.552s, episode steps:   5, steps per second:   9, episode reward: 19.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 187.916724, mean_q: 80.330225, mean_eps: 0.687070\n"," 1 : 3  \n","--->:165116:<---\n","   3493/500000: episode: 125, duration: 1.199s, episode steps:  13, steps per second:  11, episode reward: 57.000, mean reward:  4.385 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 219.229409, mean_q: 81.935266, mean_eps: 0.686260\n"," 1 : 9  \n","--->:166424:<---\n","   3500/500000: episode: 126, duration: 0.717s, episode steps:   7, steps per second:  10, episode reward: -125.000, mean reward: -17.857 [-25.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 253.121388, mean_q: 81.036137, mean_eps: 0.685360\n"," 1 : 5  \n","--->:167731:<---\n","   3510/500000: episode: 127, duration: 0.945s, episode steps:  10, steps per second:  11, episode reward: -6.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 235.166313, mean_q: 83.711119, mean_eps: 0.684595\n"," 1 : 4  \n","--->:169038:<---\n","   3520/500000: episode: 128, duration: 0.964s, episode steps:  10, steps per second:  10, episode reward: -56.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 202.552177, mean_q: 80.896315, mean_eps: 0.683695\n"," 1 : 6  \n","--->:170347:<---\n","   3544/500000: episode: 129, duration: 2.103s, episode steps:  24, steps per second:  11, episode reward: 76.000, mean reward:  3.167 [-25.000, 22.000], mean action: 1.292 [0.000, 2.000],  loss: 202.974501, mean_q: 83.990893, mean_eps: 0.682165\n"," 1 : 12  \n","--->:171650:<---\n","   3555/500000: episode: 130, duration: 1.088s, episode steps:  11, steps per second:  10, episode reward: 16.000, mean reward:  1.455 [-25.000, 22.000], mean action: 1.364 [0.000, 2.000],  loss: 264.160054, mean_q: 81.805592, mean_eps: 0.680590\n"," 1 : 5  \n","--->:172956:<---\n","   3575/500000: episode: 131, duration: 1.810s, episode steps:  20, steps per second:  11, episode reward: -37.000, mean reward: -1.850 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 240.391171, mean_q: 83.228058, mean_eps: 0.679195\n"," 1 : 9  \n","--->:174263:<---\n","   3600/500000: episode: 132, duration: 2.193s, episode steps:  25, steps per second:  11, episode reward: -49.000, mean reward: -1.960 [-25.000, 22.000], mean action: 1.040 [0.000, 2.000],  loss: 230.279274, mean_q: 81.735102, mean_eps: 0.677170\n"," 1 : 17  \n","--->:175570:<---\n","   3619/500000: episode: 133, duration: 1.713s, episode steps:  19, steps per second:  11, episode reward: 29.000, mean reward:  1.526 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 234.999264, mean_q: 83.269534, mean_eps: 0.675190\n"," 1 : 12  \n","--->:176874:<---\n","   3636/500000: episode: 134, duration: 1.545s, episode steps:  17, steps per second:  11, episode reward: -65.000, mean reward: -3.824 [-25.000, 22.000], mean action: 0.882 [0.000, 2.000],  loss: 228.927199, mean_q: 83.769455, mean_eps: 0.673570\n"," 1 : 12  \n","--->:178176:<---\n","   3653/500000: episode: 135, duration: 1.582s, episode steps:  17, steps per second:  11, episode reward:  4.000, mean reward:  0.235 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 263.570906, mean_q: 81.416255, mean_eps: 0.672040\n"," 1 : 13  \n","--->:179482:<---\n","   3675/500000: episode: 136, duration: 1.958s, episode steps:  22, steps per second:  11, episode reward: 186.000, mean reward:  8.455 [-25.000, 22.000], mean action: 0.773 [0.000, 2.000],  loss: 251.958787, mean_q: 83.930496, mean_eps: 0.670285\n"," 1 : 17  \n","--->:180785:<---\n","   3699/500000: episode: 137, duration: 2.160s, episode steps:  24, steps per second:  11, episode reward: 92.000, mean reward:  3.833 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 232.571575, mean_q: 83.020345, mean_eps: 0.668215\n"," 1 : 17  \n","--->:182094:<---\n","   3745/500000: episode: 138, duration: 3.900s, episode steps:  46, steps per second:  12, episode reward: 105.000, mean reward:  2.283 [-25.000, 22.000], mean action: 1.304 [0.000, 2.000],  loss: 277.787494, mean_q: 83.711090, mean_eps: 0.665065\n"," 1 : 24  \n","--->:183395:<---\n","   3764/500000: episode: 139, duration: 1.698s, episode steps:  19, steps per second:  11, episode reward: 23.000, mean reward:  1.211 [-25.000, 22.000], mean action: 0.895 [0.000, 2.000],  loss: 251.507328, mean_q: 83.048698, mean_eps: 0.662140\n"," 1 : 16  \n","--->:184699:<---\n","   3786/500000: episode: 140, duration: 1.941s, episode steps:  22, steps per second:  11, episode reward: 23.000, mean reward:  1.045 [-25.000, 22.000], mean action: 0.773 [0.000, 2.000],  loss: 238.735011, mean_q: 82.018910, mean_eps: 0.660295\n"," 1 : 16  \n","--->:186009:<---\n","   3805/500000: episode: 141, duration: 1.723s, episode steps:  19, steps per second:  11, episode reward: 189.000, mean reward:  9.947 [-25.000, 22.000], mean action: 0.789 [0.000, 2.000],  loss: 244.529210, mean_q: 80.375667, mean_eps: 0.658450\n"," 1 : 15  \n","--->:187316:<---\n","   3819/500000: episode: 142, duration: 1.278s, episode steps:  14, steps per second:  11, episode reward: -12.000, mean reward: -0.857 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 220.121632, mean_q: 81.288021, mean_eps: 0.656965\n"," 1 : 8  \n","--->:188622:<---\n","   3853/500000: episode: 143, duration: 2.862s, episode steps:  34, steps per second:  12, episode reward: -83.000, mean reward: -2.441 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 243.482138, mean_q: 84.052481, mean_eps: 0.654805\n"," 1 : 24  \n","--->:189929:<---\n","   3893/500000: episode: 144, duration: 3.415s, episode steps:  40, steps per second:  12, episode reward: 130.000, mean reward:  3.250 [-25.000, 22.000], mean action: 1.275 [0.000, 2.000],  loss: 237.036898, mean_q: 82.192091, mean_eps: 0.651475\n"," 1 : 23  \n","--->:191234:<---\n","   3906/500000: episode: 145, duration: 1.183s, episode steps:  13, steps per second:  11, episode reward: -128.000, mean reward: -9.846 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 207.361127, mean_q: 85.400615, mean_eps: 0.649090\n"," 1 : 7  \n","--->:192535:<---\n","   3925/500000: episode: 146, duration: 1.645s, episode steps:  19, steps per second:  12, episode reward: 79.000, mean reward:  4.158 [-25.000, 22.000], mean action: 1.211 [0.000, 2.000],  loss: 237.492137, mean_q: 81.228895, mean_eps: 0.647650\n"," 1 : 10  \n","--->:193837:<---\n","   3940/500000: episode: 147, duration: 1.337s, episode steps:  15, steps per second:  11, episode reward: 104.000, mean reward:  6.933 [-25.000, 22.000], mean action: 1.067 [0.000, 2.000],  loss: 201.113034, mean_q: 82.960757, mean_eps: 0.646120\n"," 1 : 9  \n","--->:195143:<---\n","   3959/500000: episode: 148, duration: 1.666s, episode steps:  19, steps per second:  11, episode reward: -71.000, mean reward: -3.737 [-25.000, 22.000], mean action: 0.842 [0.000, 2.000],  loss: 239.716519, mean_q: 81.707860, mean_eps: 0.644590\n"," 1 : 16  \n","--->:196446:<---\n","   3992/500000: episode: 149, duration: 2.831s, episode steps:  33, steps per second:  12, episode reward: -8.000, mean reward: -0.242 [-25.000, 22.000], mean action: 0.939 [0.000, 2.000],  loss: 242.457589, mean_q: 82.716167, mean_eps: 0.642250\n"," 1 : 21  \n","--->:197755:<---\n","   4041/500000: episode: 150, duration: 4.128s, episode steps:  49, steps per second:  12, episode reward:  8.000, mean reward:  0.163 [-25.000, 22.000], mean action: 1.265 [0.000, 2.000],  loss: 238.844132, mean_q: 82.464760, mean_eps: 0.638560\n"," 1 : 26  \n","--->:199062:<---\n","   4179/500000: episode: 151, duration: 11.243s, episode steps: 138, steps per second:  12, episode reward: 302.000, mean reward:  2.188 [-25.000, 22.000], mean action: 0.696 [0.000, 2.000],  loss: 233.376752, mean_q: 83.213652, mean_eps: 0.630145\n"," 1 : 112  \n","--->:200367:<---\n","   4333/500000: episode: 152, duration: 12.582s, episode steps: 154, steps per second:  12, episode reward: 550.000, mean reward:  3.571 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 246.269254, mean_q: 82.502763, mean_eps: 0.617005\n"," 1 : 119  \n","--->:201668:<---\n","   4484/500000: episode: 153, duration: 12.275s, episode steps: 151, steps per second:  12, episode reward: 587.000, mean reward:  3.887 [-25.000, 22.000], mean action: 0.868 [0.000, 2.000],  loss: 244.481024, mean_q: 82.424874, mean_eps: 0.603280\n"," 1 : 110  \n","--->:202971:<---\n","   4571/500000: episode: 154, duration: 7.247s, episode steps:  87, steps per second:  12, episode reward: -106.000, mean reward: -1.218 [-25.000, 22.000], mean action: 1.138 [0.000, 2.000],  loss: 264.129186, mean_q: 82.220206, mean_eps: 0.592570\n"," 1 : 55  \n","--->:204273:<---\n","   4644/500000: episode: 155, duration: 6.363s, episode steps:  73, steps per second:  11, episode reward: 229.000, mean reward:  3.137 [-25.000, 22.000], mean action: 0.959 [0.000, 2.000],  loss: 234.386382, mean_q: 84.244286, mean_eps: 0.585370\n"," 1 : 51  \n","--->:205581:<---\n","   4710/500000: episode: 156, duration: 5.737s, episode steps:  66, steps per second:  12, episode reward: 56.000, mean reward:  0.848 [-25.000, 22.000], mean action: 1.076 [0.000, 2.000],  loss: 232.231069, mean_q: 82.801725, mean_eps: 0.579115\n"," 1 : 41  \n","--->:206883:<---\n","   4780/500000: episode: 157, duration: 5.998s, episode steps:  70, steps per second:  12, episode reward: 182.000, mean reward:  2.600 [-25.000, 22.000], mean action: 0.814 [0.000, 2.000],  loss: 236.620431, mean_q: 83.739771, mean_eps: 0.572995\n"," 1 : 51  \n","--->:208188:<---\n","   4870/500000: episode: 158, duration: 7.827s, episode steps:  90, steps per second:  11, episode reward: -15.000, mean reward: -0.167 [-25.000, 22.000], mean action: 1.056 [0.000, 2.000],  loss: 247.274336, mean_q: 81.976384, mean_eps: 0.565795\n"," 1 : 57  \n","--->:209489:<---\n","   4921/500000: episode: 159, duration: 4.266s, episode steps:  51, steps per second:  12, episode reward: 156.000, mean reward:  3.059 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 231.460606, mean_q: 82.834399, mean_eps: 0.559450\n"," 1 : 37  \n","--->:210793:<---\n","   4986/500000: episode: 160, duration: 5.551s, episode steps:  65, steps per second:  12, episode reward: 194.000, mean reward:  2.985 [-25.000, 22.000], mean action: 1.046 [0.000, 2.000],  loss: 247.001877, mean_q: 82.780405, mean_eps: 0.554230\n"," 1 : 43  \n","--->:212095:<---\n","   5067/500000: episode: 161, duration: 6.681s, episode steps:  81, steps per second:  12, episode reward: 245.000, mean reward:  3.025 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 234.839526, mean_q: 82.181810, mean_eps: 0.547660\n"," 1 : 56  \n","--->:213396:<---\n","   5096/500000: episode: 162, duration: 2.451s, episode steps:  29, steps per second:  12, episode reward: 92.000, mean reward:  3.172 [-25.000, 22.000], mean action: 1.103 [0.000, 2.000],  loss: 245.901404, mean_q: 81.379365, mean_eps: 0.542710\n"," 1 : 17  \n","--->:214704:<---\n","   5133/500000: episode: 163, duration: 3.209s, episode steps:  37, steps per second:  12, episode reward: 64.000, mean reward:  1.730 [-25.000, 22.000], mean action: 1.162 [0.000, 2.000],  loss: 257.729401, mean_q: 83.793307, mean_eps: 0.539740\n"," 1 : 20  \n","--->:216008:<---\n","   5186/500000: episode: 164, duration: 4.471s, episode steps:  53, steps per second:  12, episode reward: 56.000, mean reward:  1.057 [-25.000, 22.000], mean action: 0.811 [0.000, 2.000],  loss: 243.306211, mean_q: 81.464551, mean_eps: 0.535690\n"," 1 : 41  \n","--->:217312:<---\n","   5238/500000: episode: 165, duration: 4.406s, episode steps:  52, steps per second:  12, episode reward: 150.000, mean reward:  2.885 [-25.000, 22.000], mean action: 0.769 [0.000, 2.000],  loss: 229.120673, mean_q: 84.096861, mean_eps: 0.530965\n"," 1 : 41  \n","--->:218621:<---\n","   5271/500000: episode: 166, duration: 2.867s, episode steps:  33, steps per second:  12, episode reward: -80.000, mean reward: -2.424 [-25.000, 22.000], mean action: 1.121 [0.000, 2.000],  loss: 234.277672, mean_q: 83.445264, mean_eps: 0.527140\n"," 1 : 22  \n","--->:219931:<---\n","   5291/500000: episode: 167, duration: 1.732s, episode steps:  20, steps per second:  12, episode reward: -162.000, mean reward: -8.100 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 262.605730, mean_q: 83.110705, mean_eps: 0.524755\n"," 1 : 14  \n","--->:221236:<---\n","   5319/500000: episode: 168, duration: 2.414s, episode steps:  28, steps per second:  12, episode reward: 92.000, mean reward:  3.286 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 252.277881, mean_q: 80.919951, mean_eps: 0.522595\n"," 1 : 17  \n","--->:222541:<---\n","   5341/500000: episode: 169, duration: 1.916s, episode steps:  22, steps per second:  11, episode reward: -40.000, mean reward: -1.818 [-25.000, 22.000], mean action: 1.227 [0.000, 2.000],  loss: 239.640420, mean_q: 82.213297, mean_eps: 0.520345\n"," 1 : 11  \n","--->:223843:<---\n","   5393/500000: episode: 170, duration: 4.331s, episode steps:  52, steps per second:  12, episode reward: -32.000, mean reward: -0.615 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 228.958535, mean_q: 82.637976, mean_eps: 0.517015\n"," 1 : 37  \n","--->:225151:<---\n","   5446/500000: episode: 171, duration: 4.467s, episode steps:  53, steps per second:  12, episode reward: 407.000, mean reward:  7.679 [-25.000, 22.000], mean action: 0.717 [0.000, 2.000],  loss: 226.613151, mean_q: 82.519222, mean_eps: 0.512290\n"," 1 : 42  \n","--->:226461:<---\n","   5500/500000: episode: 172, duration: 4.572s, episode steps:  54, steps per second:  12, episode reward: 265.000, mean reward:  4.907 [-25.000, 22.000], mean action: 1.241 [0.000, 2.000],  loss: 233.530538, mean_q: 84.198540, mean_eps: 0.507475\n"," 1 : 27  \n","--->:227765:<---\n","   5559/500000: episode: 173, duration: 4.899s, episode steps:  59, steps per second:  12, episode reward: 37.000, mean reward:  0.627 [-25.000, 22.000], mean action: 1.017 [0.000, 2.000],  loss: 246.039576, mean_q: 82.900292, mean_eps: 0.502390\n"," 1 : 38  \n","--->:229066:<---\n","   5592/500000: episode: 174, duration: 2.795s, episode steps:  33, steps per second:  12, episode reward: -36.000, mean reward: -1.091 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 226.127975, mean_q: 83.493318, mean_eps: 0.498250\n"," 1 : 24  \n","--->:230376:<---\n","   5655/500000: episode: 175, duration: 5.263s, episode steps:  63, steps per second:  12, episode reward: 238.000, mean reward:  3.778 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 226.568573, mean_q: 82.424540, mean_eps: 0.493930\n"," 1 : 45  \n","--->:231681:<---\n","   5739/500000: episode: 176, duration: 6.945s, episode steps:  84, steps per second:  12, episode reward: 147.000, mean reward:  1.750 [-25.000, 22.000], mean action: 1.107 [0.000, 2.000],  loss: 249.234071, mean_q: 82.817762, mean_eps: 0.487315\n"," 1 : 43  \n","--->:232988:<---\n","   5794/500000: episode: 177, duration: 4.656s, episode steps:  55, steps per second:  12, episode reward: -126.000, mean reward: -2.291 [-25.000, 22.000], mean action: 0.891 [0.000, 2.000],  loss: 234.672552, mean_q: 83.497580, mean_eps: 0.481060\n"," 1 : 37  \n","--->:234294:<---\n","   5861/500000: episode: 178, duration: 5.562s, episode steps:  67, steps per second:  12, episode reward: 75.000, mean reward:  1.119 [-25.000, 22.000], mean action: 0.985 [0.000, 2.000],  loss: 247.210251, mean_q: 82.886700, mean_eps: 0.475570\n"," 1 : 44  \n","--->:235599:<---\n","   5904/500000: episode: 179, duration: 3.629s, episode steps:  43, steps per second:  12, episode reward: -174.000, mean reward: -4.047 [-25.000, 22.000], mean action: 1.256 [0.000, 2.000],  loss: 259.143270, mean_q: 82.440681, mean_eps: 0.470620\n"," 1 : 22  \n","--->:236902:<---\n","   5965/500000: episode: 180, duration: 5.045s, episode steps:  61, steps per second:  12, episode reward: 181.000, mean reward:  2.967 [-25.000, 22.000], mean action: 1.148 [0.000, 2.000],  loss: 230.932926, mean_q: 82.345220, mean_eps: 0.465940\n"," 1 : 36  \n","--->:238205:<---\n","   6027/500000: episode: 181, duration: 5.228s, episode steps:  62, steps per second:  12, episode reward: 169.000, mean reward:  2.726 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 232.559045, mean_q: 83.402364, mean_eps: 0.460405\n"," 1 : 44  \n","--->:239509:<---\n","   6055/500000: episode: 182, duration: 2.424s, episode steps:  28, steps per second:  12, episode reward: 83.000, mean reward:  2.964 [-25.000, 22.000], mean action: 1.036 [0.000, 2.000],  loss: 230.187164, mean_q: 83.238886, mean_eps: 0.456355\n"," 1 : 23  \n","--->:240811:<---\n","   6100/500000: episode: 183, duration: 3.782s, episode steps:  45, steps per second:  12, episode reward: 281.000, mean reward:  6.244 [-25.000, 22.000], mean action: 0.933 [0.000, 2.000],  loss: 219.430242, mean_q: 83.117668, mean_eps: 0.453070\n"," 1 : 32  \n","--->:242118:<---\n","   6141/500000: episode: 184, duration: 3.518s, episode steps:  41, steps per second:  12, episode reward: -4.000, mean reward: -0.098 [-25.000, 22.000], mean action: 0.707 [0.000, 2.000],  loss: 217.077157, mean_q: 82.705348, mean_eps: 0.449200\n"," 1 : 34  \n","--->:243428:<---\n","   6178/500000: episode: 185, duration: 3.097s, episode steps:  37, steps per second:  12, episode reward: 105.000, mean reward:  2.838 [-25.000, 22.000], mean action: 1.162 [0.000, 2.000],  loss: 221.166043, mean_q: 82.686732, mean_eps: 0.445690\n"," 1 : 24  \n","--->:244734:<---\n","   6218/500000: episode: 186, duration: 3.373s, episode steps:  40, steps per second:  12, episode reward: -99.000, mean reward: -2.475 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 261.927554, mean_q: 81.847947, mean_eps: 0.442225\n"," 1 : 19  \n","--->:246036:<---\n","   6234/500000: episode: 187, duration: 1.430s, episode steps:  16, steps per second:  11, episode reward: 76.000, mean reward:  4.750 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 216.230327, mean_q: 84.083189, mean_eps: 0.439705\n"," 1 : 12  \n","--->:247337:<---\n","   6260/500000: episode: 188, duration: 2.241s, episode steps:  26, steps per second:  12, episode reward: 70.000, mean reward:  2.692 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 247.164005, mean_q: 84.122418, mean_eps: 0.437815\n"," 1 : 16  \n","--->:248642:<---\n","   6287/500000: episode: 189, duration: 2.295s, episode steps:  27, steps per second:  12, episode reward: 86.000, mean reward:  3.185 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 266.915556, mean_q: 82.631469, mean_eps: 0.435430\n"," 1 : 21  \n","--->:249949:<---\n","   6311/500000: episode: 190, duration: 2.086s, episode steps:  24, steps per second:  12, episode reward: 45.000, mean reward:  1.875 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 199.924765, mean_q: 82.547508, mean_eps: 0.433135\n"," 1 : 17  \n","--->:251253:<---\n","   6337/500000: episode: 191, duration: 2.245s, episode steps:  26, steps per second:  12, episode reward: 48.000, mean reward:  1.846 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 231.471308, mean_q: 83.611370, mean_eps: 0.430885\n"," 1 : 15  \n","--->:252554:<---\n","   6369/500000: episode: 192, duration: 2.743s, episode steps:  32, steps per second:  12, episode reward: 186.000, mean reward:  5.812 [-25.000, 22.000], mean action: 1.219 [0.000, 2.000],  loss: 233.646068, mean_q: 82.681287, mean_eps: 0.428275\n"," 1 : 17  \n","--->:253864:<---\n","   6392/500000: episode: 193, duration: 2.008s, episode steps:  23, steps per second:  11, episode reward: -2.000, mean reward: -0.087 [-25.000, 22.000], mean action: 0.826 [0.000, 2.000],  loss: 214.365435, mean_q: 80.666800, mean_eps: 0.425800\n"," 1 : 17  \n","--->:255172:<---\n","   6431/500000: episode: 194, duration: 3.304s, episode steps:  39, steps per second:  12, episode reward: 246.000, mean reward:  6.308 [-25.000, 22.000], mean action: 0.974 [0.000, 2.000],  loss: 238.326087, mean_q: 83.317635, mean_eps: 0.423010\n"," 1 : 24  \n","--->:256473:<---\n","   6462/500000: episode: 195, duration: 2.627s, episode steps:  31, steps per second:  12, episode reward: 11.000, mean reward:  0.355 [-25.000, 22.000], mean action: 0.871 [0.000, 2.000],  loss: 239.639825, mean_q: 80.723610, mean_eps: 0.419860\n"," 1 : 24  \n","--->:257775:<---\n","   6499/500000: episode: 196, duration: 3.125s, episode steps:  37, steps per second:  12, episode reward: 149.000, mean reward:  4.027 [-25.000, 22.000], mean action: 1.054 [0.000, 2.000],  loss: 241.299051, mean_q: 83.269994, mean_eps: 0.416800\n"," 1 : 26  \n","--->:259084:<---\n","   6509/500000: episode: 197, duration: 0.919s, episode steps:  10, steps per second:  11, episode reward: 82.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 231.551439, mean_q: 86.843727, mean_eps: 0.414685\n"," 1 : 8  \n","--->:260388:<---\n","   6534/500000: episode: 198, duration: 2.140s, episode steps:  25, steps per second:  12, episode reward: 48.000, mean reward:  1.920 [-25.000, 22.000], mean action: 0.960 [0.000, 2.000],  loss: 222.354233, mean_q: 84.395958, mean_eps: 0.413110\n"," 1 : 15  \n","--->:261690:<---\n","   6556/500000: episode: 199, duration: 1.918s, episode steps:  22, steps per second:  11, episode reward: 117.000, mean reward:  5.318 [-25.000, 22.000], mean action: 0.955 [0.000, 2.000],  loss: 264.027136, mean_q: 82.058657, mean_eps: 0.410995\n"," 1 : 16  \n","--->:262999:<---\n","   6579/500000: episode: 200, duration: 2.059s, episode steps:  23, steps per second:  11, episode reward: -49.000, mean reward: -2.130 [-25.000, 22.000], mean action: 0.826 [0.000, 2.000],  loss: 242.311792, mean_q: 82.669899, mean_eps: 0.408970\n"," 1 : 17  \n","--->:264300:<---\n","   6599/500000: episode: 201, duration: 1.790s, episode steps:  20, steps per second:  11, episode reward: -2.000, mean reward: -0.100 [-25.000, 22.000], mean action: 0.650 [0.000, 2.000],  loss: 240.969445, mean_q: 82.532235, mean_eps: 0.407035\n"," 1 : 17  \n","--->:265604:<---\n","   6614/500000: episode: 202, duration: 1.374s, episode steps:  15, steps per second:  11, episode reward: 79.000, mean reward:  5.267 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 217.765264, mean_q: 82.676829, mean_eps: 0.405460\n"," 1 : 10  \n","--->:266905:<---\n","   6632/500000: episode: 203, duration: 1.643s, episode steps:  18, steps per second:  11, episode reward: 145.000, mean reward:  8.056 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 239.524917, mean_q: 81.309324, mean_eps: 0.403975\n"," 1 : 13  \n","--->:268208:<---\n","   6652/500000: episode: 204, duration: 1.732s, episode steps:  20, steps per second:  12, episode reward: -27.000, mean reward: -1.350 [-25.000, 22.000], mean action: 0.550 [0.000, 2.000],  loss: 259.368317, mean_q: 84.499412, mean_eps: 0.402265\n"," 1 : 18  \n","--->:269514:<---\n","   6687/500000: episode: 205, duration: 2.989s, episode steps:  35, steps per second:  12, episode reward: 114.000, mean reward:  3.257 [-25.000, 22.000], mean action: 1.171 [0.000, 2.000],  loss: 223.159901, mean_q: 82.766589, mean_eps: 0.399790\n"," 1 : 18  \n","--->:270816:<---\n","   6726/500000: episode: 206, duration: 3.319s, episode steps:  39, steps per second:  12, episode reward: 99.000, mean reward:  2.538 [-25.000, 22.000], mean action: 0.872 [0.000, 2.000],  loss: 243.998719, mean_q: 82.597101, mean_eps: 0.396460\n"," 1 : 28  \n","--->:272125:<---\n","   6751/500000: episode: 207, duration: 2.163s, episode steps:  25, steps per second:  12, episode reward: 208.000, mean reward:  8.320 [-25.000, 22.000], mean action: 0.760 [0.000, 2.000],  loss: 243.313200, mean_q: 82.563904, mean_eps: 0.393580\n"," 1 : 18  \n","--->:273429:<---\n","   6778/500000: episode: 208, duration: 2.308s, episode steps:  27, steps per second:  12, episode reward: 54.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.370 [0.000, 2.000],  loss: 243.720411, mean_q: 81.260680, mean_eps: 0.391240\n"," 1 : 11  \n","--->:274737:<---\n","   6796/500000: episode: 209, duration: 1.604s, episode steps:  18, steps per second:  11, episode reward:  7.000, mean reward:  0.389 [-25.000, 22.000], mean action: 1.056 [0.000, 2.000],  loss: 225.437992, mean_q: 81.291539, mean_eps: 0.389215\n"," 1 : 11  \n","--->:276044:<---\n","   6823/500000: episode: 210, duration: 2.328s, episode steps:  27, steps per second:  12, episode reward: 180.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.815 [0.000, 2.000],  loss: 211.128309, mean_q: 82.963138, mean_eps: 0.387190\n"," 1 : 21  \n","--->:277347:<---\n","   6864/500000: episode: 211, duration: 3.465s, episode steps:  41, steps per second:  12, episode reward: 93.000, mean reward:  2.268 [-25.000, 22.000], mean action: 0.756 [0.000, 2.000],  loss: 218.046477, mean_q: 83.902607, mean_eps: 0.384130\n"," 1 : 32  \n","--->:278655:<---\n","   6908/500000: episode: 212, duration: 3.687s, episode steps:  44, steps per second:  12, episode reward: 42.000, mean reward:  0.955 [-25.000, 22.000], mean action: 1.432 [0.000, 2.000],  loss: 235.563340, mean_q: 83.561605, mean_eps: 0.380305\n"," 1 : 19  \n","--->:279965:<---\n","   6947/500000: episode: 213, duration: 3.235s, episode steps:  39, steps per second:  12, episode reward: 121.000, mean reward:  3.103 [-25.000, 22.000], mean action: 0.872 [0.000, 2.000],  loss: 234.951973, mean_q: 84.628220, mean_eps: 0.376570\n"," 1 : 29  \n","--->:281272:<---\n","   6972/500000: episode: 214, duration: 2.170s, episode steps:  25, steps per second:  12, episode reward: 89.000, mean reward:  3.560 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 240.709432, mean_q: 81.026953, mean_eps: 0.373690\n"," 1 : 19  \n","--->:282581:<---\n","   7002/500000: episode: 215, duration: 2.537s, episode steps:  30, steps per second:  12, episode reward: -43.000, mean reward: -1.433 [-25.000, 22.000], mean action: 1.300 [0.000, 2.000],  loss: 269.015804, mean_q: 84.705860, mean_eps: 0.371215\n"," 1 : 13  \n","--->:283887:<---\n","   7020/500000: episode: 216, duration: 1.576s, episode steps:  18, steps per second:  11, episode reward: 79.000, mean reward:  4.389 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 271.587417, mean_q: 81.734262, mean_eps: 0.369055\n"," 1 : 10  \n","--->:285189:<---\n","   7045/500000: episode: 217, duration: 2.165s, episode steps:  25, steps per second:  12, episode reward: 23.000, mean reward:  0.920 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 263.880480, mean_q: 82.652348, mean_eps: 0.367120\n"," 1 : 16  \n","--->:286492:<---\n","   7063/500000: episode: 218, duration: 1.591s, episode steps:  18, steps per second:  11, episode reward: 54.000, mean reward:  3.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 216.736454, mean_q: 82.011995, mean_eps: 0.365185\n"," 1 : 11  \n","--->:287802:<---\n","   7072/500000: episode: 219, duration: 0.882s, episode steps:   9, steps per second:  10, episode reward: 13.000, mean reward:  1.444 [-25.000, 22.000], mean action: 0.556 [0.000, 2.000],  loss: 230.395113, mean_q: 84.095341, mean_eps: 0.363970\n"," 1 : 7  \n","--->:289104:<---\n","   7088/500000: episode: 220, duration: 1.429s, episode steps:  16, steps per second:  11, episode reward: -87.000, mean reward: -5.438 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 223.998893, mean_q: 84.615395, mean_eps: 0.362845\n"," 1 : 11  \n","--->:290411:<---\n","   7117/500000: episode: 221, duration: 2.458s, episode steps:  29, steps per second:  12, episode reward: 86.000, mean reward:  2.966 [-25.000, 22.000], mean action: 0.897 [0.000, 2.000],  loss: 261.505518, mean_q: 81.967269, mean_eps: 0.360820\n"," 1 : 21  \n","--->:291714:<---\n","   7146/500000: episode: 222, duration: 2.393s, episode steps:  29, steps per second:  12, episode reward:  1.000, mean reward:  0.034 [-25.000, 22.000], mean action: 1.241 [0.000, 2.000],  loss: 272.656339, mean_q: 82.645624, mean_eps: 0.358210\n"," 1 : 15  \n","--->:293018:<---\n","   7167/500000: episode: 223, duration: 1.793s, episode steps:  21, steps per second:  12, episode reward: 73.000, mean reward:  3.476 [-25.000, 22.000], mean action: 0.905 [0.000, 2.000],  loss: 203.013589, mean_q: 81.229206, mean_eps: 0.355960\n"," 1 : 14  \n","--->:294319:<---\n","   7237/500000: episode: 224, duration: 5.773s, episode steps:  70, steps per second:  12, episode reward: 159.000, mean reward:  2.271 [-25.000, 22.000], mean action: 1.243 [0.000, 2.000],  loss: 245.690151, mean_q: 82.794474, mean_eps: 0.351865\n"," 1 : 35  \n","--->:295623:<---\n","   7283/500000: episode: 225, duration: 3.860s, episode steps:  46, steps per second:  12, episode reward: 375.000, mean reward:  8.152 [-25.000, 22.000], mean action: 1.174 [0.000, 2.000],  loss: 250.213334, mean_q: 81.311293, mean_eps: 0.346645\n"," 1 : 32  \n","--->:296929:<---\n","   7320/500000: episode: 226, duration: 3.129s, episode steps:  37, steps per second:  12, episode reward: 83.000, mean reward:  2.243 [-25.000, 22.000], mean action: 1.108 [0.000, 2.000],  loss: 245.882628, mean_q: 83.379966, mean_eps: 0.342910\n"," 1 : 23  \n","--->:298236:<---\n","   7365/500000: episode: 227, duration: 3.827s, episode steps:  45, steps per second:  12, episode reward: 43.000, mean reward:  0.956 [-25.000, 22.000], mean action: 0.978 [0.000, 2.000],  loss: 259.470749, mean_q: 81.390833, mean_eps: 0.339220\n"," 1 : 34  \n","--->:299537:<---\n","   7425/500000: episode: 228, duration: 4.980s, episode steps:  60, steps per second:  12, episode reward: 510.000, mean reward:  8.500 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 234.273972, mean_q: 82.745709, mean_eps: 0.334495\n"," 1 : 36  \n","--->:300845:<---\n","   7471/500000: episode: 229, duration: 3.949s, episode steps:  46, steps per second:  12, episode reward: 134.000, mean reward:  2.913 [-25.000, 22.000], mean action: 0.891 [0.000, 2.000],  loss: 243.145760, mean_q: 81.948781, mean_eps: 0.329725\n"," 1 : 36  \n","--->:302148:<---\n","   7548/500000: episode: 230, duration: 6.362s, episode steps:  77, steps per second:  12, episode reward: 626.000, mean reward:  8.130 [-25.000, 22.000], mean action: 1.260 [0.000, 2.000],  loss: 241.173520, mean_q: 83.035791, mean_eps: 0.324190\n"," 1 : 37  \n","--->:303455:<---\n","   7612/500000: episode: 231, duration: 5.339s, episode steps:  64, steps per second:  12, episode reward: 125.000, mean reward:  1.953 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 254.841271, mean_q: 83.822058, mean_eps: 0.317845\n"," 1 : 42  \n","--->:304757:<---\n","   7654/500000: episode: 232, duration: 3.531s, episode steps:  42, steps per second:  12, episode reward: 121.000, mean reward:  2.881 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 259.700995, mean_q: 83.734332, mean_eps: 0.313075\n"," 1 : 29  \n","--->:306059:<---\n","   7723/500000: episode: 233, duration: 5.726s, episode steps:  69, steps per second:  12, episode reward: 40.000, mean reward:  0.580 [-25.000, 22.000], mean action: 1.275 [0.000, 2.000],  loss: 269.474895, mean_q: 81.155061, mean_eps: 0.308080\n"," 1 : 36  \n","--->:307366:<---\n","   7788/500000: episode: 234, duration: 5.391s, episode steps:  65, steps per second:  12, episode reward: 401.000, mean reward:  6.169 [-25.000, 22.000], mean action: 0.862 [0.000, 2.000],  loss: 238.306717, mean_q: 82.374569, mean_eps: 0.302050\n"," 1 : 46  \n","--->:308670:<---\n","   7840/500000: episode: 235, duration: 4.356s, episode steps:  52, steps per second:  12, episode reward: -13.000, mean reward: -0.250 [-25.000, 22.000], mean action: 0.654 [0.000, 2.000],  loss: 245.473393, mean_q: 82.102997, mean_eps: 0.296785\n"," 1 : 40  \n","--->:309975:<---\n","   7871/500000: episode: 236, duration: 2.666s, episode steps:  31, steps per second:  12, episode reward: 58.000, mean reward:  1.871 [-25.000, 22.000], mean action: 0.806 [0.000, 2.000],  loss: 219.514186, mean_q: 85.985068, mean_eps: 0.293050\n"," 1 : 24  \n","--->:311284:<---\n","   7910/500000: episode: 237, duration: 3.292s, episode steps:  39, steps per second:  12, episode reward: 199.000, mean reward:  5.103 [-25.000, 22.000], mean action: 1.205 [0.000, 2.000],  loss: 241.863761, mean_q: 82.071092, mean_eps: 0.289900\n"," 1 : 24  \n","--->:312594:<---\n","   7922/500000: episode: 238, duration: 1.091s, episode steps:  12, steps per second:  11, episode reward: -12.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 179.464975, mean_q: 83.264383, mean_eps: 0.287605\n"," 1 : 8  \n","--->:313899:<---\n","   7939/500000: episode: 239, duration: 1.560s, episode steps:  17, steps per second:  11, episode reward: -15.000, mean reward: -0.882 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 238.334097, mean_q: 83.914843, mean_eps: 0.286300\n"," 1 : 10  \n","--->:315208:<---\n","   7949/500000: episode: 240, duration: 0.978s, episode steps:  10, steps per second:  10, episode reward: 129.000, mean reward: 12.900 [-25.000, 22.000], mean action: 1.200 [1.000, 2.000],  loss: 249.204089, mean_q: 80.400343, mean_eps: 0.285085\n"," 1 : 8  \n","--->:316518:<---\n","   7957/500000: episode: 241, duration: 0.783s, episode steps:   8, steps per second:  10, episode reward: 63.000, mean reward:  7.875 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 250.472101, mean_q: 80.993310, mean_eps: 0.284275\n"," 1 : 5  \n","--->:317823:<---\n","   7972/500000: episode: 242, duration: 1.368s, episode steps:  15, steps per second:  11, episode reward: 129.000, mean reward:  8.600 [-25.000, 22.000], mean action: 1.067 [0.000, 2.000],  loss: 291.813349, mean_q: 79.948889, mean_eps: 0.283240\n"," 1 : 8  \n","--->:319131:<---\n","   8019/500000: episode: 243, duration: 3.961s, episode steps:  47, steps per second:  12, episode reward: 62.000, mean reward:  1.319 [-25.000, 22.000], mean action: 0.723 [0.000, 2.000],  loss: 233.314468, mean_q: 84.759447, mean_eps: 0.280450\n"," 1 : 37  \n","--->:320435:<---\n","   8050/500000: episode: 244, duration: 2.662s, episode steps:  31, steps per second:  12, episode reward: -11.000, mean reward: -0.355 [-25.000, 22.000], mean action: 0.806 [0.000, 2.000],  loss: 235.251778, mean_q: 83.256760, mean_eps: 0.276940\n"," 1 : 23  \n","--->:321744:<---\n","   8086/500000: episode: 245, duration: 3.091s, episode steps:  36, steps per second:  12, episode reward: 309.000, mean reward:  8.583 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 228.142225, mean_q: 83.248333, mean_eps: 0.273925\n"," 1 : 29  \n","--->:323049:<---\n","   8134/500000: episode: 246, duration: 4.013s, episode steps:  48, steps per second:  12, episode reward: 297.000, mean reward:  6.188 [-25.000, 22.000], mean action: 0.771 [0.000, 2.000],  loss: 239.135600, mean_q: 82.906997, mean_eps: 0.270145\n"," 1 : 37  \n","--->:324355:<---\n","   8190/500000: episode: 247, duration: 4.738s, episode steps:  56, steps per second:  12, episode reward: 103.000, mean reward:  1.839 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 211.550061, mean_q: 83.104765, mean_eps: 0.265465\n"," 1 : 41  \n","--->:325658:<---\n","   8270/500000: episode: 248, duration: 6.627s, episode steps:  80, steps per second:  12, episode reward: 182.000, mean reward:  2.275 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 225.786730, mean_q: 83.806407, mean_eps: 0.259345\n"," 1 : 51  \n","--->:326959:<---\n","   8323/500000: episode: 249, duration: 4.413s, episode steps:  53, steps per second:  12, episode reward: 385.000, mean reward:  7.264 [-25.000, 22.000], mean action: 0.943 [0.000, 2.000],  loss: 241.365291, mean_q: 83.080342, mean_eps: 0.253360\n"," 1 : 41  \n","--->:328262:<---\n","   8402/500000: episode: 250, duration: 6.545s, episode steps:  79, steps per second:  12, episode reward: 244.000, mean reward:  3.089 [-25.000, 22.000], mean action: 1.215 [0.000, 2.000],  loss: 245.238322, mean_q: 82.793099, mean_eps: 0.247420\n"," 1 : 41  \n","--->:329572:<---\n","   8471/500000: episode: 251, duration: 5.865s, episode steps:  69, steps per second:  12, episode reward: 364.000, mean reward:  5.275 [-25.000, 22.000], mean action: 0.710 [0.000, 2.000],  loss: 259.660377, mean_q: 81.850478, mean_eps: 0.240760\n"," 1 : 55  \n","--->:330875:<---\n","   8566/500000: episode: 252, duration: 7.992s, episode steps:  95, steps per second:  12, episode reward: 351.000, mean reward:  3.695 [-25.000, 22.000], mean action: 1.253 [0.000, 2.000],  loss: 253.274278, mean_q: 82.610659, mean_eps: 0.233380\n"," 1 : 48  \n","--->:332178:<---\n","   8644/500000: episode: 253, duration: 6.645s, episode steps:  78, steps per second:  12, episode reward: 189.000, mean reward:  2.423 [-25.000, 22.000], mean action: 0.795 [0.000, 2.000],  loss: 232.172881, mean_q: 82.493340, mean_eps: 0.225595\n"," 1 : 62  \n","--->:333486:<---\n","   8670/500000: episode: 254, duration: 2.329s, episode steps:  26, steps per second:  11, episode reward: 158.000, mean reward:  6.077 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 237.691355, mean_q: 84.225570, mean_eps: 0.220915\n"," 1 : 20  \n","--->:334794:<---\n","   8767/500000: episode: 255, duration: 8.314s, episode steps:  97, steps per second:  12, episode reward: 492.000, mean reward:  5.072 [-25.000, 22.000], mean action: 1.216 [0.000, 2.000],  loss: 253.583265, mean_q: 81.621023, mean_eps: 0.215380\n"," 1 : 48  \n","--->:336095:<---\n","   8896/500000: episode: 256, duration: 10.938s, episode steps: 129, steps per second:  12, episode reward: 765.000, mean reward:  5.930 [-25.000, 22.000], mean action: 0.845 [0.000, 2.000],  loss: 243.485551, mean_q: 82.538290, mean_eps: 0.205210\n"," 1 : 101  \n","--->:337405:<---\n","   9014/500000: episode: 257, duration: 9.897s, episode steps: 118, steps per second:  12, episode reward: 324.000, mean reward:  2.746 [-25.000, 22.000], mean action: 1.186 [0.000, 2.000],  loss: 259.506136, mean_q: 81.833639, mean_eps: 0.194095\n"," 1 : 66  \n","--->:338709:<---\n","   9064/500000: episode: 258, duration: 4.205s, episode steps:  50, steps per second:  12, episode reward: 36.000, mean reward:  0.720 [-25.000, 22.000], mean action: 1.180 [0.000, 2.000],  loss: 236.874426, mean_q: 82.833698, mean_eps: 0.186535\n"," 1 : 23  \n","--->:340018:<---\n","   9095/500000: episode: 259, duration: 2.694s, episode steps:  31, steps per second:  12, episode reward: 221.000, mean reward:  7.129 [-25.000, 22.000], mean action: 0.677 [0.000, 2.000],  loss: 248.568358, mean_q: 83.733250, mean_eps: 0.182890\n"," 1 : 25  \n","--->:341320:<---\n","   9160/500000: episode: 260, duration: 5.424s, episode steps:  65, steps per second:  12, episode reward: 140.000, mean reward:  2.154 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 223.559046, mean_q: 82.464390, mean_eps: 0.178570\n"," 1 : 32  \n","--->:342628:<---\n","   9251/500000: episode: 261, duration: 7.489s, episode steps:  91, steps per second:  12, episode reward: 508.000, mean reward:  5.582 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 214.272237, mean_q: 83.716890, mean_eps: 0.171550\n"," 1 : 53  \n","--->:343932:<---\n","   9359/500000: episode: 262, duration: 8.862s, episode steps: 108, steps per second:  12, episode reward: 606.000, mean reward:  5.611 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 239.588820, mean_q: 82.808535, mean_eps: 0.162595\n"," 1 : 66  \n","--->:345239:<---\n","   9401/500000: episode: 263, duration: 3.578s, episode steps:  42, steps per second:  12, episode reward: 49.000, mean reward:  1.167 [-25.000, 22.000], mean action: 1.119 [0.000, 2.000],  loss: 296.212883, mean_q: 83.254607, mean_eps: 0.155845\n"," 1 : 30  \n","--->:346543:<---\n","   9444/500000: episode: 264, duration: 3.627s, episode steps:  43, steps per second:  12, episode reward: 328.000, mean reward:  7.628 [-25.000, 22.000], mean action: 0.907 [0.000, 2.000],  loss: 246.343436, mean_q: 82.945429, mean_eps: 0.152020\n"," 1 : 32  \n","--->:347853:<---\n","   9483/500000: episode: 265, duration: 3.319s, episode steps:  39, steps per second:  12, episode reward: 27.000, mean reward:  0.692 [-25.000, 22.000], mean action: 0.795 [0.000, 2.000],  loss: 245.151182, mean_q: 81.793194, mean_eps: 0.148330\n"," 1 : 29  \n","--->:349162:<---\n","   9586/500000: episode: 266, duration: 8.759s, episode steps: 103, steps per second:  12, episode reward: 484.000, mean reward:  4.699 [-25.000, 22.000], mean action: 1.078 [0.000, 2.000],  loss: 233.191909, mean_q: 82.798675, mean_eps: 0.141940\n"," 1 : 69  \n","--->:350469:<---\n","   9695/500000: episode: 267, duration: 9.072s, episode steps: 109, steps per second:  12, episode reward: 455.000, mean reward:  4.174 [-25.000, 22.000], mean action: 1.257 [0.000, 2.000],  loss: 230.851973, mean_q: 83.738725, mean_eps: 0.132400\n"," 1 : 57  \n","--->:351778:<---\n","   9759/500000: episode: 268, duration: 5.392s, episode steps:  64, steps per second:  12, episode reward: 279.000, mean reward:  4.359 [-25.000, 22.000], mean action: 0.781 [0.000, 2.000],  loss: 258.927458, mean_q: 82.664231, mean_eps: 0.124615\n"," 1 : 49  \n","--->:353086:<---\n","   9855/500000: episode: 269, duration: 8.100s, episode steps:  96, steps per second:  12, episode reward: 277.000, mean reward:  2.885 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 256.780431, mean_q: 81.900703, mean_eps: 0.117415\n"," 1 : 66  \n","--->:354390:<---\n","   9974/500000: episode: 270, duration: 9.971s, episode steps: 119, steps per second:  12, episode reward: 638.000, mean reward:  5.361 [-25.000, 22.000], mean action: 0.992 [0.000, 2.000],  loss: 232.795464, mean_q: 82.973861, mean_eps: 0.107740\n"," 1 : 76  \n","--->:355697:<---\n","  10075/500000: episode: 271, duration: 8.547s, episode steps: 101, steps per second:  12, episode reward: 533.000, mean reward:  5.277 [-25.000, 22.000], mean action: 1.218 [0.000, 2.000],  loss: 247.872182, mean_q: 82.842753, mean_eps: 0.100313\n"," 1 : 52  \n","--->:357004:<---\n","  10170/500000: episode: 272, duration: 8.040s, episode steps:  95, steps per second:  12, episode reward: 644.000, mean reward:  6.779 [-25.000, 22.000], mean action: 0.747 [0.000, 2.000],  loss: 242.447588, mean_q: 83.931095, mean_eps: 0.100000\n"," 1 : 72  \n","--->:358310:<---\n","  10219/500000: episode: 273, duration: 4.246s, episode steps:  49, steps per second:  12, episode reward: 206.000, mean reward:  4.204 [-25.000, 22.000], mean action: 0.918 [0.000, 2.000],  loss: 237.152032, mean_q: 84.462470, mean_eps: 0.100000\n"," 1 : 35  \n","--->:359619:<---\n","  10254/500000: episode: 274, duration: 2.972s, episode steps:  35, steps per second:  12, episode reward: 26.000, mean reward:  0.743 [-25.000, 22.000], mean action: 1.371 [0.000, 2.000],  loss: 257.534611, mean_q: 82.815823, mean_eps: 0.100000\n"," 1 : 14  \n","--->:360922:<---\n","  10311/500000: episode: 275, duration: 4.802s, episode steps:  57, steps per second:  12, episode reward: 340.000, mean reward:  5.965 [-25.000, 22.000], mean action: 1.351 [0.000, 2.000],  loss: 243.638193, mean_q: 85.009870, mean_eps: 0.100000\n"," 1 : 24  \n","--->:362232:<---\n","  10330/500000: episode: 276, duration: 1.690s, episode steps:  19, steps per second:  11, episode reward: 255.000, mean reward: 13.421 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 222.533956, mean_q: 84.058006, mean_eps: 0.100000\n"," 1 : 18  \n","--->:363533:<---\n","  10365/500000: episode: 277, duration: 2.990s, episode steps:  35, steps per second:  12, episode reward: 177.000, mean reward:  5.057 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 249.534641, mean_q: 84.291765, mean_eps: 0.100000\n"," 1 : 23  \n","--->:364837:<---\n","  10403/500000: episode: 278, duration: 3.245s, episode steps:  38, steps per second:  12, episode reward: -33.000, mean reward: -0.868 [-25.000, 22.000], mean action: 1.132 [0.000, 2.000],  loss: 257.400369, mean_q: 83.406409, mean_eps: 0.100000\n"," 1 : 22  \n","--->:366146:<---\n","  10418/500000: episode: 279, duration: 1.393s, episode steps:  15, steps per second:  11, episode reward: 123.000, mean reward:  8.200 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 222.504225, mean_q: 85.988054, mean_eps: 0.100000\n"," 1 : 12  \n","--->:367456:<---\n","  10445/500000: episode: 280, duration: 2.347s, episode steps:  27, steps per second:  12, episode reward: 64.000, mean reward:  2.370 [-25.000, 22.000], mean action: 0.852 [0.000, 2.000],  loss: 240.168960, mean_q: 83.369788, mean_eps: 0.100000\n"," 1 : 20  \n","--->:368763:<---\n","  10485/500000: episode: 281, duration: 3.416s, episode steps:  40, steps per second:  12, episode reward: -95.000, mean reward: -2.375 [-25.000, 22.000], mean action: 0.675 [0.000, 2.000],  loss: 242.983249, mean_q: 83.655610, mean_eps: 0.100000\n"," 1 : 32  \n","--->:370065:<---\n","  10524/500000: episode: 282, duration: 3.328s, episode steps:  39, steps per second:  12, episode reward: 80.000, mean reward:  2.051 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 235.085019, mean_q: 84.421895, mean_eps: 0.100000\n"," 1 : 25  \n","--->:371366:<---\n","  10558/500000: episode: 283, duration: 2.905s, episode steps:  34, steps per second:  12, episode reward: 258.000, mean reward:  7.588 [-25.000, 22.000], mean action: 1.324 [0.000, 2.000],  loss: 258.440470, mean_q: 83.792211, mean_eps: 0.100000\n"," 1 : 16  \n","--->:372674:<---\n","  10609/500000: episode: 284, duration: 4.351s, episode steps:  51, steps per second:  12, episode reward: 58.000, mean reward:  1.137 [-25.000, 22.000], mean action: 1.294 [0.000, 2.000],  loss: 282.643095, mean_q: 83.249818, mean_eps: 0.100000\n"," 1 : 24  \n","--->:373976:<---\n","  10637/500000: episode: 285, duration: 2.449s, episode steps:  28, steps per second:  11, episode reward: 199.000, mean reward:  7.107 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 250.835640, mean_q: 85.002680, mean_eps: 0.100000\n"," 1 : 24  \n","--->:375285:<---\n","  10659/500000: episode: 286, duration: 1.975s, episode steps:  22, steps per second:  11, episode reward: -62.000, mean reward: -2.818 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 223.595360, mean_q: 83.459786, mean_eps: 0.100000\n"," 1 : 10  \n","--->:376592:<---\n","  10675/500000: episode: 287, duration: 1.449s, episode steps:  16, steps per second:  11, episode reward: 29.000, mean reward:  1.812 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 282.018693, mean_q: 80.748605, mean_eps: 0.100000\n"," 1 : 12  \n","--->:377897:<---\n","  10682/500000: episode: 288, duration: 0.703s, episode steps:   7, steps per second:  10, episode reward: 16.000, mean reward:  2.286 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 206.746412, mean_q: 85.639854, mean_eps: 0.100000\n"," 1 : 5  \n","--->:379200:<---\n","  10704/500000: episode: 289, duration: 1.964s, episode steps:  22, steps per second:  11, episode reward: 117.000, mean reward:  5.318 [-25.000, 22.000], mean action: 0.864 [0.000, 2.000],  loss: 234.675013, mean_q: 85.664414, mean_eps: 0.100000\n"," 1 : 16  \n","--->:380508:<---\n","  10728/500000: episode: 290, duration: 2.117s, episode steps:  24, steps per second:  11, episode reward: 117.000, mean reward:  4.875 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 263.178027, mean_q: 84.623242, mean_eps: 0.100000\n"," 1 : 16  \n","--->:381812:<---\n","  10752/500000: episode: 291, duration: 2.149s, episode steps:  24, steps per second:  11, episode reward: 167.000, mean reward:  6.958 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 219.455366, mean_q: 82.923474, mean_eps: 0.100000\n"," 1 : 14  \n","--->:383119:<---\n","  10777/500000: episode: 292, duration: 2.248s, episode steps:  25, steps per second:  11, episode reward: -40.000, mean reward: -1.600 [-25.000, 22.000], mean action: 1.360 [0.000, 2.000],  loss: 269.448152, mean_q: 82.113690, mean_eps: 0.100000\n"," 1 : 11  \n","--->:384429:<---\n","  10840/500000: episode: 293, duration: 5.323s, episode steps:  63, steps per second:  12, episode reward: 268.000, mean reward:  4.254 [-25.000, 22.000], mean action: 1.413 [0.000, 2.000],  loss: 240.586315, mean_q: 82.806242, mean_eps: 0.100000\n"," 1 : 25  \n","--->:385735:<---\n","  10845/500000: episode: 294, duration: 0.547s, episode steps:   5, steps per second:   9, episode reward: -53.000, mean reward: -10.600 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 234.020773, mean_q: 84.196104, mean_eps: 0.100000\n"," 1 : 4  \n","--->:387040:<---\n","  10863/500000: episode: 295, duration: 1.654s, episode steps:  18, steps per second:  11, episode reward: -9.000, mean reward: -0.500 [-25.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 275.390503, mean_q: 83.998318, mean_eps: 0.100000\n"," 1 : 6  \n","--->:388350:<---\n","  10873/500000: episode: 296, duration: 0.984s, episode steps:  10, steps per second:  10, episode reward: 63.000, mean reward:  6.300 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 182.860310, mean_q: 83.409777, mean_eps: 0.100000\n"," 1 : 5  \n","--->:389652:<---\n","  10914/500000: episode: 297, duration: 3.571s, episode steps:  41, steps per second:  11, episode reward: 149.000, mean reward:  3.634 [-25.000, 22.000], mean action: 1.049 [0.000, 2.000],  loss: 256.243990, mean_q: 83.688890, mean_eps: 0.100000\n"," 1 : 26  \n","--->:390954:<---\n","  10925/500000: episode: 298, duration: 1.065s, episode steps:  11, steps per second:  10, episode reward: 10.000, mean reward:  0.909 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 203.707575, mean_q: 86.036163, mean_eps: 0.100000\n"," 1 : 9  \n","--->:392262:<---\n","  10951/500000: episode: 299, duration: 2.375s, episode steps:  26, steps per second:  11, episode reward: 32.000, mean reward:  1.231 [-25.000, 22.000], mean action: 1.423 [0.000, 2.000],  loss: 237.575463, mean_q: 84.704526, mean_eps: 0.100000\n"," 1 : 10  \n","--->:393563:<---\n","  10976/500000: episode: 300, duration: 2.311s, episode steps:  25, steps per second:  11, episode reward:  7.000, mean reward:  0.280 [-25.000, 22.000], mean action: 1.360 [0.000, 2.000],  loss: 225.844624, mean_q: 83.395157, mean_eps: 0.100000\n"," 1 : 11  \n","--->:394871:<---\n","  10982/500000: episode: 301, duration: 0.643s, episode steps:   6, steps per second:   9, episode reward: 85.000, mean reward: 14.167 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 229.369975, mean_q: 86.058746, mean_eps: 0.100000\n"," 1 : 6  \n","--->:396179:<---\n","  11000/500000: episode: 302, duration: 1.695s, episode steps:  18, steps per second:  11, episode reward: 10.000, mean reward:  0.556 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 241.869060, mean_q: 83.715238, mean_eps: 0.100000\n"," 1 : 9  \n","--->:397480:<---\n","  11011/500000: episode: 303, duration: 1.097s, episode steps:  11, steps per second:  10, episode reward: 198.000, mean reward: 18.000 [ 0.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 208.324794, mean_q: 84.548933, mean_eps: 0.100000\n"," 1 : 9  \n","--->:398784:<---\n","  11025/500000: episode: 304, duration: 1.335s, episode steps:  14, steps per second:  10, episode reward: 85.000, mean reward:  6.071 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 222.653012, mean_q: 83.126974, mean_eps: 0.100000\n"," 1 : 6  \n","--->:400094:<---\n","  11033/500000: episode: 305, duration: 0.809s, episode steps:   8, steps per second:  10, episode reward: 132.000, mean reward: 16.500 [ 0.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 286.586399, mean_q: 83.372370, mean_eps: 0.100000\n"," 1 : 6  \n","--->:401395:<---\n","  11048/500000: episode: 306, duration: 1.444s, episode steps:  15, steps per second:  10, episode reward: -15.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 233.880921, mean_q: 85.030974, mean_eps: 0.100000\n"," 1 : 10  \n","--->:402699:<---\n","  11066/500000: episode: 307, duration: 1.656s, episode steps:  18, steps per second:  11, episode reward: 51.000, mean reward:  2.833 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 216.346491, mean_q: 82.583000, mean_eps: 0.100000\n"," 1 : 13  \n","--->:404007:<---\n","  11090/500000: episode: 308, duration: 2.195s, episode steps:  24, steps per second:  11, episode reward: 161.000, mean reward:  6.708 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 239.922286, mean_q: 83.241727, mean_eps: 0.100000\n"," 1 : 18  \n","--->:405315:<---\n","  11136/500000: episode: 309, duration: 4.093s, episode steps:  46, steps per second:  11, episode reward: 230.000, mean reward:  5.000 [-25.000, 22.000], mean action: 1.413 [0.000, 2.000],  loss: 248.422416, mean_q: 84.616910, mean_eps: 0.100000\n"," 1 : 19  \n","--->:406618:<---\n","  11177/500000: episode: 310, duration: 3.587s, episode steps:  41, steps per second:  11, episode reward: 67.000, mean reward:  1.634 [-25.000, 22.000], mean action: 1.366 [0.000, 2.000],  loss: 252.076064, mean_q: 82.722741, mean_eps: 0.100000\n"," 1 : 18  \n","--->:407927:<---\n","  11212/500000: episode: 311, duration: 3.051s, episode steps:  35, steps per second:  11, episode reward: 155.000, mean reward:  4.429 [-25.000, 22.000], mean action: 0.886 [0.000, 2.000],  loss: 237.949510, mean_q: 83.131488, mean_eps: 0.100000\n"," 1 : 22  \n","--->:409235:<---\n","  11244/500000: episode: 312, duration: 2.867s, episode steps:  32, steps per second:  11, episode reward: 48.000, mean reward:  1.500 [-25.000, 22.000], mean action: 1.281 [0.000, 2.000],  loss: 234.478686, mean_q: 84.387136, mean_eps: 0.100000\n"," 1 : 15  \n","--->:410536:<---\n","  11306/500000: episode: 313, duration: 5.305s, episode steps:  62, steps per second:  12, episode reward: 221.000, mean reward:  3.565 [-25.000, 22.000], mean action: 1.435 [0.000, 2.000],  loss: 259.984517, mean_q: 83.051865, mean_eps: 0.100000\n"," 1 : 25  \n","--->:411841:<---\n","  11341/500000: episode: 314, duration: 3.078s, episode steps:  35, steps per second:  11, episode reward: 14.000, mean reward:  0.400 [-25.000, 22.000], mean action: 1.171 [0.000, 2.000],  loss: 221.220621, mean_q: 84.166775, mean_eps: 0.100000\n"," 1 : 22  \n","--->:413146:<---\n","  11358/500000: episode: 315, duration: 1.617s, episode steps:  17, steps per second:  11, episode reward: 107.000, mean reward:  6.294 [-25.000, 22.000], mean action: 1.529 [0.000, 2.000],  loss: 256.853020, mean_q: 84.154388, mean_eps: 0.100000\n"," 1 : 7  \n","--->:414456:<---\n","  11380/500000: episode: 316, duration: 1.978s, episode steps:  22, steps per second:  11, episode reward: 54.000, mean reward:  2.455 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 242.621969, mean_q: 84.649480, mean_eps: 0.100000\n"," 1 : 11  \n","--->:415760:<---\n","  11395/500000: episode: 317, duration: 1.402s, episode steps:  15, steps per second:  11, episode reward:  7.000, mean reward:  0.467 [-25.000, 22.000], mean action: 0.733 [0.000, 2.000],  loss: 271.374589, mean_q: 83.211670, mean_eps: 0.100000\n"," 1 : 11  \n","--->:417069:<---\n","  11438/500000: episode: 318, duration: 3.767s, episode steps:  43, steps per second:  11, episode reward: 79.000, mean reward:  1.837 [-25.000, 22.000], mean action: 1.744 [0.000, 2.000],  loss: 250.143633, mean_q: 83.364043, mean_eps: 0.100000\n"," 1 : 10  \n","--->:418379:<---\n","  11465/500000: episode: 319, duration: 2.494s, episode steps:  27, steps per second:  11, episode reward: 167.000, mean reward:  6.185 [-25.000, 22.000], mean action: 1.296 [0.000, 2.000],  loss: 216.591659, mean_q: 84.535366, mean_eps: 0.100000\n"," 1 : 14  \n","--->:419685:<---\n","  11500/500000: episode: 320, duration: 3.063s, episode steps:  35, steps per second:  11, episode reward: 32.000, mean reward:  0.914 [-25.000, 22.000], mean action: 1.686 [0.000, 2.000],  loss: 255.402244, mean_q: 82.378124, mean_eps: 0.100000\n"," 1 : 10  \n","--->:420995:<---\n","  11518/500000: episode: 321, duration: 1.632s, episode steps:  18, steps per second:  11, episode reward: -46.000, mean reward: -2.556 [-25.000, 22.000], mean action: 0.611 [0.000, 2.000],  loss: 211.514747, mean_q: 82.976358, mean_eps: 0.100000\n"," 1 : 15  \n","--->:422298:<---\n","  11541/500000: episode: 322, duration: 2.038s, episode steps:  23, steps per second:  11, episode reward: 23.000, mean reward:  1.000 [-25.000, 22.000], mean action: 0.783 [0.000, 2.000],  loss: 202.511292, mean_q: 85.441727, mean_eps: 0.100000\n"," 1 : 16  \n","--->:423599:<---\n","  11586/500000: episode: 323, duration: 3.897s, episode steps:  45, steps per second:  12, episode reward: 98.000, mean reward:  2.178 [-25.000, 22.000], mean action: 1.578 [0.000, 2.000],  loss: 258.695370, mean_q: 83.164238, mean_eps: 0.100000\n"," 1 : 13  \n","--->:424903:<---\n","  11617/500000: episode: 324, duration: 2.702s, episode steps:  31, steps per second:  11, episode reward: -18.000, mean reward: -0.581 [-25.000, 22.000], mean action: 1.516 [0.000, 2.000],  loss: 248.891855, mean_q: 81.546626, mean_eps: 0.100000\n"," 1 : 12  \n","--->:426213:<---\n","  11679/500000: episode: 325, duration: 5.358s, episode steps:  62, steps per second:  12, episode reward: 293.000, mean reward:  4.726 [-25.000, 22.000], mean action: 1.452 [0.000, 2.000],  loss: 278.961065, mean_q: 82.536620, mean_eps: 0.100000\n"," 1 : 24  \n","--->:427521:<---\n","  11735/500000: episode: 326, duration: 4.845s, episode steps:  56, steps per second:  12, episode reward: 183.000, mean reward:  3.268 [-25.000, 22.000], mean action: 1.518 [0.000, 2.000],  loss: 243.317919, mean_q: 82.662883, mean_eps: 0.100000\n"," 1 : 19  \n","--->:428826:<---\n","  11773/500000: episode: 327, duration: 3.332s, episode steps:  38, steps per second:  11, episode reward: 255.000, mean reward:  6.711 [-25.000, 22.000], mean action: 1.368 [0.000, 2.000],  loss: 260.213339, mean_q: 82.580272, mean_eps: 0.100000\n"," 1 : 18  \n","--->:430134:<---\n","  11786/500000: episode: 328, duration: 1.199s, episode steps:  13, steps per second:  11, episode reward: 173.000, mean reward: 13.308 [-25.000, 22.000], mean action: 0.692 [0.000, 2.000],  loss: 230.907057, mean_q: 86.597860, mean_eps: 0.100000\n"," 1 : 10  \n","--->:431436:<---\n","  11809/500000: episode: 329, duration: 2.132s, episode steps:  23, steps per second:  11, episode reward: 167.000, mean reward:  7.261 [-25.000, 22.000], mean action: 0.870 [0.000, 2.000],  loss: 228.693435, mean_q: 83.476791, mean_eps: 0.100000\n"," 1 : 14  \n","--->:432746:<---\n","  11833/500000: episode: 330, duration: 2.165s, episode steps:  24, steps per second:  11, episode reward: 236.000, mean reward:  9.833 [-25.000, 22.000], mean action: 1.042 [0.000, 2.000],  loss: 251.251547, mean_q: 82.276590, mean_eps: 0.100000\n"," 1 : 15  \n","--->:434048:<---\n","  11864/500000: episode: 331, duration: 2.702s, episode steps:  31, steps per second:  11, episode reward: 161.000, mean reward:  5.194 [-25.000, 22.000], mean action: 1.129 [0.000, 2.000],  loss: 210.524480, mean_q: 86.264535, mean_eps: 0.100000\n"," 1 : 18  \n","--->:435351:<---\n","  11883/500000: episode: 332, duration: 1.687s, episode steps:  19, steps per second:  11, episode reward: 167.000, mean reward:  8.789 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 229.829991, mean_q: 84.532503, mean_eps: 0.100000\n"," 1 : 14  \n","--->:436652:<---\n","  11898/500000: episode: 333, duration: 1.390s, episode steps:  15, steps per second:  11, episode reward: 120.000, mean reward:  8.000 [-25.000, 22.000], mean action: 1.067 [1.000, 2.000],  loss: 270.405780, mean_q: 82.976716, mean_eps: 0.100000\n"," 1 : 14  \n","--->:437953:<---\n","  11920/500000: episode: 334, duration: 1.990s, episode steps:  22, steps per second:  11, episode reward: 95.000, mean reward:  4.318 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 245.245667, mean_q: 85.149134, mean_eps: 0.100000\n"," 1 : 15  \n","--->:439257:<---\n","  11957/500000: episode: 335, duration: 3.166s, episode steps:  37, steps per second:  12, episode reward: 312.000, mean reward:  8.432 [-25.000, 22.000], mean action: 0.838 [0.000, 2.000],  loss: 225.694423, mean_q: 83.317056, mean_eps: 0.100000\n"," 1 : 27  \n","--->:440567:<---\n","  12060/500000: episode: 336, duration: 8.763s, episode steps: 103, steps per second:  12, episode reward: 347.000, mean reward:  3.369 [-25.000, 22.000], mean action: 0.825 [0.000, 2.000],  loss: 238.410595, mean_q: 83.395885, mean_eps: 0.100000\n"," 1 : 82  \n","--->:441874:<---\n","  12342/500000: episode: 337, duration: 23.676s, episode steps: 282, steps per second:  12, episode reward: 663.000, mean reward:  2.351 [-25.000, 22.000], mean action: 1.216 [0.000, 2.000],  loss: 238.089055, mean_q: 83.711647, mean_eps: 0.100000\n"," 1 : 169  \n","--->:443177:<---\n","  12359/500000: episode: 338, duration: 1.564s, episode steps:  17, steps per second:  11, episode reward:  4.000, mean reward:  0.235 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 246.141831, mean_q: 83.942138, mean_eps: 0.100000\n"," 1 : 13  \n","--->:444483:<---\n","  12370/500000: episode: 339, duration: 1.086s, episode steps:  11, steps per second:  10, episode reward: 13.000, mean reward:  1.182 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 301.837949, mean_q: 81.249890, mean_eps: 0.100000\n"," 1 : 7  \n","--->:445793:<---\n","  12382/500000: episode: 340, duration: 1.133s, episode steps:  12, steps per second:  11, episode reward: 63.000, mean reward:  5.250 [-25.000, 22.000], mean action: 1.417 [0.000, 2.000],  loss: 246.520532, mean_q: 85.119841, mean_eps: 0.100000\n"," 1 : 5  \n","--->:447096:<---\n","  12385/500000: episode: 341, duration: 0.406s, episode steps:   3, steps per second:   7, episode reward: 19.000, mean reward:  6.333 [-25.000, 22.000], mean action: 0.333 [0.000, 1.000],  loss: 180.555649, mean_q: 83.532789, mean_eps: 0.100000\n"," 1 : 3  \n","--->:448397:<---\n","  12393/500000: episode: 342, duration: 0.803s, episode steps:   8, steps per second:  10, episode reward: 16.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 264.895819, mean_q: 81.466534, mean_eps: 0.100000\n"," 1 : 5  \n","--->:449702:<---\n","  12415/500000: episode: 343, duration: 2.021s, episode steps:  22, steps per second:  11, episode reward: 173.000, mean reward:  7.864 [-25.000, 22.000], mean action: 1.318 [0.000, 2.000],  loss: 225.676686, mean_q: 83.029845, mean_eps: 0.100000\n"," 1 : 10  \n","--->:451010:<---\n","  12438/500000: episode: 344, duration: 2.086s, episode steps:  23, steps per second:  11, episode reward: 142.000, mean reward:  6.174 [-25.000, 22.000], mean action: 1.087 [0.000, 2.000],  loss: 276.669666, mean_q: 83.033198, mean_eps: 0.100000\n"," 1 : 15  \n","--->:452317:<---\n","  12451/500000: episode: 345, duration: 1.211s, episode steps:  13, steps per second:  11, episode reward: 110.000, mean reward:  8.462 [ 0.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 245.744932, mean_q: 83.158794, mean_eps: 0.100000\n"," 1 : 5  \n","--->:453622:<---\n","  12463/500000: episode: 346, duration: 1.223s, episode steps:  12, steps per second:  10, episode reward: -12.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 311.787403, mean_q: 83.191584, mean_eps: 0.100000\n"," 1 : 8  \n","--->:454928:<---\n","  12469/500000: episode: 347, duration: 0.676s, episode steps:   6, steps per second:   9, episode reward: 63.000, mean reward: 10.500 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 275.644608, mean_q: 82.344278, mean_eps: 0.100000\n"," 1 : 5  \n","--->:456234:<---\n","  12477/500000: episode: 348, duration: 0.855s, episode steps:   8, steps per second:   9, episode reward: 38.000, mean reward:  4.750 [-25.000, 22.000], mean action: 1.250 [1.000, 2.000],  loss: 242.549959, mean_q: 87.487874, mean_eps: 0.100000\n"," 1 : 6  \n","--->:457539:<---\n","  12485/500000: episode: 349, duration: 0.849s, episode steps:   8, steps per second:   9, episode reward: 38.000, mean reward:  4.750 [-25.000, 22.000], mean action: 0.500 [0.000, 2.000],  loss: 237.886158, mean_q: 84.393655, mean_eps: 0.100000\n"," 1 : 6  \n","--->:458841:<---\n","  12505/500000: episode: 350, duration: 1.932s, episode steps:  20, steps per second:  10, episode reward:  7.000, mean reward:  0.350 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 234.680850, mean_q: 84.649959, mean_eps: 0.100000\n"," 1 : 11  \n","--->:460143:<---\n","  12523/500000: episode: 351, duration: 1.756s, episode steps:  18, steps per second:  10, episode reward: 98.000, mean reward:  5.444 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 228.901415, mean_q: 84.499268, mean_eps: 0.100000\n"," 1 : 13  \n","--->:461449:<---\n","  12532/500000: episode: 352, duration: 0.934s, episode steps:   9, steps per second:  10, episode reward: -31.000, mean reward: -3.444 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 226.380831, mean_q: 84.471823, mean_eps: 0.100000\n"," 1 : 5  \n","--->:462758:<---\n","  12544/500000: episode: 353, duration: 1.160s, episode steps:  12, steps per second:  10, episode reward: 151.000, mean reward: 12.583 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 250.194861, mean_q: 83.953373, mean_eps: 0.100000\n"," 1 : 9  \n","--->:464064:<---\n","  12556/500000: episode: 354, duration: 1.211s, episode steps:  12, steps per second:  10, episode reward: -75.000, mean reward: -6.250 [-25.000,  0.000], mean action: 1.583 [0.000, 2.000],  loss: 245.977586, mean_q: 79.728482, mean_eps: 0.100000\n"," 1 : 3  \n","--->:465373:<---\n","  12562/500000: episode: 355, duration: 0.683s, episode steps:   6, steps per second:   9, episode reward: 38.000, mean reward:  6.333 [-25.000, 22.000], mean action: 0.167 [0.000, 1.000],  loss: 213.186948, mean_q: 83.565717, mean_eps: 0.100000\n"," 1 : 6  \n","--->:466678:<---\n","  12614/500000: episode: 356, duration: 4.643s, episode steps:  52, steps per second:  11, episode reward: 199.000, mean reward:  3.827 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 217.720266, mean_q: 82.855279, mean_eps: 0.100000\n"," 1 : 24  \n","--->:467981:<---\n","  12652/500000: episode: 357, duration: 3.437s, episode steps:  38, steps per second:  11, episode reward: 118.000, mean reward:  3.105 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 237.695913, mean_q: 83.762721, mean_eps: 0.100000\n"," 1 : 31  \n","--->:469291:<---\n","  12703/500000: episode: 358, duration: 4.716s, episode steps:  51, steps per second:  11, episode reward: 319.000, mean reward:  6.255 [-25.000, 22.000], mean action: 0.863 [0.000, 2.000],  loss: 254.727089, mean_q: 83.429925, mean_eps: 0.100000\n"," 1 : 38  \n","--->:470595:<---\n","  12750/500000: episode: 359, duration: 4.304s, episode steps:  47, steps per second:  11, episode reward: 309.000, mean reward:  6.574 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 225.141466, mean_q: 83.496471, mean_eps: 0.100000\n"," 1 : 29  \n","--->:471901:<---\n","  12776/500000: episode: 360, duration: 2.383s, episode steps:  26, steps per second:  11, episode reward: 271.000, mean reward: 10.423 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 266.093076, mean_q: 84.408262, mean_eps: 0.100000\n"," 1 : 23  \n","--->:473210:<---\n","  12822/500000: episode: 361, duration: 3.998s, episode steps:  46, steps per second:  12, episode reward: 20.000, mean reward:  0.435 [-25.000, 22.000], mean action: 1.326 [0.000, 2.000],  loss: 235.911602, mean_q: 81.986502, mean_eps: 0.100000\n"," 1 : 18  \n","--->:474516:<---\n","  12860/500000: episode: 362, duration: 3.470s, episode steps:  38, steps per second:  11, episode reward: 186.000, mean reward:  4.895 [-25.000, 22.000], mean action: 1.316 [0.000, 2.000],  loss: 232.570547, mean_q: 83.036804, mean_eps: 0.100000\n"," 1 : 17  \n","--->:475826:<---\n","  12897/500000: episode: 363, duration: 3.401s, episode steps:  37, steps per second:  11, episode reward: 82.000, mean reward:  2.216 [-25.000, 22.000], mean action: 1.730 [0.000, 2.000],  loss: 251.343240, mean_q: 83.311545, mean_eps: 0.100000\n"," 1 : 8  \n","--->:477136:<---\n","  12917/500000: episode: 364, duration: 1.762s, episode steps:  20, steps per second:  11, episode reward: 192.000, mean reward:  9.600 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 270.659977, mean_q: 82.788704, mean_eps: 0.100000\n"," 1 : 13  \n","--->:478443:<---\n","  12948/500000: episode: 365, duration: 2.683s, episode steps:  31, steps per second:  12, episode reward: 79.000, mean reward:  2.548 [-25.000, 22.000], mean action: 1.581 [0.000, 2.000],  loss: 246.245991, mean_q: 82.817021, mean_eps: 0.100000\n"," 1 : 10  \n","--->:479753:<---\n","  12966/500000: episode: 366, duration: 1.669s, episode steps:  18, steps per second:  11, episode reward: -37.000, mean reward: -2.056 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 244.445647, mean_q: 80.420017, mean_eps: 0.100000\n"," 1 : 9  \n","--->:481061:<---\n","  12980/500000: episode: 367, duration: 1.311s, episode steps:  14, steps per second:  11, episode reward: 54.000, mean reward:  3.857 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 271.713658, mean_q: 87.613702, mean_eps: 0.100000\n"," 1 : 11  \n","--->:482365:<---\n","  13014/500000: episode: 368, duration: 2.978s, episode steps:  34, steps per second:  11, episode reward: 208.000, mean reward:  6.118 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 242.629234, mean_q: 84.361777, mean_eps: 0.100000\n"," 1 : 18  \n","--->:483674:<---\n","  13052/500000: episode: 369, duration: 3.321s, episode steps:  38, steps per second:  11, episode reward: 92.000, mean reward:  2.421 [-25.000, 22.000], mean action: 1.368 [0.000, 2.000],  loss: 239.394102, mean_q: 82.203617, mean_eps: 0.100000\n"," 1 : 17  \n","--->:484983:<---\n","  13093/500000: episode: 370, duration: 3.577s, episode steps:  41, steps per second:  11, episode reward: 224.000, mean reward:  5.463 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 245.410395, mean_q: 83.683235, mean_eps: 0.100000\n"," 1 : 23  \n","--->:486286:<---\n","  13119/500000: episode: 371, duration: 2.335s, episode steps:  26, steps per second:  11, episode reward: 39.000, mean reward:  1.500 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 247.051412, mean_q: 84.914640, mean_eps: 0.100000\n"," 1 : 21  \n","--->:487595:<---\n","  13146/500000: episode: 372, duration: 2.453s, episode steps:  27, steps per second:  11, episode reward: 189.000, mean reward:  7.000 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 238.594348, mean_q: 84.390490, mean_eps: 0.100000\n"," 1 : 15  \n","--->:488900:<---\n","  13177/500000: episode: 373, duration: 2.700s, episode steps:  31, steps per second:  11, episode reward: 177.000, mean reward:  5.710 [-25.000, 22.000], mean action: 0.710 [0.000, 2.000],  loss: 274.035237, mean_q: 84.738679, mean_eps: 0.100000\n"," 1 : 23  \n","--->:490210:<---\n","  13189/500000: episode: 374, duration: 1.170s, episode steps:  12, steps per second:  10, episode reward: 176.000, mean reward: 14.667 [ 0.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 222.768862, mean_q: 81.613991, mean_eps: 0.100000\n"," 1 : 8  \n","--->:491514:<---\n","  13214/500000: episode: 375, duration: 2.189s, episode steps:  25, steps per second:  11, episode reward: -43.000, mean reward: -1.720 [-25.000, 22.000], mean action: 1.280 [0.000, 2.000],  loss: 248.302411, mean_q: 82.228308, mean_eps: 0.100000\n"," 1 : 13  \n","--->:492819:<---\n","  13246/500000: episode: 376, duration: 2.803s, episode steps:  32, steps per second:  11, episode reward: 177.000, mean reward:  5.531 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 248.349829, mean_q: 82.373588, mean_eps: 0.100000\n"," 1 : 23  \n","--->:494124:<---\n","  13261/500000: episode: 377, duration: 1.363s, episode steps:  15, steps per second:  11, episode reward: 176.000, mean reward: 11.733 [ 0.000, 22.000], mean action: 1.267 [0.000, 2.000],  loss: 242.587522, mean_q: 81.505705, mean_eps: 0.100000\n"," 1 : 8  \n","--->:495425:<---\n","  13276/500000: episode: 378, duration: 1.390s, episode steps:  15, steps per second:  11, episode reward: 239.000, mean reward: 15.933 [-25.000, 22.000], mean action: 0.533 [0.000, 2.000],  loss: 240.298430, mean_q: 82.607142, mean_eps: 0.100000\n"," 1 : 13  \n","--->:496733:<---\n","  13295/500000: episode: 379, duration: 1.745s, episode steps:  19, steps per second:  11, episode reward: 117.000, mean reward:  6.158 [-25.000, 22.000], mean action: 0.632 [0.000, 2.000],  loss: 251.853904, mean_q: 82.743763, mean_eps: 0.100000\n"," 1 : 16  \n","--->:498043:<---\n","  13326/500000: episode: 380, duration: 2.762s, episode steps:  31, steps per second:  11, episode reward: 26.000, mean reward:  0.839 [-25.000, 22.000], mean action: 1.452 [0.000, 2.000],  loss: 233.561459, mean_q: 82.480792, mean_eps: 0.100000\n"," 1 : 14  \n","--->:499345:<---\n","  13353/500000: episode: 381, duration: 2.357s, episode steps:  27, steps per second:  11, episode reward: 327.000, mean reward: 12.111 [-25.000, 22.000], mean action: 1.037 [0.000, 2.000],  loss: 241.336659, mean_q: 82.789588, mean_eps: 0.100000\n"," 1 : 17  \n","--->:500648:<---\n","  13369/500000: episode: 382, duration: 1.475s, episode steps:  16, steps per second:  11, episode reward: 26.000, mean reward:  1.625 [-25.000, 22.000], mean action: 0.812 [0.000, 2.000],  loss: 239.344124, mean_q: 82.110350, mean_eps: 0.100000\n"," 1 : 14  \n","--->:501955:<---\n","  13399/500000: episode: 383, duration: 2.680s, episode steps:  30, steps per second:  11, episode reward: 195.000, mean reward:  6.500 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 227.843182, mean_q: 84.034454, mean_eps: 0.100000\n"," 1 : 11  \n","--->:503258:<---\n","  13415/500000: episode: 384, duration: 1.471s, episode steps:  16, steps per second:  11, episode reward: 10.000, mean reward:  0.625 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 241.238601, mean_q: 84.769140, mean_eps: 0.100000\n"," 1 : 9  \n","--->:504567:<---\n","  13435/500000: episode: 385, duration: 1.787s, episode steps:  20, steps per second:  11, episode reward: 208.000, mean reward: 10.400 [-25.000, 22.000], mean action: 0.550 [0.000, 2.000],  loss: 285.737217, mean_q: 83.386960, mean_eps: 0.100000\n"," 1 : 18  \n","--->:505869:<---\n","  13488/500000: episode: 386, duration: 4.543s, episode steps:  53, steps per second:  12, episode reward: -34.000, mean reward: -0.642 [-25.000, 22.000], mean action: 1.849 [0.000, 2.000],  loss: 237.377945, mean_q: 82.887800, mean_eps: 0.100000\n"," 1 : 7  \n","--->:507179:<---\n","  13502/500000: episode: 387, duration: 1.335s, episode steps:  14, steps per second:  10, episode reward: 54.000, mean reward:  3.857 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 226.823666, mean_q: 84.101242, mean_eps: 0.100000\n"," 1 : 11  \n","--->:508480:<---\n","  13519/500000: episode: 388, duration: 1.587s, episode steps:  17, steps per second:  11, episode reward: -12.000, mean reward: -0.706 [-25.000, 22.000], mean action: 1.294 [0.000, 2.000],  loss: 259.132640, mean_q: 81.962480, mean_eps: 0.100000\n"," 1 : 8  \n","--->:509788:<---\n","  13544/500000: episode: 389, duration: 2.216s, episode steps:  25, steps per second:  11, episode reward: 186.000, mean reward:  7.440 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 247.910990, mean_q: 82.452515, mean_eps: 0.100000\n"," 1 : 17  \n","--->:511096:<---\n","  13598/500000: episode: 390, duration: 4.716s, episode steps:  54, steps per second:  11, episode reward: 73.000, mean reward:  1.352 [-25.000, 22.000], mean action: 1.574 [0.000, 2.000],  loss: 254.194571, mean_q: 83.143792, mean_eps: 0.100000\n"," 1 : 14  \n","--->:512404:<---\n","  13652/500000: episode: 391, duration: 4.892s, episode steps:  54, steps per second:  11, episode reward: 105.000, mean reward:  1.944 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 239.900817, mean_q: 83.802998, mean_eps: 0.100000\n"," 1 : 24  \n","--->:513706:<---\n","  13667/500000: episode: 392, duration: 1.535s, episode steps:  15, steps per second:  10, episode reward: 57.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 274.282431, mean_q: 83.095818, mean_eps: 0.100000\n"," 1 : 9  \n","--->:515013:<---\n","  13686/500000: episode: 393, duration: 1.954s, episode steps:  19, steps per second:  10, episode reward: 167.000, mean reward:  8.789 [-25.000, 22.000], mean action: 0.895 [0.000, 2.000],  loss: 206.264124, mean_q: 84.481196, mean_eps: 0.100000\n"," 1 : 14  \n","--->:516323:<---\n","  13715/500000: episode: 394, duration: 2.879s, episode steps:  29, steps per second:  10, episode reward: 45.000, mean reward:  1.552 [-25.000, 22.000], mean action: 1.103 [0.000, 2.000],  loss: 249.991134, mean_q: 81.365331, mean_eps: 0.100000\n"," 1 : 17  \n","--->:517631:<---\n","  13750/500000: episode: 395, duration: 3.146s, episode steps:  35, steps per second:  11, episode reward: -12.000, mean reward: -0.343 [-25.000, 22.000], mean action: 1.600 [0.000, 2.000],  loss: 257.306131, mean_q: 83.347186, mean_eps: 0.100000\n"," 1 : 8  \n","--->:518932:<---\n","  13781/500000: episode: 396, duration: 2.822s, episode steps:  31, steps per second:  11, episode reward: -12.000, mean reward: -0.387 [-25.000, 22.000], mean action: 1.677 [0.000, 2.000],  loss: 254.565605, mean_q: 85.107849, mean_eps: 0.100000\n"," 1 : 8  \n","--->:520235:<---\n","  13792/500000: episode: 397, duration: 1.071s, episode steps:  11, steps per second:  10, episode reward: 35.000, mean reward:  3.182 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 198.489230, mean_q: 87.601474, mean_eps: 0.100000\n"," 1 : 8  \n","--->:521544:<---\n","  13805/500000: episode: 398, duration: 1.256s, episode steps:  13, steps per second:  10, episode reward: 195.000, mean reward: 15.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 219.933931, mean_q: 83.547651, mean_eps: 0.100000\n"," 1 : 11  \n","--->:522849:<---\n","  14054/500000: episode: 399, duration: 21.182s, episode steps: 249, steps per second:  12, episode reward: 994.000, mean reward:  3.992 [-25.000, 22.000], mean action: 1.068 [0.000, 2.000],  loss: 236.847874, mean_q: 83.974676, mean_eps: 0.100000\n"," 1 : 152  \n","--->:524158:<---\n","  14101/500000: episode: 400, duration: 4.118s, episode steps:  47, steps per second:  11, episode reward: 24.000, mean reward:  0.511 [-25.000, 22.000], mean action: 1.085 [0.000, 2.000],  loss: 253.250712, mean_q: 82.180601, mean_eps: 0.100000\n"," 1 : 31  \n","--->:525460:<---\n","  14123/500000: episode: 401, duration: 2.054s, episode steps:  22, steps per second:  11, episode reward: 117.000, mean reward:  5.318 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 261.858134, mean_q: 82.619542, mean_eps: 0.100000\n"," 1 : 16  \n","--->:526770:<---\n","  14140/500000: episode: 402, duration: 1.584s, episode steps:  17, steps per second:  11, episode reward: 79.000, mean reward:  4.647 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 238.609689, mean_q: 84.029502, mean_eps: 0.100000\n"," 1 : 10  \n","--->:528073:<---\n","  14149/500000: episode: 403, duration: 0.905s, episode steps:   9, steps per second:  10, episode reward: -6.000, mean reward: -0.667 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 275.605021, mean_q: 80.967755, mean_eps: 0.100000\n"," 1 : 4  \n","--->:529380:<---\n","  14167/500000: episode: 404, duration: 1.668s, episode steps:  18, steps per second:  11, episode reward: 13.000, mean reward:  0.722 [-25.000, 22.000], mean action: 1.389 [0.000, 2.000],  loss: 240.303508, mean_q: 84.254213, mean_eps: 0.100000\n"," 1 : 7  \n","--->:530688:<---\n","  14168/500000: episode: 405, duration: 0.219s, episode steps:   1, steps per second:   5, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 1.000 [1.000, 1.000],  loss: 381.777771, mean_q: 79.162369, mean_eps: 0.100000\n"," 1 : 1  \n","--->:531993:<---\n","  14171/500000: episode: 406, duration: 0.405s, episode steps:   3, steps per second:   7, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 0.667 [0.000, 1.000],  loss: 347.465062, mean_q: 81.076780, mean_eps: 0.100000\n"," 1 : 3  \n","--->:533299:<---\n","  14172/500000: episode: 407, duration: 0.212s, episode steps:   1, steps per second:   5, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.000 [0.000, 0.000],  loss: 284.601013, mean_q: 83.845009, mean_eps: 0.100000\n"," 1 : 1  \n","--->:534609:<---\n","  14183/500000: episode: 408, duration: 1.062s, episode steps:  11, steps per second:  10, episode reward: 63.000, mean reward:  5.727 [-25.000, 22.000], mean action: 1.455 [0.000, 2.000],  loss: 263.978560, mean_q: 81.197422, mean_eps: 0.100000\n"," 1 : 5  \n","--->:535917:<---\n","  14192/500000: episode: 409, duration: 0.917s, episode steps:   9, steps per second:  10, episode reward: 66.000, mean reward:  7.333 [ 0.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 229.669696, mean_q: 80.437348, mean_eps: 0.100000\n"," 1 : 3  \n","--->:537223:<---\n","  14196/500000: episode: 410, duration: 0.455s, episode steps:   4, steps per second:   9, episode reward: -6.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.750 [0.000, 1.000],  loss: 166.495056, mean_q: 80.035522, mean_eps: 0.100000\n"," 1 : 4  \n","--->:538527:<---\n","  14197/500000: episode: 411, duration: 0.225s, episode steps:   1, steps per second:   4, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.000 [0.000, 0.000],  loss: 301.430359, mean_q: 80.017822, mean_eps: 0.100000\n"," 1 : 1  \n","--->:539831:<---\n","  14222/500000: episode: 412, duration: 2.258s, episode steps:  25, steps per second:  11, episode reward: -15.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.320 [0.000, 2.000],  loss: 226.918286, mean_q: 83.536612, mean_eps: 0.100000\n"," 1 : 10  \n","--->:541132:<---\n","  14228/500000: episode: 413, duration: 0.649s, episode steps:   6, steps per second:   9, episode reward: -53.000, mean reward: -8.833 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 202.562370, mean_q: 79.946332, mean_eps: 0.100000\n"," 1 : 4  \n","--->:542436:<---\n","  14241/500000: episode: 414, duration: 1.229s, episode steps:  13, steps per second:  11, episode reward: 35.000, mean reward:  2.692 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 261.169157, mean_q: 86.789178, mean_eps: 0.100000\n"," 1 : 8  \n","--->:543739:<---\n","  14261/500000: episode: 415, duration: 1.751s, episode steps:  20, steps per second:  11, episode reward: -12.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.600 [1.000, 2.000],  loss: 263.200494, mean_q: 83.685947, mean_eps: 0.100000\n"," 1 : 8  \n","--->:545040:<---\n","  14276/500000: episode: 416, duration: 1.376s, episode steps:  15, steps per second:  11, episode reward: -9.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 260.466182, mean_q: 82.413277, mean_eps: 0.100000\n"," 1 : 6  \n","--->:546344:<---\n","  14294/500000: episode: 417, duration: 1.621s, episode steps:  18, steps per second:  11, episode reward: 129.000, mean reward:  7.167 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 246.524312, mean_q: 81.495414, mean_eps: 0.100000\n"," 1 : 8  \n","--->:547654:<---\n","  14313/500000: episode: 418, duration: 1.721s, episode steps:  19, steps per second:  11, episode reward: -53.000, mean reward: -2.789 [-25.000, 22.000], mean action: 1.684 [0.000, 2.000],  loss: 234.639285, mean_q: 83.762430, mean_eps: 0.100000\n"," 1 : 4  \n","--->:548961:<---\n","  14360/500000: episode: 419, duration: 4.011s, episode steps:  47, steps per second:  12, episode reward: 161.000, mean reward:  3.426 [-25.000, 22.000], mean action: 1.553 [0.000, 2.000],  loss: 275.583418, mean_q: 83.200008, mean_eps: 0.100000\n"," 1 : 18  \n","--->:550267:<---\n","  14385/500000: episode: 420, duration: 2.217s, episode steps:  25, steps per second:  11, episode reward: 214.000, mean reward:  8.560 [-25.000, 22.000], mean action: 1.240 [0.000, 2.000],  loss: 223.866831, mean_q: 83.566976, mean_eps: 0.100000\n"," 1 : 14  \n","--->:551575:<---\n","  14393/500000: episode: 421, duration: 0.815s, episode steps:   8, steps per second:  10, episode reward: 82.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.875 [0.000, 1.000],  loss: 284.131771, mean_q: 87.936460, mean_eps: 0.100000\n"," 1 : 8  \n","--->:552883:<---\n","  14406/500000: episode: 422, duration: 1.288s, episode steps:  13, steps per second:  10, episode reward: -12.000, mean reward: -0.923 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.313970, mean_q: 83.780604, mean_eps: 0.100000\n"," 1 : 8  \n","--->:554186:<---\n","  14432/500000: episode: 423, duration: 2.333s, episode steps:  26, steps per second:  11, episode reward:  7.000, mean reward:  0.269 [-25.000, 22.000], mean action: 1.385 [0.000, 2.000],  loss: 234.672553, mean_q: 82.626643, mean_eps: 0.100000\n"," 1 : 11  \n","--->:555492:<---\n","  14481/500000: episode: 424, duration: 4.301s, episode steps:  49, steps per second:  11, episode reward: 118.000, mean reward:  2.408 [-25.000, 22.000], mean action: 1.020 [0.000, 2.000],  loss: 265.984363, mean_q: 83.021461, mean_eps: 0.100000\n"," 1 : 31  \n","--->:556801:<---\n","  14509/500000: episode: 425, duration: 2.544s, episode steps:  28, steps per second:  11, episode reward: 233.000, mean reward:  8.321 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 239.094113, mean_q: 86.337573, mean_eps: 0.100000\n"," 1 : 17  \n","--->:558106:<---\n","  14522/500000: episode: 426, duration: 1.223s, episode steps:  13, steps per second:  11, episode reward: 101.000, mean reward:  7.769 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 265.724953, mean_q: 81.245818, mean_eps: 0.100000\n"," 1 : 11  \n","--->:559407:<---\n","  14540/500000: episode: 427, duration: 1.647s, episode steps:  18, steps per second:  11, episode reward: 57.000, mean reward:  3.167 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 217.327055, mean_q: 84.161626, mean_eps: 0.100000\n"," 1 : 9  \n","--->:560711:<---\n","  14549/500000: episode: 428, duration: 0.914s, episode steps:   9, steps per second:  10, episode reward: 63.000, mean reward:  7.000 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 197.063907, mean_q: 85.947500, mean_eps: 0.100000\n"," 1 : 5  \n","--->:562012:<---\n","  14551/500000: episode: 429, duration: 0.310s, episode steps:   2, steps per second:   6, episode reward: -3.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 283.668335, mean_q: 84.539463, mean_eps: 0.100000\n"," 1 : 2  \n","--->:563313:<---\n","  14570/500000: episode: 430, duration: 1.824s, episode steps:  19, steps per second:  10, episode reward: 88.000, mean reward:  4.632 [ 0.000, 22.000], mean action: 1.684 [0.000, 2.000],  loss: 249.892479, mean_q: 85.488776, mean_eps: 0.100000\n"," 1 : 4  \n","--->:564616:<---\n","  14573/500000: episode: 431, duration: 0.407s, episode steps:   3, steps per second:   7, episode reward: -50.000, mean reward: -16.667 [-25.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 248.192734, mean_q: 84.561404, mean_eps: 0.100000\n"," 1 : 2  \n","--->:565925:<---\n","  14577/500000: episode: 432, duration: 0.487s, episode steps:   4, steps per second:   8, episode reward: 44.000, mean reward: 11.000 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 190.659765, mean_q: 79.182104, mean_eps: 0.100000\n"," 1 : 2  \n","--->:567234:<---\n","  14588/500000: episode: 433, duration: 1.044s, episode steps:  11, steps per second:  11, episode reward: 13.000, mean reward:  1.182 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 218.365864, mean_q: 86.287425, mean_eps: 0.100000\n"," 1 : 7  \n","--->:568544:<---\n","  14594/500000: episode: 434, duration: 0.633s, episode steps:   6, steps per second:   9, episode reward: -9.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 265.067477, mean_q: 84.457774, mean_eps: 0.100000\n"," 1 : 6  \n","--->:569851:<---\n","  14612/500000: episode: 435, duration: 1.629s, episode steps:  18, steps per second:  11, episode reward: 148.000, mean reward:  8.222 [-25.000, 22.000], mean action: 1.056 [0.000, 2.000],  loss: 244.545210, mean_q: 81.565070, mean_eps: 0.100000\n"," 1 : 11  \n","--->:571155:<---\n","  14619/500000: episode: 436, duration: 0.710s, episode steps:   7, steps per second:  10, episode reward: 60.000, mean reward:  8.571 [-25.000, 22.000], mean action: 0.571 [0.000, 1.000],  loss: 285.952214, mean_q: 85.273454, mean_eps: 0.100000\n"," 1 : 7  \n","--->:572465:<---\n","  14630/500000: episode: 437, duration: 1.052s, episode steps:  11, steps per second:  10, episode reward: 82.000, mean reward:  7.455 [-25.000, 22.000], mean action: 0.636 [0.000, 2.000],  loss: 254.344627, mean_q: 85.493025, mean_eps: 0.100000\n"," 1 : 8  \n","--->:573766:<---\n","  14639/500000: episode: 438, duration: 0.889s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 1.222 [1.000, 2.000],  loss: 258.072762, mean_q: 83.042572, mean_eps: 0.100000\n"," 1 : 7  \n","--->:575073:<---\n","  14653/500000: episode: 439, duration: 1.302s, episode steps:  14, steps per second:  11, episode reward: 60.000, mean reward:  4.286 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 261.315314, mean_q: 80.932412, mean_eps: 0.100000\n"," 1 : 7  \n","--->:576378:<---\n","  14675/500000: episode: 440, duration: 1.937s, episode steps:  22, steps per second:  11, episode reward: 126.000, mean reward:  5.727 [-25.000, 22.000], mean action: 1.318 [0.000, 2.000],  loss: 226.454256, mean_q: 83.485952, mean_eps: 0.100000\n"," 1 : 10  \n","--->:577681:<---\n","  14699/500000: episode: 441, duration: 2.098s, episode steps:  24, steps per second:  11, episode reward: 111.000, mean reward:  4.625 [-25.000, 22.000], mean action: 0.708 [0.000, 2.000],  loss: 255.831432, mean_q: 82.682651, mean_eps: 0.100000\n"," 1 : 20  \n","--->:578985:<---\n","  14717/500000: episode: 442, duration: 1.634s, episode steps:  18, steps per second:  11, episode reward: 101.000, mean reward:  5.611 [-25.000, 22.000], mean action: 1.056 [0.000, 2.000],  loss: 227.114324, mean_q: 86.328000, mean_eps: 0.100000\n"," 1 : 11  \n","--->:580295:<---\n","  14723/500000: episode: 443, duration: 0.634s, episode steps:   6, steps per second:   9, episode reward: 38.000, mean reward:  6.333 [-25.000, 22.000], mean action: 0.667 [0.000, 1.000],  loss: 262.214643, mean_q: 80.633219, mean_eps: 0.100000\n"," 1 : 6  \n","--->:581597:<---\n","  14735/500000: episode: 444, duration: 1.152s, episode steps:  12, steps per second:  10, episode reward: 38.000, mean reward:  3.167 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 224.036578, mean_q: 81.307714, mean_eps: 0.100000\n"," 1 : 6  \n","--->:582904:<---\n","  14760/500000: episode: 445, duration: 2.226s, episode steps:  25, steps per second:  11, episode reward: 82.000, mean reward:  3.280 [-25.000, 22.000], mean action: 1.560 [0.000, 2.000],  loss: 248.820919, mean_q: 83.513213, mean_eps: 0.100000\n"," 1 : 8  \n","--->:584213:<---\n","  14763/500000: episode: 446, duration: 0.407s, episode steps:   3, steps per second:   7, episode reward: -3.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 225.420680, mean_q: 81.151779, mean_eps: 0.100000\n"," 1 : 2  \n","--->:585523:<---\n","  14798/500000: episode: 447, duration: 3.072s, episode steps:  35, steps per second:  11, episode reward: 38.000, mean reward:  1.086 [-25.000, 22.000], mean action: 1.743 [0.000, 2.000],  loss: 257.294570, mean_q: 83.655407, mean_eps: 0.100000\n"," 1 : 6  \n","--->:586828:<---\n","  14805/500000: episode: 448, duration: 0.732s, episode steps:   7, steps per second:  10, episode reward: -9.000, mean reward: -1.286 [-25.000, 22.000], mean action: 0.286 [0.000, 2.000],  loss: 296.416846, mean_q: 84.609591, mean_eps: 0.100000\n"," 1 : 6  \n","--->:588130:<---\n","  14817/500000: episode: 449, duration: 1.133s, episode steps:  12, steps per second:  11, episode reward: 41.000, mean reward:  3.417 [-25.000, 22.000], mean action: 1.583 [0.000, 2.000],  loss: 203.168344, mean_q: 83.206617, mean_eps: 0.100000\n"," 1 : 4  \n","--->:589435:<---\n","  14830/500000: episode: 450, duration: 1.230s, episode steps:  13, steps per second:  11, episode reward: 85.000, mean reward:  6.538 [-25.000, 22.000], mean action: 1.462 [0.000, 2.000],  loss: 215.448053, mean_q: 83.937252, mean_eps: 0.100000\n"," 1 : 6  \n","--->:590741:<---\n","  14835/500000: episode: 451, duration: 0.851s, episode steps:   5, steps per second:   6, episode reward: -6.000, mean reward: -1.200 [-25.000, 22.000], mean action: 0.400 [0.000, 2.000],  loss: 207.548578, mean_q: 83.514182, mean_eps: 0.100000\n"," 1 : 4  \n","--->:592042:<---\n","  14839/500000: episode: 452, duration: 0.456s, episode steps:   4, steps per second:   9, episode reward: -75.000, mean reward: -18.750 [-25.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 206.726639, mean_q: 87.755735, mean_eps: 0.100000\n"," 1 : 3  \n","--->:593345:<---\n","  14854/500000: episode: 453, duration: 1.423s, episode steps:  15, steps per second:  11, episode reward: 98.000, mean reward:  6.533 [-25.000, 22.000], mean action: 0.733 [0.000, 2.000],  loss: 275.508584, mean_q: 83.455119, mean_eps: 0.100000\n"," 1 : 13  \n","--->:594651:<---\n","  14869/500000: episode: 454, duration: 1.397s, episode steps:  15, steps per second:  11, episode reward: 148.000, mean reward:  9.867 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 271.018280, mean_q: 84.076761, mean_eps: 0.100000\n"," 1 : 11  \n","--->:595954:<---\n","  14896/500000: episode: 455, duration: 2.363s, episode steps:  27, steps per second:  11, episode reward: 95.000, mean reward:  3.519 [-25.000, 22.000], mean action: 0.963 [0.000, 2.000],  loss: 265.903897, mean_q: 85.331008, mean_eps: 0.100000\n"," 1 : 15  \n","--->:597259:<---\n","  14937/500000: episode: 456, duration: 3.509s, episode steps:  41, steps per second:  12, episode reward: 54.000, mean reward:  1.317 [-25.000, 22.000], mean action: 1.610 [0.000, 2.000],  loss: 259.027799, mean_q: 83.352456, mean_eps: 0.100000\n"," 1 : 11  \n","--->:598566:<---\n","  14954/500000: episode: 457, duration: 1.544s, episode steps:  17, steps per second:  11, episode reward: 63.000, mean reward:  3.706 [-25.000, 22.000], mean action: 1.647 [0.000, 2.000],  loss: 262.694670, mean_q: 80.136976, mean_eps: 0.100000\n"," 1 : 5  \n","--->:599871:<---\n","  14961/500000: episode: 458, duration: 0.742s, episode steps:   7, steps per second:   9, episode reward: 38.000, mean reward:  5.429 [-25.000, 22.000], mean action: 0.571 [0.000, 2.000],  loss: 257.466655, mean_q: 79.610059, mean_eps: 0.100000\n"," 1 : 6  \n","--->:601173:<---\n","  14968/500000: episode: 459, duration: 0.733s, episode steps:   7, steps per second:  10, episode reward: -9.000, mean reward: -1.286 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 199.280071, mean_q: 78.908773, mean_eps: 0.100000\n"," 1 : 6  \n","--->:602480:<---\n","  14971/500000: episode: 460, duration: 0.400s, episode steps:   3, steps per second:   8, episode reward: 44.000, mean reward: 14.667 [ 0.000, 22.000], mean action: 1.333 [1.000, 2.000],  loss: 211.747971, mean_q: 79.024839, mean_eps: 0.100000\n"," 1 : 2  \n","--->:603788:<---\n","  14975/500000: episode: 461, duration: 0.484s, episode steps:   4, steps per second:   8, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 203.065708, mean_q: 82.697786, mean_eps: 0.100000\n"," 1 : 4  \n","--->:605093:<---\n","  14979/500000: episode: 462, duration: 0.500s, episode steps:   4, steps per second:   8, episode reward: -6.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 202.854038, mean_q: 82.313354, mean_eps: 0.100000\n"," 1 : 4  \n","--->:606400:<---\n","  14984/500000: episode: 463, duration: 0.560s, episode steps:   5, steps per second:   9, episode reward: -53.000, mean reward: -10.600 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 195.086472, mean_q: 85.487573, mean_eps: 0.100000\n"," 1 : 4  \n","--->:607708:<---\n","  14997/500000: episode: 464, duration: 1.230s, episode steps:  13, steps per second:  11, episode reward: 85.000, mean reward:  6.538 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 225.471478, mean_q: 83.895644, mean_eps: 0.100000\n"," 1 : 6  \n","--->:609011:<---\n","  15004/500000: episode: 465, duration: 0.733s, episode steps:   7, steps per second:  10, episode reward: -31.000, mean reward: -4.429 [-25.000, 22.000], mean action: 0.714 [0.000, 2.000],  loss: 245.640148, mean_q: 83.603249, mean_eps: 0.100000\n"," 1 : 5  \n","--->:610312:<---\n","  15015/500000: episode: 466, duration: 1.042s, episode steps:  11, steps per second:  11, episode reward: 132.000, mean reward: 12.000 [ 0.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 248.256104, mean_q: 81.151607, mean_eps: 0.100000\n"," 1 : 6  \n","--->:611617:<---\n","  15035/500000: episode: 467, duration: 1.796s, episode steps:  20, steps per second:  11, episode reward: 41.000, mean reward:  2.050 [-25.000, 22.000], mean action: 1.800 [1.000, 2.000],  loss: 261.088501, mean_q: 81.656147, mean_eps: 0.100000\n"," 1 : 4  \n","--->:612924:<---\n","  15046/500000: episode: 468, duration: 1.088s, episode steps:  11, steps per second:  10, episode reward: 79.000, mean reward:  7.182 [-25.000, 22.000], mean action: 0.727 [0.000, 2.000],  loss: 212.796729, mean_q: 84.466033, mean_eps: 0.100000\n"," 1 : 10  \n","--->:614225:<---\n","  15054/500000: episode: 469, duration: 0.816s, episode steps:   8, steps per second:  10, episode reward: 60.000, mean reward:  7.500 [-25.000, 22.000], mean action: 0.625 [0.000, 2.000],  loss: 205.370451, mean_q: 88.063043, mean_eps: 0.100000\n"," 1 : 7  \n","--->:615528:<---\n","  15072/500000: episode: 470, duration: 1.701s, episode steps:  18, steps per second:  11, episode reward: -37.000, mean reward: -2.056 [-25.000, 22.000], mean action: 1.278 [0.000, 2.000],  loss: 236.067314, mean_q: 82.054963, mean_eps: 0.100000\n"," 1 : 9  \n","--->:616834:<---\n","  15089/500000: episode: 471, duration: 1.549s, episode steps:  17, steps per second:  11, episode reward: 57.000, mean reward:  3.353 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 245.250337, mean_q: 83.738612, mean_eps: 0.100000\n"," 1 : 9  \n","--->:618141:<---\n","  15098/500000: episode: 472, duration: 0.904s, episode steps:   9, steps per second:  10, episode reward: -6.000, mean reward: -0.667 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 267.317832, mean_q: 83.463451, mean_eps: 0.100000\n"," 1 : 4  \n","--->:619450:<---\n","  15132/500000: episode: 473, duration: 2.951s, episode steps:  34, steps per second:  12, episode reward: 13.000, mean reward:  0.382 [-25.000, 22.000], mean action: 1.735 [0.000, 2.000],  loss: 223.363843, mean_q: 83.067174, mean_eps: 0.100000\n"," 1 : 7  \n","--->:620754:<---\n","  15161/500000: episode: 474, duration: 2.525s, episode steps:  29, steps per second:  11, episode reward: 283.000, mean reward:  9.759 [-25.000, 22.000], mean action: 1.310 [0.000, 2.000],  loss: 253.503998, mean_q: 82.984262, mean_eps: 0.100000\n"," 1 : 15  \n","--->:622060:<---\n","  15175/500000: episode: 475, duration: 1.278s, episode steps:  14, steps per second:  11, episode reward: 79.000, mean reward:  5.643 [-25.000, 22.000], mean action: 0.714 [0.000, 2.000],  loss: 235.578617, mean_q: 84.370694, mean_eps: 0.100000\n"," 1 : 10  \n","--->:623362:<---\n","  15190/500000: episode: 476, duration: 1.387s, episode steps:  15, steps per second:  11, episode reward: -15.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.733 [0.000, 2.000],  loss: 230.337271, mean_q: 81.381950, mean_eps: 0.100000\n"," 1 : 10  \n","--->:624667:<---\n","  15214/500000: episode: 477, duration: 2.116s, episode steps:  24, steps per second:  11, episode reward: 38.000, mean reward:  1.583 [-25.000, 22.000], mean action: 1.708 [0.000, 2.000],  loss: 230.751169, mean_q: 84.077392, mean_eps: 0.100000\n"," 1 : 6  \n","--->:625977:<---\n","  15219/500000: episode: 478, duration: 0.564s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 263.799054, mean_q: 83.848508, mean_eps: 0.100000\n"," 1 : 4  \n","--->:627280:<---\n","  15228/500000: episode: 479, duration: 0.867s, episode steps:   9, steps per second:  10, episode reward: -75.000, mean reward: -8.333 [-25.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 173.686360, mean_q: 85.176929, mean_eps: 0.100000\n"," 1 : 3  \n","--->:628589:<---\n","  15239/500000: episode: 480, duration: 1.012s, episode steps:  11, steps per second:  11, episode reward: 10.000, mean reward:  0.909 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 195.764927, mean_q: 85.882554, mean_eps: 0.100000\n"," 1 : 9  \n","--->:629897:<---\n","  15256/500000: episode: 481, duration: 1.551s, episode steps:  17, steps per second:  11, episode reward: 79.000, mean reward:  4.647 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 220.646586, mean_q: 85.801394, mean_eps: 0.100000\n"," 1 : 10  \n","--->:631203:<---\n","  15259/500000: episode: 482, duration: 0.380s, episode steps:   3, steps per second:   8, episode reward: -50.000, mean reward: -16.667 [-25.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 192.897868, mean_q: 81.308568, mean_eps: 0.100000\n"," 1 : 2  \n","--->:632512:<---\n","  15263/500000: episode: 483, duration: 0.450s, episode steps:   4, steps per second:   9, episode reward: 66.000, mean reward: 16.500 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 221.565666, mean_q: 82.902376, mean_eps: 0.100000\n"," 1 : 3  \n","--->:633813:<---\n","  15272/500000: episode: 484, duration: 0.956s, episode steps:   9, steps per second:   9, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 224.547568, mean_q: 85.902571, mean_eps: 0.100000\n"," 1 : 7  \n","--->:635119:<---\n","  15277/500000: episode: 485, duration: 0.555s, episode steps:   5, steps per second:   9, episode reward: 88.000, mean reward: 17.600 [ 0.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 191.955292, mean_q: 81.815933, mean_eps: 0.100000\n"," 1 : 4  \n","--->:636423:<---\n","  15282/500000: episode: 486, duration: 0.535s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 230.850287, mean_q: 85.264651, mean_eps: 0.100000\n"," 1 : 4  \n","--->:637732:<---\n","  15310/500000: episode: 487, duration: 2.456s, episode steps:  28, steps per second:  11, episode reward: 142.000, mean reward:  5.071 [-25.000, 22.000], mean action: 1.179 [0.000, 2.000],  loss: 223.666072, mean_q: 84.532745, mean_eps: 0.100000\n"," 1 : 15  \n","--->:639042:<---\n","  15324/500000: episode: 488, duration: 1.270s, episode steps:  14, steps per second:  11, episode reward: -37.000, mean reward: -2.643 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 231.383266, mean_q: 81.017787, mean_eps: 0.100000\n"," 1 : 9  \n","--->:640352:<---\n","  15333/500000: episode: 489, duration: 0.884s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 286.509184, mean_q: 87.427096, mean_eps: 0.100000\n"," 1 : 7  \n","--->:641656:<---\n","  15338/500000: episode: 490, duration: 0.539s, episode steps:   5, steps per second:   9, episode reward: -31.000, mean reward: -6.200 [-25.000, 22.000], mean action: 0.600 [0.000, 1.000],  loss: 279.653571, mean_q: 83.531699, mean_eps: 0.100000\n"," 1 : 5  \n","--->:642964:<---\n","  15363/500000: episode: 491, duration: 2.209s, episode steps:  25, steps per second:  11, episode reward: 101.000, mean reward:  4.040 [-25.000, 22.000], mean action: 1.360 [0.000, 2.000],  loss: 246.319765, mean_q: 83.576118, mean_eps: 0.100000\n"," 1 : 11  \n","--->:644269:<---\n","  15373/500000: episode: 492, duration: 0.967s, episode steps:  10, steps per second:  10, episode reward: 82.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 246.608902, mean_q: 78.306139, mean_eps: 0.100000\n"," 1 : 8  \n","--->:645578:<---\n","  15377/500000: episode: 493, duration: 0.477s, episode steps:   4, steps per second:   8, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 213.225880, mean_q: 86.060936, mean_eps: 0.100000\n"," 1 : 4  \n","--->:646881:<---\n","  15386/500000: episode: 494, duration: 0.883s, episode steps:   9, steps per second:  10, episode reward: -9.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 243.610272, mean_q: 84.571730, mean_eps: 0.100000\n"," 1 : 6  \n","--->:648182:<---\n","  15390/500000: episode: 495, duration: 0.479s, episode steps:   4, steps per second:   8, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 205.935101, mean_q: 87.116278, mean_eps: 0.100000\n"," 1 : 4  \n","--->:649486:<---\n","  15396/500000: episode: 496, duration: 0.663s, episode steps:   6, steps per second:   9, episode reward: 16.000, mean reward:  2.667 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 243.202268, mean_q: 80.167034, mean_eps: 0.100000\n"," 1 : 5  \n","--->:650794:<---\n","  15408/500000: episode: 497, duration: 1.123s, episode steps:  12, steps per second:  11, episode reward: 10.000, mean reward:  0.833 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 225.292482, mean_q: 85.315229, mean_eps: 0.100000\n"," 1 : 9  \n","--->:652100:<---\n","  15420/500000: episode: 498, duration: 1.147s, episode steps:  12, steps per second:  10, episode reward: 217.000, mean reward: 18.083 [-25.000, 22.000], mean action: 0.750 [0.000, 1.000],  loss: 218.060884, mean_q: 84.370696, mean_eps: 0.100000\n"," 1 : 12  \n","--->:653410:<---\n","  15466/500000: episode: 499, duration: 3.892s, episode steps:  46, steps per second:  12, episode reward: 73.000, mean reward:  1.587 [-25.000, 22.000], mean action: 1.543 [0.000, 2.000],  loss: 224.540845, mean_q: 83.131516, mean_eps: 0.100000\n"," 1 : 14  \n","--->:654715:<---\n","  15502/500000: episode: 500, duration: 3.080s, episode steps:  36, steps per second:  12, episode reward: 86.000, mean reward:  2.389 [-25.000, 22.000], mean action: 1.139 [0.000, 2.000],  loss: 247.169003, mean_q: 83.216461, mean_eps: 0.100000\n"," 1 : 21  \n","--->:656021:<---\n","  15510/500000: episode: 501, duration: 0.812s, episode steps:   8, steps per second:  10, episode reward: 63.000, mean reward:  7.875 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 208.539833, mean_q: 85.084552, mean_eps: 0.100000\n"," 1 : 5  \n","--->:657324:<---\n","  15517/500000: episode: 502, duration: 0.720s, episode steps:   7, steps per second:  10, episode reward: 41.000, mean reward:  5.857 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 221.951069, mean_q: 85.342182, mean_eps: 0.100000\n"," 1 : 4  \n","--->:658626:<---\n","  15526/500000: episode: 503, duration: 0.905s, episode steps:   9, steps per second:  10, episode reward: -12.000, mean reward: -1.333 [-25.000, 22.000], mean action: 0.444 [0.000, 2.000],  loss: 195.913510, mean_q: 85.491193, mean_eps: 0.100000\n"," 1 : 8  \n","--->:659933:<---\n","  15539/500000: episode: 504, duration: 1.176s, episode steps:  13, steps per second:  11, episode reward: 82.000, mean reward:  6.308 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 221.759493, mean_q: 84.032548, mean_eps: 0.100000\n"," 1 : 8  \n","--->:661235:<---\n","  15552/500000: episode: 505, duration: 1.195s, episode steps:  13, steps per second:  11, episode reward: 57.000, mean reward:  4.385 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 232.668454, mean_q: 80.005518, mean_eps: 0.100000\n"," 1 : 9  \n","--->:662541:<---\n","  15564/500000: episode: 506, duration: 1.164s, episode steps:  12, steps per second:  10, episode reward: 79.000, mean reward:  6.583 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 200.650307, mean_q: 82.718011, mean_eps: 0.100000\n"," 1 : 10  \n","--->:663848:<---\n","  15587/500000: episode: 507, duration: 2.067s, episode steps:  23, steps per second:  11, episode reward: 16.000, mean reward:  0.696 [-25.000, 22.000], mean action: 1.565 [0.000, 2.000],  loss: 246.308787, mean_q: 82.607391, mean_eps: 0.100000\n"," 1 : 5  \n","--->:665155:<---\n","  15596/500000: episode: 508, duration: 0.915s, episode steps:   9, steps per second:  10, episode reward: 41.000, mean reward:  4.556 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 193.897174, mean_q: 83.516227, mean_eps: 0.100000\n"," 1 : 4  \n","--->:666463:<---\n","  15601/500000: episode: 509, duration: 0.605s, episode steps:   5, steps per second:   8, episode reward: -28.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.400 [1.000, 2.000],  loss: 242.867316, mean_q: 87.496455, mean_eps: 0.100000\n"," 1 : 3  \n","--->:667770:<---\n","  15613/500000: episode: 510, duration: 1.147s, episode steps:  12, steps per second:  10, episode reward: 107.000, mean reward:  8.917 [-25.000, 22.000], mean action: 1.083 [0.000, 2.000],  loss: 235.542341, mean_q: 81.731148, mean_eps: 0.100000\n"," 1 : 7  \n","--->:669075:<---\n","  15632/500000: episode: 511, duration: 1.721s, episode steps:  19, steps per second:  11, episode reward: 13.000, mean reward:  0.684 [-25.000, 22.000], mean action: 1.421 [0.000, 2.000],  loss: 219.178042, mean_q: 82.499480, mean_eps: 0.100000\n"," 1 : 7  \n","--->:670384:<---\n","  15645/500000: episode: 512, duration: 1.237s, episode steps:  13, steps per second:  11, episode reward: 16.000, mean reward:  1.231 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 254.178603, mean_q: 83.519916, mean_eps: 0.100000\n"," 1 : 5  \n","--->:671693:<---\n","  15688/500000: episode: 513, duration: 3.720s, episode steps:  43, steps per second:  12, episode reward: 41.000, mean reward:  0.953 [-25.000, 22.000], mean action: 1.814 [0.000, 2.000],  loss: 233.361734, mean_q: 83.318133, mean_eps: 0.100000\n"," 1 : 4  \n","--->:673001:<---\n","  15691/500000: episode: 514, duration: 0.400s, episode steps:   3, steps per second:   8, episode reward: -3.000, mean reward: -1.000 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 267.996719, mean_q: 83.885330, mean_eps: 0.100000\n"," 1 : 2  \n","--->:674306:<---\n","  15696/500000: episode: 515, duration: 0.581s, episode steps:   5, steps per second:   9, episode reward: -6.000, mean reward: -1.200 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 159.672258, mean_q: 87.057922, mean_eps: 0.100000\n"," 1 : 4  \n","--->:675615:<---\n","  15699/500000: episode: 516, duration: 0.391s, episode steps:   3, steps per second:   8, episode reward: -3.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.333 [1.000, 2.000],  loss: 278.887291, mean_q: 86.258687, mean_eps: 0.100000\n"," 1 : 2  \n","--->:676919:<---\n","  15704/500000: episode: 517, duration: 0.565s, episode steps:   5, steps per second:   9, episode reward: 16.000, mean reward:  3.200 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 223.740364, mean_q: 87.058612, mean_eps: 0.100000\n"," 1 : 5  \n","--->:678229:<---\n","  15719/500000: episode: 518, duration: 1.428s, episode steps:  15, steps per second:  11, episode reward: 104.000, mean reward:  6.933 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 237.358876, mean_q: 80.826125, mean_eps: 0.100000\n"," 1 : 9  \n","--->:679538:<---\n","  15731/500000: episode: 519, duration: 1.189s, episode steps:  12, steps per second:  10, episode reward: -9.000, mean reward: -0.750 [-25.000, 22.000], mean action: 1.083 [0.000, 2.000],  loss: 269.586419, mean_q: 81.991873, mean_eps: 0.100000\n"," 1 : 6  \n","--->:680839:<---\n","  15737/500000: episode: 520, duration: 0.668s, episode steps:   6, steps per second:   9, episode reward: -31.000, mean reward: -5.167 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 214.590904, mean_q: 83.660467, mean_eps: 0.100000\n"," 1 : 5  \n","--->:682140:<---\n","  15746/500000: episode: 521, duration: 0.987s, episode steps:   9, steps per second:   9, episode reward: 13.000, mean reward:  1.444 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 200.395014, mean_q: 82.204369, mean_eps: 0.100000\n"," 1 : 7  \n","--->:683448:<---\n","  15759/500000: episode: 522, duration: 1.286s, episode steps:  13, steps per second:  10, episode reward: 38.000, mean reward:  2.923 [-25.000, 22.000], mean action: 1.462 [0.000, 2.000],  loss: 275.721629, mean_q: 83.676524, mean_eps: 0.100000\n"," 1 : 6  \n","--->:684753:<---\n","  15784/500000: episode: 523, duration: 2.261s, episode steps:  25, steps per second:  11, episode reward: 151.000, mean reward:  6.040 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 220.230094, mean_q: 86.768748, mean_eps: 0.100000\n"," 1 : 9  \n","--->:686062:<---\n","  15793/500000: episode: 524, duration: 0.863s, episode steps:   9, steps per second:  10, episode reward: -78.000, mean reward: -8.667 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 264.400091, mean_q: 78.003371, mean_eps: 0.100000\n"," 1 : 5  \n","--->:687367:<---\n","  15819/500000: episode: 525, duration: 2.249s, episode steps:  26, steps per second:  12, episode reward: 85.000, mean reward:  3.269 [-25.000, 22.000], mean action: 1.769 [1.000, 2.000],  loss: 254.713430, mean_q: 82.432264, mean_eps: 0.100000\n"," 1 : 6  \n","--->:688673:<---\n","  15835/500000: episode: 526, duration: 1.463s, episode steps:  16, steps per second:  11, episode reward: -15.000, mean reward: -0.938 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 235.002546, mean_q: 82.644815, mean_eps: 0.100000\n"," 1 : 10  \n","--->:689980:<---\n","  15859/500000: episode: 527, duration: 2.222s, episode steps:  24, steps per second:  11, episode reward: -2.000, mean reward: -0.083 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 296.133400, mean_q: 82.351491, mean_eps: 0.100000\n"," 1 : 17  \n","--->:691281:<---\n","  15900/500000: episode: 528, duration: 3.554s, episode steps:  41, steps per second:  12, episode reward: 296.000, mean reward:  7.220 [-25.000, 22.000], mean action: 1.268 [0.000, 2.000],  loss: 249.117662, mean_q: 84.586920, mean_eps: 0.100000\n"," 1 : 22  \n","--->:692584:<---\n","  15918/500000: episode: 529, duration: 1.663s, episode steps:  18, steps per second:  11, episode reward: 129.000, mean reward:  7.167 [-25.000, 22.000], mean action: 1.556 [1.000, 2.000],  loss: 254.033116, mean_q: 82.032228, mean_eps: 0.100000\n"," 1 : 8  \n","--->:693890:<---\n","  15931/500000: episode: 530, duration: 1.230s, episode steps:  13, steps per second:  11, episode reward: 104.000, mean reward:  8.000 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 226.216471, mean_q: 83.724831, mean_eps: 0.100000\n"," 1 : 9  \n","--->:695195:<---\n","  15943/500000: episode: 531, duration: 1.105s, episode steps:  12, steps per second:  11, episode reward: 63.000, mean reward:  5.250 [-25.000, 22.000], mean action: 1.417 [0.000, 2.000],  loss: 241.042985, mean_q: 87.263631, mean_eps: 0.100000\n"," 1 : 5  \n","--->:696498:<---\n","  15951/500000: episode: 532, duration: 0.830s, episode steps:   8, steps per second:  10, episode reward: 41.000, mean reward:  5.125 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 298.960764, mean_q: 80.220910, mean_eps: 0.100000\n"," 1 : 4  \n","--->:697800:<---\n","  15963/500000: episode: 533, duration: 1.131s, episode steps:  12, steps per second:  11, episode reward: 63.000, mean reward:  5.250 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 238.026487, mean_q: 82.869680, mean_eps: 0.100000\n"," 1 : 5  \n","--->:699108:<---\n","  15966/500000: episode: 534, duration: 0.388s, episode steps:   3, steps per second:   8, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 0.667 [0.000, 1.000],  loss: 184.145935, mean_q: 86.601657, mean_eps: 0.100000\n"," 1 : 3  \n","--->:700413:<---\n","  15968/500000: episode: 535, duration: 0.320s, episode steps:   2, steps per second:   6, episode reward: -25.000, mean reward: -12.500 [-25.000,  0.000], mean action: 1.500 [1.000, 2.000],  loss: 200.789459, mean_q: 81.911373, mean_eps: 0.100000\n"," 1 : 1  \n","--->:701716:<---\n","  15975/500000: episode: 536, duration: 0.731s, episode steps:   7, steps per second:  10, episode reward: 16.000, mean reward:  2.286 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 257.939737, mean_q: 83.348975, mean_eps: 0.100000\n"," 1 : 5  \n","--->:703017:<---\n","  16002/500000: episode: 537, duration: 2.393s, episode steps:  27, steps per second:  11, episode reward:  4.000, mean reward:  0.148 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 231.088702, mean_q: 83.020595, mean_eps: 0.100000\n"," 1 : 13  \n","--->:704326:<---\n","  16013/500000: episode: 538, duration: 0.998s, episode steps:  11, steps per second:  11, episode reward: 151.000, mean reward: 13.727 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 223.696836, mean_q: 82.208393, mean_eps: 0.100000\n"," 1 : 9  \n","--->:705631:<---\n","  16025/500000: episode: 539, duration: 1.147s, episode steps:  12, steps per second:  10, episode reward: 151.000, mean reward: 12.583 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 220.281996, mean_q: 84.193638, mean_eps: 0.100000\n"," 1 : 9  \n","--->:706933:<---\n","  16029/500000: episode: 540, duration: 0.460s, episode steps:   4, steps per second:   9, episode reward: -28.000, mean reward: -7.000 [-25.000, 22.000], mean action: 1.250 [1.000, 2.000],  loss: 218.420498, mean_q: 84.078518, mean_eps: 0.100000\n"," 1 : 3  \n","--->:708235:<---\n","  16051/500000: episode: 541, duration: 1.947s, episode steps:  22, steps per second:  11, episode reward: 32.000, mean reward:  1.455 [-25.000, 22.000], mean action: 1.409 [0.000, 2.000],  loss: 242.954764, mean_q: 84.707130, mean_eps: 0.100000\n"," 1 : 10  \n","--->:709543:<---\n","  16058/500000: episode: 542, duration: 0.732s, episode steps:   7, steps per second:  10, episode reward: -81.000, mean reward: -11.571 [-25.000, 22.000], mean action: 0.429 [0.000, 1.000],  loss: 237.299533, mean_q: 81.844404, mean_eps: 0.100000\n"," 1 : 7  \n","--->:710848:<---\n","  16083/500000: episode: 543, duration: 2.248s, episode steps:  25, steps per second:  11, episode reward: 95.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.080 [0.000, 2.000],  loss: 271.120026, mean_q: 80.952393, mean_eps: 0.100000\n"," 1 : 15  \n","--->:712151:<---\n","  16108/500000: episode: 544, duration: 2.202s, episode steps:  25, steps per second:  11, episode reward:  1.000, mean reward:  0.040 [-25.000, 22.000], mean action: 1.320 [0.000, 2.000],  loss: 267.072623, mean_q: 82.594954, mean_eps: 0.100000\n"," 1 : 15  \n","--->:713453:<---\n","  16122/500000: episode: 545, duration: 1.286s, episode steps:  14, steps per second:  11, episode reward: 35.000, mean reward:  2.500 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 265.852032, mean_q: 81.897560, mean_eps: 0.100000\n"," 1 : 8  \n","--->:714759:<---\n","  16132/500000: episode: 546, duration: 0.964s, episode steps:  10, steps per second:  10, episode reward: 63.000, mean reward:  6.300 [-25.000, 22.000], mean action: 1.500 [1.000, 2.000],  loss: 225.721391, mean_q: 83.783080, mean_eps: 0.100000\n"," 1 : 5  \n","--->:716065:<---\n","  16164/500000: episode: 547, duration: 2.721s, episode steps:  32, steps per second:  12, episode reward: 120.000, mean reward:  3.750 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 249.568145, mean_q: 84.921004, mean_eps: 0.100000\n"," 1 : 14  \n","--->:717375:<---\n","  16195/500000: episode: 548, duration: 2.746s, episode steps:  31, steps per second:  11, episode reward: 98.000, mean reward:  3.161 [-25.000, 22.000], mean action: 1.387 [0.000, 2.000],  loss: 247.049877, mean_q: 85.616081, mean_eps: 0.100000\n"," 1 : 13  \n","--->:718681:<---\n","  16200/500000: episode: 549, duration: 0.564s, episode steps:   5, steps per second:   9, episode reward: 63.000, mean reward: 12.600 [-25.000, 22.000], mean action: 0.400 [0.000, 1.000],  loss: 199.752112, mean_q: 87.591577, mean_eps: 0.100000\n"," 1 : 5  \n","--->:719984:<---\n","  16210/500000: episode: 550, duration: 0.906s, episode steps:  10, steps per second:  11, episode reward: 35.000, mean reward:  3.500 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 240.350087, mean_q: 83.954588, mean_eps: 0.100000\n"," 1 : 8  \n","--->:721286:<---\n","  16216/500000: episode: 551, duration: 0.656s, episode steps:   6, steps per second:   9, episode reward: 63.000, mean reward: 10.500 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 228.556107, mean_q: 84.115726, mean_eps: 0.100000\n"," 1 : 5  \n","--->:722595:<---\n","  16249/500000: episode: 552, duration: 2.838s, episode steps:  33, steps per second:  12, episode reward: 79.000, mean reward:  2.394 [-25.000, 22.000], mean action: 1.545 [0.000, 2.000],  loss: 227.817065, mean_q: 83.179190, mean_eps: 0.100000\n"," 1 : 10  \n","--->:723900:<---\n","  16252/500000: episode: 553, duration: 0.386s, episode steps:   3, steps per second:   8, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 371.674245, mean_q: 82.550054, mean_eps: 0.100000\n"," 1 : 3  \n","--->:725207:<---\n","  16269/500000: episode: 554, duration: 1.548s, episode steps:  17, steps per second:  11, episode reward: -15.000, mean reward: -0.882 [-25.000, 22.000], mean action: 1.059 [0.000, 2.000],  loss: 248.038168, mean_q: 82.691330, mean_eps: 0.100000\n"," 1 : 10  \n","--->:726513:<---\n","  16296/500000: episode: 555, duration: 2.329s, episode steps:  27, steps per second:  12, episode reward: 173.000, mean reward:  6.407 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 237.569350, mean_q: 82.261641, mean_eps: 0.100000\n"," 1 : 10  \n","--->:727823:<---\n","  16329/500000: episode: 556, duration: 2.880s, episode steps:  33, steps per second:  11, episode reward: 57.000, mean reward:  1.727 [-25.000, 22.000], mean action: 1.515 [0.000, 2.000],  loss: 252.595231, mean_q: 83.843493, mean_eps: 0.100000\n"," 1 : 9  \n","--->:729126:<---\n","  16338/500000: episode: 557, duration: 0.887s, episode steps:   9, steps per second:  10, episode reward: 35.000, mean reward:  3.889 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 233.708996, mean_q: 83.178045, mean_eps: 0.100000\n"," 1 : 8  \n","--->:730432:<---\n","  16369/500000: episode: 558, duration: 2.678s, episode steps:  31, steps per second:  12, episode reward: 293.000, mean reward:  9.452 [-25.000, 22.000], mean action: 0.774 [0.000, 2.000],  loss: 218.584464, mean_q: 83.500372, mean_eps: 0.100000\n"," 1 : 24  \n","--->:731733:<---\n","  16399/500000: episode: 559, duration: 2.649s, episode steps:  30, steps per second:  11, episode reward: 236.000, mean reward:  7.867 [-25.000, 22.000], mean action: 1.300 [0.000, 2.000],  loss: 221.267469, mean_q: 81.840570, mean_eps: 0.100000\n"," 1 : 15  \n","--->:733041:<---\n","  16413/500000: episode: 560, duration: 1.363s, episode steps:  14, steps per second:  10, episode reward: 242.000, mean reward: 17.286 [ 0.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 221.242045, mean_q: 85.585189, mean_eps: 0.100000\n"," 1 : 11  \n","--->:734351:<---\n","  16422/500000: episode: 561, duration: 0.921s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 257.262568, mean_q: 82.289129, mean_eps: 0.100000\n"," 1 : 7  \n","--->:735652:<---\n","  16433/500000: episode: 562, duration: 1.080s, episode steps:  11, steps per second:  10, episode reward: 35.000, mean reward:  3.182 [-25.000, 22.000], mean action: 0.727 [0.000, 2.000],  loss: 285.222180, mean_q: 81.901935, mean_eps: 0.100000\n"," 1 : 8  \n","--->:736960:<---\n","  16457/500000: episode: 563, duration: 2.090s, episode steps:  24, steps per second:  11, episode reward: 54.000, mean reward:  2.250 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 241.871173, mean_q: 83.996998, mean_eps: 0.100000\n"," 1 : 11  \n","--->:738264:<---\n","  16469/500000: episode: 564, duration: 1.143s, episode steps:  12, steps per second:  10, episode reward: 57.000, mean reward:  4.750 [-25.000, 22.000], mean action: 1.083 [0.000, 2.000],  loss: 251.226339, mean_q: 83.066463, mean_eps: 0.100000\n"," 1 : 9  \n","--->:739569:<---\n","  16474/500000: episode: 565, duration: 0.545s, episode steps:   5, steps per second:   9, episode reward: 16.000, mean reward:  3.200 [-25.000, 22.000], mean action: 0.400 [0.000, 1.000],  loss: 287.339590, mean_q: 81.371542, mean_eps: 0.100000\n"," 1 : 5  \n","--->:740877:<---\n","  16515/500000: episode: 566, duration: 3.591s, episode steps:  41, steps per second:  11, episode reward: 104.000, mean reward:  2.537 [-25.000, 22.000], mean action: 1.659 [0.000, 2.000],  loss: 236.324314, mean_q: 82.177646, mean_eps: 0.100000\n"," 1 : 9  \n","--->:742185:<---\n","  16524/500000: episode: 567, duration: 0.916s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 213.997457, mean_q: 84.639542, mean_eps: 0.100000\n"," 1 : 7  \n","--->:743486:<---\n","  16531/500000: episode: 568, duration: 0.718s, episode steps:   7, steps per second:  10, episode reward: 16.000, mean reward:  2.286 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 278.080599, mean_q: 80.812836, mean_eps: 0.100000\n"," 1 : 5  \n","--->:744796:<---\n","  16546/500000: episode: 569, duration: 1.388s, episode steps:  15, steps per second:  11, episode reward: 104.000, mean reward:  6.933 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 197.238685, mean_q: 81.067848, mean_eps: 0.100000\n"," 1 : 9  \n","--->:746101:<---\n","  16568/500000: episode: 570, duration: 1.975s, episode steps:  22, steps per second:  11, episode reward: -28.000, mean reward: -1.273 [-25.000, 22.000], mean action: 1.864 [1.000, 2.000],  loss: 249.146198, mean_q: 83.279987, mean_eps: 0.100000\n"," 1 : 3  \n","--->:747410:<---\n","  16570/500000: episode: 571, duration: 0.296s, episode steps:   2, steps per second:   7, episode reward: -50.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.000 [0.000, 0.000],  loss: 161.678383, mean_q: 86.966347, mean_eps: 0.100000\n"," 1 : 2  \n","--->:748712:<---\n","  16573/500000: episode: 572, duration: 0.396s, episode steps:   3, steps per second:   8, episode reward: 19.000, mean reward:  6.333 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 305.834452, mean_q: 80.740176, mean_eps: 0.100000\n"," 1 : 3  \n","--->:750017:<---\n","  16580/500000: episode: 573, duration: 0.721s, episode steps:   7, steps per second:  10, episode reward: 85.000, mean reward: 12.143 [-25.000, 22.000], mean action: 0.571 [0.000, 2.000],  loss: 229.771809, mean_q: 82.077500, mean_eps: 0.100000\n"," 1 : 6  \n","--->:751324:<---\n","  16604/500000: episode: 574, duration: 2.116s, episode steps:  24, steps per second:  11, episode reward: 60.000, mean reward:  2.500 [-25.000, 22.000], mean action: 1.458 [0.000, 2.000],  loss: 259.497881, mean_q: 83.643250, mean_eps: 0.100000\n"," 1 : 7  \n","--->:752627:<---\n","  16619/500000: episode: 575, duration: 1.379s, episode steps:  15, steps per second:  11, episode reward: 195.000, mean reward: 13.000 [-25.000, 22.000], mean action: 0.933 [0.000, 2.000],  loss: 244.102589, mean_q: 84.635413, mean_eps: 0.100000\n"," 1 : 11  \n","--->:753936:<---\n","  16636/500000: episode: 576, duration: 1.528s, episode steps:  17, steps per second:  11, episode reward: 126.000, mean reward:  7.412 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 205.172396, mean_q: 84.218232, mean_eps: 0.100000\n"," 1 : 10  \n","--->:755237:<---\n","  16647/500000: episode: 577, duration: 1.092s, episode steps:  11, steps per second:  10, episode reward: 129.000, mean reward: 11.727 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 265.992682, mean_q: 85.127410, mean_eps: 0.100000\n"," 1 : 8  \n","--->:756538:<---\n","  16664/500000: episode: 578, duration: 1.535s, episode steps:  17, steps per second:  11, episode reward: -106.000, mean reward: -6.235 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 232.912766, mean_q: 82.865732, mean_eps: 0.100000\n"," 1 : 8  \n","--->:757844:<---\n","  16674/500000: episode: 579, duration: 0.955s, episode steps:  10, steps per second:  10, episode reward: 38.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 274.907709, mean_q: 83.537141, mean_eps: 0.100000\n"," 1 : 6  \n","--->:759154:<---\n","  16705/500000: episode: 580, duration: 2.672s, episode steps:  31, steps per second:  12, episode reward: 82.000, mean reward:  2.645 [-25.000, 22.000], mean action: 1.645 [0.000, 2.000],  loss: 248.770289, mean_q: 82.172093, mean_eps: 0.100000\n"," 1 : 8  \n","--->:760456:<---\n","  16711/500000: episode: 581, duration: 0.616s, episode steps:   6, steps per second:  10, episode reward: 16.000, mean reward:  2.667 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 248.745244, mean_q: 85.144112, mean_eps: 0.100000\n"," 1 : 5  \n","--->:761760:<---\n","  16714/500000: episode: 582, duration: 0.377s, episode steps:   3, steps per second:   8, episode reward: 19.000, mean reward:  6.333 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 286.979370, mean_q: 83.464327, mean_eps: 0.100000\n"," 1 : 3  \n","--->:763070:<---\n","  16731/500000: episode: 583, duration: 1.609s, episode steps:  17, steps per second:  11, episode reward: -9.000, mean reward: -0.529 [-25.000, 22.000], mean action: 1.529 [0.000, 2.000],  loss: 221.403026, mean_q: 83.081451, mean_eps: 0.100000\n"," 1 : 6  \n","--->:764378:<---\n","  16738/500000: episode: 584, duration: 0.707s, episode steps:   7, steps per second:  10, episode reward: -28.000, mean reward: -4.000 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 275.021444, mean_q: 80.834879, mean_eps: 0.100000\n"," 1 : 3  \n","--->:765680:<---\n","  16747/500000: episode: 585, duration: 0.902s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 226.450072, mean_q: 82.148659, mean_eps: 0.100000\n"," 1 : 7  \n","--->:766986:<---\n","  16751/500000: episode: 586, duration: 0.482s, episode steps:   4, steps per second:   8, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.000 [0.000, 0.000],  loss: 316.839653, mean_q: 84.367035, mean_eps: 0.100000\n"," 1 : 4  \n","--->:768294:<---\n","  16766/500000: episode: 587, duration: 1.430s, episode steps:  15, steps per second:  10, episode reward: 79.000, mean reward:  5.267 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 229.877949, mean_q: 80.236449, mean_eps: 0.100000\n"," 1 : 10  \n","--->:769596:<---\n","  16785/500000: episode: 588, duration: 1.693s, episode steps:  19, steps per second:  11, episode reward: 13.000, mean reward:  0.684 [-25.000, 22.000], mean action: 1.526 [0.000, 2.000],  loss: 268.226575, mean_q: 83.090645, mean_eps: 0.100000\n"," 1 : 7  \n","--->:770899:<---\n","  16802/500000: episode: 589, duration: 1.514s, episode steps:  17, steps per second:  11, episode reward: -56.000, mean reward: -3.294 [-25.000, 22.000], mean action: 1.471 [0.000, 2.000],  loss: 212.371031, mean_q: 82.852407, mean_eps: 0.100000\n"," 1 : 6  \n","--->:772205:<---\n","  16806/500000: episode: 590, duration: 0.492s, episode steps:   4, steps per second:   8, episode reward: 19.000, mean reward:  4.750 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 257.557079, mean_q: 82.858526, mean_eps: 0.100000\n"," 1 : 3  \n","--->:773506:<---\n","  16812/500000: episode: 591, duration: 0.627s, episode steps:   6, steps per second:  10, episode reward: 38.000, mean reward:  6.333 [-25.000, 22.000], mean action: 0.167 [0.000, 1.000],  loss: 203.006767, mean_q: 85.537254, mean_eps: 0.100000\n"," 1 : 6  \n","--->:774813:<---\n","  16848/500000: episode: 592, duration: 3.051s, episode steps:  36, steps per second:  12, episode reward: -12.000, mean reward: -0.333 [-25.000, 22.000], mean action: 1.722 [0.000, 2.000],  loss: 240.592202, mean_q: 82.950459, mean_eps: 0.100000\n"," 1 : 8  \n","--->:776120:<---\n","  16887/500000: episode: 593, duration: 3.374s, episode steps:  39, steps per second:  12, episode reward: -9.000, mean reward: -0.231 [-25.000, 22.000], mean action: 1.769 [0.000, 2.000],  loss: 251.746434, mean_q: 83.145260, mean_eps: 0.100000\n"," 1 : 6  \n","--->:777430:<---\n","  16896/500000: episode: 594, duration: 0.874s, episode steps:   9, steps per second:  10, episode reward: -106.000, mean reward: -11.778 [-25.000, 22.000], mean action: 0.556 [0.000, 2.000],  loss: 271.018892, mean_q: 86.710041, mean_eps: 0.100000\n"," 1 : 8  \n","--->:778736:<---\n","  16909/500000: episode: 595, duration: 1.224s, episode steps:  13, steps per second:  11, episode reward: 79.000, mean reward:  6.077 [-25.000, 22.000], mean action: 0.769 [0.000, 2.000],  loss: 231.150188, mean_q: 80.953399, mean_eps: 0.100000\n"," 1 : 10  \n","--->:780038:<---\n","  16925/500000: episode: 596, duration: 1.448s, episode steps:  16, steps per second:  11, episode reward: 104.000, mean reward:  6.500 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 241.193966, mean_q: 84.535913, mean_eps: 0.100000\n"," 1 : 9  \n","--->:781347:<---\n","  16948/500000: episode: 597, duration: 2.005s, episode steps:  23, steps per second:  11, episode reward: 57.000, mean reward:  2.478 [-25.000, 22.000], mean action: 1.565 [0.000, 2.000],  loss: 225.684723, mean_q: 83.864094, mean_eps: 0.100000\n"," 1 : 9  \n","--->:782657:<---\n","  16977/500000: episode: 598, duration: 2.486s, episode steps:  29, steps per second:  12, episode reward: 57.000, mean reward:  1.966 [-25.000, 22.000], mean action: 1.552 [0.000, 2.000],  loss: 210.431095, mean_q: 85.122746, mean_eps: 0.100000\n"," 1 : 9  \n","--->:783965:<---\n","  17002/500000: episode: 599, duration: 2.167s, episode steps:  25, steps per second:  12, episode reward: -43.000, mean reward: -1.720 [-25.000, 22.000], mean action: 1.080 [0.000, 2.000],  loss: 261.255605, mean_q: 82.680587, mean_eps: 0.100000\n"," 1 : 13  \n","--->:785272:<---\n","  17022/500000: episode: 600, duration: 1.802s, episode steps:  20, steps per second:  11, episode reward: 13.000, mean reward:  0.650 [-25.000, 22.000], mean action: 1.550 [0.000, 2.000],  loss: 249.256629, mean_q: 83.501645, mean_eps: 0.100000\n"," 1 : 7  \n","--->:786580:<---\n","  17043/500000: episode: 601, duration: 1.805s, episode steps:  21, steps per second:  12, episode reward: -9.000, mean reward: -0.429 [-25.000, 22.000], mean action: 1.667 [0.000, 2.000],  loss: 244.755856, mean_q: 82.019945, mean_eps: 0.100000\n"," 1 : 6  \n","--->:787888:<---\n","  17047/500000: episode: 602, duration: 0.483s, episode steps:   4, steps per second:   8, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 143.315189, mean_q: 88.218777, mean_eps: 0.100000\n"," 1 : 4  \n","--->:789195:<---\n","  17052/500000: episode: 603, duration: 0.562s, episode steps:   5, steps per second:   9, episode reward: 19.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 297.683685, mean_q: 82.336319, mean_eps: 0.100000\n"," 1 : 3  \n","--->:790498:<---\n","  17071/500000: episode: 604, duration: 1.710s, episode steps:  19, steps per second:  11, episode reward: 192.000, mean reward: 10.105 [-25.000, 22.000], mean action: 0.842 [0.000, 2.000],  loss: 235.962926, mean_q: 81.652573, mean_eps: 0.100000\n"," 1 : 13  \n","--->:791804:<---\n","  17091/500000: episode: 605, duration: 1.781s, episode steps:  20, steps per second:  11, episode reward: -34.000, mean reward: -1.700 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 217.742590, mean_q: 81.901396, mean_eps: 0.100000\n"," 1 : 7  \n","--->:793105:<---\n","  17102/500000: episode: 606, duration: 1.053s, episode steps:  11, steps per second:  10, episode reward: 104.000, mean reward:  9.455 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 236.592022, mean_q: 84.418541, mean_eps: 0.100000\n"," 1 : 9  \n","--->:794415:<---\n","  17114/500000: episode: 607, duration: 1.119s, episode steps:  12, steps per second:  11, episode reward: 35.000, mean reward:  2.917 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 255.427990, mean_q: 84.924513, mean_eps: 0.100000\n"," 1 : 8  \n","--->:795725:<---\n","  17127/500000: episode: 608, duration: 1.188s, episode steps:  13, steps per second:  11, episode reward: 38.000, mean reward:  2.923 [-25.000, 22.000], mean action: 1.385 [0.000, 2.000],  loss: 238.709525, mean_q: 82.601071, mean_eps: 0.100000\n"," 1 : 6  \n","--->:797032:<---\n","  17150/500000: episode: 609, duration: 2.028s, episode steps:  23, steps per second:  11, episode reward: 60.000, mean reward:  2.609 [-25.000, 22.000], mean action: 1.478 [0.000, 2.000],  loss: 248.902051, mean_q: 85.704971, mean_eps: 0.100000\n"," 1 : 7  \n","--->:798338:<---\n","  17153/500000: episode: 610, duration: 0.411s, episode steps:   3, steps per second:   7, episode reward: -3.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 223.076141, mean_q: 79.884763, mean_eps: 0.100000\n"," 1 : 2  \n","--->:799640:<---\n","  17161/500000: episode: 611, duration: 0.796s, episode steps:   8, steps per second:  10, episode reward: 85.000, mean reward: 10.625 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 283.222939, mean_q: 83.584391, mean_eps: 0.100000\n"," 1 : 6  \n","--->:800949:<---\n","  17166/500000: episode: 612, duration: 0.564s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 255.870532, mean_q: 86.757980, mean_eps: 0.100000\n"," 1 : 4  \n","--->:802256:<---\n","  17184/500000: episode: 613, duration: 1.645s, episode steps:  18, steps per second:  11, episode reward: 35.000, mean reward:  1.944 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 270.713661, mean_q: 82.401097, mean_eps: 0.100000\n"," 1 : 8  \n","--->:803562:<---\n","  17187/500000: episode: 614, duration: 0.386s, episode steps:   3, steps per second:   8, episode reward: 44.000, mean reward: 14.667 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 265.988810, mean_q: 83.306908, mean_eps: 0.100000\n"," 1 : 2  \n","--->:804868:<---\n","  17202/500000: episode: 615, duration: 1.344s, episode steps:  15, steps per second:  11, episode reward: -9.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.533 [0.000, 2.000],  loss: 279.809718, mean_q: 84.190248, mean_eps: 0.100000\n"," 1 : 6  \n","--->:806178:<---\n","  17212/500000: episode: 616, duration: 0.997s, episode steps:  10, steps per second:  10, episode reward: -56.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 247.637121, mean_q: 84.517527, mean_eps: 0.100000\n"," 1 : 6  \n","--->:807481:<---\n","  17218/500000: episode: 617, duration: 0.655s, episode steps:   6, steps per second:   9, episode reward: -31.000, mean reward: -5.167 [-25.000, 22.000], mean action: 0.500 [0.000, 2.000],  loss: 269.200478, mean_q: 85.659170, mean_eps: 0.100000\n"," 1 : 5  \n","--->:808787:<---\n","  17238/500000: episode: 618, duration: 1.774s, episode steps:  20, steps per second:  11, episode reward: 101.000, mean reward:  5.050 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 238.309115, mean_q: 82.393716, mean_eps: 0.100000\n"," 1 : 11  \n","--->:810097:<---\n","  17247/500000: episode: 619, duration: 0.887s, episode steps:   9, steps per second:  10, episode reward: 13.000, mean reward:  1.444 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 211.146088, mean_q: 85.734208, mean_eps: 0.100000\n"," 1 : 7  \n","--->:811404:<---\n","  17262/500000: episode: 620, duration: 1.374s, episode steps:  15, steps per second:  11, episode reward: 57.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 210.162577, mean_q: 82.249122, mean_eps: 0.100000\n"," 1 : 9  \n","--->:812714:<---\n","  17266/500000: episode: 621, duration: 0.486s, episode steps:   4, steps per second:   8, episode reward: 88.000, mean reward: 22.000 [22.000, 22.000], mean action: 0.750 [0.000, 1.000],  loss: 361.070328, mean_q: 79.569256, mean_eps: 0.100000\n"," 1 : 4  \n","--->:814017:<---\n","  17275/500000: episode: 622, duration: 0.866s, episode steps:   9, steps per second:  10, episode reward: 13.000, mean reward:  1.444 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 230.935616, mean_q: 82.816144, mean_eps: 0.100000\n"," 1 : 7  \n","--->:815325:<---\n","  17291/500000: episode: 623, duration: 1.454s, episode steps:  16, steps per second:  11, episode reward: 132.000, mean reward:  8.250 [ 0.000, 22.000], mean action: 1.625 [1.000, 2.000],  loss: 204.792129, mean_q: 85.015161, mean_eps: 0.100000\n"," 1 : 6  \n","--->:816631:<---\n","  17293/500000: episode: 624, duration: 0.307s, episode steps:   2, steps per second:   7, episode reward: -3.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.000 [0.000, 0.000],  loss: 131.329407, mean_q: 86.870258, mean_eps: 0.100000\n"," 1 : 2  \n","--->:817937:<---\n","  17295/500000: episode: 625, duration: 0.300s, episode steps:   2, steps per second:   7, episode reward: -50.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.000 [0.000, 0.000],  loss: 265.984146, mean_q: 88.379124, mean_eps: 0.100000\n"," 1 : 2  \n","--->:819240:<---\n","  17300/500000: episode: 626, duration: 0.547s, episode steps:   5, steps per second:   9, episode reward: -6.000, mean reward: -1.200 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 207.192111, mean_q: 88.187405, mean_eps: 0.100000\n"," 1 : 4  \n","--->:820541:<---\n","  17315/500000: episode: 627, duration: 1.353s, episode steps:  15, steps per second:  11, episode reward: 66.000, mean reward:  4.400 [ 0.000, 22.000], mean action: 1.667 [0.000, 2.000],  loss: 208.226074, mean_q: 79.441776, mean_eps: 0.100000\n"," 1 : 3  \n","--->:821849:<---\n","  17333/500000: episode: 628, duration: 1.611s, episode steps:  18, steps per second:  11, episode reward: -78.000, mean reward: -4.333 [-25.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 248.098394, mean_q: 85.015565, mean_eps: 0.100000\n"," 1 : 5  \n","--->:823152:<---\n","  17354/500000: episode: 629, duration: 1.870s, episode steps:  21, steps per second:  11, episode reward: -6.000, mean reward: -0.286 [-25.000, 22.000], mean action: 1.714 [0.000, 2.000],  loss: 216.876235, mean_q: 82.019060, mean_eps: 0.100000\n"," 1 : 4  \n","--->:824456:<---\n","  17366/500000: episode: 630, duration: 1.105s, episode steps:  12, steps per second:  11, episode reward: 63.000, mean reward:  5.250 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 258.603896, mean_q: 80.914567, mean_eps: 0.100000\n"," 1 : 5  \n","--->:825760:<---\n","  17381/500000: episode: 631, duration: 1.400s, episode steps:  15, steps per second:  11, episode reward: -6.000, mean reward: -0.400 [-25.000, 22.000], mean action: 1.667 [0.000, 2.000],  loss: 233.844090, mean_q: 82.517977, mean_eps: 0.100000\n"," 1 : 4  \n","--->:827067:<---\n","  17384/500000: episode: 632, duration: 0.396s, episode steps:   3, steps per second:   8, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 0.333 [0.000, 1.000],  loss: 200.370316, mean_q: 82.048042, mean_eps: 0.100000\n"," 1 : 3  \n","--->:828374:<---\n","  17391/500000: episode: 633, duration: 0.743s, episode steps:   7, steps per second:   9, episode reward: -9.000, mean reward: -1.286 [-25.000, 22.000], mean action: 0.714 [0.000, 2.000],  loss: 186.113038, mean_q: 83.770251, mean_eps: 0.100000\n"," 1 : 6  \n","--->:829682:<---\n","  17411/500000: episode: 634, duration: 1.808s, episode steps:  20, steps per second:  11, episode reward: 161.000, mean reward:  8.050 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 203.091100, mean_q: 82.218098, mean_eps: 0.100000\n"," 1 : 18  \n","--->:830990:<---\n","  17431/500000: episode: 635, duration: 1.822s, episode steps:  20, steps per second:  11, episode reward: 29.000, mean reward:  1.450 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 240.215403, mean_q: 84.076496, mean_eps: 0.100000\n"," 1 : 12  \n","--->:832291:<---\n","  17436/500000: episode: 636, duration: 0.561s, episode steps:   5, steps per second:   9, episode reward: 66.000, mean reward: 13.200 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 205.198846, mean_q: 85.360081, mean_eps: 0.100000\n"," 1 : 3  \n","--->:833597:<---\n","  17462/500000: episode: 637, duration: 2.257s, episode steps:  26, steps per second:  12, episode reward: 151.000, mean reward:  5.808 [-25.000, 22.000], mean action: 1.538 [0.000, 2.000],  loss: 260.480265, mean_q: 81.651933, mean_eps: 0.100000\n"," 1 : 9  \n","--->:834905:<---\n","  17481/500000: episode: 638, duration: 1.700s, episode steps:  19, steps per second:  11, episode reward: 173.000, mean reward:  9.105 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 253.544788, mean_q: 82.626128, mean_eps: 0.100000\n"," 1 : 10  \n","--->:836210:<---\n","  17502/500000: episode: 639, duration: 1.842s, episode steps:  21, steps per second:  11, episode reward: 151.000, mean reward:  7.190 [-25.000, 22.000], mean action: 1.381 [0.000, 2.000],  loss: 236.264581, mean_q: 83.297666, mean_eps: 0.100000\n"," 1 : 9  \n","--->:837519:<---\n","  17510/500000: episode: 640, duration: 0.794s, episode steps:   8, steps per second:  10, episode reward: 35.000, mean reward:  4.375 [-25.000, 22.000], mean action: 0.875 [0.000, 1.000],  loss: 240.819048, mean_q: 79.123657, mean_eps: 0.100000\n"," 1 : 8  \n","--->:838825:<---\n","  17520/500000: episode: 641, duration: 0.976s, episode steps:  10, steps per second:  10, episode reward: -56.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 186.458018, mean_q: 82.345215, mean_eps: 0.100000\n"," 1 : 6  \n","--->:840131:<---\n","  17527/500000: episode: 642, duration: 0.712s, episode steps:   7, steps per second:  10, episode reward: 16.000, mean reward:  2.286 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 257.470226, mean_q: 83.988583, mean_eps: 0.100000\n"," 1 : 5  \n","--->:841432:<---\n","  17587/500000: episode: 643, duration: 5.067s, episode steps:  60, steps per second:  12, episode reward: 54.000, mean reward:  0.900 [-25.000, 22.000], mean action: 1.750 [0.000, 2.000],  loss: 239.047046, mean_q: 83.576565, mean_eps: 0.100000\n"," 1 : 11  \n","--->:842734:<---\n","  17611/500000: episode: 644, duration: 2.112s, episode steps:  24, steps per second:  11, episode reward:  7.000, mean reward:  0.292 [-25.000, 22.000], mean action: 1.458 [0.000, 2.000],  loss: 250.101172, mean_q: 83.722216, mean_eps: 0.100000\n"," 1 : 11  \n","--->:844040:<---\n","  17624/500000: episode: 645, duration: 1.211s, episode steps:  13, steps per second:  11, episode reward: 60.000, mean reward:  4.615 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 225.865700, mean_q: 83.233847, mean_eps: 0.100000\n"," 1 : 7  \n","--->:845346:<---\n","  17627/500000: episode: 646, duration: 0.388s, episode steps:   3, steps per second:   8, episode reward: 66.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 249.671946, mean_q: 82.972710, mean_eps: 0.100000\n"," 1 : 3  \n","--->:846651:<---\n","  17645/500000: episode: 647, duration: 1.665s, episode steps:  18, steps per second:  11, episode reward: 101.000, mean reward:  5.611 [-25.000, 22.000], mean action: 0.944 [0.000, 2.000],  loss: 242.459502, mean_q: 83.996969, mean_eps: 0.100000\n"," 1 : 11  \n","--->:847961:<---\n","  17649/500000: episode: 648, duration: 0.473s, episode steps:   4, steps per second:   8, episode reward: -53.000, mean reward: -13.250 [-25.000, 22.000], mean action: 0.750 [0.000, 1.000],  loss: 369.601501, mean_q: 78.421736, mean_eps: 0.100000\n"," 1 : 4  \n","--->:849268:<---\n","  17692/500000: episode: 649, duration: 3.807s, episode steps:  43, steps per second:  11, episode reward: 381.000, mean reward:  8.860 [-25.000, 22.000], mean action: 0.977 [0.000, 2.000],  loss: 231.298807, mean_q: 82.915506, mean_eps: 0.100000\n"," 1 : 28  \n","--->:850571:<---\n","  17712/500000: episode: 650, duration: 1.829s, episode steps:  20, steps per second:  11, episode reward: 54.000, mean reward:  2.700 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 217.820963, mean_q: 85.094738, mean_eps: 0.100000\n"," 1 : 11  \n","--->:851873:<---\n","  17723/500000: episode: 651, duration: 1.025s, episode steps:  11, steps per second:  11, episode reward: 151.000, mean reward: 13.727 [-25.000, 22.000], mean action: 0.636 [0.000, 2.000],  loss: 231.351010, mean_q: 83.745557, mean_eps: 0.100000\n"," 1 : 9  \n","--->:853178:<---\n","  17758/500000: episode: 652, duration: 3.093s, episode steps:  35, steps per second:  11, episode reward: 73.000, mean reward:  2.086 [-25.000, 22.000], mean action: 1.343 [0.000, 2.000],  loss: 254.329803, mean_q: 82.948076, mean_eps: 0.100000\n"," 1 : 14  \n","--->:854485:<---\n","  17772/500000: episode: 653, duration: 1.362s, episode steps:  14, steps per second:  10, episode reward: 145.000, mean reward: 10.357 [-25.000, 22.000], mean action: 0.643 [0.000, 2.000],  loss: 223.041028, mean_q: 81.510227, mean_eps: 0.100000\n"," 1 : 13  \n","--->:855795:<---\n","  17797/500000: episode: 654, duration: 2.286s, episode steps:  25, steps per second:  11, episode reward: 13.000, mean reward:  0.520 [-25.000, 22.000], mean action: 1.640 [0.000, 2.000],  loss: 267.390881, mean_q: 84.292804, mean_eps: 0.100000\n"," 1 : 7  \n","--->:857102:<---\n","  17809/500000: episode: 655, duration: 1.149s, episode steps:  12, steps per second:  10, episode reward: -31.000, mean reward: -2.583 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 268.389484, mean_q: 82.794395, mean_eps: 0.100000\n"," 1 : 5  \n","--->:858403:<---\n","  17828/500000: episode: 656, duration: 1.725s, episode steps:  19, steps per second:  11, episode reward: 57.000, mean reward:  3.000 [-25.000, 22.000], mean action: 1.316 [0.000, 2.000],  loss: 250.436835, mean_q: 82.932453, mean_eps: 0.100000\n"," 1 : 9  \n","--->:859705:<---\n","  17842/500000: episode: 657, duration: 1.275s, episode steps:  14, steps per second:  11, episode reward: 82.000, mean reward:  5.857 [-25.000, 22.000], mean action: 1.357 [0.000, 2.000],  loss: 287.875484, mean_q: 82.798186, mean_eps: 0.100000\n"," 1 : 8  \n","--->:861009:<---\n","  17856/500000: episode: 658, duration: 1.316s, episode steps:  14, steps per second:  11, episode reward: 110.000, mean reward:  7.857 [ 0.000, 22.000], mean action: 1.357 [0.000, 2.000],  loss: 226.325653, mean_q: 82.521954, mean_eps: 0.100000\n"," 1 : 5  \n","--->:862316:<---\n","  17870/500000: episode: 659, duration: 1.329s, episode steps:  14, steps per second:  11, episode reward: 107.000, mean reward:  7.643 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 225.021383, mean_q: 82.655176, mean_eps: 0.100000\n"," 1 : 7  \n","--->:863617:<---\n","  17885/500000: episode: 660, duration: 1.370s, episode steps:  15, steps per second:  11, episode reward: 35.000, mean reward:  2.333 [-25.000, 22.000], mean action: 1.267 [0.000, 2.000],  loss: 234.626323, mean_q: 83.818124, mean_eps: 0.100000\n"," 1 : 8  \n","--->:864927:<---\n","  17917/500000: episode: 661, duration: 2.805s, episode steps:  32, steps per second:  11, episode reward: -15.000, mean reward: -0.469 [-25.000, 22.000], mean action: 1.562 [0.000, 2.000],  loss: 252.712181, mean_q: 84.483911, mean_eps: 0.100000\n"," 1 : 10  \n","--->:866233:<---\n","  17937/500000: episode: 662, duration: 1.845s, episode steps:  20, steps per second:  11, episode reward: 164.000, mean reward:  8.200 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 211.960181, mean_q: 83.572728, mean_eps: 0.100000\n"," 1 : 16  \n","--->:867542:<---\n","  17950/500000: episode: 663, duration: 1.259s, episode steps:  13, steps per second:  10, episode reward:  4.000, mean reward:  0.308 [-25.000, 22.000], mean action: 0.462 [0.000, 1.000],  loss: 271.463028, mean_q: 83.272653, mean_eps: 0.100000\n"," 1 : 13  \n","--->:868843:<---\n","  17982/500000: episode: 664, duration: 2.818s, episode steps:  32, steps per second:  11, episode reward: 337.000, mean reward: 10.531 [-25.000, 22.000], mean action: 0.906 [0.000, 2.000],  loss: 234.916917, mean_q: 83.769552, mean_eps: 0.100000\n"," 1 : 26  \n","--->:870144:<---\n","  18046/500000: episode: 665, duration: 5.575s, episode steps:  64, steps per second:  11, episode reward: 514.000, mean reward:  8.031 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 213.634993, mean_q: 84.068663, mean_eps: 0.100000\n"," 1 : 49  \n","--->:871446:<---\n","  18083/500000: episode: 666, duration: 3.264s, episode steps:  37, steps per second:  11, episode reward: 271.000, mean reward:  7.324 [-25.000, 22.000], mean action: 1.081 [0.000, 2.000],  loss: 250.319197, mean_q: 83.710361, mean_eps: 0.100000\n"," 1 : 23  \n","--->:872756:<---\n","  18120/500000: episode: 667, duration: 3.310s, episode steps:  37, steps per second:  11, episode reward: 174.000, mean reward:  4.703 [-25.000, 22.000], mean action: 0.838 [0.000, 2.000],  loss: 233.502005, mean_q: 83.833695, mean_eps: 0.100000\n"," 1 : 25  \n","--->:874063:<---\n","  18157/500000: episode: 668, duration: 3.218s, episode steps:  37, steps per second:  11, episode reward: 17.000, mean reward:  0.459 [-25.000, 22.000], mean action: 1.108 [0.000, 2.000],  loss: 245.300818, mean_q: 82.758995, mean_eps: 0.100000\n"," 1 : 20  \n","--->:875372:<---\n","  18175/500000: episode: 669, duration: 1.608s, episode steps:  18, steps per second:  11, episode reward: 48.000, mean reward:  2.667 [-25.000, 22.000], mean action: 0.722 [0.000, 2.000],  loss: 260.874635, mean_q: 81.404427, mean_eps: 0.100000\n"," 1 : 15  \n","--->:876679:<---\n","  18188/500000: episode: 670, duration: 1.193s, episode steps:  13, steps per second:  11, episode reward: 54.000, mean reward:  4.154 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 230.860717, mean_q: 82.932697, mean_eps: 0.100000\n"," 1 : 11  \n","--->:877980:<---\n","  18214/500000: episode: 671, duration: 2.259s, episode steps:  26, steps per second:  12, episode reward: -59.000, mean reward: -2.269 [-25.000, 22.000], mean action: 1.538 [0.000, 2.000],  loss: 245.765803, mean_q: 84.013330, mean_eps: 0.100000\n"," 1 : 8  \n","--->:879289:<---\n","  18236/500000: episode: 672, duration: 1.932s, episode steps:  22, steps per second:  11, episode reward: 54.000, mean reward:  2.455 [-25.000, 22.000], mean action: 1.409 [0.000, 2.000],  loss: 260.213918, mean_q: 82.455902, mean_eps: 0.100000\n"," 1 : 11  \n","--->:880599:<---\n","  18252/500000: episode: 673, duration: 1.446s, episode steps:  16, steps per second:  11, episode reward: 79.000, mean reward:  4.938 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 191.406433, mean_q: 83.333822, mean_eps: 0.100000\n"," 1 : 10  \n","--->:881906:<---\n","  18263/500000: episode: 674, duration: 1.044s, episode steps:  11, steps per second:  11, episode reward: 35.000, mean reward:  3.182 [-25.000, 22.000], mean action: 0.636 [0.000, 2.000],  loss: 217.960538, mean_q: 81.882463, mean_eps: 0.100000\n"," 1 : 8  \n","--->:883216:<---\n","  18277/500000: episode: 675, duration: 1.295s, episode steps:  14, steps per second:  11, episode reward: -37.000, mean reward: -2.643 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 275.140416, mean_q: 82.777715, mean_eps: 0.100000\n"," 1 : 9  \n","--->:884524:<---\n","  18309/500000: episode: 676, duration: 2.801s, episode steps:  32, steps per second:  11, episode reward: 38.000, mean reward:  1.188 [-25.000, 22.000], mean action: 1.750 [0.000, 2.000],  loss: 218.415928, mean_q: 83.590158, mean_eps: 0.100000\n"," 1 : 6  \n","--->:885833:<---\n","  18324/500000: episode: 677, duration: 1.397s, episode steps:  15, steps per second:  11, episode reward: 76.000, mean reward:  5.067 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 214.014876, mean_q: 80.612008, mean_eps: 0.100000\n"," 1 : 12  \n","--->:887136:<---\n","  18353/500000: episode: 678, duration: 2.572s, episode steps:  29, steps per second:  11, episode reward: 101.000, mean reward:  3.483 [-25.000, 22.000], mean action: 1.448 [0.000, 2.000],  loss: 241.810073, mean_q: 85.155251, mean_eps: 0.100000\n"," 1 : 11  \n","--->:888439:<---\n","  18364/500000: episode: 679, duration: 1.072s, episode steps:  11, steps per second:  10, episode reward: 104.000, mean reward:  9.455 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 244.314090, mean_q: 81.693916, mean_eps: 0.100000\n"," 1 : 9  \n","--->:889746:<---\n","  18382/500000: episode: 680, duration: 1.628s, episode steps:  18, steps per second:  11, episode reward: 41.000, mean reward:  2.278 [-25.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 239.450886, mean_q: 82.849752, mean_eps: 0.100000\n"," 1 : 4  \n","--->:891055:<---\n","  18393/500000: episode: 681, duration: 1.040s, episode steps:  11, steps per second:  11, episode reward: 154.000, mean reward: 14.000 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 213.340427, mean_q: 83.155133, mean_eps: 0.100000\n"," 1 : 7  \n","--->:892365:<---\n","  18411/500000: episode: 682, duration: 1.674s, episode steps:  18, steps per second:  11, episode reward: 60.000, mean reward:  3.333 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 198.867400, mean_q: 84.715998, mean_eps: 0.100000\n"," 1 : 7  \n","--->:893668:<---\n","  18415/500000: episode: 683, duration: 0.469s, episode steps:   4, steps per second:   9, episode reward: -6.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.750 [0.000, 1.000],  loss: 174.115967, mean_q: 82.401207, mean_eps: 0.100000\n"," 1 : 4  \n","--->:894975:<---\n","  18426/500000: episode: 684, duration: 1.059s, episode steps:  11, steps per second:  10, episode reward: 10.000, mean reward:  0.909 [-25.000, 22.000], mean action: 0.636 [0.000, 2.000],  loss: 229.922935, mean_q: 80.852973, mean_eps: 0.100000\n"," 1 : 9  \n","--->:896285:<---\n","  18462/500000: episode: 685, duration: 3.167s, episode steps:  36, steps per second:  11, episode reward: 73.000, mean reward:  2.028 [-25.000, 22.000], mean action: 1.417 [0.000, 2.000],  loss: 214.948904, mean_q: 84.065625, mean_eps: 0.100000\n"," 1 : 14  \n","--->:897592:<---\n","  18477/500000: episode: 686, duration: 1.375s, episode steps:  15, steps per second:  11, episode reward: 29.000, mean reward:  1.933 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 235.481630, mean_q: 84.690908, mean_eps: 0.100000\n"," 1 : 12  \n","--->:898893:<---\n","  18501/500000: episode: 687, duration: 2.111s, episode steps:  24, steps per second:  11, episode reward: 104.000, mean reward:  4.333 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 259.495124, mean_q: 82.988406, mean_eps: 0.100000\n"," 1 : 9  \n","--->:900197:<---\n","  18517/500000: episode: 688, duration: 1.450s, episode steps:  16, steps per second:  11, episode reward: 214.000, mean reward: 13.375 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 264.361066, mean_q: 84.592838, mean_eps: 0.100000\n"," 1 : 14  \n","--->:901505:<---\n","  18532/500000: episode: 689, duration: 1.416s, episode steps:  15, steps per second:  11, episode reward: 126.000, mean reward:  8.400 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 231.680591, mean_q: 84.993296, mean_eps: 0.100000\n"," 1 : 10  \n","--->:902810:<---\n","  18550/500000: episode: 690, duration: 1.631s, episode steps:  18, steps per second:  11, episode reward: 73.000, mean reward:  4.056 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 225.187754, mean_q: 85.211421, mean_eps: 0.100000\n"," 1 : 14  \n","--->:904112:<---\n","  18587/500000: episode: 691, duration: 3.153s, episode steps:  37, steps per second:  12, episode reward: 277.000, mean reward:  7.486 [-25.000, 22.000], mean action: 1.216 [0.000, 2.000],  loss: 256.961616, mean_q: 81.909413, mean_eps: 0.100000\n"," 1 : 19  \n","--->:905415:<---\n","  18608/500000: episode: 692, duration: 1.841s, episode steps:  21, steps per second:  11, episode reward: -18.000, mean reward: -0.857 [-25.000, 22.000], mean action: 1.238 [0.000, 2.000],  loss: 228.942235, mean_q: 84.663103, mean_eps: 0.100000\n"," 1 : 12  \n","--->:906724:<---\n","  18633/500000: episode: 693, duration: 2.176s, episode steps:  25, steps per second:  11, episode reward: 139.000, mean reward:  5.560 [-25.000, 22.000], mean action: 1.240 [0.000, 2.000],  loss: 246.963427, mean_q: 83.393018, mean_eps: 0.100000\n"," 1 : 17  \n","--->:908027:<---\n","  18646/500000: episode: 694, duration: 1.216s, episode steps:  13, steps per second:  11, episode reward: 54.000, mean reward:  4.154 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 260.980978, mean_q: 80.904993, mean_eps: 0.100000\n"," 1 : 11  \n","--->:909331:<---\n","  18697/500000: episode: 695, duration: 4.393s, episode steps:  51, steps per second:  12, episode reward: 347.000, mean reward:  6.804 [-25.000, 22.000], mean action: 0.922 [0.000, 2.000],  loss: 249.574945, mean_q: 83.325660, mean_eps: 0.100000\n"," 1 : 35  \n","--->:910638:<---\n","  18732/500000: episode: 696, duration: 3.073s, episode steps:  35, steps per second:  11, episode reward: 52.000, mean reward:  1.486 [-25.000, 22.000], mean action: 0.829 [0.000, 2.000],  loss: 200.873599, mean_q: 83.628784, mean_eps: 0.100000\n"," 1 : 28  \n","--->:911948:<---\n","  18793/500000: episode: 697, duration: 5.400s, episode steps:  61, steps per second:  11, episode reward: 96.000, mean reward:  1.574 [-25.000, 22.000], mean action: 1.262 [0.000, 2.000],  loss: 242.055527, mean_q: 83.155410, mean_eps: 0.100000\n"," 1 : 30  \n","--->:913253:<---\n","  18853/500000: episode: 698, duration: 5.166s, episode steps:  60, steps per second:  12, episode reward: 262.000, mean reward:  4.367 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 262.401582, mean_q: 83.705919, mean_eps: 0.100000\n"," 1 : 29  \n","--->:914562:<---\n","  18872/500000: episode: 699, duration: 1.764s, episode steps:  19, steps per second:  11, episode reward: 233.000, mean reward: 12.263 [-25.000, 22.000], mean action: 0.895 [0.000, 2.000],  loss: 216.344145, mean_q: 84.475590, mean_eps: 0.100000\n"," 1 : 17  \n","--->:915866:<---\n","  18911/500000: episode: 700, duration: 3.410s, episode steps:  39, steps per second:  11, episode reward: 340.000, mean reward:  8.718 [-25.000, 22.000], mean action: 1.026 [0.000, 2.000],  loss: 239.997066, mean_q: 82.769204, mean_eps: 0.100000\n"," 1 : 24  \n","--->:917169:<---\n","  18939/500000: episode: 701, duration: 2.500s, episode steps:  28, steps per second:  11, episode reward: 202.000, mean reward:  7.214 [-25.000, 22.000], mean action: 0.821 [0.000, 2.000],  loss: 248.038246, mean_q: 82.466998, mean_eps: 0.100000\n"," 1 : 22  \n","--->:918479:<---\n","  18982/500000: episode: 702, duration: 3.756s, episode steps:  43, steps per second:  11, episode reward: 118.000, mean reward:  2.744 [-25.000, 22.000], mean action: 1.023 [0.000, 2.000],  loss: 237.712340, mean_q: 82.361022, mean_eps: 0.100000\n"," 1 : 31  \n","--->:919788:<---\n","  19015/500000: episode: 703, duration: 2.943s, episode steps:  33, steps per second:  11, episode reward: 126.000, mean reward:  3.818 [-25.000, 22.000], mean action: 1.606 [0.000, 2.000],  loss: 254.285404, mean_q: 83.874850, mean_eps: 0.100000\n"," 1 : 10  \n","--->:921094:<---\n","  19068/500000: episode: 704, duration: 4.589s, episode steps:  53, steps per second:  12, episode reward: -102.000, mean reward: -1.925 [-25.000, 22.000], mean action: 1.377 [0.000, 2.000],  loss: 234.246032, mean_q: 84.037636, mean_eps: 0.100000\n"," 1 : 21  \n","--->:922400:<---\n","  19127/500000: episode: 705, duration: 5.109s, episode steps:  59, steps per second:  12, episode reward: 144.000, mean reward:  2.441 [-25.000, 22.000], mean action: 0.610 [0.000, 2.000],  loss: 271.080725, mean_q: 83.656916, mean_eps: 0.100000\n"," 1 : 45  \n","--->:923708:<---\n","  19190/500000: episode: 706, duration: 5.461s, episode steps:  63, steps per second:  12, episode reward: -39.000, mean reward: -0.619 [-25.000, 22.000], mean action: 1.397 [0.000, 2.000],  loss: 244.366178, mean_q: 82.012866, mean_eps: 0.100000\n"," 1 : 26  \n","--->:925011:<---\n","  19227/500000: episode: 707, duration: 3.312s, episode steps:  37, steps per second:  11, episode reward: 92.000, mean reward:  2.486 [-25.000, 22.000], mean action: 1.216 [0.000, 2.000],  loss: 235.556372, mean_q: 82.757486, mean_eps: 0.100000\n"," 1 : 17  \n","--->:926314:<---\n","  19295/500000: episode: 708, duration: 5.954s, episode steps:  68, steps per second:  11, episode reward: 451.000, mean reward:  6.632 [-25.000, 22.000], mean action: 0.956 [0.000, 2.000],  loss: 219.900297, mean_q: 82.732496, mean_eps: 0.100000\n"," 1 : 44  \n","--->:927622:<---\n","  19390/500000: episode: 709, duration: 8.545s, episode steps:  95, steps per second:  11, episode reward: 154.000, mean reward:  1.621 [-25.000, 22.000], mean action: 1.042 [0.000, 2.000],  loss: 246.377921, mean_q: 84.188413, mean_eps: 0.100000\n"," 1 : 54  \n","--->:928923:<---\n","  19783/500000: episode: 710, duration: 34.394s, episode steps: 393, steps per second:  11, episode reward: 1469.000, mean reward:  3.738 [-25.000, 22.000], mean action: 0.888 [0.000, 2.000],  loss: 246.342723, mean_q: 83.120029, mean_eps: 0.100000\n"," 1 : 274  \n","--->:930233:<---\n","  19970/500000: episode: 711, duration: 16.187s, episode steps: 187, steps per second:  12, episode reward: 538.000, mean reward:  2.877 [-25.000, 22.000], mean action: 0.968 [0.000, 2.000],  loss: 233.695958, mean_q: 83.492002, mean_eps: 0.100000\n"," 1 : 127  \n","--->:931541:<---\n","  20142/500000: episode: 712, duration: 15.400s, episode steps: 172, steps per second:  11, episode reward: 1046.000, mean reward:  6.081 [-25.000, 22.000], mean action: 0.773 [0.000, 2.000],  loss: 236.760525, mean_q: 83.601899, mean_eps: 0.100000\n"," 1 : 133  \n","--->:932844:<---\n","  20410/500000: episode: 713, duration: 22.754s, episode steps: 268, steps per second:  12, episode reward: 914.000, mean reward:  3.410 [-25.000, 22.000], mean action: 0.970 [0.000, 2.000],  loss: 241.918313, mean_q: 82.312005, mean_eps: 0.100000\n"," 1 : 174  \n","--->:934148:<---\n","  20621/500000: episode: 714, duration: 17.985s, episode steps: 211, steps per second:  12, episode reward: 1114.000, mean reward:  5.280 [-25.000, 22.000], mean action: 0.777 [0.000, 2.000],  loss: 237.009861, mean_q: 82.224903, mean_eps: 0.100000\n"," 1 : 166  \n","--->:935456:<---\n","  20751/500000: episode: 715, duration: 11.008s, episode steps: 130, steps per second:  12, episode reward: 542.000, mean reward:  4.169 [-25.000, 22.000], mean action: 0.977 [0.000, 2.000],  loss: 228.817893, mean_q: 82.639819, mean_eps: 0.100000\n"," 1 : 93  \n","--->:936761:<---\n","  20866/500000: episode: 716, duration: 9.772s, episode steps: 115, steps per second:  12, episode reward: 597.000, mean reward:  5.191 [-25.000, 22.000], mean action: 1.113 [0.000, 2.000],  loss: 241.889620, mean_q: 81.996389, mean_eps: 0.100000\n"," 1 : 72  \n","--->:938066:<---\n","  20955/500000: episode: 717, duration: 7.652s, episode steps:  89, steps per second:  12, episode reward: 641.000, mean reward:  7.202 [-25.000, 22.000], mean action: 0.955 [0.000, 2.000],  loss: 238.246606, mean_q: 83.067632, mean_eps: 0.100000\n"," 1 : 74  \n","--->:939375:<---\n","  21033/500000: episode: 718, duration: 6.593s, episode steps:  78, steps per second:  12, episode reward: 339.000, mean reward:  4.346 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 239.794817, mean_q: 82.430889, mean_eps: 0.100000\n"," 1 : 56  \n","--->:940679:<---\n","  21109/500000: episode: 719, duration: 6.457s, episode steps:  76, steps per second:  12, episode reward: 493.000, mean reward:  6.487 [-25.000, 22.000], mean action: 0.671 [0.000, 2.000],  loss: 234.108201, mean_q: 82.630760, mean_eps: 0.100000\n"," 1 : 63  \n","--->:941989:<---\n","  21159/500000: episode: 720, duration: 4.305s, episode steps:  50, steps per second:  12, episode reward: 184.000, mean reward:  3.680 [-25.000, 22.000], mean action: 0.960 [0.000, 2.000],  loss: 242.513953, mean_q: 82.866852, mean_eps: 0.100000\n"," 1 : 34  \n","--->:943292:<---\n","  21228/500000: episode: 721, duration: 5.851s, episode steps:  69, steps per second:  12, episode reward: 267.000, mean reward:  3.870 [-25.000, 22.000], mean action: 0.725 [0.000, 2.000],  loss: 228.024797, mean_q: 81.589655, mean_eps: 0.100000\n"," 1 : 57  \n","--->:944598:<---\n","  21332/500000: episode: 722, duration: 9.048s, episode steps: 104, steps per second:  11, episode reward: 757.000, mean reward:  7.279 [-25.000, 22.000], mean action: 0.894 [0.000, 2.000],  loss: 229.222171, mean_q: 82.257833, mean_eps: 0.100000\n"," 1 : 75  \n","--->:945899:<---\n","  21439/500000: episode: 723, duration: 8.844s, episode steps: 107, steps per second:  12, episode reward: 416.000, mean reward:  3.888 [-25.000, 22.000], mean action: 0.935 [0.000, 2.000],  loss: 238.823309, mean_q: 82.360801, mean_eps: 0.100000\n"," 1 : 83  \n","--->:947209:<---\n","  21541/500000: episode: 724, duration: 8.503s, episode steps: 102, steps per second:  12, episode reward: 310.000, mean reward:  3.039 [-25.000, 22.000], mean action: 1.392 [0.000, 2.000],  loss: 227.974734, mean_q: 82.420116, mean_eps: 0.100000\n"," 1 : 44  \n","--->:948519:<---\n","  21584/500000: episode: 725, duration: 3.712s, episode steps:  43, steps per second:  12, episode reward: 306.000, mean reward:  7.116 [-25.000, 22.000], mean action: 1.116 [0.000, 2.000],  loss: 252.393024, mean_q: 80.933121, mean_eps: 0.100000\n"," 1 : 31  \n","--->:949827:<---\n","  21678/500000: episode: 726, duration: 8.097s, episode steps:  94, steps per second:  12, episode reward: 321.000, mean reward:  3.415 [-25.000, 22.000], mean action: 0.638 [0.000, 2.000],  loss: 230.765517, mean_q: 82.004440, mean_eps: 0.100000\n"," 1 : 68  \n","--->:951134:<---\n","  21788/500000: episode: 727, duration: 9.186s, episode steps: 110, steps per second:  12, episode reward: 459.000, mean reward:  4.173 [-25.000, 22.000], mean action: 0.918 [0.000, 2.000],  loss: 241.147222, mean_q: 83.528335, mean_eps: 0.100000\n"," 1 : 70  \n","--->:952441:<---\n","  21978/500000: episode: 728, duration: 15.911s, episode steps: 190, steps per second:  12, episode reward: 580.000, mean reward:  3.053 [-25.000, 22.000], mean action: 0.832 [0.000, 2.000],  loss: 235.077405, mean_q: 82.511766, mean_eps: 0.100000\n"," 1 : 146  \n","--->:953742:<---\n","  22117/500000: episode: 729, duration: 11.717s, episode steps: 139, steps per second:  12, episode reward: 719.000, mean reward:  5.173 [-25.000, 22.000], mean action: 0.763 [0.000, 2.000],  loss: 235.829319, mean_q: 82.007797, mean_eps: 0.100000\n"," 1 : 116  \n","--->:955046:<---\n","  22212/500000: episode: 730, duration: 8.005s, episode steps:  95, steps per second:  12, episode reward: 405.000, mean reward:  4.263 [-25.000, 22.000], mean action: 1.242 [0.000, 2.000],  loss: 231.731494, mean_q: 82.452375, mean_eps: 0.100000\n"," 1 : 59  \n","--->:956355:<---\n","  22311/500000: episode: 731, duration: 8.354s, episode steps:  99, steps per second:  12, episode reward: 172.000, mean reward:  1.737 [-25.000, 22.000], mean action: 1.354 [0.000, 2.000],  loss: 240.736023, mean_q: 82.561844, mean_eps: 0.100000\n"," 1 : 42  \n","--->:957658:<---\n","  22409/500000: episode: 732, duration: 8.348s, episode steps:  98, steps per second:  12, episode reward: 694.000, mean reward:  7.082 [-25.000, 22.000], mean action: 0.959 [0.000, 2.000],  loss: 241.158822, mean_q: 82.357581, mean_eps: 0.100000\n"," 1 : 70  \n","--->:958966:<---\n","  22444/500000: episode: 733, duration: 3.028s, episode steps:  35, steps per second:  12, episode reward: 52.000, mean reward:  1.486 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 215.639859, mean_q: 83.624520, mean_eps: 0.100000\n"," 1 : 28  \n","--->:960270:<---\n","  22499/500000: episode: 734, duration: 4.723s, episode steps:  55, steps per second:  12, episode reward: 155.000, mean reward:  2.818 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 238.786894, mean_q: 82.571018, mean_eps: 0.100000\n"," 1 : 22  \n","--->:961580:<---\n","  22523/500000: episode: 735, duration: 2.165s, episode steps:  24, steps per second:  11, episode reward: 139.000, mean reward:  5.792 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 247.988994, mean_q: 82.381346, mean_eps: 0.100000\n"," 1 : 17  \n","--->:962881:<---\n","  22545/500000: episode: 736, duration: 2.015s, episode steps:  22, steps per second:  11, episode reward: 23.000, mean reward:  1.045 [-25.000, 22.000], mean action: 1.045 [0.000, 2.000],  loss: 213.197954, mean_q: 81.914367, mean_eps: 0.100000\n"," 1 : 16  \n","--->:964185:<---\n","  22571/500000: episode: 737, duration: 2.301s, episode steps:  26, steps per second:  11, episode reward: 161.000, mean reward:  6.192 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 225.532740, mean_q: 84.980046, mean_eps: 0.100000\n"," 1 : 18  \n","--->:965492:<---\n","  22611/500000: episode: 738, duration: 3.482s, episode steps:  40, steps per second:  11, episode reward: 237.000, mean reward:  5.925 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 241.341367, mean_q: 82.477604, mean_eps: 0.100000\n"," 1 : 30  \n","--->:966793:<---\n","  22654/500000: episode: 739, duration: 3.709s, episode steps:  43, steps per second:  12, episode reward: 33.000, mean reward:  0.767 [-25.000, 22.000], mean action: 1.186 [0.000, 2.000],  loss: 232.856538, mean_q: 83.084431, mean_eps: 0.100000\n"," 1 : 25  \n","--->:968103:<---\n","  22735/500000: episode: 740, duration: 6.929s, episode steps:  81, steps per second:  12, episode reward: 42.000, mean reward:  0.519 [-25.000, 22.000], mean action: 1.593 [0.000, 2.000],  loss: 238.465262, mean_q: 81.078508, mean_eps: 0.100000\n"," 1 : 19  \n","--->:969413:<---\n","  22794/500000: episode: 741, duration: 4.975s, episode steps:  59, steps per second:  12, episode reward: 347.000, mean reward:  5.881 [-25.000, 22.000], mean action: 1.203 [0.000, 2.000],  loss: 244.492652, mean_q: 83.208492, mean_eps: 0.100000\n"," 1 : 35  \n","--->:970718:<---\n","  22815/500000: episode: 742, duration: 1.860s, episode steps:  21, steps per second:  11, episode reward: 142.000, mean reward:  6.762 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 211.554471, mean_q: 84.687073, mean_eps: 0.100000\n"," 1 : 15  \n","--->:972021:<---\n","  22884/500000: episode: 743, duration: 5.843s, episode steps:  69, steps per second:  12, episode reward: 235.000, mean reward:  3.406 [-25.000, 22.000], mean action: 0.913 [0.000, 2.000],  loss: 241.956663, mean_q: 82.281866, mean_eps: 0.100000\n"," 1 : 47  \n","--->:973330:<---\n","  23067/500000: episode: 744, duration: 15.281s, episode steps: 183, steps per second:  12, episode reward: 674.000, mean reward:  3.683 [-25.000, 22.000], mean action: 1.109 [0.000, 2.000],  loss: 231.239563, mean_q: 82.442205, mean_eps: 0.100000\n"," 1 : 99  \n","--->:974631:<---\n","  23212/500000: episode: 745, duration: 12.204s, episode steps: 145, steps per second:  12, episode reward: 606.000, mean reward:  4.179 [-25.000, 22.000], mean action: 0.676 [0.000, 2.000],  loss: 240.676010, mean_q: 82.477852, mean_eps: 0.100000\n"," 1 : 113  \n","--->:975939:<---\n","  23292/500000: episode: 746, duration: 6.791s, episode steps:  80, steps per second:  12, episode reward: 609.000, mean reward:  7.612 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 228.547122, mean_q: 83.170303, mean_eps: 0.100000\n"," 1 : 64  \n","--->:977244:<---\n","  23427/500000: episode: 747, duration: 11.237s, episode steps: 135, steps per second:  12, episode reward: 774.000, mean reward:  5.733 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 236.525674, mean_q: 82.933139, mean_eps: 0.100000\n"," 1 : 95  \n","--->:978553:<---\n","  23624/500000: episode: 748, duration: 16.489s, episode steps: 197, steps per second:  12, episode reward: 932.000, mean reward:  4.731 [-25.000, 22.000], mean action: 1.188 [0.000, 2.000],  loss: 234.103841, mean_q: 83.468452, mean_eps: 0.100000\n"," 1 : 115  \n","--->:979860:<---\n","  23701/500000: episode: 749, duration: 6.541s, episode steps:  77, steps per second:  12, episode reward: 847.000, mean reward: 11.000 [-25.000, 22.000], mean action: 0.870 [0.000, 2.000],  loss: 238.343138, mean_q: 82.832214, mean_eps: 0.100000\n"," 1 : 62  \n","--->:981170:<---\n","  23824/500000: episode: 750, duration: 10.329s, episode steps: 123, steps per second:  12, episode reward: 489.000, mean reward:  3.976 [-25.000, 22.000], mean action: 0.683 [0.000, 2.000],  loss: 224.340679, mean_q: 82.547271, mean_eps: 0.100000\n"," 1 : 97  \n","--->:982476:<---\n","  23929/500000: episode: 751, duration: 8.968s, episode steps: 105, steps per second:  12, episode reward: 516.000, mean reward:  4.914 [-25.000, 22.000], mean action: 0.971 [0.000, 2.000],  loss: 241.182049, mean_q: 82.493867, mean_eps: 0.100000\n"," 1 : 79  \n","--->:983777:<---\n","  24092/500000: episode: 752, duration: 14.184s, episode steps: 163, steps per second:  11, episode reward: 713.000, mean reward:  4.374 [-25.000, 22.000], mean action: 1.037 [0.000, 2.000],  loss: 241.279942, mean_q: 81.616372, mean_eps: 0.100000\n"," 1 : 120  \n","--->:985079:<---\n","  24181/500000: episode: 753, duration: 7.784s, episode steps:  89, steps per second:  11, episode reward: 264.000, mean reward:  2.966 [-25.000, 22.000], mean action: 1.034 [0.000, 2.000],  loss: 237.071446, mean_q: 82.282367, mean_eps: 0.100000\n"," 1 : 59  \n","--->:986388:<---\n","  24339/500000: episode: 754, duration: 13.728s, episode steps: 158, steps per second:  12, episode reward: 825.000, mean reward:  5.222 [-25.000, 22.000], mean action: 0.911 [0.000, 2.000],  loss: 238.617548, mean_q: 82.927371, mean_eps: 0.100000\n"," 1 : 108  \n","--->:987693:<---\n","  24407/500000: episode: 755, duration: 5.804s, episode steps:  68, steps per second:  12, episode reward: 474.000, mean reward:  6.971 [-25.000, 22.000], mean action: 0.838 [0.000, 2.000],  loss: 223.441404, mean_q: 82.784591, mean_eps: 0.100000\n"," 1 : 60  \n","--->:988994:<---\n","  24497/500000: episode: 756, duration: 7.723s, episode steps:  90, steps per second:  12, episode reward: 205.000, mean reward:  2.278 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 227.866676, mean_q: 82.695849, mean_eps: 0.100000\n"," 1 : 67  \n","--->:990301:<---\n","  24598/500000: episode: 757, duration: 8.599s, episode steps: 101, steps per second:  12, episode reward: 244.000, mean reward:  2.416 [-25.000, 22.000], mean action: 1.426 [0.000, 2.000],  loss: 235.766336, mean_q: 82.443365, mean_eps: 0.100000\n"," 1 : 41  \n","--->:991603:<---\n","  24664/500000: episode: 758, duration: 5.632s, episode steps:  66, steps per second:  12, episode reward: 300.000, mean reward:  4.545 [-25.000, 22.000], mean action: 1.273 [0.000, 2.000],  loss: 228.692979, mean_q: 82.032097, mean_eps: 0.100000\n"," 1 : 35  \n","--->:992912:<---\n","  24724/500000: episode: 759, duration: 5.126s, episode steps:  60, steps per second:  12, episode reward: 375.000, mean reward:  6.250 [-25.000, 22.000], mean action: 1.117 [0.000, 2.000],  loss: 250.097056, mean_q: 81.264374, mean_eps: 0.100000\n"," 1 : 32  \n","--->:994215:<---\n","  24746/500000: episode: 760, duration: 1.981s, episode steps:  22, steps per second:  11, episode reward: 158.000, mean reward:  7.182 [-25.000, 22.000], mean action: 0.636 [0.000, 2.000],  loss: 205.927632, mean_q: 83.360998, mean_eps: 0.100000\n"," 1 : 20  \n","--->:995519:<---\n","  24795/500000: episode: 761, duration: 4.169s, episode steps:  49, steps per second:  12, episode reward: -49.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.531 [0.000, 2.000],  loss: 243.979548, mean_q: 82.140779, mean_eps: 0.100000\n"," 1 : 17  \n","--->:996823:<---\n","  24821/500000: episode: 762, duration: 2.297s, episode steps:  26, steps per second:  11, episode reward: 76.000, mean reward:  2.923 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 225.365593, mean_q: 83.527353, mean_eps: 0.100000\n"," 1 : 12  \n","--->:998125:<---\n","  24828/500000: episode: 763, duration: 0.719s, episode steps:   7, steps per second:  10, episode reward: -53.000, mean reward: -7.571 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 200.360007, mean_q: 81.960666, mean_eps: 0.100000\n"," 1 : 4  \n","--->:999433:<---\n","  24853/500000: episode: 764, duration: 2.220s, episode steps:  25, steps per second:  11, episode reward: 258.000, mean reward: 10.320 [-25.000, 22.000], mean action: 0.960 [0.000, 2.000],  loss: 239.107647, mean_q: 84.506576, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1000739:<---\n","  24879/500000: episode: 765, duration: 2.260s, episode steps:  26, steps per second:  12, episode reward: 186.000, mean reward:  7.154 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 264.354520, mean_q: 81.050609, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1002043:<---\n","  24912/500000: episode: 766, duration: 2.887s, episode steps:  33, steps per second:  11, episode reward: 136.000, mean reward:  4.121 [-25.000, 22.000], mean action: 1.212 [0.000, 2.000],  loss: 218.963028, mean_q: 81.220763, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1003353:<---\n","  24987/500000: episode: 767, duration: 6.375s, episode steps:  75, steps per second:  12, episode reward: 442.000, mean reward:  5.893 [-25.000, 22.000], mean action: 0.933 [0.000, 2.000],  loss: 250.570860, mean_q: 82.970289, mean_eps: 0.100000\n"," 1 : 50  \n","--->:1004659:<---\n","  25078/500000: episode: 768, duration: 7.694s, episode steps:  91, steps per second:  12, episode reward: 166.000, mean reward:  1.824 [-25.000, 22.000], mean action: 1.352 [0.000, 2.000],  loss: 247.984915, mean_q: 81.708486, mean_eps: 0.100000\n"," 1 : 46  \n","--->:1005960:<---\n","  25132/500000: episode: 769, duration: 4.568s, episode steps:  54, steps per second:  12, episode reward: 134.000, mean reward:  2.481 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 235.235037, mean_q: 82.123044, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1007264:<---\n","  25184/500000: episode: 770, duration: 4.451s, episode steps:  52, steps per second:  12, episode reward: 466.000, mean reward:  8.962 [-25.000, 22.000], mean action: 1.038 [0.000, 2.000],  loss: 258.079950, mean_q: 80.695712, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1008567:<---\n","  25235/500000: episode: 771, duration: 4.374s, episode steps:  51, steps per second:  12, episode reward: 134.000, mean reward:  2.627 [-25.000, 22.000], mean action: 0.824 [0.000, 2.000],  loss: 246.116197, mean_q: 81.731501, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1009874:<---\n","  25295/500000: episode: 772, duration: 5.351s, episode steps:  60, steps per second:  11, episode reward: 567.000, mean reward:  9.450 [-25.000, 22.000], mean action: 0.783 [0.000, 2.000],  loss: 236.520171, mean_q: 81.284961, mean_eps: 0.100000\n"," 1 : 45  \n","--->:1011179:<---\n","  25350/500000: episode: 773, duration: 4.843s, episode steps:  55, steps per second:  11, episode reward: 244.000, mean reward:  4.436 [-25.000, 22.000], mean action: 0.618 [0.000, 2.000],  loss: 216.734744, mean_q: 82.899585, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1012483:<---\n","  25454/500000: episode: 774, duration: 8.890s, episode steps: 104, steps per second:  12, episode reward: 773.000, mean reward:  7.433 [-25.000, 22.000], mean action: 0.894 [0.000, 2.000],  loss: 240.037706, mean_q: 82.019620, mean_eps: 0.100000\n"," 1 : 80  \n","--->:1013789:<---\n","  25526/500000: episode: 775, duration: 6.143s, episode steps:  72, steps per second:  12, episode reward: 426.000, mean reward:  5.917 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 245.457335, mean_q: 82.773664, mean_eps: 0.100000\n"," 1 : 45  \n","--->:1015090:<---\n","  25597/500000: episode: 776, duration: 6.176s, episode steps:  71, steps per second:  11, episode reward: 312.000, mean reward:  4.394 [-25.000, 22.000], mean action: 1.437 [0.000, 2.000],  loss: 252.838661, mean_q: 81.507574, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1016396:<---\n","  25653/500000: episode: 777, duration: 4.802s, episode steps:  56, steps per second:  12, episode reward: 304.000, mean reward:  5.429 [-25.000, 22.000], mean action: 0.679 [0.000, 2.000],  loss: 233.586929, mean_q: 81.557706, mean_eps: 0.100000\n"," 1 : 48  \n","--->:1017700:<---\n","  25717/500000: episode: 778, duration: 5.502s, episode steps:  64, steps per second:  12, episode reward: 215.000, mean reward:  3.359 [-25.000, 22.000], mean action: 1.297 [0.000, 2.000],  loss: 240.080283, mean_q: 82.204415, mean_eps: 0.100000\n"," 1 : 29  \n","--->:1019010:<---\n","  25746/500000: episode: 779, duration: 2.503s, episode steps:  29, steps per second:  12, episode reward: 296.000, mean reward: 10.207 [-25.000, 22.000], mean action: 1.069 [0.000, 2.000],  loss: 243.768841, mean_q: 82.159790, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1020318:<---\n","  25799/500000: episode: 780, duration: 4.504s, episode steps:  53, steps per second:  12, episode reward: 70.000, mean reward:  1.321 [-25.000, 22.000], mean action: 1.566 [0.000, 2.000],  loss: 234.521528, mean_q: 83.411122, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1021627:<---\n","  25834/500000: episode: 781, duration: 3.026s, episode steps:  35, steps per second:  12, episode reward: 95.000, mean reward:  2.714 [-25.000, 22.000], mean action: 1.314 [0.000, 2.000],  loss: 230.296162, mean_q: 82.568333, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1022928:<---\n","  25864/500000: episode: 782, duration: 2.669s, episode steps:  30, steps per second:  11, episode reward: 174.000, mean reward:  5.800 [-25.000, 22.000], mean action: 0.567 [0.000, 2.000],  loss: 273.326718, mean_q: 83.998622, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1024230:<---\n","  25914/500000: episode: 783, duration: 4.441s, episode steps:  50, steps per second:  11, episode reward: 139.000, mean reward:  2.780 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 227.661371, mean_q: 81.465000, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1025533:<---\n","  25937/500000: episode: 784, duration: 2.057s, episode steps:  23, steps per second:  11, episode reward: 145.000, mean reward:  6.304 [-25.000, 22.000], mean action: 1.304 [0.000, 2.000],  loss: 224.536198, mean_q: 81.900236, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1026835:<---\n","  25979/500000: episode: 785, duration: 3.813s, episode steps:  42, steps per second:  11, episode reward: 70.000, mean reward:  1.667 [-25.000, 22.000], mean action: 1.452 [0.000, 2.000],  loss: 211.655765, mean_q: 82.628730, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1028144:<---\n","  25991/500000: episode: 786, duration: 1.150s, episode steps:  12, steps per second:  10, episode reward: 57.000, mean reward:  4.750 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 239.780900, mean_q: 82.834564, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1029446:<---\n","  25996/500000: episode: 787, duration: 0.580s, episode steps:   5, steps per second:   9, episode reward: -53.000, mean reward: -10.600 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 223.579297, mean_q: 81.476785, mean_eps: 0.100000\n"," 1 : 4  \n","--->:1030752:<---\n","  26022/500000: episode: 788, duration: 2.383s, episode steps:  26, steps per second:  11, episode reward: 95.000, mean reward:  3.654 [-25.000, 22.000], mean action: 1.038 [0.000, 2.000],  loss: 205.591327, mean_q: 84.578058, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1032057:<---\n","  26053/500000: episode: 789, duration: 2.868s, episode steps:  31, steps per second:  11, episode reward: 51.000, mean reward:  1.645 [-25.000, 22.000], mean action: 1.452 [0.000, 2.000],  loss: 234.546442, mean_q: 81.947256, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1033364:<---\n","  26097/500000: episode: 790, duration: 3.907s, episode steps:  44, steps per second:  11, episode reward: -37.000, mean reward: -0.841 [-25.000, 22.000], mean action: 1.591 [0.000, 2.000],  loss: 250.195516, mean_q: 82.674643, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1034671:<---\n","  26126/500000: episode: 791, duration: 2.508s, episode steps:  29, steps per second:  12, episode reward: 126.000, mean reward:  4.345 [-25.000, 22.000], mean action: 1.483 [0.000, 2.000],  loss: 223.181294, mean_q: 82.759185, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1035980:<---\n","  26150/500000: episode: 792, duration: 2.111s, episode steps:  24, steps per second:  11, episode reward: 299.000, mean reward: 12.458 [-25.000, 22.000], mean action: 0.583 [0.000, 2.000],  loss: 259.438427, mean_q: 83.803536, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1037289:<---\n","  26192/500000: episode: 793, duration: 3.623s, episode steps:  42, steps per second:  12, episode reward: 284.000, mean reward:  6.762 [-25.000, 22.000], mean action: 1.071 [0.000, 2.000],  loss: 226.863637, mean_q: 84.608943, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1038591:<---\n","  26202/500000: episode: 794, duration: 1.028s, episode steps:  10, steps per second:  10, episode reward: -12.000, mean reward: -1.200 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 292.822845, mean_q: 80.761480, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1039893:<---\n","  26231/500000: episode: 795, duration: 2.687s, episode steps:  29, steps per second:  11, episode reward: -71.000, mean reward: -2.448 [-25.000, 22.000], mean action: 1.276 [0.000, 2.000],  loss: 220.926195, mean_q: 84.850061, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1041200:<---\n","  26254/500000: episode: 796, duration: 2.048s, episode steps:  23, steps per second:  11, episode reward: 95.000, mean reward:  4.130 [-25.000, 22.000], mean action: 1.087 [0.000, 2.000],  loss: 240.992693, mean_q: 82.021181, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1042507:<---\n","  26270/500000: episode: 797, duration: 1.465s, episode steps:  16, steps per second:  11, episode reward: 10.000, mean reward:  0.625 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 208.853739, mean_q: 84.963898, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1043812:<---\n","  26302/500000: episode: 798, duration: 2.789s, episode steps:  32, steps per second:  11, episode reward: 117.000, mean reward:  3.656 [-25.000, 22.000], mean action: 1.312 [0.000, 2.000],  loss: 238.100106, mean_q: 83.504482, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1045118:<---\n","  26334/500000: episode: 799, duration: 2.743s, episode steps:  32, steps per second:  12, episode reward: 48.000, mean reward:  1.500 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 255.627566, mean_q: 82.510611, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1046424:<---\n","  26384/500000: episode: 800, duration: 4.298s, episode steps:  50, steps per second:  12, episode reward: 165.000, mean reward:  3.300 [-25.000, 22.000], mean action: 0.980 [0.000, 2.000],  loss: 223.436864, mean_q: 81.355963, mean_eps: 0.100000\n"," 1 : 31  \n","--->:1047734:<---\n","  26420/500000: episode: 801, duration: 3.040s, episode steps:  36, steps per second:  12, episode reward: 136.000, mean reward:  3.778 [-25.000, 22.000], mean action: 1.306 [0.000, 2.000],  loss: 224.084821, mean_q: 82.820929, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1049042:<---\n","  26472/500000: episode: 802, duration: 4.380s, episode steps:  52, steps per second:  12, episode reward: 183.000, mean reward:  3.519 [-25.000, 22.000], mean action: 1.462 [0.000, 2.000],  loss: 246.958798, mean_q: 80.971524, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1050352:<---\n","  26521/500000: episode: 803, duration: 4.169s, episode steps:  49, steps per second:  12, episode reward: -137.000, mean reward: -2.796 [-25.000, 22.000], mean action: 1.633 [0.000, 2.000],  loss: 223.758250, mean_q: 82.764405, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1051659:<---\n","  26548/500000: episode: 804, duration: 2.323s, episode steps:  27, steps per second:  12, episode reward: 73.000, mean reward:  2.704 [-25.000, 22.000], mean action: 1.259 [0.000, 2.000],  loss: 206.581677, mean_q: 83.733965, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1052962:<---\n","  26580/500000: episode: 805, duration: 2.748s, episode steps:  32, steps per second:  12, episode reward: 271.000, mean reward:  8.469 [-25.000, 22.000], mean action: 0.969 [0.000, 2.000],  loss: 228.012227, mean_q: 81.940114, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1054272:<---\n","  26660/500000: episode: 806, duration: 6.693s, episode steps:  80, steps per second:  12, episode reward: 297.000, mean reward:  3.712 [-25.000, 22.000], mean action: 1.337 [0.000, 2.000],  loss: 251.830749, mean_q: 82.231499, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1055580:<---\n","  26687/500000: episode: 807, duration: 2.381s, episode steps:  27, steps per second:  11, episode reward: 177.000, mean reward:  6.556 [-25.000, 22.000], mean action: 0.556 [0.000, 2.000],  loss: 203.736303, mean_q: 83.375391, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1056885:<---\n","  26765/500000: episode: 808, duration: 6.645s, episode steps:  78, steps per second:  12, episode reward: 206.000, mean reward:  2.641 [-25.000, 22.000], mean action: 1.385 [0.000, 2.000],  loss: 244.952394, mean_q: 81.657886, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1058191:<---\n","  26818/500000: episode: 809, duration: 4.515s, episode steps:  53, steps per second:  12, episode reward: 111.000, mean reward:  2.094 [-25.000, 22.000], mean action: 1.472 [0.000, 2.000],  loss: 234.084872, mean_q: 80.781039, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1059500:<---\n","  26866/500000: episode: 810, duration: 4.089s, episode steps:  48, steps per second:  12, episode reward: 303.000, mean reward:  6.312 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 221.405721, mean_q: 82.448571, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1060809:<---\n","  26900/500000: episode: 811, duration: 2.965s, episode steps:  34, steps per second:  11, episode reward: 111.000, mean reward:  3.265 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 226.964303, mean_q: 82.709046, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1062112:<---\n","  26997/500000: episode: 812, duration: 8.139s, episode steps:  97, steps per second:  12, episode reward: 197.000, mean reward:  2.031 [-25.000, 22.000], mean action: 1.361 [0.000, 2.000],  loss: 240.426379, mean_q: 82.059735, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1063418:<---\n","  27067/500000: episode: 813, duration: 5.948s, episode steps:  70, steps per second:  12, episode reward: 137.000, mean reward:  1.957 [-25.000, 22.000], mean action: 1.271 [0.000, 2.000],  loss: 241.194702, mean_q: 82.236807, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1064724:<---\n","  27124/500000: episode: 814, duration: 4.932s, episode steps:  57, steps per second:  12, episode reward: 642.000, mean reward: 11.263 [-25.000, 22.000], mean action: 0.842 [0.000, 2.000],  loss: 224.398711, mean_q: 83.470671, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1066027:<---\n","  27192/500000: episode: 815, duration: 5.826s, episode steps:  68, steps per second:  12, episode reward: 276.000, mean reward:  4.059 [-25.000, 22.000], mean action: 0.971 [0.000, 2.000],  loss: 237.261320, mean_q: 81.763433, mean_eps: 0.100000\n"," 1 : 51  \n","--->:1067334:<---\n","  27241/500000: episode: 816, duration: 4.167s, episode steps:  49, steps per second:  12, episode reward:  2.000, mean reward:  0.041 [-25.000, 22.000], mean action: 1.082 [0.000, 2.000],  loss: 228.270674, mean_q: 83.539570, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1068637:<---\n","  27287/500000: episode: 817, duration: 3.937s, episode steps:  46, steps per second:  12, episode reward: 184.000, mean reward:  4.000 [-25.000, 22.000], mean action: 0.717 [0.000, 2.000],  loss: 246.008099, mean_q: 83.700191, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1069943:<---\n","  27345/500000: episode: 818, duration: 4.895s, episode steps:  58, steps per second:  12, episode reward: 159.000, mean reward:  2.741 [-25.000, 22.000], mean action: 1.121 [0.000, 2.000],  loss: 236.711687, mean_q: 81.854477, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1071245:<---\n","  27362/500000: episode: 819, duration: 1.523s, episode steps:  17, steps per second:  11, episode reward: 51.000, mean reward:  3.000 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 232.633768, mean_q: 84.055679, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1072554:<---\n","  27422/500000: episode: 820, duration: 5.083s, episode steps:  60, steps per second:  12, episode reward: 322.000, mean reward:  5.367 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 245.510074, mean_q: 82.580848, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1073857:<---\n","  27488/500000: episode: 821, duration: 5.570s, episode steps:  66, steps per second:  12, episode reward: 37.000, mean reward:  0.561 [-25.000, 22.000], mean action: 1.197 [0.000, 2.000],  loss: 220.994566, mean_q: 83.795819, mean_eps: 0.100000\n"," 1 : 38  \n","--->:1075161:<---\n","  27528/500000: episode: 822, duration: 3.429s, episode steps:  40, steps per second:  12, episode reward: 29.000, mean reward:  0.725 [-25.000, 22.000], mean action: 1.600 [0.000, 2.000],  loss: 258.953797, mean_q: 81.026004, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1076462:<---\n","  27594/500000: episode: 823, duration: 5.518s, episode steps:  66, steps per second:  12, episode reward: 95.000, mean reward:  1.439 [-25.000, 22.000], mean action: 1.682 [0.000, 2.000],  loss: 245.350862, mean_q: 81.442102, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1077765:<---\n","  27649/500000: episode: 824, duration: 4.677s, episode steps:  55, steps per second:  12, episode reward: 281.000, mean reward:  5.109 [-25.000, 22.000], mean action: 1.255 [0.000, 2.000],  loss: 230.853700, mean_q: 82.653108, mean_eps: 0.100000\n"," 1 : 32  \n","--->:1079074:<---\n","  27683/500000: episode: 825, duration: 2.922s, episode steps:  34, steps per second:  12, episode reward: 170.000, mean reward:  5.000 [-25.000, 22.000], mean action: 1.529 [0.000, 2.000],  loss: 245.333483, mean_q: 81.745390, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1080378:<---\n","  27732/500000: episode: 826, duration: 4.161s, episode steps:  49, steps per second:  12, episode reward: 173.000, mean reward:  3.531 [-25.000, 22.000], mean action: 1.673 [0.000, 2.000],  loss: 244.884422, mean_q: 81.658654, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1081682:<---\n","  27752/500000: episode: 827, duration: 1.822s, episode steps:  20, steps per second:  11, episode reward: 13.000, mean reward:  0.650 [-25.000, 22.000], mean action: 1.300 [0.000, 2.000],  loss: 225.157048, mean_q: 82.347186, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1082989:<---\n","  27771/500000: episode: 828, duration: 1.697s, episode steps:  19, steps per second:  11, episode reward: 145.000, mean reward:  7.632 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 267.290262, mean_q: 81.436745, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1084298:<---\n","  27776/500000: episode: 829, duration: 0.551s, episode steps:   5, steps per second:   9, episode reward: 63.000, mean reward: 12.600 [-25.000, 22.000], mean action: 0.200 [0.000, 1.000],  loss: 185.338324, mean_q: 80.648799, mean_eps: 0.100000\n"," 1 : 5  \n","--->:1085599:<---\n","  27800/500000: episode: 830, duration: 2.074s, episode steps:  24, steps per second:  12, episode reward: 32.000, mean reward:  1.333 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 281.573352, mean_q: 81.985196, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1086904:<---\n","  27856/500000: episode: 831, duration: 4.683s, episode steps:  56, steps per second:  12, episode reward: -52.000, mean reward: -0.929 [-25.000, 22.000], mean action: 1.357 [0.000, 2.000],  loss: 243.986030, mean_q: 82.841635, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1088205:<---\n","  27878/500000: episode: 832, duration: 1.958s, episode steps:  22, steps per second:  11, episode reward: -71.000, mean reward: -3.227 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 259.895697, mean_q: 82.832213, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1089508:<---\n","  27918/500000: episode: 833, duration: 3.413s, episode steps:  40, steps per second:  12, episode reward: 98.000, mean reward:  2.450 [-25.000, 22.000], mean action: 1.525 [0.000, 2.000],  loss: 244.438477, mean_q: 81.847763, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1090811:<---\n","  27941/500000: episode: 834, duration: 1.989s, episode steps:  23, steps per second:  12, episode reward: 183.000, mean reward:  7.957 [-25.000, 22.000], mean action: 0.870 [0.000, 2.000],  loss: 217.867342, mean_q: 80.666582, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1092120:<---\n","  27984/500000: episode: 835, duration: 3.663s, episode steps:  43, steps per second:  12, episode reward: 180.000, mean reward:  4.186 [-25.000, 22.000], mean action: 1.395 [0.000, 2.000],  loss: 235.495297, mean_q: 81.583197, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1093423:<---\n","  28039/500000: episode: 836, duration: 4.659s, episode steps:  55, steps per second:  12, episode reward: 117.000, mean reward:  2.127 [-25.000, 22.000], mean action: 1.655 [0.000, 2.000],  loss: 240.197188, mean_q: 82.110987, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1094733:<---\n","  28091/500000: episode: 837, duration: 4.453s, episode steps:  52, steps per second:  12, episode reward: 58.000, mean reward:  1.115 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 242.780255, mean_q: 82.040994, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1096036:<---\n","  28125/500000: episode: 838, duration: 2.988s, episode steps:  34, steps per second:  11, episode reward: 64.000, mean reward:  1.882 [-25.000, 22.000], mean action: 1.059 [0.000, 2.000],  loss: 249.497487, mean_q: 82.908055, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1097342:<---\n","  28164/500000: episode: 839, duration: 3.460s, episode steps:  39, steps per second:  11, episode reward: 296.000, mean reward:  7.590 [-25.000, 22.000], mean action: 1.179 [0.000, 2.000],  loss: 218.029743, mean_q: 83.940073, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1098644:<---\n","  28188/500000: episode: 840, duration: 2.088s, episode steps:  24, steps per second:  11, episode reward: 180.000, mean reward:  7.500 [-25.000, 22.000], mean action: 0.583 [0.000, 2.000],  loss: 264.703993, mean_q: 82.819073, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1099950:<---\n","  28263/500000: episode: 841, duration: 6.327s, episode steps:  75, steps per second:  12, episode reward: 221.000, mean reward:  2.947 [-25.000, 22.000], mean action: 1.493 [0.000, 2.000],  loss: 248.874940, mean_q: 81.772738, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1101253:<---\n","  28290/500000: episode: 842, duration: 2.339s, episode steps:  27, steps per second:  12, episode reward: 67.000, mean reward:  2.481 [-25.000, 22.000], mean action: 1.148 [0.000, 2.000],  loss: 235.283457, mean_q: 83.788788, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1102555:<---\n","  28340/500000: episode: 843, duration: 4.256s, episode steps:  50, steps per second:  12, episode reward: 208.000, mean reward:  4.160 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 209.192830, mean_q: 81.999633, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1103864:<---\n","  28353/500000: episode: 844, duration: 1.184s, episode steps:  13, steps per second:  11, episode reward: 126.000, mean reward:  9.692 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 261.150945, mean_q: 83.792358, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1105165:<---\n","  28373/500000: episode: 845, duration: 1.808s, episode steps:  20, steps per second:  11, episode reward: -12.000, mean reward: -0.600 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 272.468083, mean_q: 79.435324, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1106471:<---\n","  28410/500000: episode: 846, duration: 3.198s, episode steps:  37, steps per second:  12, episode reward: -62.000, mean reward: -1.676 [-25.000, 22.000], mean action: 1.622 [0.000, 2.000],  loss: 234.041937, mean_q: 83.515215, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1107781:<---\n","  28432/500000: episode: 847, duration: 1.951s, episode steps:  22, steps per second:  11, episode reward: 114.000, mean reward:  5.182 [-25.000, 22.000], mean action: 0.591 [0.000, 2.000],  loss: 223.771584, mean_q: 81.099397, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1109086:<---\n","  28476/500000: episode: 848, duration: 3.874s, episode steps:  44, steps per second:  11, episode reward: 158.000, mean reward:  3.591 [-25.000, 22.000], mean action: 1.364 [0.000, 2.000],  loss: 237.953884, mean_q: 82.095006, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1110395:<---\n","  28505/500000: episode: 849, duration: 2.605s, episode steps:  29, steps per second:  11, episode reward: 167.000, mean reward:  5.759 [-25.000, 22.000], mean action: 1.207 [0.000, 2.000],  loss: 243.053988, mean_q: 83.208115, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1111699:<---\n","  28523/500000: episode: 850, duration: 1.653s, episode steps:  18, steps per second:  11, episode reward:  4.000, mean reward:  0.222 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 229.347218, mean_q: 81.546402, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1113002:<---\n","  28605/500000: episode: 851, duration: 7.134s, episode steps:  82, steps per second:  11, episode reward: 180.000, mean reward:  2.195 [-25.000, 22.000], mean action: 1.671 [0.000, 2.000],  loss: 260.014911, mean_q: 82.090057, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1114312:<---\n","  28626/500000: episode: 852, duration: 1.864s, episode steps:  21, steps per second:  11, episode reward: 54.000, mean reward:  2.571 [-25.000, 22.000], mean action: 1.190 [0.000, 2.000],  loss: 237.317886, mean_q: 79.647564, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1115614:<---\n","  28651/500000: episode: 853, duration: 2.199s, episode steps:  25, steps per second:  11, episode reward: -84.000, mean reward: -3.360 [-25.000, 22.000], mean action: 1.440 [0.000, 2.000],  loss: 237.272853, mean_q: 83.572902, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1116921:<---\n","  28661/500000: episode: 854, duration: 0.980s, episode steps:  10, steps per second:  10, episode reward: 63.000, mean reward:  6.300 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 210.286403, mean_q: 83.900564, mean_eps: 0.100000\n"," 1 : 5  \n","--->:1118226:<---\n","  28685/500000: episode: 855, duration: 2.142s, episode steps:  24, steps per second:  11, episode reward: 45.000, mean reward:  1.875 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 213.963088, mean_q: 84.080327, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1119535:<---\n","  28705/500000: episode: 856, duration: 1.822s, episode steps:  20, steps per second:  11, episode reward: -37.000, mean reward: -1.850 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 285.075685, mean_q: 80.279674, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1120838:<---\n","  28745/500000: episode: 857, duration: 3.480s, episode steps:  40, steps per second:  11, episode reward: 164.000, mean reward:  4.100 [-25.000, 22.000], mean action: 1.475 [0.000, 2.000],  loss: 244.531426, mean_q: 82.478166, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1122148:<---\n","  28767/500000: episode: 858, duration: 1.924s, episode steps:  22, steps per second:  11, episode reward: 148.000, mean reward:  6.727 [-25.000, 22.000], mean action: 1.364 [0.000, 2.000],  loss: 231.884355, mean_q: 81.651628, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1123457:<---\n","  28788/500000: episode: 859, duration: 1.937s, episode steps:  21, steps per second:  11, episode reward: 45.000, mean reward:  2.143 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.322085, mean_q: 82.910873, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1124765:<---\n","  28802/500000: episode: 860, duration: 1.296s, episode steps:  14, steps per second:  11, episode reward: 35.000, mean reward:  2.500 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 211.673375, mean_q: 80.789484, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1126071:<---\n","  28820/500000: episode: 861, duration: 1.652s, episode steps:  18, steps per second:  11, episode reward: 82.000, mean reward:  4.556 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 227.507829, mean_q: 82.709994, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1127380:<---\n","  28854/500000: episode: 862, duration: 2.916s, episode steps:  34, steps per second:  12, episode reward: 42.000, mean reward:  1.235 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 227.027939, mean_q: 83.122007, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1128687:<---\n","  28897/500000: episode: 863, duration: 3.681s, episode steps:  43, steps per second:  12, episode reward: 133.000, mean reward:  3.093 [-25.000, 22.000], mean action: 1.302 [0.000, 2.000],  loss: 254.386433, mean_q: 82.693839, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1129996:<---\n","  28911/500000: episode: 864, duration: 1.292s, episode steps:  14, steps per second:  11, episode reward: 82.000, mean reward:  5.857 [-25.000, 22.000], mean action: 1.071 [0.000, 2.000],  loss: 196.715855, mean_q: 83.330348, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1131297:<---\n","  28981/500000: episode: 865, duration: 6.099s, episode steps:  70, steps per second:  11, episode reward: 243.000, mean reward:  3.471 [-25.000, 22.000], mean action: 1.414 [0.000, 2.000],  loss: 227.554508, mean_q: 82.055911, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1132602:<---\n","  29019/500000: episode: 866, duration: 3.349s, episode steps:  38, steps per second:  11, episode reward: 268.000, mean reward:  7.053 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 245.154394, mean_q: 83.027726, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1133907:<---\n","  29058/500000: episode: 867, duration: 3.428s, episode steps:  39, steps per second:  11, episode reward: 199.000, mean reward:  5.103 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 231.948880, mean_q: 82.200013, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1135215:<---\n","  29104/500000: episode: 868, duration: 4.024s, episode steps:  46, steps per second:  11, episode reward: 11.000, mean reward:  0.239 [-25.000, 22.000], mean action: 1.239 [0.000, 2.000],  loss: 238.237839, mean_q: 82.959529, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1136525:<---\n","  29127/500000: episode: 869, duration: 2.115s, episode steps:  23, steps per second:  11, episode reward: 95.000, mean reward:  4.130 [-25.000, 22.000], mean action: 0.870 [0.000, 2.000],  loss: 241.763823, mean_q: 83.443815, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1137835:<---\n","  29153/500000: episode: 870, duration: 2.373s, episode steps:  26, steps per second:  11, episode reward: 155.000, mean reward:  5.962 [-25.000, 22.000], mean action: 0.808 [0.000, 2.000],  loss: 223.948074, mean_q: 82.696713, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1139140:<---\n","  29196/500000: episode: 871, duration: 3.893s, episode steps:  43, steps per second:  11, episode reward: 193.000, mean reward:  4.488 [-25.000, 22.000], mean action: 1.070 [0.000, 2.000],  loss: 223.495950, mean_q: 83.447443, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1140441:<---\n","  29221/500000: episode: 872, duration: 2.338s, episode steps:  25, steps per second:  11, episode reward: 39.000, mean reward:  1.560 [-25.000, 22.000], mean action: 0.960 [0.000, 2.000],  loss: 204.231652, mean_q: 85.056787, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1141746:<---\n","  29252/500000: episode: 873, duration: 2.836s, episode steps:  31, steps per second:  11, episode reward: 148.000, mean reward:  4.774 [-25.000, 22.000], mean action: 1.484 [0.000, 2.000],  loss: 229.531103, mean_q: 82.927282, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1143052:<---\n","  29260/500000: episode: 874, duration: 0.850s, episode steps:   8, steps per second:   9, episode reward: 13.000, mean reward:  1.625 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 233.160633, mean_q: 81.239334, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1144353:<---\n","  29283/500000: episode: 875, duration: 2.182s, episode steps:  23, steps per second:  11, episode reward: 208.000, mean reward:  9.043 [-25.000, 22.000], mean action: 0.957 [0.000, 2.000],  loss: 211.168827, mean_q: 82.666951, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1145663:<---\n","  29303/500000: episode: 876, duration: 1.888s, episode steps:  20, steps per second:  11, episode reward: 167.000, mean reward:  8.350 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 234.277613, mean_q: 82.272853, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1146967:<---\n","  29331/500000: episode: 877, duration: 2.614s, episode steps:  28, steps per second:  11, episode reward: 126.000, mean reward:  4.500 [-25.000, 22.000], mean action: 1.607 [0.000, 2.000],  loss: 219.082651, mean_q: 80.542281, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1148274:<---\n","  29355/500000: episode: 878, duration: 2.223s, episode steps:  24, steps per second:  11, episode reward: 35.000, mean reward:  1.458 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 263.036441, mean_q: 82.607116, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1149584:<---\n","  29381/500000: episode: 879, duration: 2.411s, episode steps:  26, steps per second:  11, episode reward: 151.000, mean reward:  5.808 [-25.000, 22.000], mean action: 1.423 [0.000, 2.000],  loss: 256.675368, mean_q: 82.814255, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1150889:<---\n","  29388/500000: episode: 880, duration: 0.767s, episode steps:   7, steps per second:   9, episode reward: 16.000, mean reward:  2.286 [-25.000, 22.000], mean action: 1.286 [1.000, 2.000],  loss: 222.241847, mean_q: 80.881765, mean_eps: 0.100000\n"," 1 : 5  \n","--->:1152190:<---\n","  29391/500000: episode: 881, duration: 0.422s, episode steps:   3, steps per second:   7, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 0.333 [0.000, 1.000],  loss: 291.315348, mean_q: 80.679553, mean_eps: 0.100000\n"," 1 : 3  \n","--->:1153495:<---\n","  29413/500000: episode: 882, duration: 2.043s, episode steps:  22, steps per second:  11, episode reward: 92.000, mean reward:  4.182 [-25.000, 22.000], mean action: 0.864 [0.000, 2.000],  loss: 236.377642, mean_q: 81.601137, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1154796:<---\n","  29436/500000: episode: 883, duration: 2.092s, episode steps:  23, steps per second:  11, episode reward: 164.000, mean reward:  7.130 [-25.000, 22.000], mean action: 0.739 [0.000, 2.000],  loss: 225.279213, mean_q: 83.363637, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1156100:<---\n","  29456/500000: episode: 884, duration: 1.930s, episode steps:  20, steps per second:  10, episode reward: 189.000, mean reward:  9.450 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 254.372974, mean_q: 84.207556, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1157403:<---\n","  29480/500000: episode: 885, duration: 2.332s, episode steps:  24, steps per second:  10, episode reward: 73.000, mean reward:  3.042 [-25.000, 22.000], mean action: 1.083 [0.000, 2.000],  loss: 248.054257, mean_q: 83.120833, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1158711:<---\n","  29512/500000: episode: 886, duration: 2.900s, episode steps:  32, steps per second:  11, episode reward: 142.000, mean reward:  4.438 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 221.489631, mean_q: 82.895013, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1160017:<---\n","  29540/500000: episode: 887, duration: 2.466s, episode steps:  28, steps per second:  11, episode reward: 211.000, mean reward:  7.536 [-25.000, 22.000], mean action: 1.179 [0.000, 2.000],  loss: 227.451591, mean_q: 83.946473, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1161319:<---\n","  29609/500000: episode: 888, duration: 5.896s, episode steps:  69, steps per second:  12, episode reward: 360.000, mean reward:  5.217 [-25.000, 22.000], mean action: 0.942 [0.000, 2.000],  loss: 247.436203, mean_q: 82.529752, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1162620:<---\n","  29660/500000: episode: 889, duration: 4.318s, episode steps:  51, steps per second:  12, episode reward: 379.000, mean reward:  7.431 [-25.000, 22.000], mean action: 0.588 [0.000, 2.000],  loss: 210.511203, mean_q: 83.218782, mean_eps: 0.100000\n"," 1 : 45  \n","--->:1163924:<---\n","  29704/500000: episode: 890, duration: 3.838s, episode steps:  44, steps per second:  11, episode reward: 397.000, mean reward:  9.023 [-25.000, 22.000], mean action: 1.045 [0.000, 2.000],  loss: 251.172337, mean_q: 82.596298, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1165234:<---\n","  29769/500000: episode: 891, duration: 5.482s, episode steps:  65, steps per second:  12, episode reward: 551.000, mean reward:  8.477 [-25.000, 22.000], mean action: 1.015 [0.000, 2.000],  loss: 236.673324, mean_q: 82.420234, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1166542:<---\n","  29906/500000: episode: 892, duration: 11.560s, episode steps: 137, steps per second:  12, episode reward: 492.000, mean reward:  3.591 [-25.000, 22.000], mean action: 0.679 [0.000, 2.000],  loss: 249.049664, mean_q: 82.345879, mean_eps: 0.100000\n"," 1 : 95  \n","--->:1167852:<---\n","  30137/500000: episode: 893, duration: 19.331s, episode steps: 231, steps per second:  12, episode reward: 1218.000, mean reward:  5.273 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 235.166162, mean_q: 82.635492, mean_eps: 0.100000\n"," 1 : 175  \n","--->:1169161:<---\n","  30291/500000: episode: 894, duration: 12.896s, episode steps: 154, steps per second:  12, episode reward: 753.000, mean reward:  4.890 [-25.000, 22.000], mean action: 0.864 [0.000, 2.000],  loss: 248.985792, mean_q: 83.274025, mean_eps: 0.100000\n"," 1 : 109  \n","--->:1170470:<---\n","  30383/500000: episode: 895, duration: 7.809s, episode steps:  92, steps per second:  12, episode reward: 365.000, mean reward:  3.967 [-25.000, 22.000], mean action: 0.913 [0.000, 2.000],  loss: 253.037088, mean_q: 83.820450, mean_eps: 0.100000\n"," 1 : 70  \n","--->:1171776:<---\n","  30438/500000: episode: 896, duration: 4.653s, episode steps:  55, steps per second:  12, episode reward: 335.000, mean reward:  6.091 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 246.339569, mean_q: 83.270228, mean_eps: 0.100000\n"," 1 : 43  \n","--->:1173086:<---\n","  30515/500000: episode: 897, duration: 6.381s, episode steps:  77, steps per second:  12, episode reward: 697.000, mean reward:  9.052 [-25.000, 22.000], mean action: 0.649 [0.000, 2.000],  loss: 250.876852, mean_q: 83.119450, mean_eps: 0.100000\n"," 1 : 68  \n","--->:1174390:<---\n","  30601/500000: episode: 898, duration: 7.245s, episode steps:  86, steps per second:  12, episode reward: 603.000, mean reward:  7.012 [-25.000, 22.000], mean action: 0.709 [0.000, 2.000],  loss: 216.383855, mean_q: 83.251026, mean_eps: 0.100000\n"," 1 : 68  \n","--->:1175693:<---\n","  30681/500000: episode: 899, duration: 6.774s, episode steps:  80, steps per second:  12, episode reward: 367.000, mean reward:  4.588 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 224.734576, mean_q: 83.041117, mean_eps: 0.100000\n"," 1 : 53  \n","--->:1177000:<---\n","  30751/500000: episode: 900, duration: 5.922s, episode steps:  70, steps per second:  12, episode reward: 448.000, mean reward:  6.400 [-25.000, 22.000], mean action: 1.014 [0.000, 2.000],  loss: 244.434601, mean_q: 83.805138, mean_eps: 0.100000\n"," 1 : 46  \n","--->:1178305:<---\n","  30833/500000: episode: 901, duration: 6.839s, episode steps:  82, steps per second:  12, episode reward: 509.000, mean reward:  6.207 [-25.000, 22.000], mean action: 0.659 [0.000, 2.000],  loss: 241.495499, mean_q: 83.069417, mean_eps: 0.100000\n"," 1 : 68  \n","--->:1179609:<---\n","  30906/500000: episode: 902, duration: 6.249s, episode steps:  73, steps per second:  12, episode reward: 339.000, mean reward:  4.644 [-25.000, 22.000], mean action: 0.671 [0.000, 2.000],  loss: 241.717417, mean_q: 82.849405, mean_eps: 0.100000\n"," 1 : 56  \n","--->:1180918:<---\n","  30980/500000: episode: 903, duration: 6.367s, episode steps:  74, steps per second:  12, episode reward: 358.000, mean reward:  4.838 [-25.000, 22.000], mean action: 0.932 [0.000, 2.000],  loss: 243.168533, mean_q: 83.762705, mean_eps: 0.100000\n"," 1 : 59  \n","--->:1182223:<---\n","  31109/500000: episode: 904, duration: 10.703s, episode steps: 129, steps per second:  12, episode reward: 688.000, mean reward:  5.333 [-25.000, 22.000], mean action: 1.109 [0.000, 2.000],  loss: 227.286651, mean_q: 83.156273, mean_eps: 0.100000\n"," 1 : 74  \n","--->:1183531:<---\n","  31201/500000: episode: 905, duration: 7.726s, episode steps:  92, steps per second:  12, episode reward: 299.000, mean reward:  3.250 [-25.000, 22.000], mean action: 0.924 [0.000, 2.000],  loss: 241.545736, mean_q: 83.716172, mean_eps: 0.100000\n"," 1 : 67  \n","--->:1184836:<---\n","  31284/500000: episode: 906, duration: 6.958s, episode steps:  83, steps per second:  12, episode reward: 612.000, mean reward:  7.373 [-25.000, 22.000], mean action: 0.904 [0.000, 2.000],  loss: 256.046378, mean_q: 83.224400, mean_eps: 0.100000\n"," 1 : 62  \n","--->:1186143:<---\n","  31342/500000: episode: 907, duration: 4.939s, episode steps:  58, steps per second:  12, episode reward: 159.000, mean reward:  2.741 [-25.000, 22.000], mean action: 1.172 [0.000, 2.000],  loss: 253.486796, mean_q: 82.967175, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1187446:<---\n","  31413/500000: episode: 908, duration: 5.920s, episode steps:  71, steps per second:  12, episode reward: 297.000, mean reward:  4.183 [-25.000, 22.000], mean action: 1.183 [0.000, 2.000],  loss: 243.363960, mean_q: 81.680112, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1188755:<---\n","  31485/500000: episode: 909, duration: 6.032s, episode steps:  72, steps per second:  12, episode reward: 351.000, mean reward:  4.875 [-25.000, 22.000], mean action: 1.028 [0.000, 2.000],  loss: 234.643549, mean_q: 81.810537, mean_eps: 0.100000\n"," 1 : 48  \n","--->:1190059:<---\n","  31556/500000: episode: 910, duration: 5.978s, episode steps:  71, steps per second:  12, episode reward: 452.000, mean reward:  6.366 [-25.000, 22.000], mean action: 0.606 [0.000, 2.000],  loss: 236.063963, mean_q: 84.122279, mean_eps: 0.100000\n"," 1 : 59  \n","--->:1191368:<---\n","  31629/500000: episode: 911, duration: 6.205s, episode steps:  73, steps per second:  12, episode reward: 265.000, mean reward:  3.630 [-25.000, 22.000], mean action: 1.479 [0.000, 2.000],  loss: 224.815776, mean_q: 83.936467, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1192677:<---\n","  31718/500000: episode: 912, duration: 7.496s, episode steps:  89, steps per second:  12, episode reward: 549.000, mean reward:  6.169 [-25.000, 22.000], mean action: 1.112 [0.000, 2.000],  loss: 240.002566, mean_q: 82.836160, mean_eps: 0.100000\n"," 1 : 57  \n","--->:1193978:<---\n","  31760/500000: episode: 913, duration: 3.565s, episode steps:  42, steps per second:  12, episode reward: 397.000, mean reward:  9.452 [-25.000, 22.000], mean action: 1.095 [0.000, 2.000],  loss: 232.111804, mean_q: 83.484203, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1195285:<---\n","  31858/500000: episode: 914, duration: 8.212s, episode steps:  98, steps per second:  12, episode reward: 382.000, mean reward:  3.898 [-25.000, 22.000], mean action: 1.316 [0.000, 2.000],  loss: 241.488008, mean_q: 82.583238, mean_eps: 0.100000\n"," 1 : 43  \n","--->:1196591:<---\n","  31933/500000: episode: 915, duration: 6.277s, episode steps:  75, steps per second:  12, episode reward: 442.000, mean reward:  5.893 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 228.777604, mean_q: 82.734566, mean_eps: 0.100000\n"," 1 : 50  \n","--->:1197899:<---\n","  32023/500000: episode: 916, duration: 7.581s, episode steps:  90, steps per second:  12, episode reward: 691.000, mean reward:  7.678 [-25.000, 22.000], mean action: 0.589 [0.000, 2.000],  loss: 234.306057, mean_q: 82.461240, mean_eps: 0.100000\n"," 1 : 72  \n","--->:1199200:<---\n","  32165/500000: episode: 917, duration: 11.786s, episode steps: 142, steps per second:  12, episode reward: 551.000, mean reward:  3.880 [-25.000, 22.000], mean action: 1.099 [0.000, 2.000],  loss: 246.315475, mean_q: 82.475955, mean_eps: 0.100000\n"," 1 : 87  \n","--->:1200504:<---\n","  32276/500000: episode: 918, duration: 9.141s, episode steps: 111, steps per second:  12, episode reward: 849.000, mean reward:  7.649 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 228.028217, mean_q: 83.918583, mean_eps: 0.100000\n"," 1 : 92  \n","--->:1201813:<---\n","  32409/500000: episode: 919, duration: 11.160s, episode steps: 133, steps per second:  12, episode reward: 612.000, mean reward:  4.602 [-25.000, 22.000], mean action: 0.895 [0.000, 2.000],  loss: 231.554051, mean_q: 83.693869, mean_eps: 0.100000\n"," 1 : 109  \n","--->:1203114:<---\n","  32504/500000: episode: 920, duration: 7.830s, episode steps:  95, steps per second:  12, episode reward: 631.000, mean reward:  6.642 [-25.000, 22.000], mean action: 0.958 [0.000, 2.000],  loss: 226.743314, mean_q: 83.355328, mean_eps: 0.100000\n"," 1 : 65  \n","--->:1204420:<---\n","  32597/500000: episode: 921, duration: 7.843s, episode steps:  93, steps per second:  12, episode reward: 578.000, mean reward:  6.215 [-25.000, 22.000], mean action: 0.753 [0.000, 2.000],  loss: 243.908598, mean_q: 84.546365, mean_eps: 0.100000\n"," 1 : 69  \n","--->:1205724:<---\n","  32714/500000: episode: 922, duration: 9.889s, episode steps: 117, steps per second:  12, episode reward: 655.000, mean reward:  5.598 [-25.000, 22.000], mean action: 0.590 [0.000, 2.000],  loss: 241.616134, mean_q: 83.819659, mean_eps: 0.100000\n"," 1 : 96  \n","--->:1207034:<---\n","  32783/500000: episode: 923, duration: 5.924s, episode steps:  69, steps per second:  12, episode reward: 366.000, mean reward:  5.304 [-25.000, 22.000], mean action: 1.203 [0.000, 2.000],  loss: 245.204125, mean_q: 82.718269, mean_eps: 0.100000\n"," 1 : 38  \n","--->:1208342:<---\n","  32809/500000: episode: 924, duration: 2.330s, episode steps:  26, steps per second:  11, episode reward: 36.000, mean reward:  1.385 [-25.000, 22.000], mean action: 0.615 [0.000, 2.000],  loss: 197.738770, mean_q: 83.787364, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1209645:<---\n","  32872/500000: episode: 925, duration: 5.356s, episode steps:  63, steps per second:  12, episode reward: 319.000, mean reward:  5.063 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 255.746809, mean_q: 83.118253, mean_eps: 0.100000\n"," 1 : 38  \n","--->:1210948:<---\n","  32940/500000: episode: 926, duration: 5.702s, episode steps:  68, steps per second:  12, episode reward: 313.000, mean reward:  4.603 [-25.000, 22.000], mean action: 1.074 [0.000, 2.000],  loss: 226.811736, mean_q: 82.711523, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1212252:<---\n","  33014/500000: episode: 927, duration: 6.237s, episode steps:  74, steps per second:  12, episode reward: 402.000, mean reward:  5.432 [-25.000, 22.000], mean action: 0.892 [0.000, 2.000],  loss: 252.734952, mean_q: 82.597338, mean_eps: 0.100000\n"," 1 : 61  \n","--->:1213555:<---\n","  33086/500000: episode: 928, duration: 6.142s, episode steps:  72, steps per second:  12, episode reward: 307.000, mean reward:  4.264 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 256.705834, mean_q: 82.697551, mean_eps: 0.100000\n"," 1 : 46  \n","--->:1214856:<---\n","  33137/500000: episode: 929, duration: 4.442s, episode steps:  51, steps per second:  11, episode reward: 162.000, mean reward:  3.176 [-25.000, 22.000], mean action: 1.078 [0.000, 2.000],  loss: 222.726024, mean_q: 83.842546, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1216162:<---\n","  33174/500000: episode: 930, duration: 3.254s, episode steps:  37, steps per second:  11, episode reward: 96.000, mean reward:  2.595 [-25.000, 22.000], mean action: 0.865 [0.000, 2.000],  loss: 277.155776, mean_q: 83.988512, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1217472:<---\n","  33227/500000: episode: 931, duration: 4.623s, episode steps:  53, steps per second:  11, episode reward: 291.000, mean reward:  5.491 [-25.000, 22.000], mean action: 0.566 [0.000, 2.000],  loss: 247.423761, mean_q: 83.348865, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1218778:<---\n","  33295/500000: episode: 932, duration: 6.009s, episode steps:  68, steps per second:  11, episode reward: 398.000, mean reward:  5.853 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 238.375881, mean_q: 83.586272, mean_eps: 0.100000\n"," 1 : 48  \n","--->:1220084:<---\n","  33348/500000: episode: 933, duration: 4.681s, episode steps:  53, steps per second:  11, episode reward: 65.000, mean reward:  1.226 [-25.000, 22.000], mean action: 0.849 [0.000, 2.000],  loss: 246.883402, mean_q: 82.085377, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1221394:<---\n","  33497/500000: episode: 934, duration: 12.694s, episode steps: 149, steps per second:  12, episode reward: 1102.000, mean reward:  7.396 [-25.000, 22.000], mean action: 0.752 [0.000, 2.000],  loss: 214.797055, mean_q: 83.630669, mean_eps: 0.100000\n"," 1 : 127  \n","--->:1222696:<---\n","  33682/500000: episode: 935, duration: 15.375s, episode steps: 185, steps per second:  12, episode reward: 641.000, mean reward:  3.465 [-25.000, 22.000], mean action: 0.897 [0.000, 2.000],  loss: 226.814127, mean_q: 83.401306, mean_eps: 0.100000\n"," 1 : 121  \n","--->:1224000:<---\n","  33834/500000: episode: 936, duration: 12.883s, episode steps: 152, steps per second:  12, episode reward: 750.000, mean reward:  4.934 [-25.000, 22.000], mean action: 0.921 [0.000, 2.000],  loss: 237.705868, mean_q: 82.701810, mean_eps: 0.100000\n"," 1 : 111  \n","--->:1225301:<---\n","  33942/500000: episode: 937, duration: 8.999s, episode steps: 108, steps per second:  12, episode reward: 507.000, mean reward:  4.694 [-25.000, 22.000], mean action: 0.713 [0.000, 2.000],  loss: 231.743119, mean_q: 84.150188, mean_eps: 0.100000\n"," 1 : 85  \n","--->:1226604:<---\n","  34032/500000: episode: 938, duration: 7.499s, episode steps:  90, steps per second:  12, episode reward: 521.000, mean reward:  5.789 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 242.359137, mean_q: 83.016247, mean_eps: 0.100000\n"," 1 : 60  \n","--->:1227908:<---\n","  34124/500000: episode: 939, duration: 7.752s, episode steps:  92, steps per second:  12, episode reward: 562.000, mean reward:  6.109 [-25.000, 22.000], mean action: 0.989 [0.000, 2.000],  loss: 242.758770, mean_q: 83.597371, mean_eps: 0.100000\n"," 1 : 64  \n","--->:1229211:<---\n","  34191/500000: episode: 940, duration: 6.635s, episode steps:  67, steps per second:  10, episode reward: 263.000, mean reward:  3.925 [-25.000, 22.000], mean action: 0.925 [0.000, 2.000],  loss: 234.678866, mean_q: 83.189338, mean_eps: 0.100000\n"," 1 : 44  \n","--->:1230512:<---\n","  34237/500000: episode: 941, duration: 3.909s, episode steps:  46, steps per second:  12, episode reward: -38.000, mean reward: -0.826 [-25.000, 22.000], mean action: 0.630 [0.000, 2.000],  loss: 234.029529, mean_q: 85.058014, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1231818:<---\n","  34286/500000: episode: 942, duration: 4.210s, episode steps:  49, steps per second:  12, episode reward: 200.000, mean reward:  4.082 [-25.000, 22.000], mean action: 0.755 [0.000, 2.000],  loss: 240.802501, mean_q: 83.138848, mean_eps: 0.100000\n"," 1 : 39  \n","--->:1233121:<---\n","  34318/500000: episode: 943, duration: 2.736s, episode steps:  32, steps per second:  12, episode reward: 287.000, mean reward:  8.969 [-25.000, 22.000], mean action: 0.844 [0.000, 2.000],  loss: 221.743124, mean_q: 82.730853, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1234428:<---\n","  34483/500000: episode: 944, duration: 13.678s, episode steps: 165, steps per second:  12, episode reward: 867.000, mean reward:  5.255 [-25.000, 22.000], mean action: 1.279 [0.000, 2.000],  loss: 222.653911, mean_q: 83.429432, mean_eps: 0.100000\n"," 1 : 80  \n","--->:1235738:<---\n","  34598/500000: episode: 945, duration: 9.657s, episode steps: 115, steps per second:  12, episode reward: 241.000, mean reward:  2.096 [-25.000, 22.000], mean action: 0.965 [0.000, 2.000],  loss: 234.595329, mean_q: 83.225714, mean_eps: 0.100000\n"," 1 : 90  \n","--->:1237044:<---\n","  34680/500000: episode: 946, duration: 6.746s, episode steps:  82, steps per second:  12, episode reward: 569.000, mean reward:  6.939 [-25.000, 22.000], mean action: 0.476 [0.000, 2.000],  loss: 235.406341, mean_q: 84.281664, mean_eps: 0.100000\n"," 1 : 75  \n","--->:1238345:<---\n","  34847/500000: episode: 947, duration: 13.870s, episode steps: 167, steps per second:  12, episode reward: 1140.000, mean reward:  6.826 [-25.000, 22.000], mean action: 0.886 [0.000, 2.000],  loss: 237.912146, mean_q: 82.622910, mean_eps: 0.100000\n"," 1 : 133  \n","--->:1239646:<---\n","  35036/500000: episode: 948, duration: 15.864s, episode steps: 189, steps per second:  12, episode reward: 1198.000, mean reward:  6.339 [-25.000, 22.000], mean action: 0.661 [0.000, 2.000],  loss: 231.148890, mean_q: 83.592878, mean_eps: 0.100000\n"," 1 : 157  \n","--->:1240950:<---\n","  35166/500000: episode: 949, duration: 10.838s, episode steps: 130, steps per second:  12, episode reward: 603.000, mean reward:  4.638 [-25.000, 22.000], mean action: 0.692 [0.000, 2.000],  loss: 263.221309, mean_q: 82.488884, mean_eps: 0.100000\n"," 1 : 115  \n","--->:1242253:<---\n","  35205/500000: episode: 950, duration: 3.424s, episode steps:  39, steps per second:  11, episode reward: 162.000, mean reward:  4.154 [-25.000, 22.000], mean action: 0.821 [0.000, 2.000],  loss: 235.386015, mean_q: 81.845526, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1243555:<---\n","  35351/500000: episode: 951, duration: 12.159s, episode steps: 146, steps per second:  12, episode reward: 871.000, mean reward:  5.966 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 233.057885, mean_q: 83.094075, mean_eps: 0.100000\n"," 1 : 93  \n","--->:1244857:<---\n","  35537/500000: episode: 952, duration: 15.288s, episode steps: 186, steps per second:  12, episode reward: 856.000, mean reward:  4.602 [-25.000, 22.000], mean action: 1.145 [0.000, 2.000],  loss: 236.423716, mean_q: 83.772366, mean_eps: 0.100000\n"," 1 : 103  \n","--->:1246163:<---\n","  35658/500000: episode: 953, duration: 10.278s, episode steps: 121, steps per second:  12, episode reward: 645.000, mean reward:  5.331 [-25.000, 22.000], mean action: 1.132 [0.000, 2.000],  loss: 249.516595, mean_q: 82.503765, mean_eps: 0.100000\n"," 1 : 87  \n","--->:1247465:<---\n","  35778/500000: episode: 954, duration: 10.017s, episode steps: 120, steps per second:  12, episode reward: 506.000, mean reward:  4.217 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 251.927139, mean_q: 83.754902, mean_eps: 0.100000\n"," 1 : 70  \n","--->:1248774:<---\n","  35882/500000: episode: 955, duration: 8.829s, episode steps: 104, steps per second:  12, episode reward: 734.000, mean reward:  7.058 [-25.000, 22.000], mean action: 1.192 [0.000, 2.000],  loss: 242.135086, mean_q: 83.100094, mean_eps: 0.100000\n"," 1 : 59  \n","--->:1250082:<---\n","  35975/500000: episode: 956, duration: 8.045s, episode steps:  93, steps per second:  12, episode reward: 223.000, mean reward:  2.398 [-25.000, 22.000], mean action: 1.161 [0.000, 2.000],  loss: 231.485106, mean_q: 83.791387, mean_eps: 0.100000\n"," 1 : 55  \n","--->:1251390:<---\n","  36061/500000: episode: 957, duration: 7.492s, episode steps:  86, steps per second:  11, episode reward: 420.000, mean reward:  4.884 [-25.000, 22.000], mean action: 1.233 [0.000, 2.000],  loss: 232.188231, mean_q: 82.755672, mean_eps: 0.100000\n"," 1 : 49  \n","--->:1252693:<---\n","  36112/500000: episode: 958, duration: 4.533s, episode steps:  51, steps per second:  11, episode reward: 140.000, mean reward:  2.745 [-25.000, 22.000], mean action: 1.020 [0.000, 2.000],  loss: 222.869532, mean_q: 84.447708, mean_eps: 0.100000\n"," 1 : 32  \n","--->:1254003:<---\n","  36165/500000: episode: 959, duration: 4.548s, episode steps:  53, steps per second:  12, episode reward: 143.000, mean reward:  2.698 [-25.000, 22.000], mean action: 1.226 [0.000, 2.000],  loss: 222.273646, mean_q: 82.678899, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1255310:<---\n","  36228/500000: episode: 960, duration: 5.541s, episode steps:  63, steps per second:  11, episode reward: 297.000, mean reward:  4.714 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 247.495180, mean_q: 82.872280, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1256617:<---\n","  36297/500000: episode: 961, duration: 6.073s, episode steps:  69, steps per second:  11, episode reward: 102.000, mean reward:  1.478 [-25.000, 22.000], mean action: 1.522 [0.000, 2.000],  loss: 247.439754, mean_q: 83.334723, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1257920:<---\n","  36375/500000: episode: 962, duration: 6.637s, episode steps:  78, steps per second:  12, episode reward: -8.000, mean reward: -0.103 [-25.000, 22.000], mean action: 1.577 [0.000, 2.000],  loss: 243.773610, mean_q: 82.855531, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1259230:<---\n","  36406/500000: episode: 963, duration: 2.779s, episode steps:  31, steps per second:  11, episode reward: 174.000, mean reward:  5.613 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.902901, mean_q: 83.840799, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1260537:<---\n","  36435/500000: episode: 964, duration: 2.543s, episode steps:  29, steps per second:  11, episode reward: 305.000, mean reward: 10.517 [-25.000, 22.000], mean action: 1.103 [0.000, 2.000],  loss: 200.023649, mean_q: 82.904326, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1261847:<---\n","  36469/500000: episode: 965, duration: 2.984s, episode steps:  34, steps per second:  11, episode reward: 64.000, mean reward:  1.882 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 241.655961, mean_q: 82.485649, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1263155:<---\n","  36532/500000: episode: 966, duration: 5.363s, episode steps:  63, steps per second:  12, episode reward: 228.000, mean reward:  3.619 [-25.000, 22.000], mean action: 1.079 [0.000, 2.000],  loss: 252.600322, mean_q: 84.145814, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1264465:<---\n","  36570/500000: episode: 967, duration: 3.323s, episode steps:  38, steps per second:  11, episode reward: 246.000, mean reward:  6.474 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 224.978992, mean_q: 83.653223, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1265769:<---\n","  36595/500000: episode: 968, duration: 2.226s, episode steps:  25, steps per second:  11, episode reward: 111.000, mean reward:  4.440 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 251.437438, mean_q: 82.306042, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1267072:<---\n","  36614/500000: episode: 969, duration: 1.696s, episode steps:  19, steps per second:  11, episode reward: 70.000, mean reward:  3.684 [-25.000, 22.000], mean action: 0.579 [0.000, 2.000],  loss: 239.094747, mean_q: 86.536748, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1268376:<---\n","  36662/500000: episode: 970, duration: 4.139s, episode steps:  48, steps per second:  12, episode reward: 387.000, mean reward:  8.062 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 224.690586, mean_q: 83.529624, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1269682:<---\n","  36703/500000: episode: 971, duration: 3.567s, episode steps:  41, steps per second:  11, episode reward: 252.000, mean reward:  6.146 [-25.000, 22.000], mean action: 1.341 [0.000, 2.000],  loss: 213.684944, mean_q: 84.355942, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1270992:<---\n","  36754/500000: episode: 972, duration: 4.451s, episode steps:  51, steps per second:  11, episode reward: 394.000, mean reward:  7.725 [-25.000, 22.000], mean action: 0.980 [0.000, 2.000],  loss: 238.282520, mean_q: 83.050764, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1272298:<---\n","  36802/500000: episode: 973, duration: 4.084s, episode steps:  48, steps per second:  12, episode reward: 152.000, mean reward:  3.167 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 224.427199, mean_q: 84.266956, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1273604:<---\n","  36839/500000: episode: 974, duration: 3.262s, episode steps:  37, steps per second:  11, episode reward: 227.000, mean reward:  6.135 [-25.000, 22.000], mean action: 1.108 [0.000, 2.000],  loss: 252.565434, mean_q: 83.996221, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1274905:<---\n","  36858/500000: episode: 975, duration: 1.725s, episode steps:  19, steps per second:  11, episode reward: 123.000, mean reward:  6.474 [-25.000, 22.000], mean action: 1.105 [0.000, 2.000],  loss: 224.736254, mean_q: 83.675830, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1276208:<---\n","  36886/500000: episode: 976, duration: 2.453s, episode steps:  28, steps per second:  11, episode reward: -2.000, mean reward: -0.071 [-25.000, 22.000], mean action: 1.107 [0.000, 2.000],  loss: 223.376923, mean_q: 83.691055, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1277513:<---\n","  36920/500000: episode: 977, duration: 3.003s, episode steps:  34, steps per second:  11, episode reward: 318.000, mean reward:  9.353 [-25.000, 22.000], mean action: 0.882 [0.000, 2.000],  loss: 216.466352, mean_q: 83.154095, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1278819:<---\n","  36999/500000: episode: 978, duration: 6.706s, episode steps:  79, steps per second:  12, episode reward: 316.000, mean reward:  4.000 [-25.000, 22.000], mean action: 1.177 [0.000, 2.000],  loss: 234.828624, mean_q: 83.952375, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1280120:<---\n","  37070/500000: episode: 979, duration: 6.096s, episode steps:  71, steps per second:  12, episode reward: 341.000, mean reward:  4.803 [-25.000, 22.000], mean action: 1.141 [0.000, 2.000],  loss: 248.986538, mean_q: 83.160366, mean_eps: 0.100000\n"," 1 : 39  \n","--->:1281426:<---\n","  37109/500000: episode: 980, duration: 3.455s, episode steps:  39, steps per second:  11, episode reward: 240.000, mean reward:  6.154 [-25.000, 22.000], mean action: 0.769 [0.000, 2.000],  loss: 237.588827, mean_q: 83.120183, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1282732:<---\n","  37144/500000: episode: 981, duration: 3.032s, episode steps:  35, steps per second:  12, episode reward: 177.000, mean reward:  5.057 [-25.000, 22.000], mean action: 1.229 [0.000, 2.000],  loss: 264.862845, mean_q: 83.156152, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1284034:<---\n","  37197/500000: episode: 982, duration: 4.484s, episode steps:  53, steps per second:  12, episode reward: 55.000, mean reward:  1.038 [-25.000, 22.000], mean action: 1.264 [0.000, 2.000],  loss: 228.947822, mean_q: 83.161426, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1285343:<---\n","  37262/500000: episode: 983, duration: 5.633s, episode steps:  65, steps per second:  12, episode reward: 454.000, mean reward:  6.985 [-25.000, 22.000], mean action: 0.969 [0.000, 2.000],  loss: 230.305893, mean_q: 83.892242, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1286650:<---\n","  37322/500000: episode: 984, duration: 5.191s, episode steps:  60, steps per second:  12, episode reward: 410.000, mean reward:  6.833 [-25.000, 22.000], mean action: 1.217 [0.000, 2.000],  loss: 235.928773, mean_q: 84.316956, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1287953:<---\n","  37332/500000: episode: 985, duration: 0.975s, episode steps:  10, steps per second:  10, episode reward: -12.000, mean reward: -1.200 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 266.790642, mean_q: 79.575620, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1289254:<---\n","  37400/500000: episode: 986, duration: 5.879s, episode steps:  68, steps per second:  12, episode reward: 111.000, mean reward:  1.632 [-25.000, 22.000], mean action: 1.544 [0.000, 2.000],  loss: 230.868615, mean_q: 83.918594, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1290558:<---\n","  37456/500000: episode: 987, duration: 4.923s, episode steps:  56, steps per second:  11, episode reward: 64.000, mean reward:  1.143 [-25.000, 22.000], mean action: 1.625 [0.000, 2.000],  loss: 239.958331, mean_q: 84.489314, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1291861:<---\n","  37489/500000: episode: 988, duration: 2.923s, episode steps:  33, steps per second:  11, episode reward: 36.000, mean reward:  1.091 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 233.413695, mean_q: 82.548941, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1293169:<---\n","  37517/500000: episode: 989, duration: 2.505s, episode steps:  28, steps per second:  11, episode reward: 183.000, mean reward:  6.536 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 251.395863, mean_q: 84.166224, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1294471:<---\n","  37541/500000: episode: 990, duration: 2.113s, episode steps:  24, steps per second:  11, episode reward: 114.000, mean reward:  4.750 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 236.762803, mean_q: 81.890044, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1295776:<---\n","  37582/500000: episode: 991, duration: 3.607s, episode steps:  41, steps per second:  11, episode reward: 58.000, mean reward:  1.415 [-25.000, 22.000], mean action: 1.317 [0.000, 2.000],  loss: 215.142097, mean_q: 83.975823, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1297080:<---\n","  37621/500000: episode: 992, duration: 3.360s, episode steps:  39, steps per second:  12, episode reward: 211.000, mean reward:  5.410 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 264.821083, mean_q: 83.657164, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1298389:<---\n","  37666/500000: episode: 993, duration: 3.861s, episode steps:  45, steps per second:  12, episode reward: 146.000, mean reward:  3.244 [-25.000, 22.000], mean action: 0.978 [0.000, 2.000],  loss: 251.269003, mean_q: 82.521874, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1299690:<---\n","  37690/500000: episode: 994, duration: 2.150s, episode steps:  24, steps per second:  11, episode reward: 264.000, mean reward: 11.000 [ 0.000, 22.000], mean action: 1.292 [0.000, 2.000],  loss: 220.199076, mean_q: 83.532889, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1300997:<---\n","  37754/500000: episode: 995, duration: 5.522s, episode steps:  64, steps per second:  12, episode reward: 256.000, mean reward:  4.000 [-25.000, 22.000], mean action: 1.156 [0.000, 2.000],  loss: 257.175889, mean_q: 82.467353, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1302300:<---\n","  37797/500000: episode: 996, duration: 3.785s, episode steps:  43, steps per second:  11, episode reward: 152.000, mean reward:  3.535 [-25.000, 22.000], mean action: 1.047 [0.000, 2.000],  loss: 240.470966, mean_q: 83.094278, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1303610:<---\n","  37815/500000: episode: 997, duration: 1.631s, episode steps:  18, steps per second:  11, episode reward: 120.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 255.212511, mean_q: 80.550034, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1304916:<---\n","  37839/500000: episode: 998, duration: 2.168s, episode steps:  24, steps per second:  11, episode reward: 296.000, mean reward: 12.333 [-25.000, 22.000], mean action: 0.417 [0.000, 2.000],  loss: 220.467686, mean_q: 82.452176, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1306217:<---\n","  37874/500000: episode: 999, duration: 3.135s, episode steps:  35, steps per second:  11, episode reward: 230.000, mean reward:  6.571 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 227.189072, mean_q: 83.174080, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1307519:<---\n","  37912/500000: episode: 1000, duration: 3.316s, episode steps:  38, steps per second:  11, episode reward: 315.000, mean reward:  8.289 [-25.000, 22.000], mean action: 0.974 [0.000, 2.000],  loss: 235.644073, mean_q: 83.154361, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1308825:<---\n","  37948/500000: episode: 1001, duration: 3.201s, episode steps:  36, steps per second:  11, episode reward: 67.000, mean reward:  1.861 [-25.000, 22.000], mean action: 1.194 [0.000, 2.000],  loss: 228.434162, mean_q: 82.389617, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1310132:<---\n","  37994/500000: episode: 1002, duration: 3.978s, episode steps:  46, steps per second:  12, episode reward: 108.000, mean reward:  2.348 [-25.000, 22.000], mean action: 1.217 [0.000, 2.000],  loss: 259.812661, mean_q: 83.471383, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1311442:<---\n","  38073/500000: episode: 1003, duration: 7.037s, episode steps:  79, steps per second:  11, episode reward: 152.000, mean reward:  1.924 [-25.000, 22.000], mean action: 1.519 [0.000, 2.000],  loss: 236.526409, mean_q: 83.578929, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1312748:<---\n","  38088/500000: episode: 1004, duration: 1.433s, episode steps:  15, steps per second:  10, episode reward: 79.000, mean reward:  5.267 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 225.827155, mean_q: 81.961633, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1314053:<---\n","  38117/500000: episode: 1005, duration: 2.671s, episode steps:  29, steps per second:  11, episode reward: 280.000, mean reward:  9.655 [-25.000, 22.000], mean action: 1.207 [0.000, 2.000],  loss: 251.986496, mean_q: 81.992416, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1315354:<---\n","  38131/500000: episode: 1006, duration: 1.347s, episode steps:  14, steps per second:  10, episode reward: 13.000, mean reward:  0.929 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 285.995938, mean_q: 82.633286, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1316662:<---\n","  38193/500000: episode: 1007, duration: 5.393s, episode steps:  62, steps per second:  11, episode reward: 287.000, mean reward:  4.629 [-25.000, 22.000], mean action: 1.339 [0.000, 2.000],  loss: 234.489565, mean_q: 81.425120, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1317972:<---\n","  38257/500000: episode: 1008, duration: 5.510s, episode steps:  64, steps per second:  12, episode reward: 410.000, mean reward:  6.406 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 226.920130, mean_q: 83.018496, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1319277:<---\n","  38319/500000: episode: 1009, duration: 5.484s, episode steps:  62, steps per second:  11, episode reward: 397.000, mean reward:  6.403 [-25.000, 22.000], mean action: 1.226 [0.000, 2.000],  loss: 244.244586, mean_q: 84.036036, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1320587:<---\n","  38406/500000: episode: 1010, duration: 7.615s, episode steps:  87, steps per second:  11, episode reward: 593.000, mean reward:  6.816 [-25.000, 22.000], mean action: 0.816 [0.000, 2.000],  loss: 217.355892, mean_q: 84.221727, mean_eps: 0.100000\n"," 1 : 59  \n","--->:1321896:<---\n","  38469/500000: episode: 1011, duration: 5.558s, episode steps:  63, steps per second:  11, episode reward: 342.000, mean reward:  5.429 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 235.647633, mean_q: 84.159041, mean_eps: 0.100000\n"," 1 : 54  \n","--->:1323204:<---\n","  38535/500000: episode: 1012, duration: 5.796s, episode steps:  66, steps per second:  11, episode reward: 250.000, mean reward:  3.788 [-25.000, 22.000], mean action: 1.197 [0.000, 2.000],  loss: 224.915231, mean_q: 83.000772, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1324509:<---\n","  38604/500000: episode: 1013, duration: 5.980s, episode steps:  69, steps per second:  12, episode reward: 564.000, mean reward:  8.174 [-25.000, 22.000], mean action: 1.014 [0.000, 2.000],  loss: 245.033612, mean_q: 82.676586, mean_eps: 0.100000\n"," 1 : 47  \n","--->:1325817:<---\n","  38652/500000: episode: 1014, duration: 4.248s, episode steps:  48, steps per second:  11, episode reward: 124.000, mean reward:  2.583 [-25.000, 22.000], mean action: 1.292 [0.000, 2.000],  loss: 238.031170, mean_q: 80.912101, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1327121:<---\n","  38729/500000: episode: 1015, duration: 6.668s, episode steps:  77, steps per second:  12, episode reward: 212.000, mean reward:  2.753 [-25.000, 22.000], mean action: 1.481 [0.000, 2.000],  loss: 241.547958, mean_q: 82.178562, mean_eps: 0.100000\n"," 1 : 31  \n","--->:1328428:<---\n","  38758/500000: episode: 1016, duration: 2.626s, episode steps:  29, steps per second:  11, episode reward: 17.000, mean reward:  0.586 [-25.000, 22.000], mean action: 1.207 [0.000, 2.000],  loss: 239.117388, mean_q: 83.851636, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1329729:<---\n","  38806/500000: episode: 1017, duration: 4.415s, episode steps:  48, steps per second:  11, episode reward:  4.000, mean reward:  0.083 [-25.000, 22.000], mean action: 1.625 [0.000, 2.000],  loss: 210.351964, mean_q: 83.127300, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1331039:<---\n","  38857/500000: episode: 1018, duration: 4.530s, episode steps:  51, steps per second:  11, episode reward: 86.000, mean reward:  1.686 [-25.000, 22.000], mean action: 1.451 [0.000, 2.000],  loss: 249.401675, mean_q: 84.325026, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1332347:<---\n","  38892/500000: episode: 1019, duration: 3.146s, episode steps:  35, steps per second:  11, episode reward: 233.000, mean reward:  6.657 [-25.000, 22.000], mean action: 1.343 [0.000, 2.000],  loss: 252.415341, mean_q: 82.667273, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1333655:<---\n","  38913/500000: episode: 1020, duration: 1.955s, episode steps:  21, steps per second:  11, episode reward: 54.000, mean reward:  2.571 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 242.549955, mean_q: 82.087765, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1334957:<---\n","  38947/500000: episode: 1021, duration: 3.041s, episode steps:  34, steps per second:  11, episode reward: 98.000, mean reward:  2.882 [-25.000, 22.000], mean action: 1.294 [0.000, 2.000],  loss: 221.074303, mean_q: 82.268366, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1336259:<---\n","  38974/500000: episode: 1022, duration: 2.465s, episode steps:  27, steps per second:  11, episode reward: 117.000, mean reward:  4.333 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 214.068793, mean_q: 84.227140, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1337568:<---\n","  39003/500000: episode: 1023, duration: 2.641s, episode steps:  29, steps per second:  11, episode reward: 123.000, mean reward:  4.241 [-25.000, 22.000], mean action: 1.414 [0.000, 2.000],  loss: 214.389521, mean_q: 84.144008, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1338872:<---\n","  39030/500000: episode: 1024, duration: 2.465s, episode steps:  27, steps per second:  11, episode reward: 29.000, mean reward:  1.074 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 245.607874, mean_q: 85.012826, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1340176:<---\n","  39050/500000: episode: 1025, duration: 1.858s, episode steps:  20, steps per second:  11, episode reward: 123.000, mean reward:  6.150 [-25.000, 22.000], mean action: 1.150 [0.000, 2.000],  loss: 228.683088, mean_q: 82.129163, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1341478:<---\n","  39075/500000: episode: 1026, duration: 2.321s, episode steps:  25, steps per second:  11, episode reward: 10.000, mean reward:  0.400 [-25.000, 22.000], mean action: 1.600 [0.000, 2.000],  loss: 292.950978, mean_q: 83.315602, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1342783:<---\n","  39091/500000: episode: 1027, duration: 1.549s, episode steps:  16, steps per second:  10, episode reward: 129.000, mean reward:  8.062 [-25.000, 22.000], mean action: 1.500 [1.000, 2.000],  loss: 248.118665, mean_q: 85.065676, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1344084:<---\n","  39115/500000: episode: 1028, duration: 2.243s, episode steps:  24, steps per second:  11, episode reward: -40.000, mean reward: -1.667 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 251.268548, mean_q: 82.056721, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1345386:<---\n","  39171/500000: episode: 1029, duration: 5.150s, episode steps:  56, steps per second:  11, episode reward: 76.000, mean reward:  1.357 [-25.000, 22.000], mean action: 1.661 [0.000, 2.000],  loss: 237.530430, mean_q: 82.981883, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1346691:<---\n","  39184/500000: episode: 1030, duration: 1.310s, episode steps:  13, steps per second:  10, episode reward: 10.000, mean reward:  0.769 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 235.629215, mean_q: 84.539848, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1348000:<---\n","  39207/500000: episode: 1031, duration: 2.175s, episode steps:  23, steps per second:  11, episode reward: 13.000, mean reward:  0.565 [-25.000, 22.000], mean action: 1.522 [0.000, 2.000],  loss: 220.202673, mean_q: 84.056488, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1349304:<---\n","  39226/500000: episode: 1032, duration: 1.775s, episode steps:  19, steps per second:  11, episode reward: 192.000, mean reward: 10.105 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 249.743631, mean_q: 84.266942, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1350606:<---\n","  39251/500000: episode: 1033, duration: 2.268s, episode steps:  25, steps per second:  11, episode reward:  1.000, mean reward:  0.040 [-25.000, 22.000], mean action: 1.080 [0.000, 2.000],  loss: 247.765059, mean_q: 82.492281, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1351908:<---\n","  39289/500000: episode: 1034, duration: 3.353s, episode steps:  38, steps per second:  11, episode reward: 102.000, mean reward:  2.684 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 236.767898, mean_q: 83.498182, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1353212:<---\n","  39335/500000: episode: 1035, duration: 4.033s, episode steps:  46, steps per second:  11, episode reward: 39.000, mean reward:  0.848 [-25.000, 22.000], mean action: 1.370 [0.000, 2.000],  loss: 243.475093, mean_q: 82.997054, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1354518:<---\n","  39352/500000: episode: 1036, duration: 1.598s, episode steps:  17, steps per second:  11, episode reward: -37.000, mean reward: -2.176 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 232.372344, mean_q: 84.409873, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1355825:<---\n","  39390/500000: episode: 1037, duration: 3.374s, episode steps:  38, steps per second:  11, episode reward: 202.000, mean reward:  5.316 [-25.000, 22.000], mean action: 1.132 [0.000, 2.000],  loss: 240.408657, mean_q: 83.638906, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1357130:<---\n","  39450/500000: episode: 1038, duration: 5.293s, episode steps:  60, steps per second:  11, episode reward: 143.000, mean reward:  2.383 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 240.267729, mean_q: 82.863825, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1358436:<---\n","  39501/500000: episode: 1039, duration: 4.447s, episode steps:  51, steps per second:  11, episode reward: 259.000, mean reward:  5.078 [-25.000, 22.000], mean action: 1.020 [0.000, 2.000],  loss: 223.537716, mean_q: 83.768619, mean_eps: 0.100000\n"," 1 : 31  \n","--->:1359745:<---\n","  39538/500000: episode: 1040, duration: 3.343s, episode steps:  37, steps per second:  11, episode reward: 284.000, mean reward:  7.676 [-25.000, 22.000], mean action: 0.919 [0.000, 2.000],  loss: 236.008688, mean_q: 82.187554, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1361052:<---\n","  39598/500000: episode: 1041, duration: 5.412s, episode steps:  60, steps per second:  11, episode reward: 445.000, mean reward:  7.417 [-25.000, 22.000], mean action: 0.933 [0.000, 2.000],  loss: 230.596688, mean_q: 85.012729, mean_eps: 0.100000\n"," 1 : 48  \n","--->:1362358:<---\n","  39683/500000: episode: 1042, duration: 7.513s, episode steps:  85, steps per second:  11, episode reward: 597.000, mean reward:  7.024 [-25.000, 22.000], mean action: 0.506 [0.000, 2.000],  loss: 228.433932, mean_q: 83.100129, mean_eps: 0.100000\n"," 1 : 72  \n","--->:1363666:<---\n","  39763/500000: episode: 1043, duration: 7.037s, episode steps:  80, steps per second:  11, episode reward: 314.000, mean reward:  3.925 [-25.000, 22.000], mean action: 0.900 [0.000, 2.000],  loss: 236.311122, mean_q: 82.722802, mean_eps: 0.100000\n"," 1 : 57  \n","--->:1364973:<---\n","  39839/500000: episode: 1044, duration: 6.856s, episode steps:  76, steps per second:  11, episode reward: 637.000, mean reward:  8.382 [-25.000, 22.000], mean action: 0.697 [0.000, 2.000],  loss: 232.968696, mean_q: 83.965300, mean_eps: 0.100000\n"," 1 : 61  \n","--->:1366276:<---\n","  39957/500000: episode: 1045, duration: 10.557s, episode steps: 118, steps per second:  11, episode reward: 607.000, mean reward:  5.144 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.955346, mean_q: 83.317055, mean_eps: 0.100000\n"," 1 : 81  \n","--->:1367585:<---\n","  40016/500000: episode: 1046, duration: 5.341s, episode steps:  59, steps per second:  11, episode reward: 103.000, mean reward:  1.746 [-25.000, 22.000], mean action: 0.814 [0.000, 2.000],  loss: 238.778155, mean_q: 83.568206, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1368894:<---\n","  40058/500000: episode: 1047, duration: 3.913s, episode steps:  42, steps per second:  11, episode reward: 306.000, mean reward:  7.286 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 243.832071, mean_q: 83.420327, mean_eps: 0.100000\n"," 1 : 31  \n","--->:1370201:<---\n","  40099/500000: episode: 1048, duration: 3.807s, episode steps:  41, steps per second:  11, episode reward: -23.000, mean reward: -0.561 [-25.000, 22.000], mean action: 0.878 [0.000, 2.000],  loss: 244.294164, mean_q: 83.002340, mean_eps: 0.100000\n"," 1 : 31  \n","--->:1371502:<---\n","  40134/500000: episode: 1049, duration: 3.296s, episode steps:  35, steps per second:  11, episode reward: 174.000, mean reward:  4.971 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 230.055818, mean_q: 80.360600, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1372807:<---\n","  40159/500000: episode: 1050, duration: 2.326s, episode steps:  25, steps per second:  11, episode reward: 114.000, mean reward:  4.560 [-25.000, 22.000], mean action: 0.920 [0.000, 2.000],  loss: 244.674524, mean_q: 81.000656, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1374109:<---\n","  40193/500000: episode: 1051, duration: 3.078s, episode steps:  34, steps per second:  11, episode reward: 237.000, mean reward:  6.971 [-25.000, 22.000], mean action: 0.529 [0.000, 2.000],  loss: 273.906358, mean_q: 84.271733, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1375418:<---\n","  40216/500000: episode: 1052, duration: 2.150s, episode steps:  23, steps per second:  11, episode reward: -52.000, mean reward: -2.261 [-25.000, 22.000], mean action: 0.652 [0.000, 2.000],  loss: 248.087956, mean_q: 85.925726, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1376727:<---\n","  40239/500000: episode: 1053, duration: 2.139s, episode steps:  23, steps per second:  11, episode reward: 98.000, mean reward:  4.261 [-25.000, 22.000], mean action: 1.174 [0.000, 2.000],  loss: 213.311485, mean_q: 83.723644, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1378028:<---\n","  40260/500000: episode: 1054, duration: 1.984s, episode steps:  21, steps per second:  11, episode reward: 164.000, mean reward:  7.810 [-25.000, 22.000], mean action: 0.476 [0.000, 2.000],  loss: 219.617286, mean_q: 83.406599, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1379338:<---\n","  40328/500000: episode: 1055, duration: 5.891s, episode steps:  68, steps per second:  12, episode reward: 265.000, mean reward:  3.897 [-25.000, 22.000], mean action: 1.441 [0.000, 2.000],  loss: 228.478578, mean_q: 84.539074, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1380648:<---\n","  40357/500000: episode: 1056, duration: 2.570s, episode steps:  29, steps per second:  11, episode reward: 89.000, mean reward:  3.069 [-25.000, 22.000], mean action: 0.931 [0.000, 2.000],  loss: 234.784587, mean_q: 84.331928, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1381951:<---\n","  40372/500000: episode: 1057, duration: 1.430s, episode steps:  15, steps per second:  10, episode reward: -40.000, mean reward: -2.667 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 236.342875, mean_q: 85.053330, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1383261:<---\n","  40415/500000: episode: 1058, duration: 3.861s, episode steps:  43, steps per second:  11, episode reward: 315.000, mean reward:  7.326 [-25.000, 22.000], mean action: 0.953 [0.000, 2.000],  loss: 234.112086, mean_q: 83.649381, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1384564:<---\n","  40486/500000: episode: 1059, duration: 6.526s, episode steps:  71, steps per second:  11, episode reward: 492.000, mean reward:  6.930 [-25.000, 22.000], mean action: 0.915 [0.000, 2.000],  loss: 215.238501, mean_q: 84.619018, mean_eps: 0.100000\n"," 1 : 48  \n","--->:1385868:<---\n","  40583/500000: episode: 1060, duration: 8.838s, episode steps:  97, steps per second:  11, episode reward: 457.000, mean reward:  4.711 [-25.000, 22.000], mean action: 1.381 [0.000, 2.000],  loss: 242.009318, mean_q: 83.289185, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1387174:<---\n","  40641/500000: episode: 1061, duration: 5.268s, episode steps:  58, steps per second:  11, episode reward: 322.000, mean reward:  5.552 [-25.000, 22.000], mean action: 0.914 [0.000, 2.000],  loss: 237.240867, mean_q: 83.763468, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1388482:<---\n","  40716/500000: episode: 1062, duration: 6.699s, episode steps:  75, steps per second:  11, episode reward: 643.000, mean reward:  8.573 [-25.000, 22.000], mean action: 0.880 [0.000, 2.000],  loss: 235.760011, mean_q: 82.804969, mean_eps: 0.100000\n"," 1 : 57  \n","--->:1389785:<---\n","  40776/500000: episode: 1063, duration: 5.560s, episode steps:  60, steps per second:  11, episode reward: 190.000, mean reward:  3.167 [-25.000, 22.000], mean action: 1.367 [0.000, 2.000],  loss: 250.634773, mean_q: 82.656370, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1391087:<---\n","  40829/500000: episode: 1064, duration: 4.862s, episode steps:  53, steps per second:  11, episode reward: 297.000, mean reward:  5.604 [-25.000, 22.000], mean action: 1.038 [0.000, 2.000],  loss: 267.879281, mean_q: 83.806043, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1392390:<---\n","  40880/500000: episode: 1065, duration: 4.545s, episode steps:  51, steps per second:  11, episode reward: 199.000, mean reward:  3.902 [-25.000, 22.000], mean action: 1.275 [0.000, 2.000],  loss: 246.658356, mean_q: 84.113938, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1393698:<---\n","  40934/500000: episode: 1066, duration: 4.748s, episode steps:  54, steps per second:  11, episode reward: 363.000, mean reward:  6.722 [-25.000, 22.000], mean action: 0.963 [0.000, 2.000],  loss: 246.532847, mean_q: 83.596447, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1395003:<---\n","  41016/500000: episode: 1067, duration: 7.045s, episode steps:  82, steps per second:  12, episode reward: 136.000, mean reward:  1.659 [-25.000, 22.000], mean action: 1.671 [0.000, 2.000],  loss: 257.390384, mean_q: 83.578794, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1396310:<---\n","  41043/500000: episode: 1068, duration: 2.441s, episode steps:  27, steps per second:  11, episode reward: 61.000, mean reward:  2.259 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 229.236052, mean_q: 85.237533, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1397612:<---\n","  41106/500000: episode: 1069, duration: 5.459s, episode steps:  63, steps per second:  12, episode reward: 155.000, mean reward:  2.460 [-25.000, 22.000], mean action: 1.429 [0.000, 2.000],  loss: 250.581840, mean_q: 82.225297, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1398916:<---\n","  41165/500000: episode: 1070, duration: 5.162s, episode steps:  59, steps per second:  11, episode reward: 102.000, mean reward:  1.729 [-25.000, 22.000], mean action: 1.271 [0.000, 2.000],  loss: 244.770741, mean_q: 83.511042, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1400221:<---\n","  41202/500000: episode: 1071, duration: 3.257s, episode steps:  37, steps per second:  11, episode reward: 227.000, mean reward:  6.135 [-25.000, 22.000], mean action: 1.135 [0.000, 2.000],  loss: 257.994592, mean_q: 82.951243, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1401526:<---\n","  41219/500000: episode: 1072, duration: 1.627s, episode steps:  17, steps per second:  10, episode reward: 51.000, mean reward:  3.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 259.900360, mean_q: 82.068682, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1402832:<---\n","  41266/500000: episode: 1073, duration: 4.138s, episode steps:  47, steps per second:  11, episode reward: 237.000, mean reward:  5.043 [-25.000, 22.000], mean action: 1.170 [0.000, 2.000],  loss: 261.293183, mean_q: 83.842866, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1404134:<---\n","  41283/500000: episode: 1074, duration: 1.604s, episode steps:  17, steps per second:  11, episode reward: 151.000, mean reward:  8.882 [-25.000, 22.000], mean action: 1.235 [0.000, 2.000],  loss: 235.814897, mean_q: 87.725171, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1405442:<---\n","  41308/500000: episode: 1075, duration: 2.232s, episode steps:  25, steps per second:  11, episode reward: 239.000, mean reward:  9.560 [-25.000, 22.000], mean action: 1.320 [0.000, 2.000],  loss: 231.197474, mean_q: 85.096942, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1406749:<---\n","  41336/500000: episode: 1076, duration: 2.530s, episode steps:  28, steps per second:  11, episode reward: 98.000, mean reward:  3.500 [-25.000, 22.000], mean action: 1.321 [0.000, 2.000],  loss: 263.652401, mean_q: 81.732773, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1408050:<---\n","  41349/500000: episode: 1077, duration: 1.233s, episode steps:  13, steps per second:  11, episode reward: 79.000, mean reward:  6.077 [-25.000, 22.000], mean action: 0.769 [0.000, 2.000],  loss: 267.913221, mean_q: 83.094032, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1409355:<---\n","  41379/500000: episode: 1078, duration: 2.660s, episode steps:  30, steps per second:  11, episode reward: 189.000, mean reward:  6.300 [-25.000, 22.000], mean action: 1.300 [0.000, 2.000],  loss: 237.126241, mean_q: 83.255432, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1410658:<---\n","  41443/500000: episode: 1079, duration: 5.531s, episode steps:  64, steps per second:  12, episode reward: 186.000, mean reward:  2.906 [-25.000, 22.000], mean action: 1.594 [0.000, 2.000],  loss: 249.531170, mean_q: 82.836940, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1411968:<---\n","  41469/500000: episode: 1080, duration: 2.363s, episode steps:  26, steps per second:  11, episode reward: 142.000, mean reward:  5.462 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 269.801227, mean_q: 82.945895, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1413277:<---\n","  41509/500000: episode: 1081, duration: 3.506s, episode steps:  40, steps per second:  11, episode reward: 214.000, mean reward:  5.350 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 243.611681, mean_q: 82.596329, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1414581:<---\n","  41547/500000: episode: 1082, duration: 3.417s, episode steps:  38, steps per second:  11, episode reward: -33.000, mean reward: -0.868 [-25.000, 22.000], mean action: 1.158 [0.000, 2.000],  loss: 253.916838, mean_q: 82.480591, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1415887:<---\n","  41583/500000: episode: 1083, duration: 3.234s, episode steps:  36, steps per second:  11, episode reward: 255.000, mean reward:  7.083 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 254.379520, mean_q: 82.019346, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1417197:<---\n","  41612/500000: episode: 1084, duration: 2.590s, episode steps:  29, steps per second:  11, episode reward: 227.000, mean reward:  7.828 [-25.000, 22.000], mean action: 0.931 [0.000, 2.000],  loss: 241.658550, mean_q: 83.680350, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1418503:<---\n","  41643/500000: episode: 1085, duration: 2.741s, episode steps:  31, steps per second:  11, episode reward: -78.000, mean reward: -2.516 [-25.000, 22.000], mean action: 1.710 [0.000, 2.000],  loss: 209.851377, mean_q: 83.115737, mean_eps: 0.100000\n"," 1 : 5  \n","--->:1419804:<---\n","  41651/500000: episode: 1086, duration: 0.818s, episode steps:   8, steps per second:  10, episode reward: 16.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 227.012169, mean_q: 85.013421, mean_eps: 0.100000\n"," 1 : 5  \n","--->:1421107:<---\n","  41668/500000: episode: 1087, duration: 1.568s, episode steps:  17, steps per second:  11, episode reward: 123.000, mean reward:  7.235 [-25.000, 22.000], mean action: 0.765 [0.000, 2.000],  loss: 226.822316, mean_q: 83.469622, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1422412:<---\n","  41692/500000: episode: 1088, duration: 2.178s, episode steps:  24, steps per second:  11, episode reward: 192.000, mean reward:  8.000 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 223.798701, mean_q: 83.189384, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1423719:<---\n","  41722/500000: episode: 1089, duration: 2.679s, episode steps:  30, steps per second:  11, episode reward: 57.000, mean reward:  1.900 [-25.000, 22.000], mean action: 1.533 [0.000, 2.000],  loss: 235.653088, mean_q: 84.525325, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1425029:<---\n","  41751/500000: episode: 1090, duration: 2.564s, episode steps:  29, steps per second:  11, episode reward: -68.000, mean reward: -2.345 [-25.000, 22.000], mean action: 1.310 [0.000, 2.000],  loss: 228.089733, mean_q: 82.742738, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1426337:<---\n","  41789/500000: episode: 1091, duration: 3.384s, episode steps:  38, steps per second:  11, episode reward: 211.000, mean reward:  5.553 [-25.000, 22.000], mean action: 1.421 [0.000, 2.000],  loss: 215.128018, mean_q: 82.470337, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1427641:<---\n","  41821/500000: episode: 1092, duration: 2.829s, episode steps:  32, steps per second:  11, episode reward: 142.000, mean reward:  4.438 [-25.000, 22.000], mean action: 1.469 [0.000, 2.000],  loss: 224.182106, mean_q: 85.841704, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1428951:<---\n","  41847/500000: episode: 1093, duration: 2.352s, episode steps:  26, steps per second:  11, episode reward: 54.000, mean reward:  2.077 [-25.000, 22.000], mean action: 1.423 [0.000, 2.000],  loss: 235.338673, mean_q: 84.198869, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1430252:<---\n","  41872/500000: episode: 1094, duration: 2.264s, episode steps:  25, steps per second:  11, episode reward: 117.000, mean reward:  4.680 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 233.778987, mean_q: 84.042914, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1431554:<---\n","  41896/500000: episode: 1095, duration: 2.246s, episode steps:  24, steps per second:  11, episode reward: 67.000, mean reward:  2.792 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 240.217494, mean_q: 84.838815, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1432860:<---\n","  41966/500000: episode: 1096, duration: 6.088s, episode steps:  70, steps per second:  11, episode reward: 331.000, mean reward:  4.729 [-25.000, 22.000], mean action: 1.386 [0.000, 2.000],  loss: 240.258713, mean_q: 82.158534, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1434169:<---\n","  42003/500000: episode: 1097, duration: 3.291s, episode steps:  37, steps per second:  11, episode reward: 102.000, mean reward:  2.757 [-25.000, 22.000], mean action: 0.730 [0.000, 2.000],  loss: 233.946742, mean_q: 83.725719, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1435470:<---\n","  42033/500000: episode: 1098, duration: 2.680s, episode steps:  30, steps per second:  11, episode reward: 61.000, mean reward:  2.033 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 280.006978, mean_q: 83.224474, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1436775:<---\n","  42077/500000: episode: 1099, duration: 3.913s, episode steps:  44, steps per second:  11, episode reward:  1.000, mean reward:  0.023 [-25.000, 22.000], mean action: 1.409 [0.000, 2.000],  loss: 216.221154, mean_q: 84.635944, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1438080:<---\n","  42102/500000: episode: 1100, duration: 2.258s, episode steps:  25, steps per second:  11, episode reward: 280.000, mean reward: 11.200 [-25.000, 22.000], mean action: 0.960 [0.000, 2.000],  loss: 234.733873, mean_q: 83.676303, mean_eps: 0.100000\n"," 1 : 17  \n","--->:1439383:<---\n","  42147/500000: episode: 1101, duration: 3.953s, episode steps:  45, steps per second:  11, episode reward: 193.000, mean reward:  4.289 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 251.927512, mean_q: 83.185471, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1440684:<---\n","  42178/500000: episode: 1102, duration: 2.736s, episode steps:  31, steps per second:  11, episode reward: 82.000, mean reward:  2.645 [-25.000, 22.000], mean action: 1.613 [0.000, 2.000],  loss: 239.598232, mean_q: 82.260536, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1441991:<---\n","  42201/500000: episode: 1103, duration: 2.136s, episode steps:  23, steps per second:  11, episode reward: 101.000, mean reward:  4.391 [-25.000, 22.000], mean action: 1.304 [0.000, 2.000],  loss: 207.042323, mean_q: 83.234151, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1443298:<---\n","  42213/500000: episode: 1104, duration: 1.227s, episode steps:  12, steps per second:  10, episode reward: 101.000, mean reward:  8.417 [-25.000, 22.000], mean action: 0.583 [0.000, 2.000],  loss: 223.151127, mean_q: 86.275863, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1444603:<---\n","  42223/500000: episode: 1105, duration: 1.047s, episode steps:  10, steps per second:  10, episode reward: 38.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 218.902296, mean_q: 83.173005, mean_eps: 0.100000\n"," 1 : 6  \n","--->:1445905:<---\n","  42249/500000: episode: 1106, duration: 2.498s, episode steps:  26, steps per second:  10, episode reward: 249.000, mean reward:  9.577 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 235.347216, mean_q: 84.454201, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1447212:<---\n","  42342/500000: episode: 1107, duration: 8.457s, episode steps:  93, steps per second:  11, episode reward: 300.000, mean reward:  3.226 [-25.000, 22.000], mean action: 1.484 [0.000, 2.000],  loss: 239.851663, mean_q: 83.723120, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1448513:<---\n","  42423/500000: episode: 1108, duration: 7.889s, episode steps:  81, steps per second:  10, episode reward: 147.000, mean reward:  1.815 [-25.000, 22.000], mean action: 1.136 [0.000, 2.000],  loss: 233.325842, mean_q: 84.316357, mean_eps: 0.100000\n"," 1 : 43  \n","--->:1449816:<---\n","  42592/500000: episode: 1109, duration: 15.108s, episode steps: 169, steps per second:  11, episode reward: 1230.000, mean reward:  7.278 [-25.000, 22.000], mean action: 1.006 [0.000, 2.000],  loss: 247.827662, mean_q: 83.143221, mean_eps: 0.100000\n"," 1 : 120  \n","--->:1451120:<---\n","  42652/500000: episode: 1110, duration: 5.212s, episode steps:  60, steps per second:  12, episode reward: 291.000, mean reward:  4.850 [-25.000, 22.000], mean action: 0.983 [0.000, 2.000],  loss: 245.497476, mean_q: 84.292414, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1452422:<---\n","  42739/500000: episode: 1111, duration: 7.502s, episode steps:  87, steps per second:  12, episode reward: 465.000, mean reward:  5.345 [-25.000, 22.000], mean action: 0.920 [0.000, 2.000],  loss: 251.796789, mean_q: 83.680660, mean_eps: 0.100000\n"," 1 : 66  \n","--->:1453724:<---\n","  42790/500000: episode: 1112, duration: 4.490s, episode steps:  51, steps per second:  11, episode reward: 256.000, mean reward:  5.020 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 247.131131, mean_q: 82.858549, mean_eps: 0.100000\n"," 1 : 33  \n","--->:1455029:<---\n","  42866/500000: episode: 1113, duration: 6.537s, episode steps:  76, steps per second:  12, episode reward: 545.000, mean reward:  7.171 [-25.000, 22.000], mean action: 1.197 [0.000, 2.000],  loss: 229.076522, mean_q: 83.242667, mean_eps: 0.100000\n"," 1 : 44  \n","--->:1456335:<---\n","  42924/500000: episode: 1114, duration: 5.082s, episode steps:  58, steps per second:  11, episode reward: 197.000, mean reward:  3.397 [-25.000, 22.000], mean action: 0.879 [0.000, 2.000],  loss: 238.376001, mean_q: 82.957796, mean_eps: 0.100000\n"," 1 : 41  \n","--->:1457640:<---\n","  43011/500000: episode: 1115, duration: 7.586s, episode steps:  87, steps per second:  11, episode reward: 502.000, mean reward:  5.770 [-25.000, 22.000], mean action: 1.069 [0.000, 2.000],  loss: 224.536148, mean_q: 82.598616, mean_eps: 0.100000\n"," 1 : 57  \n","--->:1458945:<---\n","  43159/500000: episode: 1116, duration: 12.593s, episode steps: 148, steps per second:  12, episode reward: 875.000, mean reward:  5.912 [-25.000, 22.000], mean action: 0.777 [0.000, 2.000],  loss: 250.934916, mean_q: 83.405523, mean_eps: 0.100000\n"," 1 : 106  \n","--->:1460250:<---\n","  43306/500000: episode: 1117, duration: 12.755s, episode steps: 147, steps per second:  12, episode reward: 1006.000, mean reward:  6.844 [-25.000, 22.000], mean action: 1.061 [0.000, 2.000],  loss: 241.161600, mean_q: 83.009275, mean_eps: 0.100000\n"," 1 : 97  \n","--->:1461555:<---\n","  43377/500000: episode: 1118, duration: 6.152s, episode steps:  71, steps per second:  12, episode reward: 222.000, mean reward:  3.127 [-25.000, 22.000], mean action: 1.225 [0.000, 2.000],  loss: 267.395796, mean_q: 83.182077, mean_eps: 0.100000\n"," 1 : 40  \n","--->:1462864:<---\n","  43455/500000: episode: 1119, duration: 6.741s, episode steps:  78, steps per second:  12, episode reward: 196.000, mean reward:  2.513 [-25.000, 22.000], mean action: 1.538 [0.000, 2.000],  loss: 232.398249, mean_q: 83.796818, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1464171:<---\n","  43519/500000: episode: 1120, duration: 5.589s, episode steps:  64, steps per second:  11, episode reward: 388.000, mean reward:  6.062 [-25.000, 22.000], mean action: 0.922 [0.000, 2.000],  loss: 264.024217, mean_q: 83.517188, mean_eps: 0.100000\n"," 1 : 39  \n","--->:1465473:<---\n","  43552/500000: episode: 1121, duration: 2.962s, episode steps:  33, steps per second:  11, episode reward: 309.000, mean reward:  9.364 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 216.950805, mean_q: 82.851109, mean_eps: 0.100000\n"," 1 : 29  \n","--->:1466774:<---\n","  43631/500000: episode: 1122, duration: 6.832s, episode steps:  79, steps per second:  12, episode reward: 689.000, mean reward:  8.722 [-25.000, 22.000], mean action: 1.203 [0.000, 2.000],  loss: 253.757123, mean_q: 83.483694, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1468078:<---\n","  43733/500000: episode: 1123, duration: 8.772s, episode steps: 102, steps per second:  12, episode reward: 579.000, mean reward:  5.676 [-25.000, 22.000], mean action: 0.569 [0.000, 2.000],  loss: 236.651619, mean_q: 83.734718, mean_eps: 0.100000\n"," 1 : 84  \n","--->:1469382:<---\n","  43842/500000: episode: 1124, duration: 9.393s, episode steps: 109, steps per second:  12, episode reward: 858.000, mean reward:  7.872 [-25.000, 22.000], mean action: 0.899 [0.000, 2.000],  loss: 234.838731, mean_q: 82.301496, mean_eps: 0.100000\n"," 1 : 86  \n","--->:1470691:<---\n","  43909/500000: episode: 1125, duration: 5.791s, episode steps:  67, steps per second:  12, episode reward: 172.000, mean reward:  2.567 [-25.000, 22.000], mean action: 0.970 [0.000, 2.000],  loss: 231.247561, mean_q: 84.359674, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1471992:<---\n","  43962/500000: episode: 1126, duration: 4.685s, episode steps:  53, steps per second:  11, episode reward: 407.000, mean reward:  7.679 [-25.000, 22.000], mean action: 0.887 [0.000, 2.000],  loss: 230.307831, mean_q: 82.893862, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1473295:<---\n","  43999/500000: episode: 1127, duration: 3.350s, episode steps:  37, steps per second:  11, episode reward: 469.000, mean reward: 12.676 [-25.000, 22.000], mean action: 0.865 [0.000, 2.000],  loss: 256.091109, mean_q: 84.700058, mean_eps: 0.100000\n"," 1 : 32  \n","--->:1474604:<---\n","  44032/500000: episode: 1128, duration: 2.953s, episode steps:  33, steps per second:  11, episode reward: 309.000, mean reward:  9.364 [-25.000, 22.000], mean action: 0.606 [0.000, 2.000],  loss: 271.848360, mean_q: 82.449271, mean_eps: 0.100000\n"," 1 : 29  \n","--->:1475906:<---\n","  44073/500000: episode: 1129, duration: 3.676s, episode steps:  41, steps per second:  11, episode reward: 161.000, mean reward:  3.927 [-25.000, 22.000], mean action: 1.390 [0.000, 2.000],  loss: 224.162006, mean_q: 83.329237, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1477214:<---\n","  44156/500000: episode: 1130, duration: 7.298s, episode steps:  83, steps per second:  11, episode reward: 270.000, mean reward:  3.253 [-25.000, 22.000], mean action: 0.892 [0.000, 2.000],  loss: 234.895911, mean_q: 84.164791, mean_eps: 0.100000\n"," 1 : 55  \n","--->:1478523:<---\n","  44182/500000: episode: 1131, duration: 2.397s, episode steps:  26, steps per second:  11, episode reward: 271.000, mean reward: 10.423 [-25.000, 22.000], mean action: 0.885 [0.000, 2.000],  loss: 256.447208, mean_q: 84.126422, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1479833:<---\n","  44236/500000: episode: 1132, duration: 4.741s, episode steps:  54, steps per second:  11, episode reward: 366.000, mean reward:  6.778 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 245.130635, mean_q: 84.255453, mean_eps: 0.100000\n"," 1 : 38  \n","--->:1481137:<---\n","  44261/500000: episode: 1133, duration: 2.264s, episode steps:  25, steps per second:  11, episode reward: 136.000, mean reward:  5.440 [-25.000, 22.000], mean action: 0.920 [0.000, 2.000],  loss: 273.176562, mean_q: 83.997249, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1482442:<---\n","  44273/500000: episode: 1134, duration: 1.171s, episode steps:  12, steps per second:  10, episode reward: -34.000, mean reward: -2.833 [-25.000, 22.000], mean action: 1.417 [1.000, 2.000],  loss: 225.121816, mean_q: 87.222579, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1483751:<---\n","  44301/500000: episode: 1135, duration: 2.589s, episode steps:  28, steps per second:  11, episode reward: 76.000, mean reward:  2.714 [-25.000, 22.000], mean action: 1.393 [0.000, 2.000],  loss: 240.917719, mean_q: 83.450614, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1485052:<---\n","  44352/500000: episode: 1136, duration: 4.535s, episode steps:  51, steps per second:  11, episode reward: 111.000, mean reward:  2.176 [-25.000, 22.000], mean action: 1.412 [0.000, 2.000],  loss: 232.427994, mean_q: 82.946050, mean_eps: 0.100000\n"," 1 : 20  \n","--->:1486355:<---\n","  44376/500000: episode: 1137, duration: 2.189s, episode steps:  24, steps per second:  11, episode reward: 164.000, mean reward:  6.833 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 224.612483, mean_q: 82.841726, mean_eps: 0.100000\n"," 1 : 16  \n","--->:1487657:<---\n","  44396/500000: episode: 1138, duration: 1.854s, episode steps:  20, steps per second:  11, episode reward: 76.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 264.819809, mean_q: 84.063745, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1488966:<---\n","  44438/500000: episode: 1139, duration: 3.756s, episode steps:  42, steps per second:  11, episode reward: 80.000, mean reward:  1.905 [-25.000, 22.000], mean action: 1.095 [0.000, 2.000],  loss: 229.786289, mean_q: 84.005393, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1490274:<---\n","  44484/500000: episode: 1140, duration: 4.043s, episode steps:  46, steps per second:  11, episode reward: 26.000, mean reward:  0.565 [-25.000, 22.000], mean action: 1.565 [0.000, 2.000],  loss: 242.815085, mean_q: 83.014217, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1491582:<---\n","  44503/500000: episode: 1141, duration: 1.755s, episode steps:  19, steps per second:  11, episode reward: -137.000, mean reward: -7.211 [-25.000, 22.000], mean action: 1.105 [0.000, 2.000],  loss: 226.782153, mean_q: 84.273113, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1492885:<---\n","  44547/500000: episode: 1142, duration: 3.866s, episode steps:  44, steps per second:  11, episode reward: 369.000, mean reward:  8.386 [-25.000, 22.000], mean action: 0.705 [0.000, 2.000],  loss: 220.411989, mean_q: 84.357412, mean_eps: 0.100000\n"," 1 : 36  \n","--->:1494193:<---\n","  44582/500000: episode: 1143, duration: 3.168s, episode steps:  35, steps per second:  11, episode reward: 227.000, mean reward:  6.486 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 257.365451, mean_q: 84.051805, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1495501:<---\n","  44629/500000: episode: 1144, duration: 4.215s, episode steps:  47, steps per second:  11, episode reward: -36.000, mean reward: -0.766 [-25.000, 22.000], mean action: 1.170 [0.000, 2.000],  loss: 232.881537, mean_q: 83.583821, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1496809:<---\n","  44679/500000: episode: 1145, duration: 4.465s, episode steps:  50, steps per second:  11, episode reward: 208.000, mean reward:  4.160 [-25.000, 22.000], mean action: 1.520 [0.000, 2.000],  loss: 237.823145, mean_q: 83.585377, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1498110:<---\n","  44751/500000: episode: 1146, duration: 6.237s, episode steps:  72, steps per second:  12, episode reward: 429.000, mean reward:  5.958 [-25.000, 22.000], mean action: 1.014 [0.000, 2.000],  loss: 220.105494, mean_q: 85.752851, mean_eps: 0.100000\n"," 1 : 43  \n","--->:1499418:<---\n","  44805/500000: episode: 1147, duration: 4.717s, episode steps:  54, steps per second:  11, episode reward: 381.000, mean reward:  7.056 [-25.000, 22.000], mean action: 1.315 [0.000, 2.000],  loss: 256.530519, mean_q: 83.633783, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1500721:<---\n","  44857/500000: episode: 1148, duration: 4.617s, episode steps:  52, steps per second:  11, episode reward: 253.000, mean reward:  4.865 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 219.505912, mean_q: 82.632005, mean_eps: 0.100000\n"," 1 : 35  \n","--->:1502029:<---\n","  44924/500000: episode: 1149, duration: 5.893s, episode steps:  67, steps per second:  11, episode reward: 297.000, mean reward:  4.433 [-25.000, 22.000], mean action: 1.045 [0.000, 2.000],  loss: 226.121508, mean_q: 83.721709, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1503330:<---\n","  44967/500000: episode: 1150, duration: 3.828s, episode steps:  43, steps per second:  11, episode reward: 202.000, mean reward:  4.698 [-25.000, 22.000], mean action: 1.279 [0.000, 2.000],  loss: 257.412695, mean_q: 83.488230, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1504637:<---\n","  45014/500000: episode: 1151, duration: 4.231s, episode steps:  47, steps per second:  11, episode reward: 319.000, mean reward:  6.787 [-25.000, 22.000], mean action: 0.447 [0.000, 2.000],  loss: 214.127731, mean_q: 82.019908, mean_eps: 0.100000\n"," 1 : 38  \n","--->:1505943:<---\n","  45065/500000: episode: 1152, duration: 4.469s, episode steps:  51, steps per second:  11, episode reward: 357.000, mean reward:  7.000 [-25.000, 22.000], mean action: 0.412 [0.000, 2.000],  loss: 244.578911, mean_q: 83.289464, mean_eps: 0.100000\n"," 1 : 44  \n","--->:1507248:<---\n","  45146/500000: episode: 1153, duration: 6.990s, episode steps:  81, steps per second:  12, episode reward: 317.000, mean reward:  3.914 [-25.000, 22.000], mean action: 1.074 [0.000, 2.000],  loss: 239.793210, mean_q: 83.572833, mean_eps: 0.100000\n"," 1 : 55  \n","--->:1508554:<---\n","  45209/500000: episode: 1154, duration: 5.545s, episode steps:  63, steps per second:  11, episode reward: 278.000, mean reward:  4.413 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 237.551911, mean_q: 84.344311, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1509858:<---\n","  45252/500000: episode: 1155, duration: 3.803s, episode steps:  43, steps per second:  11, episode reward: 196.000, mean reward:  4.558 [-25.000, 22.000], mean action: 1.233 [0.000, 2.000],  loss: 252.471147, mean_q: 83.888199, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1511164:<---\n","  45331/500000: episode: 1156, duration: 6.919s, episode steps:  79, steps per second:  11, episode reward: 325.000, mean reward:  4.114 [-25.000, 22.000], mean action: 1.228 [0.000, 2.000],  loss: 245.152645, mean_q: 82.939585, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1512472:<---\n","  45402/500000: episode: 1157, duration: 6.797s, episode steps:  71, steps per second:  10, episode reward: 190.000, mean reward:  2.676 [-25.000, 22.000], mean action: 1.380 [0.000, 2.000],  loss: 257.907347, mean_q: 84.242542, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1513778:<---\n","  45482/500000: episode: 1158, duration: 7.195s, episode steps:  80, steps per second:  11, episode reward: 307.000, mean reward:  3.837 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 242.696230, mean_q: 82.514816, mean_eps: 0.100000\n"," 1 : 46  \n","--->:1515082:<---\n","  45534/500000: episode: 1159, duration: 4.653s, episode steps:  52, steps per second:  11, episode reward: 312.000, mean reward:  6.000 [-25.000, 22.000], mean action: 1.404 [0.000, 2.000],  loss: 222.765890, mean_q: 84.696493, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1516387:<---\n","  45592/500000: episode: 1160, duration: 5.090s, episode steps:  58, steps per second:  11, episode reward: 372.000, mean reward:  6.414 [-25.000, 22.000], mean action: 1.086 [0.000, 2.000],  loss: 244.348387, mean_q: 83.624378, mean_eps: 0.100000\n"," 1 : 34  \n","--->:1517696:<---\n","  45627/500000: episode: 1161, duration: 3.174s, episode steps:  35, steps per second:  11, episode reward: 26.000, mean reward:  0.743 [-25.000, 22.000], mean action: 1.486 [0.000, 2.000],  loss: 252.873525, mean_q: 83.602517, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1519001:<---\n","  45649/500000: episode: 1162, duration: 2.080s, episode steps:  22, steps per second:  11, episode reward: 145.000, mean reward:  6.591 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 255.531031, mean_q: 82.373172, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1520311:<---\n","  45664/500000: episode: 1163, duration: 1.429s, episode steps:  15, steps per second:  10, episode reward: 120.000, mean reward:  8.000 [-25.000, 22.000], mean action: 0.733 [0.000, 2.000],  loss: 244.809721, mean_q: 84.170326, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1521617:<---\n","  45712/500000: episode: 1164, duration: 4.368s, episode steps:  48, steps per second:  11, episode reward: 95.000, mean reward:  1.979 [-25.000, 22.000], mean action: 1.542 [0.000, 2.000],  loss: 231.603635, mean_q: 84.510417, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1522927:<---\n","  45771/500000: episode: 1165, duration: 5.176s, episode steps:  59, steps per second:  11, episode reward: 152.000, mean reward:  2.576 [-25.000, 22.000], mean action: 1.356 [0.000, 2.000],  loss: 232.830160, mean_q: 83.479784, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1524232:<---\n","  45804/500000: episode: 1166, duration: 2.950s, episode steps:  33, steps per second:  11, episode reward: 145.000, mean reward:  4.394 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 250.321977, mean_q: 81.269777, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1525535:<---\n","  45846/500000: episode: 1167, duration: 3.710s, episode steps:  42, steps per second:  11, episode reward: 174.000, mean reward:  4.143 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.959578, mean_q: 83.828658, mean_eps: 0.100000\n"," 1 : 25  \n","--->:1526842:<---\n","  45888/500000: episode: 1168, duration: 3.724s, episode steps:  42, steps per second:  11, episode reward: 30.000, mean reward:  0.714 [-25.000, 22.000], mean action: 1.024 [0.000, 2.000],  loss: 230.451572, mean_q: 83.777714, mean_eps: 0.100000\n"," 1 : 27  \n","--->:1528148:<---\n","  45945/500000: episode: 1169, duration: 4.909s, episode steps:  57, steps per second:  12, episode reward: 240.000, mean reward:  4.211 [-25.000, 22.000], mean action: 1.211 [0.000, 2.000],  loss: 228.282360, mean_q: 85.009509, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1529458:<---\n","  45994/500000: episode: 1170, duration: 4.315s, episode steps:  49, steps per second:  11, episode reward: 61.000, mean reward:  1.245 [-25.000, 22.000], mean action: 1.408 [0.000, 2.000],  loss: 266.031434, mean_q: 82.108116, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1530761:<---\n","  46058/500000: episode: 1171, duration: 5.921s, episode steps:  64, steps per second:  11, episode reward: 239.000, mean reward:  3.734 [-25.000, 22.000], mean action: 1.734 [0.000, 2.000],  loss: 253.315735, mean_q: 82.333080, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1532069:<---\n","  46085/500000: episode: 1172, duration: 2.559s, episode steps:  27, steps per second:  11, episode reward: 208.000, mean reward:  7.704 [-25.000, 22.000], mean action: 1.074 [0.000, 2.000],  loss: 235.838044, mean_q: 86.546304, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1533374:<---\n","  46133/500000: episode: 1173, duration: 4.503s, episode steps:  48, steps per second:  11, episode reward: 274.000, mean reward:  5.708 [-25.000, 22.000], mean action: 1.417 [0.000, 2.000],  loss: 253.546979, mean_q: 84.006077, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1534681:<---\n","  46220/500000: episode: 1174, duration: 7.936s, episode steps:  87, steps per second:  11, episode reward: 237.000, mean reward:  2.724 [-25.000, 22.000], mean action: 1.448 [0.000, 2.000],  loss: 237.251679, mean_q: 82.642630, mean_eps: 0.100000\n"," 1 : 30  \n","--->:1535990:<---\n","  46233/500000: episode: 1175, duration: 1.234s, episode steps:  13, steps per second:  11, episode reward: 192.000, mean reward: 14.769 [-25.000, 22.000], mean action: 0.846 [0.000, 1.000],  loss: 237.020052, mean_q: 82.604703, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1537291:<---\n","  46281/500000: episode: 1176, duration: 4.312s, episode steps:  48, steps per second:  11, episode reward: 192.000, mean reward:  4.000 [-25.000, 22.000], mean action: 1.625 [0.000, 2.000],  loss: 244.748183, mean_q: 82.481618, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1538593:<---\n","  46311/500000: episode: 1177, duration: 2.864s, episode steps:  30, steps per second:  10, episode reward: 214.000, mean reward:  7.133 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 236.531685, mean_q: 84.394234, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1539895:<---\n","  46336/500000: episode: 1178, duration: 2.486s, episode steps:  25, steps per second:  10, episode reward: 26.000, mean reward:  1.040 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 231.775827, mean_q: 83.800648, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1541199:<---\n","  46373/500000: episode: 1179, duration: 3.351s, episode steps:  37, steps per second:  11, episode reward: 246.000, mean reward:  6.649 [-25.000, 22.000], mean action: 1.108 [0.000, 2.000],  loss: 249.820290, mean_q: 83.815220, mean_eps: 0.100000\n"," 1 : 24  \n","--->:1542502:<---\n","  46395/500000: episode: 1180, duration: 1.992s, episode steps:  22, steps per second:  11, episode reward:  1.000, mean reward:  0.045 [-25.000, 22.000], mean action: 1.045 [0.000, 2.000],  loss: 258.534952, mean_q: 83.470254, mean_eps: 0.100000\n"," 1 : 15  \n","--->:1543807:<---\n","  46434/500000: episode: 1181, duration: 3.509s, episode steps:  39, steps per second:  11, episode reward: 120.000, mean reward:  3.077 [-25.000, 22.000], mean action: 1.538 [0.000, 2.000],  loss: 253.364970, mean_q: 83.009115, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1545116:<---\n","  46450/500000: episode: 1182, duration: 1.517s, episode steps:  16, steps per second:  11, episode reward: 26.000, mean reward:  1.625 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 216.002314, mean_q: 87.488600, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1546423:<---\n","  46470/500000: episode: 1183, duration: 1.833s, episode steps:  20, steps per second:  11, episode reward: 192.000, mean reward:  9.600 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 261.549405, mean_q: 83.796473, mean_eps: 0.100000\n"," 1 : 13  \n","--->:1547729:<---\n","  46501/500000: episode: 1184, duration: 2.706s, episode steps:  31, steps per second:  11, episode reward: 20.000, mean reward:  0.645 [-25.000, 22.000], mean action: 1.226 [0.000, 2.000],  loss: 237.165996, mean_q: 82.523855, mean_eps: 0.100000\n"," 1 : 18  \n","--->:1549036:<---\n","  46519/500000: episode: 1185, duration: 1.689s, episode steps:  18, steps per second:  11, episode reward: 10.000, mean reward:  0.556 [-25.000, 22.000], mean action: 1.278 [0.000, 2.000],  loss: 252.896801, mean_q: 81.258202, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1550344:<---\n","  46541/500000: episode: 1186, duration: 2.009s, episode steps:  22, steps per second:  11, episode reward: -34.000, mean reward: -1.545 [-25.000, 22.000], mean action: 1.455 [0.000, 2.000],  loss: 226.585307, mean_q: 85.902844, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1551650:<---\n","  46554/500000: episode: 1187, duration: 1.243s, episode steps:  13, steps per second:  10, episode reward: -37.000, mean reward: -2.846 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 211.437495, mean_q: 83.497658, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1552954:<---\n","  46590/500000: episode: 1188, duration: 3.214s, episode steps:  36, steps per second:  11, episode reward: 101.000, mean reward:  2.806 [-25.000, 22.000], mean action: 1.528 [0.000, 2.000],  loss: 206.105945, mean_q: 84.557286, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1554260:<---\n","  46621/500000: episode: 1189, duration: 2.754s, episode steps:  31, steps per second:  11, episode reward: 108.000, mean reward:  3.484 [-25.000, 22.000], mean action: 1.097 [0.000, 2.000],  loss: 219.906620, mean_q: 83.009087, mean_eps: 0.100000\n"," 1 : 22  \n","--->:1555569:<---\n","  46638/500000: episode: 1190, duration: 1.591s, episode steps:  17, steps per second:  11, episode reward: -84.000, mean reward: -4.941 [-25.000, 22.000], mean action: 1.235 [0.000, 2.000],  loss: 270.117956, mean_q: 83.637866, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1556870:<---\n","  46658/500000: episode: 1191, duration: 1.833s, episode steps:  20, steps per second:  11, episode reward: 60.000, mean reward:  3.000 [-25.000, 22.000], mean action: 1.650 [1.000, 2.000],  loss: 242.224569, mean_q: 84.273302, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1558178:<---\n","  46670/500000: episode: 1192, duration: 1.179s, episode steps:  12, steps per second:  10, episode reward: 151.000, mean reward: 12.583 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 236.214494, mean_q: 84.513308, mean_eps: 0.100000\n"," 1 : 9  \n","--->:1559487:<---\n","  46678/500000: episode: 1193, duration: 0.811s, episode steps:   8, steps per second:  10, episode reward: 82.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.625 [0.000, 1.000],  loss: 201.538872, mean_q: 86.498213, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1560794:<---\n","  46689/500000: episode: 1194, duration: 1.082s, episode steps:  11, steps per second:  10, episode reward: 38.000, mean reward:  3.455 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 272.308924, mean_q: 84.033530, mean_eps: 0.100000\n"," 1 : 6  \n","--->:1562102:<---\n","  46696/500000: episode: 1195, duration: 0.725s, episode steps:   7, steps per second:  10, episode reward: 38.000, mean reward:  5.429 [-25.000, 22.000], mean action: 0.429 [0.000, 2.000],  loss: 223.797893, mean_q: 81.770600, mean_eps: 0.100000\n"," 1 : 6  \n","--->:1563403:<---\n","  46715/500000: episode: 1196, duration: 1.827s, episode steps:  19, steps per second:  10, episode reward: 79.000, mean reward:  4.158 [-25.000, 22.000], mean action: 1.158 [0.000, 2.000],  loss: 218.133984, mean_q: 85.571826, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1564713:<---\n","  46788/500000: episode: 1197, duration: 6.349s, episode steps:  73, steps per second:  11, episode reward: 183.000, mean reward:  2.507 [-25.000, 22.000], mean action: 1.575 [0.000, 2.000],  loss: 249.002889, mean_q: 82.963695, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1566022:<---\n","  46828/500000: episode: 1198, duration: 3.566s, episode steps:  40, steps per second:  11, episode reward: 46.000, mean reward:  1.150 [-25.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 240.379185, mean_q: 84.446988, mean_eps: 0.100000\n"," 1 : 32  \n","--->:1567329:<---\n","  46859/500000: episode: 1199, duration: 2.806s, episode steps:  31, steps per second:  11, episode reward: -65.000, mean reward: -2.097 [-25.000, 22.000], mean action: 1.290 [0.000, 2.000],  loss: 249.548469, mean_q: 82.389498, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1568632:<---\n","  46883/500000: episode: 1200, duration: 2.150s, episode steps:  24, steps per second:  11, episode reward: -62.000, mean reward: -2.583 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 204.615101, mean_q: 84.612582, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1569933:<---\n","  46890/500000: episode: 1201, duration: 0.740s, episode steps:   7, steps per second:   9, episode reward: 13.000, mean reward:  1.857 [-25.000, 22.000], mean action: 0.429 [0.000, 1.000],  loss: 261.334540, mean_q: 77.652424, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1571241:<---\n","  46906/500000: episode: 1202, duration: 1.522s, episode steps:  16, steps per second:  11, episode reward: 76.000, mean reward:  4.750 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 214.227609, mean_q: 83.880026, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1572549:<---\n","  46948/500000: episode: 1203, duration: 3.695s, episode steps:  42, steps per second:  11, episode reward: 196.000, mean reward:  4.667 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 200.922088, mean_q: 83.518892, mean_eps: 0.100000\n"," 1 : 26  \n","--->:1573858:<---\n","  46970/500000: episode: 1204, duration: 2.041s, episode steps:  22, steps per second:  11, episode reward: -87.000, mean reward: -3.955 [-25.000, 22.000], mean action: 1.273 [0.000, 2.000],  loss: 233.399406, mean_q: 81.973007, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1575165:<---\n","  47043/500000: episode: 1205, duration: 6.412s, episode steps:  73, steps per second:  11, episode reward: 297.000, mean reward:  4.068 [-25.000, 22.000], mean action: 1.233 [0.000, 2.000],  loss: 222.012096, mean_q: 84.298185, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1576472:<---\n","  47100/500000: episode: 1206, duration: 5.017s, episode steps:  57, steps per second:  11, episode reward: 313.000, mean reward:  5.491 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 259.131936, mean_q: 83.665461, mean_eps: 0.100000\n"," 1 : 42  \n","--->:1577779:<---\n","  47127/500000: episode: 1207, duration: 2.435s, episode steps:  27, steps per second:  11, episode reward: 101.000, mean reward:  3.741 [-25.000, 22.000], mean action: 1.370 [0.000, 2.000],  loss: 227.561340, mean_q: 85.191659, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1579083:<---\n","  47144/500000: episode: 1208, duration: 1.596s, episode steps:  17, steps per second:  11, episode reward: 26.000, mean reward:  1.529 [-25.000, 22.000], mean action: 0.824 [0.000, 2.000],  loss: 225.388117, mean_q: 83.591577, mean_eps: 0.100000\n"," 1 : 14  \n","--->:1580393:<---\n","  47162/500000: episode: 1209, duration: 1.732s, episode steps:  18, steps per second:  10, episode reward: 82.000, mean reward:  4.556 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 273.946037, mean_q: 79.943472, mean_eps: 0.100000\n"," 1 : 8  \n","--->:1581700:<---\n","  47175/500000: episode: 1210, duration: 1.247s, episode steps:  13, steps per second:  10, episode reward: 79.000, mean reward:  6.077 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 261.857201, mean_q: 85.326818, mean_eps: 0.100000\n"," 1 : 10  \n","--->:1583008:<---\n","  47190/500000: episode: 1211, duration: 1.417s, episode steps:  15, steps per second:  11, episode reward: 101.000, mean reward:  6.733 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 278.351108, mean_q: 84.508732, mean_eps: 0.100000\n"," 1 : 11  \n","--->:1584309:<---\n","  47201/500000: episode: 1212, duration: 1.060s, episode steps:  11, steps per second:  10, episode reward: -56.000, mean reward: -5.091 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 233.265962, mean_q: 82.985795, mean_eps: 0.100000\n"," 1 : 6  \n","--->:1585611:<---\n","  47206/500000: episode: 1213, duration: 0.562s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.600 [0.000, 2.000],  loss: 180.896286, mean_q: 85.658926, mean_eps: 0.100000\n"," 1 : 4  \n","--->:1586920:<---\n","  47215/500000: episode: 1214, duration: 0.875s, episode steps:   9, steps per second:  10, episode reward: 60.000, mean reward:  6.667 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 211.830624, mean_q: 84.531114, mean_eps: 0.100000\n"," 1 : 7  \n","--->:1588223:<---\n","  47227/500000: episode: 1215, duration: 1.165s, episode steps:  12, steps per second:  10, episode reward: 76.000, mean reward:  6.333 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 269.793115, mean_q: 81.639396, mean_eps: 0.100000\n"," 1 : 12  \n","--->:1589524:<---\n","  47251/500000: episode: 1216, duration: 2.175s, episode steps:  24, steps per second:  11, episode reward: 277.000, mean reward: 11.542 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 248.060723, mean_q: 83.799768, mean_eps: 0.100000\n"," 1 : 19  \n","--->:1590826:<---\n","  47286/500000: episode: 1217, duration: 3.132s, episode steps:  35, steps per second:  11, episode reward: 86.000, mean reward:  2.457 [-25.000, 22.000], mean action: 1.057 [0.000, 2.000],  loss: 224.349053, mean_q: 83.591078, mean_eps: 0.100000\n"," 1 : 21  \n","--->:1592131:<---\n","  47346/500000: episode: 1218, duration: 5.312s, episode steps:  60, steps per second:  11, episode reward: 318.000, mean reward:  5.300 [-25.000, 22.000], mean action: 1.483 [0.000, 2.000],  loss: 247.831354, mean_q: 83.606864, mean_eps: 0.100000\n"," 1 : 23  \n","--->:1593441:<---\n","  47430/500000: episode: 1219, duration: 7.339s, episode steps:  84, steps per second:  11, episode reward: 182.000, mean reward:  2.167 [-25.000, 22.000], mean action: 1.119 [0.000, 2.000],  loss: 230.473792, mean_q: 83.301768, mean_eps: 0.100000\n"," 1 : 51  \n","--->:1594744:<---\n","  47482/500000: episode: 1220, duration: 4.519s, episode steps:  52, steps per second:  12, episode reward: 193.000, mean reward:  3.712 [-25.000, 22.000], mean action: 1.288 [0.000, 2.000],  loss: 211.592868, mean_q: 84.225542, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1596051:<---\n","  47527/500000: episode: 1221, duration: 3.922s, episode steps:  45, steps per second:  11, episode reward: 287.000, mean reward:  6.378 [-25.000, 22.000], mean action: 0.978 [0.000, 2.000],  loss: 238.034456, mean_q: 84.040230, mean_eps: 0.100000\n"," 1 : 28  \n","--->:1597359:<---\n","  47572/500000: episode: 1222, duration: 3.911s, episode steps:  45, steps per second:  12, episode reward: 250.000, mean reward:  5.556 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 223.383439, mean_q: 83.633254, mean_eps: 0.100000\n"," 1 : 37  \n","--->:1598664:<---\n","  47595/500000: episode: 1223, duration: 2.067s, episode steps:  23, steps per second:  11, episode reward: 142.000, mean reward:  6.174 [-25.000, 22.000], mean action: 1.130 [0.000, 2.000],  loss: 236.324996, mean_q: 80.702661, mean_eps: 0.100000\n"," 1 : 15  \n","--->:2000:<---\n","  47613/500000: episode: 1224, duration: 1.715s, episode steps:  18, steps per second:  10, episode reward: -93.000, mean reward: -5.167 [-25.000, 22.000], mean action: 0.611 [0.000, 2.000],  loss: 251.430475, mean_q: 85.328570, mean_eps: 0.100000\n"," 1 : 15  \n","--->:3304:<---\n","  47627/500000: episode: 1225, duration: 1.336s, episode steps:  14, steps per second:  10, episode reward: 192.000, mean reward: 13.714 [-25.000, 22.000], mean action: 0.714 [0.000, 2.000],  loss: 236.284588, mean_q: 86.095889, mean_eps: 0.100000\n"," 1 : 13  \n","--->:4608:<---\n","  47639/500000: episode: 1226, duration: 1.162s, episode steps:  12, steps per second:  10, episode reward: -15.000, mean reward: -1.250 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 211.923864, mean_q: 81.827246, mean_eps: 0.100000\n"," 1 : 10  \n","--->:5913:<---\n","  47668/500000: episode: 1227, duration: 2.661s, episode steps:  29, steps per second:  11, episode reward: 64.000, mean reward:  2.207 [-25.000, 22.000], mean action: 0.724 [0.000, 2.000],  loss: 259.350266, mean_q: 83.028287, mean_eps: 0.100000\n"," 1 : 20  \n","--->:7221:<---\n","  47708/500000: episode: 1228, duration: 3.517s, episode steps:  40, steps per second:  11, episode reward: 337.000, mean reward:  8.425 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 248.538455, mean_q: 82.197526, mean_eps: 0.100000\n"," 1 : 26  \n","--->:8524:<---\n","  47749/500000: episode: 1229, duration: 3.735s, episode steps:  41, steps per second:  11, episode reward: 218.000, mean reward:  5.317 [-25.000, 22.000], mean action: 1.049 [0.000, 2.000],  loss: 241.263187, mean_q: 85.486593, mean_eps: 0.100000\n"," 1 : 27  \n","--->:9826:<---\n","  47814/500000: episode: 1230, duration: 6.116s, episode steps:  65, steps per second:  11, episode reward: 545.000, mean reward:  8.385 [-25.000, 22.000], mean action: 0.892 [0.000, 2.000],  loss: 248.256708, mean_q: 83.527026, mean_eps: 0.100000\n"," 1 : 44  \n","--->:11134:<---\n","  47921/500000: episode: 1231, duration: 9.306s, episode steps: 107, steps per second:  11, episode reward: 876.000, mean reward:  8.187 [-25.000, 22.000], mean action: 0.907 [0.000, 2.000],  loss: 229.465640, mean_q: 83.230377, mean_eps: 0.100000\n"," 1 : 74  \n","--->:12441:<---\n","  47985/500000: episode: 1232, duration: 5.595s, episode steps:  64, steps per second:  11, episode reward: 504.000, mean reward:  7.875 [-25.000, 22.000], mean action: 1.109 [0.000, 2.000],  loss: 266.969453, mean_q: 82.805923, mean_eps: 0.100000\n"," 1 : 40  \n","--->:13751:<---\n","  48006/500000: episode: 1233, duration: 1.938s, episode steps:  21, steps per second:  11, episode reward: 92.000, mean reward:  4.381 [-25.000, 22.000], mean action: 0.905 [0.000, 2.000],  loss: 244.041936, mean_q: 84.357319, mean_eps: 0.100000\n"," 1 : 17  \n","--->:15055:<---\n","  48060/500000: episode: 1234, duration: 4.775s, episode steps:  54, steps per second:  11, episode reward: 212.000, mean reward:  3.926 [-25.000, 22.000], mean action: 1.130 [0.000, 2.000],  loss: 236.373771, mean_q: 84.415834, mean_eps: 0.100000\n"," 1 : 31  \n","--->:16356:<---\n","  48092/500000: episode: 1235, duration: 2.928s, episode steps:  32, steps per second:  11, episode reward: 265.000, mean reward:  8.281 [-25.000, 22.000], mean action: 0.844 [0.000, 2.000],  loss: 246.046270, mean_q: 81.760041, mean_eps: 0.100000\n"," 1 : 27  \n","--->:17660:<---\n","  48141/500000: episode: 1236, duration: 4.390s, episode steps:  49, steps per second:  11, episode reward: 413.000, mean reward:  8.429 [-25.000, 22.000], mean action: 0.898 [0.000, 2.000],  loss: 245.166498, mean_q: 82.751174, mean_eps: 0.100000\n"," 1 : 38  \n","--->:18970:<---\n","  48175/500000: episode: 1237, duration: 3.084s, episode steps:  34, steps per second:  11, episode reward: 42.000, mean reward:  1.235 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 209.536019, mean_q: 84.246678, mean_eps: 0.100000\n"," 1 : 19  \n","--->:20271:<---\n","  48240/500000: episode: 1238, duration: 5.662s, episode steps:  65, steps per second:  11, episode reward: 127.000, mean reward:  1.954 [-25.000, 22.000], mean action: 1.415 [0.000, 2.000],  loss: 237.921774, mean_q: 83.376905, mean_eps: 0.100000\n"," 1 : 25  \n","--->:21577:<---\n","  48286/500000: episode: 1239, duration: 4.107s, episode steps:  46, steps per second:  11, episode reward: 363.000, mean reward:  7.891 [-25.000, 22.000], mean action: 0.543 [0.000, 2.000],  loss: 244.919480, mean_q: 83.839383, mean_eps: 0.100000\n"," 1 : 40  \n","--->:22883:<---\n","  48351/500000: episode: 1240, duration: 5.772s, episode steps:  65, steps per second:  11, episode reward: 489.000, mean reward:  7.523 [-25.000, 22.000], mean action: 0.708 [0.000, 2.000],  loss: 226.897070, mean_q: 83.252546, mean_eps: 0.100000\n"," 1 : 50  \n","--->:24184:<---\n","  48399/500000: episode: 1241, duration: 4.294s, episode steps:  48, steps per second:  11, episode reward: 269.000, mean reward:  5.604 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 224.021715, mean_q: 83.254228, mean_eps: 0.100000\n"," 1 : 40  \n","--->:25494:<---\n","  48482/500000: episode: 1242, duration: 7.241s, episode steps:  83, steps per second:  11, episode reward: 492.000, mean reward:  5.928 [-25.000, 22.000], mean action: 1.012 [0.000, 2.000],  loss: 238.057300, mean_q: 83.849055, mean_eps: 0.100000\n"," 1 : 48  \n","--->:26797:<---\n","  48568/500000: episode: 1243, duration: 7.453s, episode steps:  86, steps per second:  12, episode reward: 483.000, mean reward:  5.616 [-25.000, 22.000], mean action: 1.070 [0.000, 2.000],  loss: 241.745567, mean_q: 84.649354, mean_eps: 0.100000\n"," 1 : 54  \n","--->:28100:<---\n","  48625/500000: episode: 1244, duration: 5.016s, episode steps:  57, steps per second:  11, episode reward: 322.000, mean reward:  5.649 [-25.000, 22.000], mean action: 1.158 [0.000, 2.000],  loss: 249.339857, mean_q: 83.971804, mean_eps: 0.100000\n"," 1 : 36  \n","--->:29401:<---\n","  48675/500000: episode: 1245, duration: 4.472s, episode steps:  50, steps per second:  11, episode reward: 284.000, mean reward:  5.680 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 246.137920, mean_q: 83.211549, mean_eps: 0.100000\n"," 1 : 30  \n","--->:30705:<---\n","  48715/500000: episode: 1246, duration: 3.595s, episode steps:  40, steps per second:  11, episode reward: 177.000, mean reward:  4.425 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 257.982525, mean_q: 83.232641, mean_eps: 0.100000\n"," 1 : 23  \n","--->:32015:<---\n","  48793/500000: episode: 1247, duration: 6.905s, episode steps:  78, steps per second:  11, episode reward: 161.000, mean reward:  2.064 [-25.000, 22.000], mean action: 1.628 [0.000, 2.000],  loss: 248.476841, mean_q: 82.463833, mean_eps: 0.100000\n"," 1 : 18  \n","--->:33319:<---\n","  48819/500000: episode: 1248, duration: 2.347s, episode steps:  26, steps per second:  11, episode reward: -5.000, mean reward: -0.192 [-25.000, 22.000], mean action: 1.038 [0.000, 2.000],  loss: 238.543243, mean_q: 84.282179, mean_eps: 0.100000\n"," 1 : 19  \n","--->:34628:<---\n","  48851/500000: episode: 1249, duration: 2.971s, episode steps:  32, steps per second:  11, episode reward: 158.000, mean reward:  4.938 [-25.000, 22.000], mean action: 1.094 [0.000, 2.000],  loss: 228.726417, mean_q: 83.947191, mean_eps: 0.100000\n"," 1 : 20  \n","--->:35932:<---\n","  48930/500000: episode: 1250, duration: 6.731s, episode steps:  79, steps per second:  12, episode reward: 83.000, mean reward:  1.051 [-25.000, 22.000], mean action: 1.582 [0.000, 2.000],  loss: 233.389934, mean_q: 83.785250, mean_eps: 0.100000\n"," 1 : 23  \n","--->:37239:<---\n","  48970/500000: episode: 1251, duration: 3.565s, episode steps:  40, steps per second:  11, episode reward: -81.000, mean reward: -2.025 [-25.000, 22.000], mean action: 1.675 [0.000, 2.000],  loss: 225.811816, mean_q: 84.318728, mean_eps: 0.100000\n"," 1 : 7  \n","--->:38548:<---\n","  48989/500000: episode: 1252, duration: 1.764s, episode steps:  19, steps per second:  11, episode reward: 82.000, mean reward:  4.316 [-25.000, 22.000], mean action: 1.474 [0.000, 2.000],  loss: 213.320665, mean_q: 83.270232, mean_eps: 0.100000\n"," 1 : 8  \n","--->:39857:<---\n","  49024/500000: episode: 1253, duration: 3.170s, episode steps:  35, steps per second:  11, episode reward: 32.000, mean reward:  0.914 [-25.000, 22.000], mean action: 1.514 [0.000, 2.000],  loss: 251.767618, mean_q: 84.171197, mean_eps: 0.100000\n"," 1 : 10  \n","--->:41165:<---\n","  49043/500000: episode: 1254, duration: 1.775s, episode steps:  19, steps per second:  11, episode reward: 41.000, mean reward:  2.158 [-25.000, 22.000], mean action: 1.632 [0.000, 2.000],  loss: 267.941600, mean_q: 83.371311, mean_eps: 0.100000\n"," 1 : 4  \n","--->:42471:<---\n","  49087/500000: episode: 1255, duration: 3.894s, episode steps:  44, steps per second:  11, episode reward: 26.000, mean reward:  0.591 [-25.000, 22.000], mean action: 1.591 [0.000, 2.000],  loss: 217.161415, mean_q: 82.603593, mean_eps: 0.100000\n"," 1 : 14  \n","--->:43779:<---\n","  49104/500000: episode: 1256, duration: 1.618s, episode steps:  17, steps per second:  11, episode reward: 214.000, mean reward: 12.588 [-25.000, 22.000], mean action: 0.882 [0.000, 2.000],  loss: 208.498318, mean_q: 83.832008, mean_eps: 0.100000\n"," 1 : 14  \n","--->:45088:<---\n","  49111/500000: episode: 1257, duration: 0.729s, episode steps:   7, steps per second:  10, episode reward: 41.000, mean reward:  5.857 [-25.000, 22.000], mean action: 1.429 [1.000, 2.000],  loss: 245.616937, mean_q: 77.649049, mean_eps: 0.100000\n"," 1 : 4  \n","--->:46393:<---\n","  49113/500000: episode: 1258, duration: 0.314s, episode steps:   2, steps per second:   6, episode reward: 44.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 193.723526, mean_q: 87.235619, mean_eps: 0.100000\n"," 1 : 2  \n","--->:47697:<---\n","  49121/500000: episode: 1259, duration: 0.846s, episode steps:   8, steps per second:   9, episode reward: -9.000, mean reward: -1.125 [-25.000, 22.000], mean action: 0.625 [0.000, 2.000],  loss: 253.874425, mean_q: 83.448877, mean_eps: 0.100000\n"," 1 : 6  \n","--->:49003:<---\n","  49132/500000: episode: 1260, duration: 1.062s, episode steps:  11, steps per second:  10, episode reward: -56.000, mean reward: -5.091 [-25.000, 22.000], mean action: 1.273 [0.000, 2.000],  loss: 267.780336, mean_q: 83.101510, mean_eps: 0.100000\n"," 1 : 6  \n","--->:50312:<---\n","  49148/500000: episode: 1261, duration: 1.476s, episode steps:  16, steps per second:  11, episode reward: 13.000, mean reward:  0.812 [-25.000, 22.000], mean action: 1.562 [1.000, 2.000],  loss: 216.578930, mean_q: 82.667865, mean_eps: 0.100000\n"," 1 : 7  \n","--->:51620:<---\n","  49162/500000: episode: 1262, duration: 1.367s, episode steps:  14, steps per second:  10, episode reward: 57.000, mean reward:  4.071 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 238.070566, mean_q: 79.094905, mean_eps: 0.100000\n"," 1 : 9  \n","--->:52924:<---\n","  49181/500000: episode: 1263, duration: 1.764s, episode steps:  19, steps per second:  11, episode reward: -53.000, mean reward: -2.789 [-25.000, 22.000], mean action: 1.737 [0.000, 2.000],  loss: 259.617462, mean_q: 82.504462, mean_eps: 0.100000\n"," 1 : 4  \n","--->:54233:<---\n","  49202/500000: episode: 1264, duration: 1.875s, episode steps:  21, steps per second:  11, episode reward: 92.000, mean reward:  4.381 [-25.000, 22.000], mean action: 0.619 [0.000, 2.000],  loss: 271.645865, mean_q: 85.875650, mean_eps: 0.100000\n"," 1 : 17  \n","--->:55543:<---\n","  49213/500000: episode: 1265, duration: 1.046s, episode steps:  11, steps per second:  11, episode reward: 126.000, mean reward: 11.455 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 257.377405, mean_q: 83.534296, mean_eps: 0.100000\n"," 1 : 10  \n","--->:56853:<---\n","  49254/500000: episode: 1266, duration: 3.636s, episode steps:  41, steps per second:  11, episode reward: 205.000, mean reward:  5.000 [-25.000, 22.000], mean action: 1.146 [0.000, 2.000],  loss: 239.612312, mean_q: 82.470562, mean_eps: 0.100000\n"," 1 : 20  \n","--->:58161:<---\n","  49300/500000: episode: 1267, duration: 4.077s, episode steps:  46, steps per second:  11, episode reward: 67.000, mean reward:  1.457 [-25.000, 22.000], mean action: 1.304 [0.000, 2.000],  loss: 258.434275, mean_q: 83.323035, mean_eps: 0.100000\n"," 1 : 18  \n","--->:59467:<---\n","  49349/500000: episode: 1268, duration: 4.371s, episode steps:  49, steps per second:  11, episode reward: 192.000, mean reward:  3.918 [-25.000, 22.000], mean action: 1.612 [0.000, 2.000],  loss: 224.669581, mean_q: 81.511874, mean_eps: 0.100000\n"," 1 : 13  \n","--->:60774:<---\n","  49406/500000: episode: 1269, duration: 4.981s, episode steps:  57, steps per second:  11, episode reward: 347.000, mean reward:  6.088 [-25.000, 22.000], mean action: 1.035 [0.000, 2.000],  loss: 216.836684, mean_q: 84.335745, mean_eps: 0.100000\n"," 1 : 35  \n","--->:62076:<---\n","  49436/500000: episode: 1270, duration: 2.695s, episode steps:  30, steps per second:  11, episode reward: 170.000, mean reward:  5.667 [-25.000, 22.000], mean action: 1.433 [0.000, 2.000],  loss: 257.750477, mean_q: 82.784757, mean_eps: 0.100000\n"," 1 : 12  \n","--->:63379:<---\n","  49457/500000: episode: 1271, duration: 1.996s, episode steps:  21, steps per second:  11, episode reward: 98.000, mean reward:  4.667 [-25.000, 22.000], mean action: 0.952 [0.000, 2.000],  loss: 225.096150, mean_q: 86.328590, mean_eps: 0.100000\n"," 1 : 13  \n","--->:64683:<---\n","  49509/500000: episode: 1272, duration: 4.623s, episode steps:  52, steps per second:  11, episode reward: 221.000, mean reward:  4.250 [-25.000, 22.000], mean action: 1.442 [0.000, 2.000],  loss: 251.926378, mean_q: 83.264953, mean_eps: 0.100000\n"," 1 : 25  \n","--->:65984:<---\n","  49528/500000: episode: 1273, duration: 1.789s, episode steps:  19, steps per second:  11, episode reward: -115.000, mean reward: -6.053 [-25.000, 22.000], mean action: 0.789 [0.000, 2.000],  loss: 239.051222, mean_q: 86.423336, mean_eps: 0.100000\n"," 1 : 14  \n","--->:67286:<---\n","  49543/500000: episode: 1274, duration: 1.450s, episode steps:  15, steps per second:  10, episode reward: 126.000, mean reward:  8.400 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 242.336630, mean_q: 84.374425, mean_eps: 0.100000\n"," 1 : 10  \n","--->:68590:<---\n","  49561/500000: episode: 1275, duration: 1.659s, episode steps:  18, steps per second:  11, episode reward: 51.000, mean reward:  2.833 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 234.989614, mean_q: 84.528009, mean_eps: 0.100000\n"," 1 : 13  \n","--->:69894:<---\n","  49584/500000: episode: 1276, duration: 2.081s, episode steps:  23, steps per second:  11, episode reward: 252.000, mean reward: 10.957 [-25.000, 22.000], mean action: 0.478 [0.000, 2.000],  loss: 231.763109, mean_q: 83.419158, mean_eps: 0.100000\n"," 1 : 20  \n","--->:71199:<---\n","  49640/500000: episode: 1277, duration: 4.976s, episode steps:  56, steps per second:  11, episode reward: 231.000, mean reward:  4.125 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 230.138978, mean_q: 83.879736, mean_eps: 0.100000\n"," 1 : 34  \n","--->:72507:<---\n","  49695/500000: episode: 1278, duration: 4.837s, episode steps:  55, steps per second:  11, episode reward: 67.000, mean reward:  1.218 [-25.000, 22.000], mean action: 1.418 [0.000, 2.000],  loss: 237.941720, mean_q: 83.344576, mean_eps: 0.100000\n"," 1 : 18  \n","--->:73810:<---\n","  49719/500000: episode: 1279, duration: 2.195s, episode steps:  24, steps per second:  11, episode reward: 236.000, mean reward:  9.833 [-25.000, 22.000], mean action: 1.208 [0.000, 2.000],  loss: 262.485837, mean_q: 82.213712, mean_eps: 0.100000\n"," 1 : 15  \n","--->:75120:<---\n","  49756/500000: episode: 1280, duration: 3.323s, episode steps:  37, steps per second:  11, episode reward: 224.000, mean reward:  6.054 [-25.000, 22.000], mean action: 0.946 [0.000, 2.000],  loss: 253.132756, mean_q: 81.038456, mean_eps: 0.100000\n"," 1 : 23  \n","--->:76423:<---\n","  49821/500000: episode: 1281, duration: 5.653s, episode steps:  65, steps per second:  11, episode reward: 444.000, mean reward:  6.831 [-25.000, 22.000], mean action: 1.215 [0.000, 2.000],  loss: 235.798601, mean_q: 84.040318, mean_eps: 0.100000\n"," 1 : 33  \n","--->:77725:<---\n","  49865/500000: episode: 1282, duration: 3.850s, episode steps:  44, steps per second:  11, episode reward: 139.000, mean reward:  3.159 [-25.000, 22.000], mean action: 1.409 [0.000, 2.000],  loss: 261.208652, mean_q: 83.221963, mean_eps: 0.100000\n"," 1 : 17  \n","--->:79033:<---\n","  49889/500000: episode: 1283, duration: 2.118s, episode steps:  24, steps per second:  11, episode reward: 299.000, mean reward: 12.458 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 260.749176, mean_q: 81.531519, mean_eps: 0.100000\n"," 1 : 20  \n","--->:80338:<---\n","  49911/500000: episode: 1284, duration: 1.956s, episode steps:  22, steps per second:  11, episode reward: -68.000, mean reward: -3.091 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 232.110252, mean_q: 84.220422, mean_eps: 0.100000\n"," 1 : 14  \n","--->:81642:<---\n","  49936/500000: episode: 1285, duration: 2.329s, episode steps:  25, steps per second:  11, episode reward:  7.000, mean reward:  0.280 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 273.309966, mean_q: 81.152838, mean_eps: 0.100000\n"," 1 : 11  \n","--->:82946:<---\n","  49966/500000: episode: 1286, duration: 2.725s, episode steps:  30, steps per second:  11, episode reward: 20.000, mean reward:  0.667 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 235.321029, mean_q: 84.437210, mean_eps: 0.100000\n"," 1 : 18  \n","--->:84252:<---\n","  49982/500000: episode: 1287, duration: 1.533s, episode steps:  16, steps per second:  10, episode reward: 85.000, mean reward:  5.312 [-25.000, 22.000], mean action: 1.562 [0.000, 2.000],  loss: 250.517700, mean_q: 85.372923, mean_eps: 0.100000\n"," 1 : 6  \n","--->:85553:<---\n","  50004/500000: episode: 1288, duration: 2.025s, episode steps:  22, steps per second:  11, episode reward: 104.000, mean reward:  4.727 [-25.000, 22.000], mean action: 1.364 [0.000, 2.000],  loss: 243.095403, mean_q: 81.418425, mean_eps: 0.100000\n"," 1 : 9  \n","--->:86862:<---\n","  50031/500000: episode: 1289, duration: 2.462s, episode steps:  27, steps per second:  11, episode reward: 161.000, mean reward:  5.963 [-25.000, 22.000], mean action: 0.963 [0.000, 2.000],  loss: 263.871442, mean_q: 81.759861, mean_eps: 0.100000\n"," 1 : 18  \n","--->:88165:<---\n","  50047/500000: episode: 1290, duration: 1.492s, episode steps:  16, steps per second:  11, episode reward: 173.000, mean reward: 10.812 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 288.018250, mean_q: 80.289420, mean_eps: 0.100000\n"," 1 : 10  \n","--->:89467:<---\n","  50067/500000: episode: 1291, duration: 1.865s, episode steps:  20, steps per second:  11, episode reward: 107.000, mean reward:  5.350 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 212.093313, mean_q: 83.557400, mean_eps: 0.100000\n"," 1 : 7  \n","--->:90773:<---\n","  50092/500000: episode: 1292, duration: 2.301s, episode steps:  25, steps per second:  11, episode reward: 48.000, mean reward:  1.920 [-25.000, 22.000], mean action: 1.040 [0.000, 2.000],  loss: 248.397328, mean_q: 81.569701, mean_eps: 0.100000\n"," 1 : 15  \n","--->:92079:<---\n","  50110/500000: episode: 1293, duration: 1.752s, episode steps:  18, steps per second:  10, episode reward: 51.000, mean reward:  2.833 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 214.551477, mean_q: 81.366435, mean_eps: 0.100000\n"," 1 : 13  \n","--->:93383:<---\n","  50135/500000: episode: 1294, duration: 2.283s, episode steps:  25, steps per second:  11, episode reward: 120.000, mean reward:  4.800 [-25.000, 22.000], mean action: 1.160 [0.000, 2.000],  loss: 227.100485, mean_q: 83.126738, mean_eps: 0.100000\n"," 1 : 14  \n","--->:94693:<---\n","  50168/500000: episode: 1295, duration: 3.058s, episode steps:  33, steps per second:  11, episode reward: 45.000, mean reward:  1.364 [-25.000, 22.000], mean action: 1.273 [0.000, 2.000],  loss: 251.694039, mean_q: 81.900775, mean_eps: 0.100000\n"," 1 : 17  \n","--->:95994:<---\n","  50207/500000: episode: 1296, duration: 3.546s, episode steps:  39, steps per second:  11, episode reward: 54.000, mean reward:  1.385 [-25.000, 22.000], mean action: 1.538 [0.000, 2.000],  loss: 233.166711, mean_q: 82.197854, mean_eps: 0.100000\n"," 1 : 11  \n","--->:97300:<---\n","  50223/500000: episode: 1297, duration: 1.540s, episode steps:  16, steps per second:  10, episode reward: 167.000, mean reward: 10.438 [-25.000, 22.000], mean action: 0.688 [0.000, 2.000],  loss: 224.799314, mean_q: 83.625286, mean_eps: 0.100000\n"," 1 : 14  \n","--->:98606:<---\n","  50270/500000: episode: 1298, duration: 4.127s, episode steps:  47, steps per second:  11, episode reward: 36.000, mean reward:  0.766 [-25.000, 22.000], mean action: 1.213 [0.000, 2.000],  loss: 247.318516, mean_q: 82.164293, mean_eps: 0.100000\n"," 1 : 23  \n","--->:99913:<---\n","  50340/500000: episode: 1299, duration: 6.263s, episode steps:  70, steps per second:  11, episode reward: 419.000, mean reward:  5.986 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 234.994576, mean_q: 83.042498, mean_eps: 0.100000\n"," 1 : 34  \n","--->:101222:<---\n","  50392/500000: episode: 1300, duration: 4.759s, episode steps:  52, steps per second:  11, episode reward: 137.000, mean reward:  2.635 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 205.533539, mean_q: 81.342357, mean_eps: 0.100000\n"," 1 : 34  \n","--->:102529:<---\n","  50414/500000: episode: 1301, duration: 2.098s, episode steps:  22, steps per second:  10, episode reward: 167.000, mean reward:  7.591 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 246.906476, mean_q: 82.116428, mean_eps: 0.100000\n"," 1 : 14  \n","--->:103832:<---\n","  50446/500000: episode: 1302, duration: 3.068s, episode steps:  32, steps per second:  10, episode reward: 32.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.531 [0.000, 2.000],  loss: 236.569426, mean_q: 81.609251, mean_eps: 0.100000\n"," 1 : 10  \n","--->:105139:<---\n","  50477/500000: episode: 1303, duration: 2.973s, episode steps:  31, steps per second:  10, episode reward: -71.000, mean reward: -2.290 [-25.000, 22.000], mean action: 1.355 [0.000, 2.000],  loss: 212.480131, mean_q: 81.889504, mean_eps: 0.100000\n"," 1 : 16  \n","--->:106445:<---\n","  50515/500000: episode: 1304, duration: 3.727s, episode steps:  38, steps per second:  10, episode reward: 221.000, mean reward:  5.816 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 237.136899, mean_q: 81.498637, mean_eps: 0.100000\n"," 1 : 25  \n","--->:107747:<---\n","  50532/500000: episode: 1305, duration: 1.650s, episode steps:  17, steps per second:  10, episode reward: 148.000, mean reward:  8.706 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 221.842229, mean_q: 79.276616, mean_eps: 0.100000\n"," 1 : 11  \n","--->:109057:<---\n","  50559/500000: episode: 1306, duration: 2.542s, episode steps:  27, steps per second:  11, episode reward: -34.000, mean reward: -1.259 [-25.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 235.525737, mean_q: 82.617893, mean_eps: 0.100000\n"," 1 : 7  \n","--->:110363:<---\n","  50596/500000: episode: 1307, duration: 3.512s, episode steps:  37, steps per second:  11, episode reward: 64.000, mean reward:  1.730 [-25.000, 22.000], mean action: 1.243 [0.000, 2.000],  loss: 253.297467, mean_q: 80.426466, mean_eps: 0.100000\n"," 1 : 20  \n","--->:111673:<---\n","  50620/500000: episode: 1308, duration: 2.267s, episode steps:  24, steps per second:  11, episode reward: -46.000, mean reward: -1.917 [-25.000, 22.000], mean action: 0.792 [0.000, 2.000],  loss: 226.320569, mean_q: 83.170546, mean_eps: 0.100000\n"," 1 : 15  \n","--->:112977:<---\n","  50639/500000: episode: 1309, duration: 1.848s, episode steps:  19, steps per second:  10, episode reward: 26.000, mean reward:  1.368 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 229.562691, mean_q: 81.970023, mean_eps: 0.100000\n"," 1 : 14  \n","--->:114283:<---\n","  50655/500000: episode: 1310, duration: 1.597s, episode steps:  16, steps per second:  10, episode reward: 57.000, mean reward:  3.562 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 182.814021, mean_q: 78.952899, mean_eps: 0.100000\n"," 1 : 9  \n","--->:115589:<---\n","  50664/500000: episode: 1311, duration: 0.982s, episode steps:   9, steps per second:   9, episode reward: 38.000, mean reward:  4.222 [-25.000, 22.000], mean action: 1.333 [1.000, 2.000],  loss: 295.113571, mean_q: 84.308531, mean_eps: 0.100000\n"," 1 : 6  \n","--->:116898:<---\n","  50690/500000: episode: 1312, duration: 2.518s, episode steps:  26, steps per second:  10, episode reward: 60.000, mean reward:  2.308 [-25.000, 22.000], mean action: 1.577 [0.000, 2.000],  loss: 247.044233, mean_q: 83.788899, mean_eps: 0.100000\n"," 1 : 7  \n","--->:118203:<---\n","  50711/500000: episode: 1313, duration: 2.067s, episode steps:  21, steps per second:  10, episode reward: 51.000, mean reward:  2.429 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 206.371158, mean_q: 83.947912, mean_eps: 0.100000\n"," 1 : 13  \n","--->:119507:<---\n","  50712/500000: episode: 1314, duration: 0.229s, episode steps:   1, steps per second:   4, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 1.000 [1.000, 1.000],  loss: 347.625580, mean_q: 92.850433, mean_eps: 0.100000\n"," 1 : 1  \n","--->:120816:<---\n","  50722/500000: episode: 1315, duration: 1.050s, episode steps:  10, steps per second:  10, episode reward: -59.000, mean reward: -5.900 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 218.519736, mean_q: 80.266811, mean_eps: 0.100000\n"," 1 : 8  \n","--->:122120:<---\n","  50741/500000: episode: 1316, duration: 1.875s, episode steps:  19, steps per second:  10, episode reward: 95.000, mean reward:  5.000 [-25.000, 22.000], mean action: 0.789 [0.000, 2.000],  loss: 231.363951, mean_q: 81.726679, mean_eps: 0.100000\n"," 1 : 15  \n","--->:123423:<---\n","  50760/500000: episode: 1317, duration: 1.812s, episode steps:  19, steps per second:  10, episode reward: 10.000, mean reward:  0.526 [-25.000, 22.000], mean action: 1.421 [0.000, 2.000],  loss: 216.108135, mean_q: 81.354940, mean_eps: 0.100000\n"," 1 : 9  \n","--->:124732:<---\n","  50788/500000: episode: 1318, duration: 2.604s, episode steps:  28, steps per second:  11, episode reward: 32.000, mean reward:  1.143 [-25.000, 22.000], mean action: 1.393 [0.000, 2.000],  loss: 237.055076, mean_q: 82.284113, mean_eps: 0.100000\n"," 1 : 10  \n","--->:126035:<---\n","  50805/500000: episode: 1319, duration: 1.627s, episode steps:  17, steps per second:  10, episode reward: -43.000, mean reward: -2.529 [-25.000, 22.000], mean action: 0.765 [0.000, 2.000],  loss: 256.245340, mean_q: 83.517214, mean_eps: 0.100000\n"," 1 : 13  \n","--->:127344:<---\n","  50827/500000: episode: 1320, duration: 2.004s, episode steps:  22, steps per second:  11, episode reward: 117.000, mean reward:  5.318 [-25.000, 22.000], mean action: 0.773 [0.000, 2.000],  loss: 241.967183, mean_q: 81.074803, mean_eps: 0.100000\n"," 1 : 16  \n","--->:128649:<---\n","  50848/500000: episode: 1321, duration: 1.900s, episode steps:  21, steps per second:  11, episode reward: 120.000, mean reward:  5.714 [-25.000, 22.000], mean action: 0.952 [0.000, 2.000],  loss: 233.724545, mean_q: 82.342672, mean_eps: 0.100000\n"," 1 : 14  \n","--->:129957:<---\n","  50874/500000: episode: 1322, duration: 2.340s, episode steps:  26, steps per second:  11, episode reward: 136.000, mean reward:  5.231 [-25.000, 22.000], mean action: 0.885 [0.000, 2.000],  loss: 262.400561, mean_q: 83.175451, mean_eps: 0.100000\n"," 1 : 19  \n","--->:131267:<---\n","  50916/500000: episode: 1323, duration: 3.750s, episode steps:  42, steps per second:  11, episode reward: 92.000, mean reward:  2.190 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 226.154013, mean_q: 81.284617, mean_eps: 0.100000\n"," 1 : 17  \n","--->:132575:<---\n","  50937/500000: episode: 1324, duration: 1.977s, episode steps:  21, steps per second:  11, episode reward:  4.000, mean reward:  0.190 [-25.000, 22.000], mean action: 1.048 [0.000, 2.000],  loss: 237.008920, mean_q: 82.472476, mean_eps: 0.100000\n"," 1 : 13  \n","--->:133879:<---\n","  50975/500000: episode: 1325, duration: 3.456s, episode steps:  38, steps per second:  11, episode reward: 73.000, mean reward:  1.921 [-25.000, 22.000], mean action: 1.474 [0.000, 2.000],  loss: 219.522836, mean_q: 81.738376, mean_eps: 0.100000\n"," 1 : 14  \n","--->:135189:<---\n","  50989/500000: episode: 1326, duration: 1.338s, episode steps:  14, steps per second:  10, episode reward: 148.000, mean reward: 10.571 [-25.000, 22.000], mean action: 1.071 [0.000, 2.000],  loss: 211.051885, mean_q: 78.232103, mean_eps: 0.100000\n"," 1 : 11  \n","--->:136495:<---\n","  51024/500000: episode: 1327, duration: 3.203s, episode steps:  35, steps per second:  11, episode reward: 148.000, mean reward:  4.229 [-25.000, 22.000], mean action: 1.514 [0.000, 2.000],  loss: 249.047819, mean_q: 80.705713, mean_eps: 0.100000\n"," 1 : 11  \n","--->:137800:<---\n","  51052/500000: episode: 1328, duration: 2.574s, episode steps:  28, steps per second:  11, episode reward: 255.000, mean reward:  9.107 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 223.415700, mean_q: 83.059087, mean_eps: 0.100000\n"," 1 : 18  \n","--->:139109:<---\n","  51087/500000: episode: 1329, duration: 3.159s, episode steps:  35, steps per second:  11, episode reward: -33.000, mean reward: -0.943 [-25.000, 22.000], mean action: 1.086 [0.000, 2.000],  loss: 246.988051, mean_q: 82.324160, mean_eps: 0.100000\n"," 1 : 22  \n","--->:140412:<---\n","  51096/500000: episode: 1330, duration: 0.935s, episode steps:   9, steps per second:  10, episode reward: 154.000, mean reward: 17.111 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 229.259689, mean_q: 84.725721, mean_eps: 0.100000\n"," 1 : 7  \n","--->:141721:<---\n","  51131/500000: episode: 1331, duration: 3.153s, episode steps:  35, steps per second:  11, episode reward: 111.000, mean reward:  3.171 [-25.000, 22.000], mean action: 1.114 [0.000, 2.000],  loss: 233.595406, mean_q: 80.603064, mean_eps: 0.100000\n"," 1 : 20  \n","--->:143023:<---\n","  51138/500000: episode: 1332, duration: 0.746s, episode steps:   7, steps per second:   9, episode reward: 63.000, mean reward:  9.000 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 264.025870, mean_q: 81.828566, mean_eps: 0.100000\n"," 1 : 5  \n","--->:144324:<---\n","  51163/500000: episode: 1333, duration: 2.304s, episode steps:  25, steps per second:  11, episode reward: 101.000, mean reward:  4.040 [-25.000, 22.000], mean action: 1.440 [0.000, 2.000],  loss: 223.241287, mean_q: 82.608676, mean_eps: 0.100000\n"," 1 : 11  \n","--->:145631:<---\n","  51174/500000: episode: 1334, duration: 1.142s, episode steps:  11, steps per second:  10, episode reward: 107.000, mean reward:  9.727 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 214.142885, mean_q: 82.434426, mean_eps: 0.100000\n"," 1 : 7  \n","--->:146941:<---\n","  51195/500000: episode: 1335, duration: 1.927s, episode steps:  21, steps per second:  11, episode reward: 151.000, mean reward:  7.190 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 284.457302, mean_q: 80.472804, mean_eps: 0.100000\n"," 1 : 9  \n","--->:148250:<---\n","  51216/500000: episode: 1336, duration: 1.995s, episode steps:  21, steps per second:  11, episode reward:  4.000, mean reward:  0.190 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 260.377634, mean_q: 81.302694, mean_eps: 0.100000\n"," 1 : 13  \n","--->:149559:<---\n","  51250/500000: episode: 1337, duration: 3.178s, episode steps:  34, steps per second:  11, episode reward: 130.000, mean reward:  3.824 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 225.601611, mean_q: 82.297276, mean_eps: 0.100000\n"," 1 : 23  \n","--->:150862:<---\n","  51303/500000: episode: 1338, duration: 4.741s, episode steps:  53, steps per second:  11, episode reward: 77.000, mean reward:  1.453 [-25.000, 22.000], mean action: 1.113 [0.000, 2.000],  loss: 227.816938, mean_q: 82.189968, mean_eps: 0.100000\n"," 1 : 27  \n","--->:152171:<---\n","  51364/500000: episode: 1339, duration: 5.465s, episode steps:  61, steps per second:  11, episode reward: 193.000, mean reward:  3.164 [-25.000, 22.000], mean action: 1.311 [0.000, 2.000],  loss: 249.653602, mean_q: 81.480410, mean_eps: 0.100000\n"," 1 : 28  \n","--->:153472:<---\n","  51380/500000: episode: 1340, duration: 1.535s, episode steps:  16, steps per second:  10, episode reward: 170.000, mean reward: 10.625 [-25.000, 22.000], mean action: 1.062 [0.000, 2.000],  loss: 222.082827, mean_q: 81.147413, mean_eps: 0.100000\n"," 1 : 12  \n","--->:154778:<---\n","  51385/500000: episode: 1341, duration: 0.570s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 0.800 [0.000, 2.000],  loss: 191.320102, mean_q: 78.983066, mean_eps: 0.100000\n"," 1 : 4  \n","--->:156080:<---\n","  51408/500000: episode: 1342, duration: 2.124s, episode steps:  23, steps per second:  11, episode reward: -5.000, mean reward: -0.217 [-25.000, 22.000], mean action: 0.826 [0.000, 2.000],  loss: 202.618364, mean_q: 84.873495, mean_eps: 0.100000\n"," 1 : 19  \n","--->:157381:<---\n","  51434/500000: episode: 1343, duration: 2.383s, episode steps:  26, steps per second:  11, episode reward: 155.000, mean reward:  5.962 [-25.000, 22.000], mean action: 0.462 [0.000, 2.000],  loss: 200.986737, mean_q: 83.806772, mean_eps: 0.100000\n"," 1 : 22  \n","--->:158688:<---\n","  51474/500000: episode: 1344, duration: 3.641s, episode steps:  40, steps per second:  11, episode reward: 306.000, mean reward:  7.650 [-25.000, 22.000], mean action: 0.825 [0.000, 2.000],  loss: 245.059474, mean_q: 81.101439, mean_eps: 0.100000\n"," 1 : 31  \n","--->:159993:<---\n","  51498/500000: episode: 1345, duration: 2.232s, episode steps:  24, steps per second:  11, episode reward: 136.000, mean reward:  5.667 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 251.209462, mean_q: 81.824455, mean_eps: 0.100000\n"," 1 : 19  \n","--->:161301:<---\n","  51527/500000: episode: 1346, duration: 2.625s, episode steps:  29, steps per second:  11, episode reward: 86.000, mean reward:  2.966 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 220.408338, mean_q: 82.948645, mean_eps: 0.100000\n"," 1 : 21  \n","--->:162605:<---\n","  51555/500000: episode: 1347, duration: 2.586s, episode steps:  28, steps per second:  11, episode reward: 183.000, mean reward:  6.536 [-25.000, 22.000], mean action: 1.107 [0.000, 2.000],  loss: 225.182924, mean_q: 81.232427, mean_eps: 0.100000\n"," 1 : 19  \n","--->:163915:<---\n","  51561/500000: episode: 1348, duration: 0.655s, episode steps:   6, steps per second:   9, episode reward: 16.000, mean reward:  2.667 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 187.590563, mean_q: 80.480376, mean_eps: 0.100000\n"," 1 : 5  \n","--->:165220:<---\n","  51570/500000: episode: 1349, duration: 0.962s, episode steps:   9, steps per second:   9, episode reward: 41.000, mean reward:  4.556 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 218.770579, mean_q: 82.708499, mean_eps: 0.100000\n"," 1 : 4  \n","--->:166523:<---\n","  51583/500000: episode: 1350, duration: 1.289s, episode steps:  13, steps per second:  10, episode reward: 57.000, mean reward:  4.385 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 274.698093, mean_q: 83.687614, mean_eps: 0.100000\n"," 1 : 9  \n","--->:167828:<---\n","  51596/500000: episode: 1351, duration: 1.277s, episode steps:  13, steps per second:  10, episode reward: 13.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 267.885878, mean_q: 77.692028, mean_eps: 0.100000\n"," 1 : 7  \n","--->:169135:<---\n","  51614/500000: episode: 1352, duration: 1.777s, episode steps:  18, steps per second:  10, episode reward: 35.000, mean reward:  1.944 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 243.703203, mean_q: 85.379809, mean_eps: 0.100000\n"," 1 : 8  \n","--->:170438:<---\n","  51625/500000: episode: 1353, duration: 1.125s, episode steps:  11, steps per second:  10, episode reward: 60.000, mean reward:  5.455 [-25.000, 22.000], mean action: 1.182 [0.000, 2.000],  loss: 221.318896, mean_q: 79.471614, mean_eps: 0.100000\n"," 1 : 7  \n","--->:171742:<---\n","  51635/500000: episode: 1354, duration: 1.073s, episode steps:  10, steps per second:   9, episode reward: 60.000, mean reward:  6.000 [-25.000, 22.000], mean action: 0.900 [0.000, 2.000],  loss: 218.649212, mean_q: 85.050047, mean_eps: 0.100000\n"," 1 : 7  \n","--->:173051:<---\n","  51651/500000: episode: 1355, duration: 1.557s, episode steps:  16, steps per second:  10, episode reward: 101.000, mean reward:  6.312 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 225.685075, mean_q: 81.532052, mean_eps: 0.100000\n"," 1 : 11  \n","--->:174361:<---\n","  51672/500000: episode: 1356, duration: 2.028s, episode steps:  21, steps per second:  10, episode reward: 170.000, mean reward:  8.095 [-25.000, 22.000], mean action: 1.095 [0.000, 2.000],  loss: 241.046387, mean_q: 81.531650, mean_eps: 0.100000\n"," 1 : 12  \n","--->:175662:<---\n","  51693/500000: episode: 1357, duration: 1.951s, episode steps:  21, steps per second:  11, episode reward: 98.000, mean reward:  4.667 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 245.655274, mean_q: 80.474636, mean_eps: 0.100000\n"," 1 : 13  \n","--->:176966:<---\n","  51719/500000: episode: 1358, duration: 2.392s, episode steps:  26, steps per second:  11, episode reward: 98.000, mean reward:  3.769 [-25.000, 22.000], mean action: 1.423 [0.000, 2.000],  loss: 240.788077, mean_q: 81.292889, mean_eps: 0.100000\n"," 1 : 13  \n","--->:178276:<---\n","  51739/500000: episode: 1359, duration: 1.925s, episode steps:  20, steps per second:  10, episode reward: 23.000, mean reward:  1.150 [-25.000, 22.000], mean action: 0.700 [0.000, 2.000],  loss: 248.194986, mean_q: 85.755204, mean_eps: 0.100000\n"," 1 : 16  \n","--->:179579:<---\n","  51762/500000: episode: 1360, duration: 2.242s, episode steps:  23, steps per second:  10, episode reward: 139.000, mean reward:  6.043 [-25.000, 22.000], mean action: 1.043 [0.000, 2.000],  loss: 202.852667, mean_q: 83.183032, mean_eps: 0.100000\n"," 1 : 17  \n","--->:180889:<---\n","  51814/500000: episode: 1361, duration: 4.742s, episode steps:  52, steps per second:  11, episode reward: 23.000, mean reward:  0.442 [-25.000, 22.000], mean action: 1.442 [0.000, 2.000],  loss: 234.028566, mean_q: 81.690088, mean_eps: 0.100000\n"," 1 : 16  \n","--->:182190:<---\n","  51846/500000: episode: 1362, duration: 2.920s, episode steps:  32, steps per second:  11, episode reward: 230.000, mean reward:  7.188 [-25.000, 22.000], mean action: 1.156 [0.000, 2.000],  loss: 210.705709, mean_q: 82.436512, mean_eps: 0.100000\n"," 1 : 19  \n","--->:183493:<---\n","  51870/500000: episode: 1363, duration: 2.197s, episode steps:  24, steps per second:  11, episode reward: 92.000, mean reward:  3.833 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 251.015381, mean_q: 82.846250, mean_eps: 0.100000\n"," 1 : 17  \n","--->:184798:<---\n","  51901/500000: episode: 1364, duration: 2.862s, episode steps:  31, steps per second:  11, episode reward: 133.000, mean reward:  4.290 [-25.000, 22.000], mean action: 0.871 [0.000, 2.000],  loss: 207.195581, mean_q: 84.565603, mean_eps: 0.100000\n"," 1 : 21  \n","--->:186104:<---\n","  51913/500000: episode: 1365, duration: 1.230s, episode steps:  12, steps per second:  10, episode reward: 57.000, mean reward:  4.750 [-25.000, 22.000], mean action: 1.167 [0.000, 2.000],  loss: 204.312827, mean_q: 83.575854, mean_eps: 0.100000\n"," 1 : 9  \n","--->:187406:<---\n","  51926/500000: episode: 1366, duration: 1.357s, episode steps:  13, steps per second:  10, episode reward: -12.000, mean reward: -0.923 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 218.110337, mean_q: 80.942944, mean_eps: 0.100000\n"," 1 : 8  \n","--->:188709:<---\n","  51980/500000: episode: 1367, duration: 4.906s, episode steps:  54, steps per second:  11, episode reward: 20.000, mean reward:  0.370 [-25.000, 22.000], mean action: 1.407 [0.000, 2.000],  loss: 251.158062, mean_q: 82.450624, mean_eps: 0.100000\n"," 1 : 18  \n","--->:190019:<---\n","  52014/500000: episode: 1368, duration: 3.131s, episode steps:  34, steps per second:  11, episode reward: 161.000, mean reward:  4.735 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 250.489881, mean_q: 81.618172, mean_eps: 0.100000\n"," 1 : 18  \n","--->:191323:<---\n","  52033/500000: episode: 1369, duration: 1.789s, episode steps:  19, steps per second:  11, episode reward: 10.000, mean reward:  0.526 [-25.000, 22.000], mean action: 1.211 [0.000, 2.000],  loss: 248.632884, mean_q: 79.982224, mean_eps: 0.100000\n"," 1 : 9  \n","--->:192631:<---\n","  52055/500000: episode: 1370, duration: 2.110s, episode steps:  22, steps per second:  10, episode reward: 170.000, mean reward:  7.727 [-25.000, 22.000], mean action: 1.136 [0.000, 2.000],  loss: 231.712031, mean_q: 79.791143, mean_eps: 0.100000\n"," 1 : 12  \n","--->:193937:<---\n","  52099/500000: episode: 1371, duration: 4.113s, episode steps:  44, steps per second:  11, episode reward:  7.000, mean reward:  0.159 [-25.000, 22.000], mean action: 1.591 [0.000, 2.000],  loss: 261.608610, mean_q: 81.843514, mean_eps: 0.100000\n"," 1 : 11  \n","--->:195247:<---\n","  52149/500000: episode: 1372, duration: 4.512s, episode steps:  50, steps per second:  11, episode reward: 230.000, mean reward:  4.600 [-25.000, 22.000], mean action: 1.340 [0.000, 2.000],  loss: 244.525835, mean_q: 81.347424, mean_eps: 0.100000\n"," 1 : 19  \n","--->:196553:<---\n","  52226/500000: episode: 1373, duration: 6.853s, episode steps:  77, steps per second:  11, episode reward:  8.000, mean reward:  0.104 [-25.000, 22.000], mean action: 1.390 [0.000, 2.000],  loss: 238.346767, mean_q: 81.881427, mean_eps: 0.100000\n"," 1 : 26  \n","--->:197855:<---\n","  52268/500000: episode: 1374, duration: 3.812s, episode steps:  42, steps per second:  11, episode reward: 353.000, mean reward:  8.405 [-25.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 221.366677, mean_q: 85.076725, mean_eps: 0.100000\n"," 1 : 31  \n","--->:199159:<---\n","  52454/500000: episode: 1375, duration: 16.249s, episode steps: 186, steps per second:  11, episode reward: 1159.000, mean reward:  6.231 [-25.000, 22.000], mean action: 0.876 [0.000, 2.000],  loss: 229.155527, mean_q: 82.582002, mean_eps: 0.100000\n"," 1 : 136  \n","--->:200460:<---\n","  52623/500000: episode: 1376, duration: 14.945s, episode steps: 169, steps per second:  11, episode reward: 1140.000, mean reward:  6.746 [-25.000, 22.000], mean action: 0.828 [0.000, 2.000],  loss: 232.357532, mean_q: 82.204618, mean_eps: 0.100000\n"," 1 : 133  \n","--->:201768:<---\n","  52747/500000: episode: 1377, duration: 10.838s, episode steps: 124, steps per second:  11, episode reward: 1034.000, mean reward:  8.339 [-25.000, 22.000], mean action: 0.952 [0.000, 2.000],  loss: 233.535150, mean_q: 81.436353, mean_eps: 0.100000\n"," 1 : 94  \n","--->:203070:<---\n","  52843/500000: episode: 1378, duration: 8.549s, episode steps:  96, steps per second:  11, episode reward: 382.000, mean reward:  3.979 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 240.325292, mean_q: 82.598216, mean_eps: 0.100000\n"," 1 : 43  \n","--->:204372:<---\n","  52908/500000: episode: 1379, duration: 5.770s, episode steps:  65, steps per second:  11, episode reward: 53.000, mean reward:  0.815 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 247.267726, mean_q: 81.480832, mean_eps: 0.100000\n"," 1 : 43  \n","--->:205679:<---\n","  52973/500000: episode: 1380, duration: 5.893s, episode steps:  65, steps per second:  11, episode reward: 316.000, mean reward:  4.862 [-25.000, 22.000], mean action: 1.015 [0.000, 2.000],  loss: 223.161009, mean_q: 82.551963, mean_eps: 0.100000\n"," 1 : 40  \n","--->:206982:<---\n","  53042/500000: episode: 1381, duration: 7.080s, episode steps:  69, steps per second:  10, episode reward: 542.000, mean reward:  7.855 [-25.000, 22.000], mean action: 0.870 [0.000, 2.000],  loss: 216.558295, mean_q: 81.681581, mean_eps: 0.100000\n"," 1 : 46  \n","--->:208286:<---\n","  53119/500000: episode: 1382, duration: 7.370s, episode steps:  77, steps per second:  10, episode reward: 157.000, mean reward:  2.039 [-25.000, 22.000], mean action: 0.922 [0.000, 2.000],  loss: 248.890597, mean_q: 82.874938, mean_eps: 0.100000\n"," 1 : 52  \n","--->:209591:<---\n","  53202/500000: episode: 1383, duration: 7.420s, episode steps:  83, steps per second:  11, episode reward: 118.000, mean reward:  1.422 [-25.000, 22.000], mean action: 1.518 [0.000, 2.000],  loss: 241.435886, mean_q: 80.740451, mean_eps: 0.100000\n"," 1 : 31  \n","--->:210900:<---\n","  53270/500000: episode: 1384, duration: 6.094s, episode steps:  68, steps per second:  11, episode reward: 426.000, mean reward:  6.265 [-25.000, 22.000], mean action: 0.926 [0.000, 2.000],  loss: 231.074557, mean_q: 82.637086, mean_eps: 0.100000\n"," 1 : 45  \n","--->:212204:<---\n","  53333/500000: episode: 1385, duration: 5.474s, episode steps:  63, steps per second:  12, episode reward: 291.000, mean reward:  4.619 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 254.368289, mean_q: 81.677711, mean_eps: 0.100000\n"," 1 : 41  \n","--->:213511:<---\n","  53398/500000: episode: 1386, duration: 5.844s, episode steps:  65, steps per second:  11, episode reward: 158.000, mean reward:  2.431 [-25.000, 22.000], mean action: 1.662 [0.000, 2.000],  loss: 243.166909, mean_q: 81.706592, mean_eps: 0.100000\n"," 1 : 20  \n","--->:214815:<---\n","  53429/500000: episode: 1387, duration: 3.060s, episode steps:  31, steps per second:  10, episode reward: 177.000, mean reward:  5.710 [-25.000, 22.000], mean action: 1.032 [0.000, 2.000],  loss: 231.085902, mean_q: 82.469345, mean_eps: 0.100000\n"," 1 : 23  \n","--->:216118:<---\n","  53480/500000: episode: 1388, duration: 4.620s, episode steps:  51, steps per second:  11, episode reward: 247.000, mean reward:  4.843 [-25.000, 22.000], mean action: 0.824 [0.000, 2.000],  loss: 230.959499, mean_q: 82.201943, mean_eps: 0.100000\n"," 1 : 39  \n","--->:217425:<---\n","  53548/500000: episode: 1389, duration: 6.278s, episode steps:  68, steps per second:  11, episode reward: 288.000, mean reward:  4.235 [-25.000, 22.000], mean action: 1.147 [0.000, 2.000],  loss: 235.530622, mean_q: 82.068177, mean_eps: 0.100000\n"," 1 : 43  \n","--->:218733:<---\n","  53604/500000: episode: 1390, duration: 5.096s, episode steps:  56, steps per second:  11, episode reward: 99.000, mean reward:  1.768 [-25.000, 22.000], mean action: 1.411 [0.000, 2.000],  loss: 265.290900, mean_q: 82.574766, mean_eps: 0.100000\n"," 1 : 28  \n","--->:220039:<---\n","  53620/500000: episode: 1391, duration: 1.574s, episode steps:  16, steps per second:  10, episode reward: 192.000, mean reward: 12.000 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 222.975045, mean_q: 81.301680, mean_eps: 0.100000\n"," 1 : 13  \n","--->:221349:<---\n","  53646/500000: episode: 1392, duration: 2.314s, episode steps:  26, steps per second:  11, episode reward: 95.000, mean reward:  3.654 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 218.524334, mean_q: 82.632797, mean_eps: 0.100000\n"," 1 : 15  \n","--->:222650:<---\n","  53678/500000: episode: 1393, duration: 2.893s, episode steps:  32, steps per second:  11, episode reward: 54.000, mean reward:  1.688 [-25.000, 22.000], mean action: 1.594 [0.000, 2.000],  loss: 223.400358, mean_q: 83.159844, mean_eps: 0.100000\n"," 1 : 11  \n","--->:223958:<---\n","  53733/500000: episode: 1394, duration: 4.961s, episode steps:  55, steps per second:  11, episode reward: 344.000, mean reward:  6.255 [-25.000, 22.000], mean action: 1.036 [0.000, 2.000],  loss: 238.203669, mean_q: 82.260526, mean_eps: 0.100000\n"," 1 : 37  \n","--->:225262:<---\n","  53814/500000: episode: 1395, duration: 7.236s, episode steps:  81, steps per second:  11, episode reward: 347.000, mean reward:  4.284 [-25.000, 22.000], mean action: 1.321 [0.000, 2.000],  loss: 235.367485, mean_q: 81.801931, mean_eps: 0.100000\n"," 1 : 35  \n","--->:226567:<---\n","  53845/500000: episode: 1396, duration: 2.750s, episode steps:  31, steps per second:  11, episode reward: 218.000, mean reward:  7.032 [-25.000, 22.000], mean action: 0.710 [0.000, 2.000],  loss: 197.542667, mean_q: 84.443053, mean_eps: 0.100000\n"," 1 : 27  \n","--->:227869:<---\n","  53921/500000: episode: 1397, duration: 6.621s, episode steps:  76, steps per second:  11, episode reward: 165.000, mean reward:  2.171 [-25.000, 22.000], mean action: 1.408 [0.000, 2.000],  loss: 246.310683, mean_q: 81.257527, mean_eps: 0.100000\n"," 1 : 31  \n","--->:229174:<---\n","  54007/500000: episode: 1398, duration: 7.594s, episode steps:  86, steps per second:  11, episode reward: 121.000, mean reward:  1.407 [-25.000, 22.000], mean action: 1.488 [0.000, 2.000],  loss: 248.299881, mean_q: 82.003555, mean_eps: 0.100000\n"," 1 : 29  \n","--->:230484:<---\n","  54059/500000: episode: 1399, duration: 4.639s, episode steps:  52, steps per second:  11, episode reward: 260.000, mean reward:  5.000 [-25.000, 22.000], mean action: 0.673 [0.000, 2.000],  loss: 231.543323, mean_q: 82.368674, mean_eps: 0.100000\n"," 1 : 46  \n","--->:231791:<---\n","  54114/500000: episode: 1400, duration: 5.035s, episode steps:  55, steps per second:  11, episode reward: 454.000, mean reward:  8.255 [-25.000, 22.000], mean action: 0.764 [0.000, 2.000],  loss: 219.137323, mean_q: 83.342874, mean_eps: 0.100000\n"," 1 : 42  \n","--->:233097:<---\n","  54180/500000: episode: 1401, duration: 5.826s, episode steps:  66, steps per second:  11, episode reward: 18.000, mean reward:  0.273 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 246.088499, mean_q: 82.065582, mean_eps: 0.100000\n"," 1 : 35  \n","--->:234400:<---\n","  54244/500000: episode: 1402, duration: 5.596s, episode steps:  64, steps per second:  11, episode reward: 426.000, mean reward:  6.656 [-25.000, 22.000], mean action: 0.969 [0.000, 2.000],  loss: 231.272039, mean_q: 82.646484, mean_eps: 0.100000\n"," 1 : 45  \n","--->:235702:<---\n","  54277/500000: episode: 1403, duration: 2.984s, episode steps:  33, steps per second:  11, episode reward: 61.000, mean reward:  1.848 [-25.000, 22.000], mean action: 1.212 [0.000, 2.000],  loss: 243.379648, mean_q: 81.776213, mean_eps: 0.100000\n"," 1 : 22  \n","--->:237007:<---\n","  54328/500000: episode: 1404, duration: 4.513s, episode steps:  51, steps per second:  11, episode reward: 394.000, mean reward:  7.725 [-25.000, 22.000], mean action: 0.941 [0.000, 2.000],  loss: 252.451117, mean_q: 81.068432, mean_eps: 0.100000\n"," 1 : 35  \n","--->:238313:<---\n","  54436/500000: episode: 1405, duration: 9.433s, episode steps: 108, steps per second:  11, episode reward: 310.000, mean reward:  2.870 [-25.000, 22.000], mean action: 1.398 [0.000, 2.000],  loss: 230.159544, mean_q: 82.572419, mean_eps: 0.100000\n"," 1 : 44  \n","--->:239622:<---\n","  54460/500000: episode: 1406, duration: 2.198s, episode steps:  24, steps per second:  11, episode reward: 139.000, mean reward:  5.792 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 255.299227, mean_q: 80.032820, mean_eps: 0.100000\n"," 1 : 17  \n","--->:240930:<---\n","  54504/500000: episode: 1407, duration: 3.894s, episode steps:  44, steps per second:  11, episode reward: 303.000, mean reward:  6.886 [-25.000, 22.000], mean action: 0.932 [0.000, 2.000],  loss: 253.731012, mean_q: 82.311194, mean_eps: 0.100000\n"," 1 : 33  \n","--->:242240:<---\n","  54553/500000: episode: 1408, duration: 4.322s, episode steps:  49, steps per second:  11, episode reward: 378.000, mean reward:  7.714 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 239.276230, mean_q: 81.982799, mean_eps: 0.100000\n"," 1 : 30  \n","--->:243544:<---\n","  54631/500000: episode: 1409, duration: 6.860s, episode steps:  78, steps per second:  11, episode reward: 202.000, mean reward:  2.590 [-25.000, 22.000], mean action: 1.679 [0.000, 2.000],  loss: 250.699141, mean_q: 82.819845, mean_eps: 0.100000\n"," 1 : 22  \n","--->:244851:<---\n","  54663/500000: episode: 1410, duration: 2.856s, episode steps:  32, steps per second:  11, episode reward: 274.000, mean reward:  8.562 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 224.417058, mean_q: 81.302175, mean_eps: 0.100000\n"," 1 : 21  \n","--->:246153:<---\n","  54687/500000: episode: 1411, duration: 2.196s, episode steps:  24, steps per second:  11, episode reward: 54.000, mean reward:  2.250 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 242.786278, mean_q: 80.493222, mean_eps: 0.100000\n"," 1 : 11  \n","--->:247458:<---\n","  54726/500000: episode: 1412, duration: 3.447s, episode steps:  39, steps per second:  11, episode reward: 164.000, mean reward:  4.205 [-25.000, 22.000], mean action: 1.436 [0.000, 2.000],  loss: 206.759070, mean_q: 83.391277, mean_eps: 0.100000\n"," 1 : 16  \n","--->:248762:<---\n","  54772/500000: episode: 1413, duration: 4.148s, episode steps:  46, steps per second:  11, episode reward: 271.000, mean reward:  5.891 [-25.000, 22.000], mean action: 1.196 [0.000, 2.000],  loss: 238.753394, mean_q: 81.780730, mean_eps: 0.100000\n"," 1 : 23  \n","--->:250063:<---\n","  54794/500000: episode: 1414, duration: 2.032s, episode steps:  22, steps per second:  11, episode reward: 89.000, mean reward:  4.045 [-25.000, 22.000], mean action: 0.955 [0.000, 2.000],  loss: 195.675811, mean_q: 81.582475, mean_eps: 0.100000\n"," 1 : 19  \n","--->:251372:<---\n","  54822/500000: episode: 1415, duration: 2.539s, episode steps:  28, steps per second:  11, episode reward: -24.000, mean reward: -0.857 [-25.000, 22.000], mean action: 1.321 [0.000, 2.000],  loss: 242.947280, mean_q: 80.877119, mean_eps: 0.100000\n"," 1 : 16  \n","--->:252678:<---\n","  54837/500000: episode: 1416, duration: 1.448s, episode steps:  15, steps per second:  10, episode reward: 29.000, mean reward:  1.933 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 228.489411, mean_q: 87.497409, mean_eps: 0.100000\n"," 1 : 12  \n","--->:253988:<---\n","  54859/500000: episode: 1417, duration: 2.030s, episode steps:  22, steps per second:  11, episode reward: 139.000, mean reward:  6.318 [-25.000, 22.000], mean action: 0.909 [0.000, 2.000],  loss: 271.650899, mean_q: 82.059642, mean_eps: 0.100000\n"," 1 : 17  \n","--->:255291:<---\n","  54911/500000: episode: 1418, duration: 4.573s, episode steps:  52, steps per second:  11, episode reward: 205.000, mean reward:  3.942 [-25.000, 22.000], mean action: 1.365 [0.000, 2.000],  loss: 254.103339, mean_q: 81.968840, mean_eps: 0.100000\n"," 1 : 20  \n","--->:256596:<---\n","  54937/500000: episode: 1419, duration: 2.410s, episode steps:  26, steps per second:  11, episode reward: 152.000, mean reward:  5.846 [-25.000, 22.000], mean action: 0.885 [0.000, 2.000],  loss: 259.448246, mean_q: 81.737893, mean_eps: 0.100000\n"," 1 : 24  \n","--->:257905:<---\n","  54977/500000: episode: 1420, duration: 3.543s, episode steps:  40, steps per second:  11, episode reward: 387.000, mean reward:  9.675 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 239.588541, mean_q: 80.995605, mean_eps: 0.100000\n"," 1 : 24  \n","--->:259211:<---\n","  54995/500000: episode: 1421, duration: 1.711s, episode steps:  18, steps per second:  11, episode reward: -9.000, mean reward: -0.500 [-25.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 224.451850, mean_q: 82.231834, mean_eps: 0.100000\n"," 1 : 6  \n","--->:260518:<---\n","  55059/500000: episode: 1422, duration: 5.608s, episode steps:  64, steps per second:  11, episode reward:  1.000, mean reward:  0.016 [-25.000, 22.000], mean action: 1.688 [0.000, 2.000],  loss: 236.913517, mean_q: 82.396299, mean_eps: 0.100000\n"," 1 : 15  \n","--->:261825:<---\n","  55090/500000: episode: 1423, duration: 2.854s, episode steps:  31, steps per second:  11, episode reward: 152.000, mean reward:  4.903 [-25.000, 22.000], mean action: 1.097 [0.000, 2.000],  loss: 239.131174, mean_q: 82.490889, mean_eps: 0.100000\n"," 1 : 24  \n","--->:263128:<---\n","  55123/500000: episode: 1424, duration: 3.005s, episode steps:  33, steps per second:  11, episode reward: 76.000, mean reward:  2.303 [-25.000, 22.000], mean action: 1.545 [0.000, 2.000],  loss: 216.723413, mean_q: 82.705654, mean_eps: 0.100000\n"," 1 : 12  \n","--->:264429:<---\n","  55166/500000: episode: 1425, duration: 3.856s, episode steps:  43, steps per second:  11, episode reward:  1.000, mean reward:  0.023 [-25.000, 22.000], mean action: 1.419 [0.000, 2.000],  loss: 238.725173, mean_q: 82.507331, mean_eps: 0.100000\n"," 1 : 15  \n","--->:265734:<---\n","  55175/500000: episode: 1426, duration: 0.929s, episode steps:   9, steps per second:  10, episode reward: 35.000, mean reward:  3.889 [-25.000, 22.000], mean action: 0.556 [0.000, 2.000],  loss: 217.264511, mean_q: 79.905368, mean_eps: 0.100000\n"," 1 : 8  \n","--->:267035:<---\n","  55200/500000: episode: 1427, duration: 2.311s, episode steps:  25, steps per second:  11, episode reward: 120.000, mean reward:  4.800 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 241.895209, mean_q: 83.700800, mean_eps: 0.100000\n"," 1 : 14  \n","--->:268345:<---\n","  55227/500000: episode: 1428, duration: 2.551s, episode steps:  27, steps per second:  11, episode reward: 95.000, mean reward:  3.519 [-25.000, 22.000], mean action: 1.296 [0.000, 2.000],  loss: 266.313310, mean_q: 82.445992, mean_eps: 0.100000\n"," 1 : 15  \n","--->:269654:<---\n","  55255/500000: episode: 1429, duration: 2.635s, episode steps:  28, steps per second:  11, episode reward: 236.000, mean reward:  8.429 [-25.000, 22.000], mean action: 1.179 [0.000, 2.000],  loss: 252.170254, mean_q: 81.513368, mean_eps: 0.100000\n"," 1 : 15  \n","--->:270962:<---\n","  55293/500000: episode: 1430, duration: 3.543s, episode steps:  38, steps per second:  11, episode reward: 268.000, mean reward:  7.053 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 231.471318, mean_q: 80.919546, mean_eps: 0.100000\n"," 1 : 25  \n","--->:272267:<---\n","  55317/500000: episode: 1431, duration: 2.251s, episode steps:  24, steps per second:  11, episode reward: 161.000, mean reward:  6.708 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 209.578800, mean_q: 82.820417, mean_eps: 0.100000\n"," 1 : 18  \n","--->:273572:<---\n","  55353/500000: episode: 1432, duration: 3.278s, episode steps:  36, steps per second:  11, episode reward: 233.000, mean reward:  6.472 [-25.000, 22.000], mean action: 1.389 [0.000, 2.000],  loss: 219.872473, mean_q: 82.360771, mean_eps: 0.100000\n"," 1 : 17  \n","--->:274879:<---\n","  55391/500000: episode: 1433, duration: 3.439s, episode steps:  38, steps per second:  11, episode reward: 95.000, mean reward:  2.500 [-25.000, 22.000], mean action: 1.368 [0.000, 2.000],  loss: 241.557117, mean_q: 81.210951, mean_eps: 0.100000\n"," 1 : 15  \n","--->:276182:<---\n","  55417/500000: episode: 1434, duration: 2.373s, episode steps:  26, steps per second:  11, episode reward: -21.000, mean reward: -0.808 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 237.544148, mean_q: 80.051659, mean_eps: 0.100000\n"," 1 : 14  \n","--->:277484:<---\n","  55458/500000: episode: 1435, duration: 3.754s, episode steps:  41, steps per second:  11, episode reward: 127.000, mean reward:  3.098 [-25.000, 22.000], mean action: 1.024 [0.000, 2.000],  loss: 207.696942, mean_q: 81.430468, mean_eps: 0.100000\n"," 1 : 25  \n","--->:278793:<---\n","  55486/500000: episode: 1436, duration: 2.565s, episode steps:  28, steps per second:  11, episode reward: 227.000, mean reward:  8.107 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 255.499822, mean_q: 80.054134, mean_eps: 0.100000\n"," 1 : 21  \n","--->:280103:<---\n","  55530/500000: episode: 1437, duration: 4.029s, episode steps:  44, steps per second:  11, episode reward: 86.000, mean reward:  1.955 [-25.000, 22.000], mean action: 1.341 [0.000, 2.000],  loss: 242.583297, mean_q: 82.216532, mean_eps: 0.100000\n"," 1 : 21  \n","--->:281409:<---\n","  55555/500000: episode: 1438, duration: 2.318s, episode steps:  25, steps per second:  11, episode reward: 120.000, mean reward:  4.800 [-25.000, 22.000], mean action: 1.360 [0.000, 2.000],  loss: 267.781608, mean_q: 81.444953, mean_eps: 0.100000\n"," 1 : 14  \n","--->:282714:<---\n","  55581/500000: episode: 1439, duration: 2.444s, episode steps:  26, steps per second:  11, episode reward: 26.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.231 [0.000, 2.000],  loss: 245.960155, mean_q: 78.890822, mean_eps: 0.100000\n"," 1 : 14  \n","--->:284019:<---\n","  55618/500000: episode: 1440, duration: 3.348s, episode steps:  37, steps per second:  11, episode reward: -21.000, mean reward: -0.568 [-25.000, 22.000], mean action: 1.514 [0.000, 2.000],  loss: 222.225426, mean_q: 83.129621, mean_eps: 0.100000\n"," 1 : 14  \n","--->:285326:<---\n","  55650/500000: episode: 1441, duration: 2.998s, episode steps:  32, steps per second:  11, episode reward: 233.000, mean reward:  7.281 [-25.000, 22.000], mean action: 1.156 [0.000, 2.000],  loss: 242.494415, mean_q: 79.023932, mean_eps: 0.100000\n"," 1 : 17  \n","--->:286633:<---\n","  55663/500000: episode: 1442, duration: 1.227s, episode steps:  13, steps per second:  11, episode reward: 35.000, mean reward:  2.692 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 241.154519, mean_q: 81.662639, mean_eps: 0.100000\n"," 1 : 8  \n","--->:287938:<---\n","  55677/500000: episode: 1443, duration: 1.380s, episode steps:  14, steps per second:  10, episode reward: -31.000, mean reward: -2.214 [-25.000, 22.000], mean action: 1.429 [0.000, 2.000],  loss: 217.386918, mean_q: 81.560489, mean_eps: 0.100000\n"," 1 : 5  \n","--->:289247:<---\n","  55699/500000: episode: 1444, duration: 2.049s, episode steps:  22, steps per second:  11, episode reward: -62.000, mean reward: -2.818 [-25.000, 22.000], mean action: 1.364 [0.000, 2.000],  loss: 260.712882, mean_q: 83.816104, mean_eps: 0.100000\n"," 1 : 10  \n","--->:290557:<---\n","  55728/500000: episode: 1445, duration: 2.667s, episode steps:  29, steps per second:  11, episode reward: 139.000, mean reward:  4.793 [-25.000, 22.000], mean action: 1.138 [0.000, 2.000],  loss: 230.076317, mean_q: 83.060472, mean_eps: 0.100000\n"," 1 : 17  \n","--->:291858:<---\n","  55753/500000: episode: 1446, duration: 2.333s, episode steps:  25, steps per second:  11, episode reward: 48.000, mean reward:  1.920 [-25.000, 22.000], mean action: 1.080 [0.000, 2.000],  loss: 242.654965, mean_q: 82.740652, mean_eps: 0.100000\n"," 1 : 15  \n","--->:293160:<---\n","  55777/500000: episode: 1447, duration: 2.305s, episode steps:  24, steps per second:  10, episode reward: 117.000, mean reward:  4.875 [-25.000, 22.000], mean action: 1.042 [0.000, 2.000],  loss: 227.090321, mean_q: 82.356811, mean_eps: 0.100000\n"," 1 : 16  \n","--->:294469:<---\n","  55824/500000: episode: 1448, duration: 4.215s, episode steps:  47, steps per second:  11, episode reward: 149.000, mean reward:  3.170 [-25.000, 22.000], mean action: 1.043 [0.000, 2.000],  loss: 250.901161, mean_q: 81.657511, mean_eps: 0.100000\n"," 1 : 26  \n","--->:295774:<---\n","  55869/500000: episode: 1449, duration: 4.126s, episode steps:  45, steps per second:  11, episode reward: 230.000, mean reward:  5.111 [-25.000, 22.000], mean action: 1.422 [0.000, 2.000],  loss: 248.190887, mean_q: 81.197401, mean_eps: 0.100000\n"," 1 : 19  \n","--->:297077:<---\n","  55906/500000: episode: 1450, duration: 3.396s, episode steps:  37, steps per second:  11, episode reward: 224.000, mean reward:  6.054 [-25.000, 22.000], mean action: 1.081 [0.000, 2.000],  loss: 222.753528, mean_q: 83.369896, mean_eps: 0.100000\n"," 1 : 23  \n","--->:298385:<---\n","  55941/500000: episode: 1451, duration: 3.211s, episode steps:  35, steps per second:  11, episode reward: 202.000, mean reward:  5.771 [-25.000, 22.000], mean action: 1.029 [0.000, 2.000],  loss: 235.576005, mean_q: 81.478901, mean_eps: 0.100000\n"," 1 : 22  \n","--->:299694:<---\n","  55983/500000: episode: 1452, duration: 3.881s, episode steps:  42, steps per second:  11, episode reward: 196.000, mean reward:  4.667 [-25.000, 22.000], mean action: 1.048 [0.000, 2.000],  loss: 206.242751, mean_q: 81.994794, mean_eps: 0.100000\n"," 1 : 26  \n","--->:301003:<---\n","  56018/500000: episode: 1453, duration: 3.206s, episode steps:  35, steps per second:  11, episode reward: 246.000, mean reward:  7.029 [-25.000, 22.000], mean action: 0.914 [0.000, 2.000],  loss: 231.560108, mean_q: 80.696360, mean_eps: 0.100000\n"," 1 : 24  \n","--->:302310:<---\n","  56069/500000: episode: 1454, duration: 4.652s, episode steps:  51, steps per second:  11, episode reward: 171.000, mean reward:  3.353 [-25.000, 22.000], mean action: 1.255 [0.000, 2.000],  loss: 247.701545, mean_q: 82.808075, mean_eps: 0.100000\n"," 1 : 27  \n","--->:303619:<---\n","  56124/500000: episode: 1455, duration: 4.971s, episode steps:  55, steps per second:  11, episode reward: 284.000, mean reward:  5.164 [-25.000, 22.000], mean action: 1.273 [0.000, 2.000],  loss: 222.625420, mean_q: 82.264490, mean_eps: 0.100000\n"," 1 : 30  \n","--->:304923:<---\n","  56147/500000: episode: 1456, duration: 2.121s, episode steps:  23, steps per second:  11, episode reward: 252.000, mean reward: 10.957 [-25.000, 22.000], mean action: 0.739 [0.000, 2.000],  loss: 232.835861, mean_q: 82.260677, mean_eps: 0.100000\n"," 1 : 20  \n","--->:306228:<---\n","  56208/500000: episode: 1457, duration: 5.490s, episode steps:  61, steps per second:  11, episode reward: 268.000, mean reward:  4.393 [-25.000, 22.000], mean action: 1.492 [0.000, 2.000],  loss: 239.302157, mean_q: 81.982119, mean_eps: 0.100000\n"," 1 : 25  \n","--->:307531:<---\n","  56268/500000: episode: 1458, duration: 5.421s, episode steps:  60, steps per second:  11, episode reward: 150.000, mean reward:  2.500 [-25.000, 22.000], mean action: 0.767 [0.000, 2.000],  loss: 250.879315, mean_q: 82.554251, mean_eps: 0.100000\n"," 1 : 41  \n","--->:308837:<---\n","  56333/500000: episode: 1459, duration: 5.767s, episode steps:  65, steps per second:  11, episode reward: 156.000, mean reward:  2.400 [-25.000, 22.000], mean action: 0.969 [0.000, 2.000],  loss: 241.650850, mean_q: 81.939120, mean_eps: 0.100000\n"," 1 : 37  \n","--->:310146:<---\n","  56426/500000: episode: 1460, duration: 8.211s, episode steps:  93, steps per second:  11, episode reward: 196.000, mean reward:  2.108 [-25.000, 22.000], mean action: 1.613 [0.000, 2.000],  loss: 235.049495, mean_q: 82.124867, mean_eps: 0.100000\n"," 1 : 26  \n","--->:311452:<---\n","  56460/500000: episode: 1461, duration: 3.251s, episode steps:  34, steps per second:  10, episode reward: 42.000, mean reward:  1.235 [-25.000, 22.000], mean action: 1.235 [0.000, 2.000],  loss: 231.124848, mean_q: 82.491626, mean_eps: 0.100000\n"," 1 : 19  \n","--->:312755:<---\n","  56474/500000: episode: 1462, duration: 1.470s, episode steps:  14, steps per second:  10, episode reward: -59.000, mean reward: -4.214 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 209.203765, mean_q: 81.483970, mean_eps: 0.100000\n"," 1 : 8  \n","--->:314063:<---\n","  56489/500000: episode: 1463, duration: 1.575s, episode steps:  15, steps per second:  10, episode reward: 10.000, mean reward:  0.667 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 229.560482, mean_q: 81.399106, mean_eps: 0.100000\n"," 1 : 9  \n","--->:315371:<---\n","  56510/500000: episode: 1464, duration: 2.073s, episode steps:  21, steps per second:  10, episode reward: 54.000, mean reward:  2.571 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 260.448029, mean_q: 81.989940, mean_eps: 0.100000\n"," 1 : 11  \n","--->:316672:<---\n","  56517/500000: episode: 1465, duration: 0.828s, episode steps:   7, steps per second:   8, episode reward: -31.000, mean reward: -4.429 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 307.542539, mean_q: 82.896717, mean_eps: 0.100000\n"," 1 : 5  \n","--->:317982:<---\n","  56538/500000: episode: 1466, duration: 2.182s, episode steps:  21, steps per second:  10, episode reward: 148.000, mean reward:  7.048 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 244.119762, mean_q: 79.786341, mean_eps: 0.100000\n"," 1 : 11  \n","--->:319289:<---\n","  56588/500000: episode: 1467, duration: 4.876s, episode steps:  50, steps per second:  10, episode reward: 200.000, mean reward:  4.000 [-25.000, 22.000], mean action: 0.720 [0.000, 2.000],  loss: 231.235198, mean_q: 81.029950, mean_eps: 0.100000\n"," 1 : 39  \n","--->:320594:<---\n","  56626/500000: episode: 1468, duration: 3.549s, episode steps:  38, steps per second:  11, episode reward: 215.000, mean reward:  5.658 [-25.000, 22.000], mean action: 0.842 [0.000, 2.000],  loss: 223.585729, mean_q: 82.842300, mean_eps: 0.100000\n"," 1 : 29  \n","--->:321897:<---\n","  56686/500000: episode: 1469, duration: 5.475s, episode steps:  60, steps per second:  11, episode reward: 303.000, mean reward:  5.050 [-25.000, 22.000], mean action: 1.083 [0.000, 2.000],  loss: 232.711434, mean_q: 81.930844, mean_eps: 0.100000\n"," 1 : 33  \n","--->:323206:<---\n","  56741/500000: episode: 1470, duration: 5.046s, episode steps:  55, steps per second:  11, episode reward: 495.000, mean reward:  9.000 [-25.000, 22.000], mean action: 0.691 [0.000, 2.000],  loss: 228.489491, mean_q: 81.271414, mean_eps: 0.100000\n"," 1 : 46  \n","--->:324513:<---\n","  56812/500000: episode: 1471, duration: 6.416s, episode steps:  71, steps per second:  11, episode reward: 392.000, mean reward:  5.521 [-25.000, 22.000], mean action: 0.831 [0.000, 2.000],  loss: 256.169384, mean_q: 81.405429, mean_eps: 0.100000\n"," 1 : 52  \n","--->:325823:<---\n","  56888/500000: episode: 1472, duration: 6.788s, episode steps:  76, steps per second:  11, episode reward: 417.000, mean reward:  5.487 [-25.000, 22.000], mean action: 1.145 [0.000, 2.000],  loss: 232.182921, mean_q: 80.569348, mean_eps: 0.100000\n"," 1 : 51  \n","--->:327127:<---\n","  56946/500000: episode: 1473, duration: 5.213s, episode steps:  58, steps per second:  11, episode reward: 466.000, mean reward:  8.034 [-25.000, 22.000], mean action: 1.241 [0.000, 2.000],  loss: 228.721118, mean_q: 81.697777, mean_eps: 0.100000\n"," 1 : 34  \n","--->:328435:<---\n","  56993/500000: episode: 1474, duration: 4.272s, episode steps:  47, steps per second:  11, episode reward: 309.000, mean reward:  6.574 [-25.000, 22.000], mean action: 1.128 [0.000, 2.000],  loss: 252.361475, mean_q: 81.215236, mean_eps: 0.100000\n"," 1 : 29  \n","--->:329742:<---\n","  57067/500000: episode: 1475, duration: 6.639s, episode steps:  74, steps per second:  11, episode reward: 285.000, mean reward:  3.851 [-25.000, 22.000], mean action: 1.081 [0.000, 2.000],  loss: 230.045034, mean_q: 81.843442, mean_eps: 0.100000\n"," 1 : 45  \n","--->:331051:<---\n","  57134/500000: episode: 1476, duration: 5.981s, episode steps:  67, steps per second:  11, episode reward: 75.000, mean reward:  1.119 [-25.000, 22.000], mean action: 0.925 [0.000, 2.000],  loss: 246.732189, mean_q: 81.313686, mean_eps: 0.100000\n"," 1 : 44  \n","--->:332353:<---\n","  57205/500000: episode: 1477, duration: 6.383s, episode steps:  71, steps per second:  11, episode reward: 527.000, mean reward:  7.423 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 227.262478, mean_q: 82.025311, mean_eps: 0.100000\n"," 1 : 56  \n","--->:333657:<---\n","  57257/500000: episode: 1478, duration: 4.781s, episode steps:  52, steps per second:  11, episode reward: 431.000, mean reward:  8.288 [-25.000, 22.000], mean action: 1.269 [0.000, 2.000],  loss: 249.102781, mean_q: 80.889269, mean_eps: 0.100000\n"," 1 : 26  \n","--->:334959:<---\n","  57308/500000: episode: 1479, duration: 4.650s, episode steps:  51, steps per second:  11, episode reward: 272.000, mean reward:  5.333 [-25.000, 22.000], mean action: 0.765 [0.000, 2.000],  loss: 230.120771, mean_q: 82.780748, mean_eps: 0.100000\n"," 1 : 38  \n","--->:336260:<---\n","  57415/500000: episode: 1480, duration: 9.724s, episode steps: 107, steps per second:  11, episode reward: 792.000, mean reward:  7.402 [-25.000, 22.000], mean action: 0.832 [0.000, 2.000],  loss: 229.918001, mean_q: 82.254080, mean_eps: 0.100000\n"," 1 : 83  \n","--->:337562:<---\n","  57502/500000: episode: 1481, duration: 7.790s, episode steps:  87, steps per second:  11, episode reward: 430.000, mean reward:  4.943 [-25.000, 22.000], mean action: 1.069 [0.000, 2.000],  loss: 244.645717, mean_q: 81.834606, mean_eps: 0.100000\n"," 1 : 58  \n","--->:338864:<---\n","  57539/500000: episode: 1482, duration: 3.458s, episode steps:  37, steps per second:  11, episode reward: 249.000, mean reward:  6.730 [-25.000, 22.000], mean action: 1.027 [0.000, 2.000],  loss: 232.855663, mean_q: 81.746092, mean_eps: 0.100000\n"," 1 : 22  \n","--->:340171:<---\n","  57572/500000: episode: 1483, duration: 3.060s, episode steps:  33, steps per second:  11, episode reward: 293.000, mean reward:  8.879 [-25.000, 22.000], mean action: 0.879 [0.000, 2.000],  loss: 212.111682, mean_q: 81.258836, mean_eps: 0.100000\n"," 1 : 24  \n","--->:341473:<---\n","  57641/500000: episode: 1484, duration: 6.231s, episode steps:  69, steps per second:  11, episode reward: -35.000, mean reward: -0.507 [-25.000, 22.000], mean action: 1.159 [0.000, 2.000],  loss: 229.789987, mean_q: 81.584940, mean_eps: 0.100000\n"," 1 : 39  \n","--->:342776:<---\n","  57720/500000: episode: 1485, duration: 6.872s, episode steps:  79, steps per second:  11, episode reward: 511.000, mean reward:  6.468 [-25.000, 22.000], mean action: 1.076 [0.000, 2.000],  loss: 235.542486, mean_q: 82.170522, mean_eps: 0.100000\n"," 1 : 51  \n","--->:344084:<---\n","  57813/500000: episode: 1486, duration: 8.131s, episode steps:  93, steps per second:  11, episode reward: 336.000, mean reward:  3.613 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 238.854082, mean_q: 81.668186, mean_eps: 0.100000\n"," 1 : 58  \n","--->:345387:<---\n","  57864/500000: episode: 1487, duration: 4.676s, episode steps:  51, steps per second:  11, episode reward: 234.000, mean reward:  4.588 [-25.000, 22.000], mean action: 1.255 [0.000, 2.000],  loss: 231.516991, mean_q: 82.688712, mean_eps: 0.100000\n"," 1 : 32  \n","--->:346696:<---\n","  57906/500000: episode: 1488, duration: 3.881s, episode steps:  42, steps per second:  11, episode reward: 96.000, mean reward:  2.286 [-25.000, 22.000], mean action: 0.976 [0.000, 2.000],  loss: 233.871923, mean_q: 83.147604, mean_eps: 0.100000\n"," 1 : 30  \n","--->:348006:<---\n","  57944/500000: episode: 1489, duration: 3.473s, episode steps:  38, steps per second:  11, episode reward: 146.000, mean reward:  3.842 [-25.000, 22.000], mean action: 0.868 [0.000, 2.000],  loss: 250.265002, mean_q: 80.408883, mean_eps: 0.100000\n"," 1 : 28  \n","--->:349308:<---\n","  58025/500000: episode: 1490, duration: 7.127s, episode steps:  81, steps per second:  11, episode reward: 775.000, mean reward:  9.568 [-25.000, 22.000], mean action: 0.914 [0.000, 2.000],  loss: 222.894005, mean_q: 82.949717, mean_eps: 0.100000\n"," 1 : 63  \n","--->:350609:<---\n","  58117/500000: episode: 1491, duration: 8.159s, episode steps:  92, steps per second:  11, episode reward: 323.000, mean reward:  3.511 [-25.000, 22.000], mean action: 1.120 [0.000, 2.000],  loss: 233.326642, mean_q: 81.869093, mean_eps: 0.100000\n"," 1 : 51  \n","--->:351919:<---\n","  58175/500000: episode: 1492, duration: 5.205s, episode steps:  58, steps per second:  11, episode reward: 241.000, mean reward:  4.155 [-25.000, 22.000], mean action: 0.776 [0.000, 2.000],  loss: 219.308592, mean_q: 81.785605, mean_eps: 0.100000\n"," 1 : 43  \n","--->:353226:<---\n","  58260/500000: episode: 1493, duration: 7.710s, episode steps:  85, steps per second:  11, episode reward: 546.000, mean reward:  6.424 [-25.000, 22.000], mean action: 0.953 [0.000, 2.000],  loss: 234.387187, mean_q: 82.462592, mean_eps: 0.100000\n"," 1 : 59  \n","--->:354534:<---\n","  58379/500000: episode: 1494, duration: 10.495s, episode steps: 119, steps per second:  11, episode reward: 506.000, mean reward:  4.252 [-25.000, 22.000], mean action: 1.118 [0.000, 2.000],  loss: 254.994156, mean_q: 81.541205, mean_eps: 0.100000\n"," 1 : 70  \n","--->:355844:<---\n","  58452/500000: episode: 1495, duration: 6.522s, episode steps:  73, steps per second:  11, episode reward: 496.000, mean reward:  6.795 [-25.000, 22.000], mean action: 0.726 [0.000, 2.000],  loss: 230.152085, mean_q: 80.843092, mean_eps: 0.100000\n"," 1 : 61  \n","--->:357146:<---\n","  58553/500000: episode: 1496, duration: 9.085s, episode steps: 101, steps per second:  11, episode reward: 911.000, mean reward:  9.020 [-25.000, 22.000], mean action: 0.762 [0.000, 2.000],  loss: 238.545316, mean_q: 82.010515, mean_eps: 0.100000\n"," 1 : 82  \n","--->:358455:<---\n","  58597/500000: episode: 1497, duration: 4.073s, episode steps:  44, steps per second:  11, episode reward: 400.000, mean reward:  9.091 [-25.000, 22.000], mean action: 1.205 [0.000, 2.000],  loss: 221.791416, mean_q: 80.360508, mean_eps: 0.100000\n"," 1 : 31  \n","--->:359762:<---\n","  58618/500000: episode: 1498, duration: 1.943s, episode steps:  21, steps per second:  11, episode reward: 145.000, mean reward:  6.905 [-25.000, 22.000], mean action: 1.095 [0.000, 2.000],  loss: 248.687325, mean_q: 81.429022, mean_eps: 0.100000\n"," 1 : 13  \n","--->:361063:<---\n","  58663/500000: episode: 1499, duration: 4.093s, episode steps:  45, steps per second:  11, episode reward: -142.000, mean reward: -3.156 [-25.000, 22.000], mean action: 0.978 [0.000, 2.000],  loss: 228.463130, mean_q: 81.645731, mean_eps: 0.100000\n"," 1 : 32  \n","--->:362373:<---\n","  58701/500000: episode: 1500, duration: 3.410s, episode steps:  38, steps per second:  11, episode reward: 205.000, mean reward:  5.395 [-25.000, 22.000], mean action: 1.368 [0.000, 2.000],  loss: 238.881745, mean_q: 82.044312, mean_eps: 0.100000\n"," 1 : 20  \n","--->:363675:<---\n","  58751/500000: episode: 1501, duration: 4.514s, episode steps:  50, steps per second:  11, episode reward: 14.000, mean reward:  0.280 [-25.000, 22.000], mean action: 1.300 [0.000, 2.000],  loss: 242.862982, mean_q: 82.628191, mean_eps: 0.100000\n"," 1 : 22  \n","--->:364982:<---\n","  58810/500000: episode: 1502, duration: 5.260s, episode steps:  59, steps per second:  11, episode reward: 224.000, mean reward:  3.797 [-25.000, 22.000], mean action: 1.407 [0.000, 2.000],  loss: 207.970443, mean_q: 81.469821, mean_eps: 0.100000\n"," 1 : 23  \n","--->:366288:<---\n","  58833/500000: episode: 1503, duration: 2.166s, episode steps:  23, steps per second:  11, episode reward: 167.000, mean reward:  7.261 [-25.000, 22.000], mean action: 1.087 [0.000, 2.000],  loss: 232.655599, mean_q: 83.677676, mean_eps: 0.100000\n"," 1 : 14  \n","--->:367593:<---\n","  58867/500000: episode: 1504, duration: 3.106s, episode steps:  34, steps per second:  11, episode reward: 293.000, mean reward:  8.618 [-25.000, 22.000], mean action: 0.794 [0.000, 2.000],  loss: 237.276368, mean_q: 80.496031, mean_eps: 0.100000\n"," 1 : 24  \n","--->:368895:<---\n","  58933/500000: episode: 1505, duration: 5.874s, episode steps:  66, steps per second:  11, episode reward: 463.000, mean reward:  7.015 [-25.000, 22.000], mean action: 1.258 [0.000, 2.000],  loss: 245.755603, mean_q: 81.552463, mean_eps: 0.100000\n"," 1 : 36  \n","--->:370201:<---\n","  58990/500000: episode: 1506, duration: 5.070s, episode steps:  57, steps per second:  11, episode reward: 124.000, mean reward:  2.175 [-25.000, 22.000], mean action: 1.386 [0.000, 2.000],  loss: 233.514557, mean_q: 81.360498, mean_eps: 0.100000\n"," 1 : 27  \n","--->:371511:<---\n","  59037/500000: episode: 1507, duration: 4.128s, episode steps:  47, steps per second:  11, episode reward: 249.000, mean reward:  5.298 [-25.000, 22.000], mean action: 1.319 [0.000, 2.000],  loss: 219.421048, mean_q: 81.767997, mean_eps: 0.100000\n"," 1 : 22  \n","--->:372818:<---\n","  59082/500000: episode: 1508, duration: 4.070s, episode steps:  45, steps per second:  11, episode reward: 155.000, mean reward:  3.444 [-25.000, 22.000], mean action: 1.356 [0.000, 2.000],  loss: 212.267070, mean_q: 80.305879, mean_eps: 0.100000\n"," 1 : 22  \n","--->:374119:<---\n","  59118/500000: episode: 1509, duration: 3.233s, episode steps:  36, steps per second:  11, episode reward: 177.000, mean reward:  4.917 [-25.000, 22.000], mean action: 1.139 [0.000, 2.000],  loss: 232.024627, mean_q: 81.326062, mean_eps: 0.100000\n"," 1 : 23  \n","--->:375423:<---\n","  59150/500000: episode: 1510, duration: 2.927s, episode steps:  32, steps per second:  11, episode reward: -5.000, mean reward: -0.156 [-25.000, 22.000], mean action: 1.188 [0.000, 2.000],  loss: 254.370061, mean_q: 82.353616, mean_eps: 0.100000\n"," 1 : 19  \n","--->:376731:<---\n","  59185/500000: episode: 1511, duration: 3.181s, episode steps:  35, steps per second:  11, episode reward: 123.000, mean reward:  3.514 [-25.000, 22.000], mean action: 1.571 [0.000, 2.000],  loss: 242.866174, mean_q: 82.327484, mean_eps: 0.100000\n"," 1 : 12  \n","--->:378035:<---\n","  59205/500000: episode: 1512, duration: 1.925s, episode steps:  20, steps per second:  10, episode reward: -65.000, mean reward: -3.250 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 234.825163, mean_q: 83.577063, mean_eps: 0.100000\n"," 1 : 12  \n","--->:379337:<---\n","  59233/500000: episode: 1513, duration: 2.673s, episode steps:  28, steps per second:  10, episode reward: 98.000, mean reward:  3.500 [-25.000, 22.000], mean action: 1.321 [0.000, 2.000],  loss: 213.115532, mean_q: 81.159098, mean_eps: 0.100000\n"," 1 : 13  \n","--->:380644:<---\n","  59249/500000: episode: 1514, duration: 1.591s, episode steps:  16, steps per second:  10, episode reward: 148.000, mean reward:  9.250 [-25.000, 22.000], mean action: 0.812 [0.000, 2.000],  loss: 215.106199, mean_q: 80.298684, mean_eps: 0.100000\n"," 1 : 11  \n","--->:381951:<---\n","  59299/500000: episode: 1515, duration: 4.499s, episode steps:  50, steps per second:  11, episode reward: 98.000, mean reward:  1.960 [-25.000, 22.000], mean action: 1.720 [0.000, 2.000],  loss: 245.991634, mean_q: 82.037416, mean_eps: 0.100000\n"," 1 : 13  \n","--->:383258:<---\n","  59323/500000: episode: 1516, duration: 2.240s, episode steps:  24, steps per second:  11, episode reward: 98.000, mean reward:  4.083 [-25.000, 22.000], mean action: 1.417 [0.000, 2.000],  loss: 222.267504, mean_q: 84.379685, mean_eps: 0.100000\n"," 1 : 13  \n","--->:384561:<---\n","  59358/500000: episode: 1517, duration: 3.135s, episode steps:  35, steps per second:  11, episode reward: 142.000, mean reward:  4.057 [-25.000, 22.000], mean action: 1.457 [0.000, 2.000],  loss: 240.999277, mean_q: 81.963678, mean_eps: 0.100000\n"," 1 : 15  \n","--->:385865:<---\n","  59377/500000: episode: 1518, duration: 1.813s, episode steps:  19, steps per second:  10, episode reward: -84.000, mean reward: -4.421 [-25.000, 22.000], mean action: 1.316 [0.000, 2.000],  loss: 229.240745, mean_q: 84.021633, mean_eps: 0.100000\n"," 1 : 9  \n","--->:387167:<---\n","  59406/500000: episode: 1519, duration: 2.646s, episode steps:  29, steps per second:  11, episode reward: -56.000, mean reward: -1.931 [-25.000, 22.000], mean action: 1.690 [0.000, 2.000],  loss: 244.678045, mean_q: 80.442044, mean_eps: 0.100000\n"," 1 : 6  \n","--->:388477:<---\n","  59420/500000: episode: 1520, duration: 1.381s, episode steps:  14, steps per second:  10, episode reward: 38.000, mean reward:  2.714 [-25.000, 22.000], mean action: 1.214 [0.000, 2.000],  loss: 250.622771, mean_q: 81.790955, mean_eps: 0.100000\n"," 1 : 6  \n","--->:389780:<---\n","  59434/500000: episode: 1521, duration: 1.361s, episode steps:  14, steps per second:  10, episode reward: 26.000, mean reward:  1.857 [-25.000, 22.000], mean action: 0.357 [0.000, 1.000],  loss: 221.536153, mean_q: 81.476345, mean_eps: 0.100000\n"," 1 : 14  \n","--->:391089:<---\n","  59445/500000: episode: 1522, duration: 1.090s, episode steps:  11, steps per second:  10, episode reward: -109.000, mean reward: -9.909 [-25.000, 22.000], mean action: 0.455 [0.000, 2.000],  loss: 283.681992, mean_q: 80.514524, mean_eps: 0.100000\n"," 1 : 10  \n","--->:392396:<---\n","  59458/500000: episode: 1523, duration: 1.276s, episode steps:  13, steps per second:  10, episode reward: 104.000, mean reward:  8.000 [-25.000, 22.000], mean action: 1.154 [0.000, 2.000],  loss: 235.580619, mean_q: 81.336242, mean_eps: 0.100000\n"," 1 : 9  \n","--->:393706:<---\n","  59473/500000: episode: 1524, duration: 1.445s, episode steps:  15, steps per second:  10, episode reward: 148.000, mean reward:  9.867 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 237.014434, mean_q: 81.857720, mean_eps: 0.100000\n"," 1 : 11  \n","--->:395010:<---\n","  59482/500000: episode: 1525, duration: 0.914s, episode steps:   9, steps per second:  10, episode reward: -128.000, mean reward: -14.222 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 190.892385, mean_q: 82.338156, mean_eps: 0.100000\n"," 1 : 7  \n","--->:396319:<---\n","  59496/500000: episode: 1526, duration: 1.405s, episode steps:  14, steps per second:  10, episode reward: 104.000, mean reward:  7.429 [-25.000, 22.000], mean action: 0.929 [0.000, 2.000],  loss: 250.661158, mean_q: 82.953587, mean_eps: 0.100000\n"," 1 : 9  \n","--->:397624:<---\n","  59513/500000: episode: 1527, duration: 1.623s, episode steps:  17, steps per second:  10, episode reward: 151.000, mean reward:  8.882 [-25.000, 22.000], mean action: 1.176 [0.000, 2.000],  loss: 233.566472, mean_q: 82.166250, mean_eps: 0.100000\n"," 1 : 9  \n","--->:398927:<---\n","  59521/500000: episode: 1528, duration: 0.828s, episode steps:   8, steps per second:  10, episode reward: 85.000, mean reward: 10.625 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 207.268147, mean_q: 82.512927, mean_eps: 0.100000\n"," 1 : 6  \n","--->:400232:<---\n","  59542/500000: episode: 1529, duration: 2.085s, episode steps:  21, steps per second:  10, episode reward: 173.000, mean reward:  8.238 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 219.854391, mean_q: 84.144895, mean_eps: 0.100000\n"," 1 : 10  \n","--->:401534:<---\n","  59561/500000: episode: 1530, duration: 1.814s, episode steps:  19, steps per second:  10, episode reward: -18.000, mean reward: -0.947 [-25.000, 22.000], mean action: 1.053 [0.000, 2.000],  loss: 265.091865, mean_q: 79.674023, mean_eps: 0.100000\n"," 1 : 12  \n","--->:402837:<---\n","  59606/500000: episode: 1531, duration: 4.097s, episode steps:  45, steps per second:  11, episode reward: 45.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.467 [0.000, 2.000],  loss: 239.552097, mean_q: 81.213198, mean_eps: 0.100000\n"," 1 : 17  \n","--->:404139:<---\n","  59647/500000: episode: 1532, duration: 3.663s, episode steps:  41, steps per second:  11, episode reward: 92.000, mean reward:  2.244 [-25.000, 22.000], mean action: 1.366 [0.000, 2.000],  loss: 242.849962, mean_q: 80.645404, mean_eps: 0.100000\n"," 1 : 17  \n","--->:405443:<---\n","  59685/500000: episode: 1533, duration: 3.409s, episode steps:  38, steps per second:  11, episode reward: 130.000, mean reward:  3.421 [-25.000, 22.000], mean action: 1.132 [0.000, 2.000],  loss: 258.020433, mean_q: 81.478433, mean_eps: 0.100000\n"," 1 : 23  \n","--->:406749:<---\n","  59742/500000: episode: 1534, duration: 5.111s, episode steps:  57, steps per second:  11, episode reward: 64.000, mean reward:  1.123 [-25.000, 22.000], mean action: 1.561 [0.000, 2.000],  loss: 232.897899, mean_q: 82.962878, mean_eps: 0.100000\n"," 1 : 20  \n","--->:408056:<---\n","  59791/500000: episode: 1535, duration: 4.406s, episode steps:  49, steps per second:  11, episode reward: 149.000, mean reward:  3.041 [-25.000, 22.000], mean action: 1.041 [0.000, 2.000],  loss: 229.081421, mean_q: 82.369571, mean_eps: 0.100000\n"," 1 : 26  \n","--->:409362:<---\n","  59823/500000: episode: 1536, duration: 2.909s, episode steps:  32, steps per second:  11, episode reward: 20.000, mean reward:  0.625 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 212.230013, mean_q: 82.980007, mean_eps: 0.100000\n"," 1 : 18  \n","--->:410667:<---\n","  59863/500000: episode: 1537, duration: 3.584s, episode steps:  40, steps per second:  11, episode reward: 149.000, mean reward:  3.725 [-25.000, 22.000], mean action: 0.950 [0.000, 2.000],  loss: 261.927819, mean_q: 81.133545, mean_eps: 0.100000\n"," 1 : 26  \n","--->:411970:<---\n","  59891/500000: episode: 1538, duration: 2.578s, episode steps:  28, steps per second:  11, episode reward: 180.000, mean reward:  6.429 [-25.000, 22.000], mean action: 0.893 [0.000, 2.000],  loss: 238.526470, mean_q: 81.567277, mean_eps: 0.100000\n"," 1 : 21  \n","--->:413278:<---\n","  59916/500000: episode: 1539, duration: 2.284s, episode steps:  25, steps per second:  11, episode reward: 173.000, mean reward:  6.920 [-25.000, 22.000], mean action: 1.520 [0.000, 2.000],  loss: 221.289448, mean_q: 80.557594, mean_eps: 0.100000\n"," 1 : 10  \n","--->:414584:<---\n","  59929/500000: episode: 1540, duration: 1.270s, episode steps:  13, steps per second:  10, episode reward: 79.000, mean reward:  6.077 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 236.940231, mean_q: 81.617397, mean_eps: 0.100000\n"," 1 : 10  \n","--->:415893:<---\n","  59946/500000: episode: 1541, duration: 1.610s, episode steps:  17, steps per second:  11, episode reward: -62.000, mean reward: -3.647 [-25.000, 22.000], mean action: 1.059 [0.000, 2.000],  loss: 233.654203, mean_q: 82.236148, mean_eps: 0.100000\n"," 1 : 10  \n","--->:417194:<---\n","  59962/500000: episode: 1542, duration: 1.565s, episode steps:  16, steps per second:  10, episode reward: 79.000, mean reward:  4.938 [-25.000, 22.000], mean action: 1.125 [0.000, 2.000],  loss: 207.686494, mean_q: 80.700981, mean_eps: 0.100000\n"," 1 : 10  \n","--->:418498:<---\n","  59982/500000: episode: 1543, duration: 1.895s, episode steps:  20, steps per second:  11, episode reward: 98.000, mean reward:  4.900 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 306.470824, mean_q: 80.309156, mean_eps: 0.100000\n"," 1 : 13  \n","--->:419807:<---\n","  59999/500000: episode: 1544, duration: 1.652s, episode steps:  17, steps per second:  10, episode reward: 104.000, mean reward:  6.118 [-25.000, 22.000], mean action: 1.412 [0.000, 2.000],  loss: 250.112716, mean_q: 83.817120, mean_eps: 0.100000\n"," 1 : 9  \n","--->:421113:<---\n","  60031/500000: episode: 1545, duration: 2.954s, episode steps:  32, steps per second:  11, episode reward: 98.000, mean reward:  3.062 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 201.020545, mean_q: 80.787009, mean_eps: 0.100000\n"," 1 : 13  \n","--->:422423:<---\n","  60076/500000: episode: 1546, duration: 4.078s, episode steps:  45, steps per second:  11, episode reward: 120.000, mean reward:  2.667 [-25.000, 22.000], mean action: 1.489 [0.000, 2.000],  loss: 219.470219, mean_q: 81.250585, mean_eps: 0.100000\n"," 1 : 14  \n","--->:423725:<---\n","  60122/500000: episode: 1547, duration: 4.100s, episode steps:  46, steps per second:  11, episode reward:  4.000, mean reward:  0.087 [-25.000, 22.000], mean action: 1.696 [0.000, 2.000],  loss: 267.102425, mean_q: 79.503936, mean_eps: 0.100000\n"," 1 : 13  \n","--->:425035:<---\n","  60142/500000: episode: 1548, duration: 1.892s, episode steps:  20, steps per second:  11, episode reward: 82.000, mean reward:  4.100 [-25.000, 22.000], mean action: 1.350 [0.000, 2.000],  loss: 245.013356, mean_q: 83.212557, mean_eps: 0.100000\n"," 1 : 8  \n","--->:426343:<---\n","  60177/500000: episode: 1549, duration: 3.206s, episode steps:  35, steps per second:  11, episode reward: 205.000, mean reward:  5.857 [-25.000, 22.000], mean action: 1.057 [0.000, 2.000],  loss: 232.199303, mean_q: 81.029259, mean_eps: 0.100000\n"," 1 : 20  \n","--->:427651:<---\n","  60220/500000: episode: 1550, duration: 3.841s, episode steps:  43, steps per second:  11, episode reward: -71.000, mean reward: -1.651 [-25.000, 22.000], mean action: 1.442 [0.000, 2.000],  loss: 235.984956, mean_q: 81.350002, mean_eps: 0.100000\n"," 1 : 16  \n","--->:428954:<---\n","  60243/500000: episode: 1551, duration: 2.084s, episode steps:  23, steps per second:  11, episode reward: 261.000, mean reward: 11.348 [-25.000, 22.000], mean action: 1.261 [0.000, 2.000],  loss: 265.804743, mean_q: 80.377411, mean_eps: 0.100000\n"," 1 : 14  \n","--->:430258:<---\n","  60259/500000: episode: 1552, duration: 1.550s, episode steps:  16, steps per second:  10, episode reward: 48.000, mean reward:  3.000 [-25.000, 22.000], mean action: 0.438 [0.000, 2.000],  loss: 283.226574, mean_q: 82.271797, mean_eps: 0.100000\n"," 1 : 15  \n","--->:431567:<---\n","  60291/500000: episode: 1553, duration: 2.975s, episode steps:  32, steps per second:  11, episode reward: 67.000, mean reward:  2.094 [-25.000, 22.000], mean action: 1.031 [0.000, 2.000],  loss: 228.389614, mean_q: 83.401320, mean_eps: 0.100000\n"," 1 : 18  \n","--->:432877:<---\n","  60317/500000: episode: 1554, duration: 2.418s, episode steps:  26, steps per second:  11, episode reward: 117.000, mean reward:  4.500 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 239.002103, mean_q: 81.051971, mean_eps: 0.100000\n"," 1 : 16  \n","--->:434181:<---\n","  60386/500000: episode: 1555, duration: 6.156s, episode steps:  69, steps per second:  11, episode reward: 89.000, mean reward:  1.290 [-25.000, 22.000], mean action: 1.638 [0.000, 2.000],  loss: 231.338458, mean_q: 80.575807, mean_eps: 0.100000\n"," 1 : 19  \n","--->:435488:<---\n","  60421/500000: episode: 1556, duration: 3.173s, episode steps:  35, steps per second:  11, episode reward: 277.000, mean reward:  7.914 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 223.826457, mean_q: 82.750074, mean_eps: 0.100000\n"," 1 : 19  \n","--->:436792:<---\n","  60437/500000: episode: 1557, duration: 1.538s, episode steps:  16, steps per second:  10, episode reward: 95.000, mean reward:  5.938 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 239.810212, mean_q: 82.935606, mean_eps: 0.100000\n"," 1 : 15  \n","--->:438097:<---\n","  60451/500000: episode: 1558, duration: 1.388s, episode steps:  14, steps per second:  10, episode reward: 76.000, mean reward:  5.429 [-25.000, 22.000], mean action: 0.429 [0.000, 2.000],  loss: 215.845219, mean_q: 81.255571, mean_eps: 0.100000\n"," 1 : 12  \n","--->:439404:<---\n","  60483/500000: episode: 1559, duration: 2.923s, episode steps:  32, steps per second:  11, episode reward: 346.000, mean reward: 10.812 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 217.831539, mean_q: 80.048963, mean_eps: 0.100000\n"," 1 : 20  \n","--->:440708:<---\n","  60619/500000: episode: 1560, duration: 12.209s, episode steps: 136, steps per second:  11, episode reward: 728.000, mean reward:  5.353 [-25.000, 22.000], mean action: 0.787 [0.000, 2.000],  loss: 231.657216, mean_q: 81.045981, mean_eps: 0.100000\n"," 1 : 110  \n","--->:442014:<---\n","  60857/500000: episode: 1561, duration: 21.029s, episode steps: 238, steps per second:  11, episode reward: 722.000, mean reward:  3.034 [-25.000, 22.000], mean action: 1.374 [0.000, 2.000],  loss: 232.981520, mean_q: 81.165569, mean_eps: 0.100000\n"," 1 : 114  \n","--->:443316:<---\n","  60874/500000: episode: 1562, duration: 1.659s, episode steps:  17, steps per second:  10, episode reward: -59.000, mean reward: -3.471 [-25.000, 22.000], mean action: 1.353 [0.000, 2.000],  loss: 249.511018, mean_q: 84.398818, mean_eps: 0.100000\n"," 1 : 8  \n","--->:444621:<---\n","  60886/500000: episode: 1563, duration: 1.208s, episode steps:  12, steps per second:  10, episode reward: 13.000, mean reward:  1.083 [-25.000, 22.000], mean action: 0.917 [0.000, 2.000],  loss: 235.087425, mean_q: 81.432957, mean_eps: 0.100000\n"," 1 : 7  \n","--->:445923:<---\n","  60890/500000: episode: 1564, duration: 0.521s, episode steps:   4, steps per second:   8, episode reward: -75.000, mean reward: -18.750 [-25.000,  0.000], mean action: 1.250 [1.000, 2.000],  loss: 272.483593, mean_q: 78.658319, mean_eps: 0.100000\n"," 1 : 3  \n","--->:447224:<---\n","  60896/500000: episode: 1565, duration: 0.703s, episode steps:   6, steps per second:   9, episode reward: 88.000, mean reward: 14.667 [ 0.000, 22.000], mean action: 0.833 [0.000, 2.000],  loss: 244.049886, mean_q: 79.954928, mean_eps: 0.100000\n"," 1 : 4  \n","--->:448525:<---\n","  60911/500000: episode: 1566, duration: 1.541s, episode steps:  15, steps per second:  10, episode reward: 16.000, mean reward:  1.067 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 185.529126, mean_q: 81.179714, mean_eps: 0.100000\n"," 1 : 5  \n","--->:449835:<---\n","  60939/500000: episode: 1567, duration: 2.765s, episode steps:  28, steps per second:  10, episode reward: 123.000, mean reward:  4.393 [-25.000, 22.000], mean action: 1.357 [0.000, 2.000],  loss: 246.352763, mean_q: 82.192064, mean_eps: 0.100000\n"," 1 : 12  \n","--->:451144:<---\n","  60970/500000: episode: 1568, duration: 3.290s, episode steps:  31, steps per second:   9, episode reward: 101.000, mean reward:  3.258 [-25.000, 22.000], mean action: 1.452 [0.000, 2.000],  loss: 241.490319, mean_q: 80.454756, mean_eps: 0.100000\n"," 1 : 11  \n","--->:452445:<---\n","  60987/500000: episode: 1569, duration: 1.863s, episode steps:  17, steps per second:   9, episode reward: 60.000, mean reward:  3.529 [-25.000, 22.000], mean action: 1.353 [0.000, 2.000],  loss: 240.370219, mean_q: 80.351022, mean_eps: 0.100000\n"," 1 : 7  \n","--->:453748:<---\n","  60996/500000: episode: 1570, duration: 1.089s, episode steps:   9, steps per second:   8, episode reward: 13.000, mean reward:  1.444 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 254.384491, mean_q: 79.603026, mean_eps: 0.100000\n"," 1 : 7  \n","--->:455056:<---\n","  61030/500000: episode: 1571, duration: 3.331s, episode steps:  34, steps per second:  10, episode reward: 123.000, mean reward:  3.618 [-25.000, 22.000], mean action: 1.559 [0.000, 2.000],  loss: 253.620300, mean_q: 80.335623, mean_eps: 0.100000\n"," 1 : 12  \n","--->:456364:<---\n","  61035/500000: episode: 1572, duration: 0.611s, episode steps:   5, steps per second:   8, episode reward: 63.000, mean reward: 12.600 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 252.440518, mean_q: 81.022029, mean_eps: 0.100000\n"," 1 : 5  \n","--->:457665:<---\n","  61048/500000: episode: 1573, duration: 1.278s, episode steps:  13, steps per second:  10, episode reward: 104.000, mean reward:  8.000 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 242.704133, mean_q: 82.379781, mean_eps: 0.100000\n"," 1 : 9  \n","--->:458968:<---\n","  61073/500000: episode: 1574, duration: 2.376s, episode steps:  25, steps per second:  11, episode reward: 79.000, mean reward:  3.160 [-25.000, 22.000], mean action: 1.440 [0.000, 2.000],  loss: 250.469053, mean_q: 81.028862, mean_eps: 0.100000\n"," 1 : 10  \n","--->:460270:<---\n","  61081/500000: episode: 1575, duration: 0.883s, episode steps:   8, steps per second:   9, episode reward: 132.000, mean reward: 16.500 [ 0.000, 22.000], mean action: 1.250 [1.000, 2.000],  loss: 228.150724, mean_q: 84.800183, mean_eps: 0.100000\n"," 1 : 6  \n","--->:461573:<---\n","  61084/500000: episode: 1576, duration: 0.423s, episode steps:   3, steps per second:   7, episode reward: 19.000, mean reward:  6.333 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 242.996663, mean_q: 78.936785, mean_eps: 0.100000\n"," 1 : 3  \n","--->:462880:<---\n","  61110/500000: episode: 1577, duration: 2.568s, episode steps:  26, steps per second:  10, episode reward: 126.000, mean reward:  4.846 [-25.000, 22.000], mean action: 1.423 [0.000, 2.000],  loss: 239.650033, mean_q: 78.984850, mean_eps: 0.100000\n"," 1 : 10  \n","--->:464189:<---\n","  61126/500000: episode: 1578, duration: 1.629s, episode steps:  16, steps per second:  10, episode reward: -6.000, mean reward: -0.375 [-25.000, 22.000], mean action: 1.625 [0.000, 2.000],  loss: 253.687718, mean_q: 79.961678, mean_eps: 0.100000\n"," 1 : 4  \n","--->:465499:<---\n","  61139/500000: episode: 1579, duration: 1.411s, episode steps:  13, steps per second:   9, episode reward: -9.000, mean reward: -0.692 [-25.000, 22.000], mean action: 1.385 [0.000, 2.000],  loss: 201.398710, mean_q: 82.556630, mean_eps: 0.100000\n"," 1 : 6  \n","--->:466802:<---\n","  61172/500000: episode: 1580, duration: 3.154s, episode steps:  33, steps per second:  10, episode reward: 315.000, mean reward:  9.545 [-25.000, 22.000], mean action: 0.545 [0.000, 2.000],  loss: 251.813998, mean_q: 81.622547, mean_eps: 0.100000\n"," 1 : 25  \n","--->:468107:<---\n","  61251/500000: episode: 1581, duration: 7.021s, episode steps:  79, steps per second:  11, episode reward: 328.000, mean reward:  4.152 [-25.000, 22.000], mean action: 1.506 [0.000, 2.000],  loss: 229.889486, mean_q: 81.480047, mean_eps: 0.100000\n"," 1 : 32  \n","--->:469412:<---\n","  61348/500000: episode: 1582, duration: 8.701s, episode steps:  97, steps per second:  11, episode reward: 272.000, mean reward:  2.804 [-25.000, 22.000], mean action: 1.330 [0.000, 2.000],  loss: 245.174171, mean_q: 81.677137, mean_eps: 0.100000\n"," 1 : 38  \n","--->:470720:<---\n","  61383/500000: episode: 1583, duration: 3.181s, episode steps:  35, steps per second:  11, episode reward: 152.000, mean reward:  4.343 [-25.000, 22.000], mean action: 1.114 [0.000, 2.000],  loss: 229.046802, mean_q: 80.892594, mean_eps: 0.100000\n"," 1 : 24  \n","--->:472027:<---\n","  61404/500000: episode: 1584, duration: 2.016s, episode steps:  21, steps per second:  10, episode reward: 161.000, mean reward:  7.667 [-25.000, 22.000], mean action: 1.048 [0.000, 2.000],  loss: 244.080218, mean_q: 82.216110, mean_eps: 0.100000\n"," 1 : 18  \n","--->:473328:<---\n","  61448/500000: episode: 1585, duration: 4.010s, episode steps:  44, steps per second:  11, episode reward: -2.000, mean reward: -0.045 [-25.000, 22.000], mean action: 1.386 [0.000, 2.000],  loss: 264.043125, mean_q: 80.435499, mean_eps: 0.100000\n"," 1 : 17  \n","--->:474631:<---\n","  61478/500000: episode: 1586, duration: 2.771s, episode steps:  30, steps per second:  11, episode reward: -21.000, mean reward: -0.700 [-25.000, 22.000], mean action: 1.133 [0.000, 2.000],  loss: 233.451655, mean_q: 80.841640, mean_eps: 0.100000\n"," 1 : 14  \n","--->:475941:<---\n","  61499/500000: episode: 1587, duration: 2.014s, episode steps:  21, steps per second:  10, episode reward: 60.000, mean reward:  2.857 [-25.000, 22.000], mean action: 1.476 [0.000, 2.000],  loss: 243.910778, mean_q: 79.172014, mean_eps: 0.100000\n"," 1 : 7  \n","--->:477251:<---\n","  61571/500000: episode: 1588, duration: 6.386s, episode steps:  72, steps per second:  11, episode reward: 189.000, mean reward:  2.625 [-25.000, 22.000], mean action: 1.708 [0.000, 2.000],  loss: 245.513961, mean_q: 81.238384, mean_eps: 0.100000\n"," 1 : 15  \n","--->:478555:<---\n","  61598/500000: episode: 1589, duration: 2.474s, episode steps:  27, steps per second:  11, episode reward: -40.000, mean reward: -1.481 [-25.000, 22.000], mean action: 1.370 [0.000, 2.000],  loss: 246.066742, mean_q: 80.192904, mean_eps: 0.100000\n"," 1 : 11  \n","--->:479858:<---\n","  61612/500000: episode: 1590, duration: 1.331s, episode steps:  14, steps per second:  11, episode reward: 54.000, mean reward:  3.857 [-25.000, 22.000], mean action: 0.643 [0.000, 2.000],  loss: 210.742240, mean_q: 83.516193, mean_eps: 0.100000\n"," 1 : 11  \n","--->:481165:<---\n","  61641/500000: episode: 1591, duration: 2.649s, episode steps:  29, steps per second:  11, episode reward: 167.000, mean reward:  5.759 [-25.000, 22.000], mean action: 1.379 [0.000, 2.000],  loss: 273.937009, mean_q: 81.380719, mean_eps: 0.100000\n"," 1 : 14  \n","--->:482466:<---\n","  61673/500000: episode: 1592, duration: 2.906s, episode steps:  32, steps per second:  11, episode reward: 189.000, mean reward:  5.906 [-25.000, 22.000], mean action: 1.281 [0.000, 2.000],  loss: 238.822564, mean_q: 82.449666, mean_eps: 0.100000\n"," 1 : 15  \n","--->:483771:<---\n","  61722/500000: episode: 1593, duration: 4.432s, episode steps:  49, steps per second:  11, episode reward: 167.000, mean reward:  3.408 [-25.000, 22.000], mean action: 1.571 [0.000, 2.000],  loss: 231.056460, mean_q: 80.660087, mean_eps: 0.100000\n"," 1 : 14  \n","--->:485078:<---\n","  61755/500000: episode: 1594, duration: 2.952s, episode steps:  33, steps per second:  11, episode reward: 177.000, mean reward:  5.364 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 232.782397, mean_q: 81.466615, mean_eps: 0.100000\n"," 1 : 23  \n","--->:486383:<---\n","  61780/500000: episode: 1595, duration: 2.275s, episode steps:  25, steps per second:  11, episode reward: 136.000, mean reward:  5.440 [-25.000, 22.000], mean action: 1.080 [0.000, 2.000],  loss: 229.557856, mean_q: 83.006068, mean_eps: 0.100000\n"," 1 : 19  \n","--->:487685:<---\n","  61796/500000: episode: 1596, duration: 1.607s, episode steps:  16, steps per second:  10, episode reward: 170.000, mean reward: 10.625 [-25.000, 22.000], mean action: 0.812 [0.000, 2.000],  loss: 222.537823, mean_q: 81.763113, mean_eps: 0.100000\n"," 1 : 12  \n","--->:488995:<---\n","  61832/500000: episode: 1597, duration: 3.417s, episode steps:  36, steps per second:  11, episode reward: 199.000, mean reward:  5.528 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 260.531687, mean_q: 81.783368, mean_eps: 0.100000\n"," 1 : 24  \n","--->:490298:<---\n","  61850/500000: episode: 1598, duration: 1.691s, episode steps:  18, steps per second:  11, episode reward: 51.000, mean reward:  2.833 [-25.000, 22.000], mean action: 1.056 [0.000, 2.000],  loss: 264.743791, mean_q: 82.664235, mean_eps: 0.100000\n"," 1 : 13  \n","--->:491608:<---\n","  61875/500000: episode: 1599, duration: 2.266s, episode steps:  25, steps per second:  11, episode reward: 195.000, mean reward:  7.800 [-25.000, 22.000], mean action: 1.360 [0.000, 2.000],  loss: 218.932611, mean_q: 81.256245, mean_eps: 0.100000\n"," 1 : 11  \n","--->:492912:<---\n","  61915/500000: episode: 1600, duration: 3.569s, episode steps:  40, steps per second:  11, episode reward: 23.000, mean reward:  0.575 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 207.910116, mean_q: 81.654940, mean_eps: 0.100000\n"," 1 : 16  \n","--->:494216:<---\n","  61923/500000: episode: 1601, duration: 0.851s, episode steps:   8, steps per second:   9, episode reward: 154.000, mean reward: 19.250 [ 0.000, 22.000], mean action: 0.750 [0.000, 2.000],  loss: 267.419985, mean_q: 84.026073, mean_eps: 0.100000\n"," 1 : 7  \n","--->:495525:<---\n","  61951/500000: episode: 1602, duration: 2.524s, episode steps:  28, steps per second:  11, episode reward: 89.000, mean reward:  3.179 [-25.000, 22.000], mean action: 0.786 [0.000, 2.000],  loss: 231.824042, mean_q: 82.047152, mean_eps: 0.100000\n"," 1 : 19  \n","--->:496835:<---\n","  61983/500000: episode: 1603, duration: 2.965s, episode steps:  32, steps per second:  11, episode reward: 73.000, mean reward:  2.281 [-25.000, 22.000], mean action: 1.406 [0.000, 2.000],  loss: 240.227310, mean_q: 81.213490, mean_eps: 0.100000\n"," 1 : 14  \n","--->:498141:<---\n","  62004/500000: episode: 1604, duration: 1.999s, episode steps:  21, steps per second:  11, episode reward: -24.000, mean reward: -1.143 [-25.000, 22.000], mean action: 0.714 [0.000, 2.000],  loss: 239.014805, mean_q: 82.126015, mean_eps: 0.100000\n"," 1 : 16  \n","--->:499449:<---\n","  62030/500000: episode: 1605, duration: 2.368s, episode steps:  26, steps per second:  11, episode reward: -5.000, mean reward: -0.192 [-25.000, 22.000], mean action: 0.923 [0.000, 2.000],  loss: 215.073326, mean_q: 79.642778, mean_eps: 0.100000\n"," 1 : 19  \n","--->:500753:<---\n","  62051/500000: episode: 1606, duration: 1.937s, episode steps:  21, steps per second:  11, episode reward: 283.000, mean reward: 13.476 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 244.744749, mean_q: 78.426557, mean_eps: 0.100000\n"," 1 : 15  \n","--->:502057:<---\n","  62075/500000: episode: 1607, duration: 2.190s, episode steps:  24, steps per second:  11, episode reward: -59.000, mean reward: -2.458 [-25.000, 22.000], mean action: 1.542 [0.000, 2.000],  loss: 227.168987, mean_q: 82.270398, mean_eps: 0.100000\n"," 1 : 8  \n","--->:503367:<---\n","  62100/500000: episode: 1608, duration: 2.301s, episode steps:  25, steps per second:  11, episode reward: 60.000, mean reward:  2.400 [-25.000, 22.000], mean action: 1.560 [0.000, 2.000],  loss: 223.275634, mean_q: 78.687450, mean_eps: 0.100000\n"," 1 : 7  \n","--->:504676:<---\n","  62122/500000: episode: 1609, duration: 2.014s, episode steps:  22, steps per second:  11, episode reward: 164.000, mean reward:  7.455 [-25.000, 22.000], mean action: 0.955 [0.000, 2.000],  loss: 234.032232, mean_q: 82.028920, mean_eps: 0.100000\n"," 1 : 16  \n","--->:505986:<---\n","  62162/500000: episode: 1610, duration: 3.601s, episode steps:  40, steps per second:  11, episode reward:  7.000, mean reward:  0.175 [-25.000, 22.000], mean action: 1.600 [0.000, 2.000],  loss: 230.778296, mean_q: 79.618645, mean_eps: 0.100000\n"," 1 : 11  \n","--->:507293:<---\n","  62174/500000: episode: 1611, duration: 1.188s, episode steps:  12, steps per second:  10, episode reward: -18.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.583 [0.000, 1.000],  loss: 259.862222, mean_q: 79.977529, mean_eps: 0.100000\n"," 1 : 12  \n","--->:508603:<---\n","  62187/500000: episode: 1612, duration: 1.250s, episode steps:  13, steps per second:  10, episode reward: 57.000, mean reward:  4.385 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 283.911314, mean_q: 81.726407, mean_eps: 0.100000\n"," 1 : 9  \n","--->:509904:<---\n","  62217/500000: episode: 1613, duration: 2.739s, episode steps:  30, steps per second:  11, episode reward: 51.000, mean reward:  1.700 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 247.744519, mean_q: 80.013660, mean_eps: 0.100000\n"," 1 : 13  \n","--->:511209:<---\n","  62288/500000: episode: 1614, duration: 6.165s, episode steps:  71, steps per second:  12, episode reward: 48.000, mean reward:  0.676 [-25.000, 22.000], mean action: 1.676 [0.000, 2.000],  loss: 228.400718, mean_q: 80.965999, mean_eps: 0.100000\n"," 1 : 15  \n","--->:512516:<---\n","  62339/500000: episode: 1615, duration: 4.568s, episode steps:  51, steps per second:  11, episode reward: 77.000, mean reward:  1.510 [-25.000, 22.000], mean action: 1.078 [0.000, 2.000],  loss: 221.789366, mean_q: 81.738779, mean_eps: 0.100000\n"," 1 : 27  \n","--->:513825:<---\n","  62350/500000: episode: 1616, duration: 1.093s, episode steps:  11, steps per second:  10, episode reward: 151.000, mean reward: 13.727 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 205.964862, mean_q: 79.908275, mean_eps: 0.100000\n"," 1 : 9  \n","--->:515130:<---\n","  62373/500000: episode: 1617, duration: 2.136s, episode steps:  23, steps per second:  11, episode reward: 117.000, mean reward:  5.087 [-25.000, 22.000], mean action: 0.783 [0.000, 2.000],  loss: 256.234904, mean_q: 78.911668, mean_eps: 0.100000\n"," 1 : 16  \n","--->:516435:<---\n","  62411/500000: episode: 1618, duration: 3.405s, episode steps:  38, steps per second:  11, episode reward:  7.000, mean reward:  0.184 [-25.000, 22.000], mean action: 1.632 [0.000, 2.000],  loss: 269.779668, mean_q: 79.987052, mean_eps: 0.100000\n"," 1 : 11  \n","--->:517739:<---\n","  62438/500000: episode: 1619, duration: 2.492s, episode steps:  27, steps per second:  11, episode reward:  4.000, mean reward:  0.148 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 233.526423, mean_q: 81.066142, mean_eps: 0.100000\n"," 1 : 13  \n","--->:519042:<---\n","  62456/500000: episode: 1620, duration: 1.682s, episode steps:  18, steps per second:  11, episode reward: -40.000, mean reward: -2.222 [-25.000, 22.000], mean action: 1.278 [0.000, 2.000],  loss: 199.310493, mean_q: 80.911429, mean_eps: 0.100000\n"," 1 : 11  \n","--->:520345:<---\n","  62470/500000: episode: 1621, duration: 1.373s, episode steps:  14, steps per second:  10, episode reward: 35.000, mean reward:  2.500 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 227.710307, mean_q: 78.500330, mean_eps: 0.100000\n"," 1 : 8  \n","--->:521646:<---\n","  62487/500000: episode: 1622, duration: 1.598s, episode steps:  17, steps per second:  11, episode reward: 51.000, mean reward:  3.000 [-25.000, 22.000], mean action: 1.059 [0.000, 2.000],  loss: 197.157387, mean_q: 82.033153, mean_eps: 0.100000\n"," 1 : 13  \n","--->:522951:<---\n","  62763/500000: episode: 1623, duration: 24.257s, episode steps: 276, steps per second:  11, episode reward: 820.000, mean reward:  2.971 [-25.000, 22.000], mean action: 1.377 [0.000, 2.000],  loss: 234.336037, mean_q: 81.334724, mean_eps: 0.100000\n"," 1 : 127  \n","--->:524259:<---\n","  62832/500000: episode: 1624, duration: 6.200s, episode steps:  69, steps per second:  11, episode reward: 294.000, mean reward:  4.261 [-25.000, 22.000], mean action: 1.232 [0.000, 2.000],  loss: 231.812972, mean_q: 80.374145, mean_eps: 0.100000\n"," 1 : 39  \n","--->:525564:<---\n","  62861/500000: episode: 1625, duration: 2.664s, episode steps:  29, steps per second:  11, episode reward: 117.000, mean reward:  4.034 [-25.000, 22.000], mean action: 1.069 [0.000, 2.000],  loss: 259.142691, mean_q: 81.730642, mean_eps: 0.100000\n"," 1 : 16  \n","--->:526871:<---\n","  62876/500000: episode: 1626, duration: 1.427s, episode steps:  15, steps per second:  11, episode reward: 29.000, mean reward:  1.933 [-25.000, 22.000], mean action: 0.467 [0.000, 2.000],  loss: 261.519574, mean_q: 81.245284, mean_eps: 0.100000\n"," 1 : 12  \n","--->:528180:<---\n","  62885/500000: episode: 1627, duration: 0.936s, episode steps:   9, steps per second:  10, episode reward: 85.000, mean reward:  9.444 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 251.623689, mean_q: 82.519619, mean_eps: 0.100000\n"," 1 : 6  \n","--->:529483:<---\n","  62894/500000: episode: 1628, duration: 0.927s, episode steps:   9, steps per second:  10, episode reward: -31.000, mean reward: -3.444 [-25.000, 22.000], mean action: 1.111 [0.000, 2.000],  loss: 246.035788, mean_q: 79.628306, mean_eps: 0.100000\n"," 1 : 5  \n","--->:530786:<---\n","  62899/500000: episode: 1629, duration: 0.597s, episode steps:   5, steps per second:   8, episode reward: -28.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.200 [0.000, 2.000],  loss: 184.101825, mean_q: 76.739488, mean_eps: 0.100000\n"," 1 : 3  \n","--->:532092:<---\n","  62908/500000: episode: 1630, duration: 0.937s, episode steps:   9, steps per second:  10, episode reward: 19.000, mean reward:  2.111 [-25.000, 22.000], mean action: 1.667 [1.000, 2.000],  loss: 238.468020, mean_q: 80.983685, mean_eps: 0.100000\n"," 1 : 3  \n","--->:533396:<---\n","  62913/500000: episode: 1631, duration: 0.565s, episode steps:   5, steps per second:   9, episode reward: 41.000, mean reward:  8.200 [-25.000, 22.000], mean action: 1.200 [1.000, 2.000],  loss: 229.461472, mean_q: 81.065274, mean_eps: 0.100000\n"," 1 : 4  \n","--->:534706:<---\n","  62918/500000: episode: 1632, duration: 0.572s, episode steps:   5, steps per second:   9, episode reward: 63.000, mean reward: 12.600 [-25.000, 22.000], mean action: 0.800 [0.000, 1.000],  loss: 192.393437, mean_q: 82.713661, mean_eps: 0.100000\n"," 1 : 5  \n","--->:536009:<---\n","  62920/500000: episode: 1633, duration: 0.307s, episode steps:   2, steps per second:   7, episode reward: -3.000, mean reward: -1.500 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 225.357239, mean_q: 82.594296, mean_eps: 0.100000\n"," 1 : 2  \n","--->:537314:<---\n","  62922/500000: episode: 1634, duration: 0.319s, episode steps:   2, steps per second:   6, episode reward: 44.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 164.321529, mean_q: 79.350979, mean_eps: 0.100000\n"," 1 : 2  \n","--->:538623:<---\n","  62923/500000: episode: 1635, duration: 0.231s, episode steps:   1, steps per second:   4, episode reward: -25.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.000 [0.000, 0.000],  loss: 393.519348, mean_q: 84.136444, mean_eps: 0.100000\n"," 1 : 1  \n","--->:539930:<---\n","  62939/500000: episode: 1636, duration: 1.556s, episode steps:  16, steps per second:  10, episode reward: -15.000, mean reward: -0.938 [-25.000, 22.000], mean action: 0.938 [0.000, 2.000],  loss: 266.106285, mean_q: 82.837649, mean_eps: 0.100000\n"," 1 : 10  \n","--->:541239:<---\n","  62947/500000: episode: 1637, duration: 0.845s, episode steps:   8, steps per second:   9, episode reward: 38.000, mean reward:  4.750 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 251.329361, mean_q: 83.194569, mean_eps: 0.100000\n"," 1 : 6  \n","--->:542546:<---\n","  62970/500000: episode: 1638, duration: 2.188s, episode steps:  23, steps per second:  11, episode reward: 82.000, mean reward:  3.565 [-25.000, 22.000], mean action: 1.565 [0.000, 2.000],  loss: 197.447252, mean_q: 82.315794, mean_eps: 0.100000\n"," 1 : 8  \n","--->:543856:<---\n","  62987/500000: episode: 1639, duration: 1.646s, episode steps:  17, steps per second:  10, episode reward:  7.000, mean reward:  0.412 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 213.723328, mean_q: 80.130628, mean_eps: 0.100000\n"," 1 : 11  \n","--->:545165:<---\n","  63011/500000: episode: 1640, duration: 2.204s, episode steps:  24, steps per second:  11, episode reward: 35.000, mean reward:  1.458 [-25.000, 22.000], mean action: 1.375 [0.000, 2.000],  loss: 208.749949, mean_q: 82.500474, mean_eps: 0.100000\n"," 1 : 8  \n","--->:546469:<---\n","  63020/500000: episode: 1641, duration: 0.969s, episode steps:   9, steps per second:   9, episode reward: 151.000, mean reward: 16.778 [-25.000, 22.000], mean action: 0.778 [0.000, 1.000],  loss: 196.047939, mean_q: 78.168680, mean_eps: 0.100000\n"," 1 : 9  \n","--->:547777:<---\n","  63024/500000: episode: 1642, duration: 0.486s, episode steps:   4, steps per second:   8, episode reward: -6.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.250 [0.000, 1.000],  loss: 219.808117, mean_q: 77.201073, mean_eps: 0.100000\n"," 1 : 4  \n","--->:549086:<---\n","  63043/500000: episode: 1643, duration: 1.776s, episode steps:  19, steps per second:  11, episode reward: 45.000, mean reward:  2.368 [-25.000, 22.000], mean action: 0.947 [0.000, 2.000],  loss: 225.817931, mean_q: 83.197070, mean_eps: 0.100000\n"," 1 : 17  \n","--->:550388:<---\n","  63062/500000: episode: 1644, duration: 1.856s, episode steps:  19, steps per second:  10, episode reward: 45.000, mean reward:  2.368 [-25.000, 22.000], mean action: 0.737 [0.000, 2.000],  loss: 251.140574, mean_q: 80.692449, mean_eps: 0.100000\n"," 1 : 17  \n","--->:551692:<---\n","  63085/500000: episode: 1645, duration: 2.148s, episode steps:  23, steps per second:  11, episode reward: 101.000, mean reward:  4.391 [-25.000, 22.000], mean action: 1.304 [0.000, 2.000],  loss: 226.247169, mean_q: 80.719588, mean_eps: 0.100000\n"," 1 : 11  \n","--->:552995:<---\n","  63095/500000: episode: 1646, duration: 1.043s, episode steps:  10, steps per second:  10, episode reward: -103.000, mean reward: -10.300 [-25.000, 22.000], mean action: 0.900 [0.000, 2.000],  loss: 230.430035, mean_q: 79.910423, mean_eps: 0.100000\n"," 1 : 6  \n","--->:554299:<---\n","  63112/500000: episode: 1647, duration: 1.647s, episode steps:  17, steps per second:  10, episode reward: 151.000, mean reward:  8.882 [-25.000, 22.000], mean action: 1.294 [0.000, 2.000],  loss: 226.407237, mean_q: 83.212413, mean_eps: 0.100000\n"," 1 : 9  \n","--->:555604:<---\n","  63156/500000: episode: 1648, duration: 3.998s, episode steps:  44, steps per second:  11, episode reward: 124.000, mean reward:  2.818 [-25.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 239.013857, mean_q: 79.877690, mean_eps: 0.100000\n"," 1 : 27  \n","--->:556905:<---\n","  63186/500000: episode: 1649, duration: 2.692s, episode steps:  30, steps per second:  11, episode reward: 70.000, mean reward:  2.333 [-25.000, 22.000], mean action: 1.367 [0.000, 2.000],  loss: 227.897263, mean_q: 80.185730, mean_eps: 0.100000\n"," 1 : 16  \n","--->:558211:<---\n","  63196/500000: episode: 1650, duration: 0.987s, episode steps:  10, steps per second:  10, episode reward: 79.000, mean reward:  7.900 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 252.548462, mean_q: 83.133116, mean_eps: 0.100000\n"," 1 : 10  \n","--->:559513:<---\n","  63209/500000: episode: 1651, duration: 1.229s, episode steps:  13, steps per second:  11, episode reward: -34.000, mean reward: -2.615 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 229.536194, mean_q: 80.490379, mean_eps: 0.100000\n"," 1 : 7  \n","--->:560821:<---\n","  63226/500000: episode: 1652, duration: 1.586s, episode steps:  17, steps per second:  11, episode reward: 132.000, mean reward:  7.765 [ 0.000, 22.000], mean action: 1.529 [0.000, 2.000],  loss: 222.085572, mean_q: 82.340536, mean_eps: 0.100000\n"," 1 : 6  \n","--->:562124:<---\n","  63228/500000: episode: 1653, duration: 0.310s, episode steps:   2, steps per second:   6, episode reward: -50.000, mean reward: -25.000 [-25.000, -25.000], mean action: 1.000 [1.000, 1.000],  loss: 223.953728, mean_q: 80.054474, mean_eps: 0.100000\n"," 1 : 2  \n","--->:563427:<---\n","  63232/500000: episode: 1654, duration: 0.478s, episode steps:   4, steps per second:   8, episode reward: 66.000, mean reward: 16.500 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 263.177982, mean_q: 83.489525, mean_eps: 0.100000\n"," 1 : 3  \n","--->:564736:<---\n","  63241/500000: episode: 1655, duration: 0.913s, episode steps:   9, steps per second:  10, episode reward: 66.000, mean reward:  7.333 [ 0.000, 22.000], mean action: 1.556 [0.000, 2.000],  loss: 234.160717, mean_q: 80.735611, mean_eps: 0.100000\n"," 1 : 3  \n","--->:566038:<---\n","  63250/500000: episode: 1656, duration: 0.918s, episode steps:   9, steps per second:  10, episode reward: 38.000, mean reward:  4.222 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 217.531584, mean_q: 82.195652, mean_eps: 0.100000\n"," 1 : 6  \n","--->:567348:<---\n","  63257/500000: episode: 1657, duration: 0.720s, episode steps:   7, steps per second:  10, episode reward: 38.000, mean reward:  5.429 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 235.508015, mean_q: 82.179450, mean_eps: 0.100000\n"," 1 : 6  \n","--->:568658:<---\n","  63265/500000: episode: 1658, duration: 0.858s, episode steps:   8, steps per second:   9, episode reward: 82.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.625 [0.000, 1.000],  loss: 264.056250, mean_q: 75.158465, mean_eps: 0.100000\n"," 1 : 8  \n","--->:569964:<---\n","  63280/500000: episode: 1659, duration: 1.422s, episode steps:  15, steps per second:  11, episode reward: 145.000, mean reward:  9.667 [-25.000, 22.000], mean action: 0.667 [0.000, 2.000],  loss: 231.990339, mean_q: 81.178322, mean_eps: 0.100000\n"," 1 : 13  \n","--->:571272:<---\n","  63287/500000: episode: 1660, duration: 0.763s, episode steps:   7, steps per second:   9, episode reward: 85.000, mean reward: 12.143 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 205.720385, mean_q: 81.270558, mean_eps: 0.100000\n"," 1 : 6  \n","--->:572579:<---\n","  63302/500000: episode: 1661, duration: 1.464s, episode steps:  15, steps per second:  10, episode reward: 35.000, mean reward:  2.333 [-25.000, 22.000], mean action: 1.067 [0.000, 2.000],  loss: 239.630085, mean_q: 84.569445, mean_eps: 0.100000\n"," 1 : 8  \n","--->:573882:<---\n","  63316/500000: episode: 1662, duration: 1.354s, episode steps:  14, steps per second:  10, episode reward: 104.000, mean reward:  7.429 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 263.354152, mean_q: 82.112509, mean_eps: 0.100000\n"," 1 : 9  \n","--->:575187:<---\n","  63324/500000: episode: 1663, duration: 0.826s, episode steps:   8, steps per second:  10, episode reward: -9.000, mean reward: -1.125 [-25.000, 22.000], mean action: 0.875 [0.000, 2.000],  loss: 208.541384, mean_q: 77.307282, mean_eps: 0.100000\n"," 1 : 6  \n","--->:576496:<---\n","  63356/500000: episode: 1664, duration: 2.870s, episode steps:  32, steps per second:  11, episode reward: 76.000, mean reward:  2.375 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 257.662446, mean_q: 81.461791, mean_eps: 0.100000\n"," 1 : 12  \n","--->:577806:<---\n","  63382/500000: episode: 1665, duration: 2.367s, episode steps:  26, steps per second:  11, episode reward: 89.000, mean reward:  3.423 [-25.000, 22.000], mean action: 0.808 [0.000, 2.000],  loss: 236.061126, mean_q: 79.765225, mean_eps: 0.100000\n"," 1 : 19  \n","--->:579110:<---\n","  63395/500000: episode: 1666, duration: 1.280s, episode steps:  13, steps per second:  10, episode reward: 126.000, mean reward:  9.692 [-25.000, 22.000], mean action: 0.846 [0.000, 2.000],  loss: 227.591061, mean_q: 82.083439, mean_eps: 0.100000\n"," 1 : 10  \n","--->:580412:<---\n","  63408/500000: episode: 1667, duration: 1.302s, episode steps:  13, steps per second:  10, episode reward: 60.000, mean reward:  4.615 [-25.000, 22.000], mean action: 1.308 [0.000, 2.000],  loss: 249.679814, mean_q: 81.585249, mean_eps: 0.100000\n"," 1 : 7  \n","--->:581714:<---\n","  63414/500000: episode: 1668, duration: 0.696s, episode steps:   6, steps per second:   9, episode reward: -31.000, mean reward: -5.167 [-25.000, 22.000], mean action: 1.167 [1.000, 2.000],  loss: 257.847870, mean_q: 80.137217, mean_eps: 0.100000\n"," 1 : 5  \n","--->:583020:<---\n","  63424/500000: episode: 1669, duration: 1.014s, episode steps:  10, steps per second:  10, episode reward: 104.000, mean reward: 10.400 [-25.000, 22.000], mean action: 0.700 [0.000, 2.000],  loss: 273.253993, mean_q: 81.075435, mean_eps: 0.100000\n"," 1 : 9  \n","--->:584322:<---\n","  63435/500000: episode: 1670, duration: 1.128s, episode steps:  11, steps per second:  10, episode reward: 132.000, mean reward: 12.000 [ 0.000, 22.000], mean action: 1.091 [0.000, 2.000],  loss: 232.834975, mean_q: 83.581123, mean_eps: 0.100000\n"," 1 : 6  \n","--->:585624:<---\n","  63443/500000: episode: 1671, duration: 0.852s, episode steps:   8, steps per second:   9, episode reward: -34.000, mean reward: -4.250 [-25.000, 22.000], mean action: 0.625 [0.000, 2.000],  loss: 247.241650, mean_q: 80.231950, mean_eps: 0.100000\n"," 1 : 7  \n","--->:586934:<---\n","  63457/500000: episode: 1672, duration: 1.359s, episode steps:  14, steps per second:  10, episode reward: 107.000, mean reward:  7.643 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 276.585611, mean_q: 78.817654, mean_eps: 0.100000\n"," 1 : 7  \n","--->:588237:<---\n","  63466/500000: episode: 1673, duration: 0.921s, episode steps:   9, steps per second:  10, episode reward: 107.000, mean reward: 11.889 [-25.000, 22.000], mean action: 1.222 [1.000, 2.000],  loss: 219.719715, mean_q: 85.070354, mean_eps: 0.100000\n"," 1 : 7  \n","--->:589539:<---\n","  63472/500000: episode: 1674, duration: 0.674s, episode steps:   6, steps per second:   9, episode reward: 132.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 196.293371, mean_q: 78.901461, mean_eps: 0.100000\n"," 1 : 6  \n","--->:590840:<---\n","  63480/500000: episode: 1675, duration: 0.824s, episode steps:   8, steps per second:  10, episode reward: 16.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 226.714006, mean_q: 83.611814, mean_eps: 0.100000\n"," 1 : 5  \n","--->:592149:<---\n","  63485/500000: episode: 1676, duration: 0.549s, episode steps:   5, steps per second:   9, episode reward: 16.000, mean reward:  3.200 [-25.000, 22.000], mean action: 0.200 [0.000, 1.000],  loss: 226.634485, mean_q: 85.052252, mean_eps: 0.100000\n"," 1 : 5  \n","--->:593452:<---\n","  63502/500000: episode: 1677, duration: 1.650s, episode steps:  17, steps per second:  10, episode reward: 126.000, mean reward:  7.412 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 227.792964, mean_q: 82.840897, mean_eps: 0.100000\n"," 1 : 10  \n","--->:594758:<---\n","  63525/500000: episode: 1678, duration: 2.342s, episode steps:  23, steps per second:  10, episode reward:  4.000, mean reward:  0.174 [-25.000, 22.000], mean action: 1.087 [0.000, 2.000],  loss: 219.753417, mean_q: 83.524507, mean_eps: 0.100000\n"," 1 : 13  \n","--->:596059:<---\n","  63547/500000: episode: 1679, duration: 2.309s, episode steps:  22, steps per second:  10, episode reward: -21.000, mean reward: -0.955 [-25.000, 22.000], mean action: 0.955 [0.000, 2.000],  loss: 244.812003, mean_q: 81.053577, mean_eps: 0.100000\n"," 1 : 14  \n","--->:597363:<---\n","  63568/500000: episode: 1680, duration: 1.954s, episode steps:  21, steps per second:  11, episode reward: -18.000, mean reward: -0.857 [-25.000, 22.000], mean action: 1.095 [0.000, 2.000],  loss: 241.795486, mean_q: 82.644898, mean_eps: 0.100000\n"," 1 : 12  \n","--->:598670:<---\n","  63577/500000: episode: 1681, duration: 0.908s, episode steps:   9, steps per second:  10, episode reward: 38.000, mean reward:  4.222 [-25.000, 22.000], mean action: 0.889 [0.000, 2.000],  loss: 266.978995, mean_q: 81.439038, mean_eps: 0.100000\n"," 1 : 6  \n","--->:599973:<---\n","  63584/500000: episode: 1682, duration: 0.780s, episode steps:   7, steps per second:   9, episode reward: -6.000, mean reward: -0.857 [-25.000, 22.000], mean action: 1.286 [0.000, 2.000],  loss: 256.169351, mean_q: 78.751843, mean_eps: 0.100000\n"," 1 : 4  \n","--->:601279:<---\n","  63595/500000: episode: 1683, duration: 1.119s, episode steps:  11, steps per second:  10, episode reward: -12.000, mean reward: -1.091 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 217.171652, mean_q: 82.184790, mean_eps: 0.100000\n"," 1 : 8  \n","--->:602581:<---\n","  63597/500000: episode: 1684, duration: 0.346s, episode steps:   2, steps per second:   6, episode reward: 44.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 292.254860, mean_q: 82.243137, mean_eps: 0.100000\n"," 1 : 2  \n","--->:603889:<---\n","  63601/500000: episode: 1685, duration: 0.534s, episode steps:   4, steps per second:   7, episode reward: 41.000, mean reward: 10.250 [-25.000, 22.000], mean action: 0.500 [0.000, 1.000],  loss: 290.972355, mean_q: 82.567389, mean_eps: 0.100000\n"," 1 : 4  \n","--->:605198:<---\n","  63608/500000: episode: 1686, duration: 0.812s, episode steps:   7, steps per second:   9, episode reward: 63.000, mean reward:  9.000 [-25.000, 22.000], mean action: 1.143 [0.000, 2.000],  loss: 260.560758, mean_q: 83.248693, mean_eps: 0.100000\n"," 1 : 5  \n","--->:606504:<---\n","  63626/500000: episode: 1687, duration: 1.744s, episode steps:  18, steps per second:  10, episode reward: -75.000, mean reward: -4.167 [-25.000,  0.000], mean action: 1.722 [0.000, 2.000],  loss: 222.998334, mean_q: 81.440869, mean_eps: 0.100000\n"," 1 : 3  \n","--->:607806:<---\n","  63637/500000: episode: 1688, duration: 1.125s, episode steps:  11, steps per second:  10, episode reward: 35.000, mean reward:  3.182 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 187.289791, mean_q: 79.481277, mean_eps: 0.100000\n"," 1 : 8  \n","--->:609110:<---\n","  63664/500000: episode: 1689, duration: 2.483s, episode steps:  27, steps per second:  11, episode reward: -37.000, mean reward: -1.370 [-25.000, 22.000], mean action: 1.444 [0.000, 2.000],  loss: 202.642553, mean_q: 83.399234, mean_eps: 0.100000\n"," 1 : 9  \n","--->:610417:<---\n","  63672/500000: episode: 1690, duration: 0.875s, episode steps:   8, steps per second:   9, episode reward: 16.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 257.644929, mean_q: 81.166216, mean_eps: 0.100000\n"," 1 : 5  \n","--->:611724:<---\n","  63676/500000: episode: 1691, duration: 0.511s, episode steps:   4, steps per second:   8, episode reward: -100.000, mean reward: -25.000 [-25.000, -25.000], mean action: 0.250 [0.000, 1.000],  loss: 232.742409, mean_q: 81.867855, mean_eps: 0.100000\n"," 1 : 4  \n","--->:613028:<---\n","  63695/500000: episode: 1692, duration: 1.852s, episode steps:  19, steps per second:  10, episode reward: 82.000, mean reward:  4.316 [-25.000, 22.000], mean action: 1.474 [0.000, 2.000],  loss: 202.610810, mean_q: 81.645671, mean_eps: 0.100000\n"," 1 : 8  \n","--->:614332:<---\n","  63701/500000: episode: 1693, duration: 0.723s, episode steps:   6, steps per second:   8, episode reward: 110.000, mean reward: 18.333 [ 0.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 200.720970, mean_q: 81.226542, mean_eps: 0.100000\n"," 1 : 5  \n","--->:615633:<---\n","  63712/500000: episode: 1694, duration: 1.131s, episode steps:  11, steps per second:  10, episode reward: 32.000, mean reward:  2.909 [-25.000, 22.000], mean action: 0.818 [0.000, 2.000],  loss: 202.630064, mean_q: 80.100018, mean_eps: 0.100000\n"," 1 : 10  \n","--->:616939:<---\n","  63727/500000: episode: 1695, duration: 1.483s, episode steps:  15, steps per second:  10, episode reward: -84.000, mean reward: -5.600 [-25.000, 22.000], mean action: 1.067 [0.000, 2.000],  loss: 249.586773, mean_q: 81.666696, mean_eps: 0.100000\n"," 1 : 9  \n","--->:618247:<---\n","  63756/500000: episode: 1696, duration: 2.716s, episode steps:  29, steps per second:  11, episode reward: -56.000, mean reward: -1.931 [-25.000, 22.000], mean action: 1.759 [0.000, 2.000],  loss: 230.464861, mean_q: 82.361202, mean_eps: 0.100000\n"," 1 : 6  \n","--->:619550:<---\n","  63772/500000: episode: 1697, duration: 1.558s, episode steps:  16, steps per second:  10, episode reward: 32.000, mean reward:  2.000 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 210.432130, mean_q: 81.832862, mean_eps: 0.100000\n"," 1 : 10  \n","--->:620852:<---\n","  63806/500000: episode: 1698, duration: 3.140s, episode steps:  34, steps per second:  11, episode reward: 123.000, mean reward:  3.618 [-25.000, 22.000], mean action: 1.500 [0.000, 2.000],  loss: 235.402704, mean_q: 80.093929, mean_eps: 0.100000\n"," 1 : 12  \n","--->:622159:<---\n","  63823/500000: episode: 1699, duration: 1.624s, episode steps:  17, steps per second:  10, episode reward: -12.000, mean reward: -0.706 [-25.000, 22.000], mean action: 1.294 [0.000, 2.000],  loss: 230.673901, mean_q: 81.295762, mean_eps: 0.100000\n"," 1 : 8  \n","--->:623468:<---\n","  63837/500000: episode: 1700, duration: 1.413s, episode steps:  14, steps per second:  10, episode reward: 10.000, mean reward:  0.714 [-25.000, 22.000], mean action: 1.000 [0.000, 2.000],  loss: 266.032958, mean_q: 81.472789, mean_eps: 0.100000\n"," 1 : 9  \n","--->:624776:<---\n","  63853/500000: episode: 1701, duration: 1.572s, episode steps:  16, steps per second:  10, episode reward: 16.000, mean reward:  1.000 [-25.000, 22.000], mean action: 1.562 [0.000, 2.000],  loss: 240.284705, mean_q: 81.384876, mean_eps: 0.100000\n"," 1 : 5  \n","--->:626085:<---\n","  63859/500000: episode: 1702, duration: 0.679s, episode steps:   6, steps per second:   9, episode reward: -6.000, mean reward: -1.000 [-25.000, 22.000], mean action: 1.333 [1.000, 2.000],  loss: 253.735001, mean_q: 87.719199, mean_eps: 0.100000\n"," 1 : 4  \n","--->:627394:<---\n","  63875/500000: episode: 1703, duration: 1.542s, episode steps:  16, steps per second:  10, episode reward: 13.000, mean reward:  0.812 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 233.082494, mean_q: 80.288440, mean_eps: 0.100000\n"," 1 : 7  \n","--->:628702:<---\n","  63890/500000: episode: 1704, duration: 1.464s, episode steps:  15, steps per second:  10, episode reward: 82.000, mean reward:  5.467 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 255.664224, mean_q: 78.942058, mean_eps: 0.100000\n"," 1 : 8  \n","--->:630007:<---\n","  63895/500000: episode: 1705, duration: 0.629s, episode steps:   5, steps per second:   8, episode reward: 63.000, mean reward: 12.600 [-25.000, 22.000], mean action: 0.400 [0.000, 1.000],  loss: 207.736063, mean_q: 82.366531, mean_eps: 0.100000\n"," 1 : 5  \n","--->:631311:<---\n","  63898/500000: episode: 1706, duration: 0.421s, episode steps:   3, steps per second:   7, episode reward: -28.000, mean reward: -9.333 [-25.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 189.205098, mean_q: 80.012288, mean_eps: 0.100000\n"," 1 : 3  \n","--->:632620:<---\n","  63900/500000: episode: 1707, duration: 0.349s, episode steps:   2, steps per second:   6, episode reward: 44.000, mean reward: 22.000 [22.000, 22.000], mean action: 1.000 [1.000, 1.000],  loss: 204.723694, mean_q: 80.372200, mean_eps: 0.100000\n"," 1 : 2  \n","--->:633927:<---\n","  63920/500000: episode: 1708, duration: 1.940s, episode steps:  20, steps per second:  10, episode reward: 35.000, mean reward:  1.750 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 270.017030, mean_q: 78.329120, mean_eps: 0.100000\n"," 1 : 8  \n","--->:635230:<---\n","  63935/500000: episode: 1709, duration: 1.449s, episode steps:  15, steps per second:  10, episode reward: 19.000, mean reward:  1.267 [-25.000, 22.000], mean action: 1.667 [0.000, 2.000],  loss: 252.910845, mean_q: 80.889964, mean_eps: 0.100000\n"," 1 : 3  \n","--->:636539:<---\n","  63937/500000: episode: 1710, duration: 0.334s, episode steps:   2, steps per second:   6, episode reward: -3.000, mean reward: -1.500 [-25.000, 22.000], mean action: 0.000 [0.000, 0.000],  loss: 223.429031, mean_q: 81.452755, mean_eps: 0.100000\n"," 1 : 2  \n","--->:637848:<---\n","  63968/500000: episode: 1711, duration: 2.860s, episode steps:  31, steps per second:  11, episode reward: 211.000, mean reward:  6.806 [-25.000, 22.000], mean action: 1.194 [0.000, 2.000],  loss: 249.700001, mean_q: 79.821161, mean_eps: 0.100000\n"," 1 : 16  \n","--->:639149:<---\n","  63978/500000: episode: 1712, duration: 1.020s, episode steps:  10, steps per second:  10, episode reward: 107.000, mean reward: 10.700 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 313.837509, mean_q: 83.687406, mean_eps: 0.100000\n"," 1 : 7  \n","--->:640453:<---\n","  63998/500000: episode: 1713, duration: 1.881s, episode steps:  20, steps per second:  11, episode reward: 79.000, mean reward:  3.950 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 229.432049, mean_q: 84.113732, mean_eps: 0.100000\n"," 1 : 10  \n","--->:641761:<---\n","  64013/500000: episode: 1714, duration: 1.512s, episode steps:  15, steps per second:  10, episode reward: -34.000, mean reward: -2.267 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 238.175924, mean_q: 80.598625, mean_eps: 0.100000\n"," 1 : 7  \n","--->:643068:<---\n","  64031/500000: episode: 1715, duration: 1.750s, episode steps:  18, steps per second:  10, episode reward: -46.000, mean reward: -2.556 [-25.000, 22.000], mean action: 0.556 [0.000, 2.000],  loss: 244.398706, mean_q: 78.528001, mean_eps: 0.100000\n"," 1 : 15  \n","--->:644371:<---\n","  64057/500000: episode: 1716, duration: 2.427s, episode steps:  26, steps per second:  11, episode reward: 35.000, mean reward:  1.346 [-25.000, 22.000], mean action: 1.654 [0.000, 2.000],  loss: 216.560423, mean_q: 80.159276, mean_eps: 0.100000\n"," 1 : 8  \n","--->:645679:<---\n","  64066/500000: episode: 1717, duration: 0.953s, episode steps:   9, steps per second:   9, episode reward: 16.000, mean reward:  1.778 [-25.000, 22.000], mean action: 1.222 [0.000, 2.000],  loss: 193.714936, mean_q: 82.287406, mean_eps: 0.100000\n"," 1 : 5  \n","--->:646985:<---\n","  64081/500000: episode: 1718, duration: 1.460s, episode steps:  15, steps per second:  10, episode reward: 60.000, mean reward:  4.000 [-25.000, 22.000], mean action: 1.400 [0.000, 2.000],  loss: 218.464763, mean_q: 80.723275, mean_eps: 0.100000\n"," 1 : 7  \n","--->:648294:<---\n","  64088/500000: episode: 1719, duration: 0.784s, episode steps:   7, steps per second:   9, episode reward: -6.000, mean reward: -0.857 [-25.000, 22.000], mean action: 0.857 [0.000, 2.000],  loss: 258.228361, mean_q: 78.883344, mean_eps: 0.100000\n"," 1 : 4  \n","--->:649599:<---\n","  64112/500000: episode: 1720, duration: 2.226s, episode steps:  24, steps per second:  11, episode reward: 173.000, mean reward:  7.208 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 260.297129, mean_q: 79.101299, mean_eps: 0.100000\n"," 1 : 10  \n","--->:650908:<---\n","  64136/500000: episode: 1721, duration: 2.234s, episode steps:  24, steps per second:  11, episode reward: -40.000, mean reward: -1.667 [-25.000, 22.000], mean action: 1.333 [0.000, 2.000],  loss: 229.272694, mean_q: 82.381672, mean_eps: 0.100000\n"," 1 : 11  \n","--->:652218:<---\n","  64151/500000: episode: 1722, duration: 1.402s, episode steps:  15, steps per second:  11, episode reward: 217.000, mean reward: 14.467 [-25.000, 22.000], mean action: 0.867 [0.000, 2.000],  loss: 203.418015, mean_q: 78.492740, mean_eps: 0.100000\n"," 1 : 12  \n","--->:653523:<---\n","  64171/500000: episode: 1723, duration: 1.907s, episode steps:  20, steps per second:  10, episode reward: 214.000, mean reward: 10.700 [-25.000, 22.000], mean action: 1.050 [0.000, 2.000],  loss: 236.553807, mean_q: 79.059528, mean_eps: 0.100000\n"," 1 : 14  \n","--->:654824:<---\n","  64210/500000: episode: 1724, duration: 3.550s, episode steps:  39, steps per second:  11, episode reward: 337.000, mean reward:  8.641 [-25.000, 22.000], mean action: 1.026 [0.000, 2.000],  loss: 217.148243, mean_q: 80.433753, mean_eps: 0.100000\n"," 1 : 26  \n","--->:656126:<---\n","  64233/500000: episode: 1725, duration: 2.136s, episode steps:  23, steps per second:  11, episode reward: 60.000, mean reward:  2.609 [-25.000, 22.000], mean action: 1.565 [0.000, 2.000],  loss: 226.002640, mean_q: 83.520155, mean_eps: 0.100000\n"," 1 : 7  \n","--->:657434:<---\n","  64243/500000: episode: 1726, duration: 1.037s, episode steps:  10, steps per second:  10, episode reward: 38.000, mean reward:  3.800 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 248.249158, mean_q: 83.305482, mean_eps: 0.100000\n"," 1 : 6  \n","--->:658742:<---\n","  64256/500000: episode: 1727, duration: 1.305s, episode steps:  13, steps per second:  10, episode reward: -34.000, mean reward: -2.615 [-25.000, 22.000], mean action: 1.077 [0.000, 2.000],  loss: 244.475586, mean_q: 80.727516, mean_eps: 0.100000\n"," 1 : 7  \n","--->:660046:<---\n","  64266/500000: episode: 1728, duration: 1.037s, episode steps:  10, steps per second:  10, episode reward: -81.000, mean reward: -8.100 [-25.000, 22.000], mean action: 1.100 [0.000, 2.000],  loss: 217.401279, mean_q: 79.472481, mean_eps: 0.100000\n"," 1 : 7  \n","--->:661355:<---\n","  64275/500000: episode: 1729, duration: 0.943s, episode steps:   9, steps per second:  10, episode reward: 129.000, mean reward: 14.333 [-25.000, 22.000], mean action: 0.778 [0.000, 2.000],  loss: 247.703525, mean_q: 78.687661, mean_eps: 0.100000\n"," 1 : 8  \n","--->:662660:<---\n","  64307/500000: episode: 1730, duration: 2.947s, episode steps:  32, steps per second:  11, episode reward: -62.000, mean reward: -1.938 [-25.000, 22.000], mean action: 1.469 [0.000, 2.000],  loss: 209.299940, mean_q: 80.594139, mean_eps: 0.100000\n"," 1 : 10  \n","--->:663962:<---\n","  64315/500000: episode: 1731, duration: 0.861s, episode steps:   8, steps per second:   9, episode reward: 19.000, mean reward:  2.375 [-25.000, 22.000], mean action: 1.250 [0.000, 2.000],  loss: 234.166189, mean_q: 76.245850, mean_eps: 0.100000\n"," 1 : 3  \n","--->:665263:<---\n","done, took 5617.733 seconds\n","IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n","WSaved/Aminev7/21-13-57/WSaveddqn_21-13-57.h5f\n","IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M1a4FI2rkNpn"},"source":["Loss 180"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8PFqQvIFVuO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tOIurU49mTKC"},"source":["name  = 'W/MULTITRANING/WSaveddqn_15-21-22.h5f'\n","#name  = 'W/8H0_5M/WSaveddqn_19-26-31.h5f'\n","\n","#name  = 'W/amineV9/WSaveddqn_19-33-05.h5f'\n","#name  = 'W/amineV10/WSaveddqn_18-31-29.h5f'\n","dqn.load_weights(name)          \n","sIndex                        = 3180000\n","eIndex                        = 3200000\n","env = env1(    prices , TP, SL,1)\n","_ = dqn.test(env, nb_episodes=13, visualize=False)\n","\n","showV409(1)"]}]}