{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LiveTrading_uv23_5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"code","metadata":{"id":"RwE6PE6QJbdG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615790026217,"user_tz":0,"elapsed":2898,"user":{"displayName":"rtergdr gregr","photoUrl":"","userId":"06451125374650332616"}},"outputId":"2c5e6eda-6c9b-4fb6-b64a-2844ba8ef3e4"},"source":["from google.colab import drive ; drive.mount('/content/drive')\n","!pip install gym keras keras-rl2 tensorflow==2.1\n","import os , sys\n","import pandas as pd \n","from datetime import datetime as dt\n","import time\n","from gym import Env\n","from gym.spaces import Discrete, Box\n","import numpy as np\n","from numpy import loadtxt\n","from datetime import datetime\n","from numpy import savetxt\n","import random ; from random import randint\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, Dense , Flatten \n","from tensorflow.keras.optimizers import Adam\n","from rl.policy import * ; from rl.memory import * ; from rl.agents import *"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Successfully installed gast-0.2.2 keras-applications-1.0.8 keras-rl2-1.0.4 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QjaUMPtOKM6c"},"source":["#input_shape = ( Memoryin_window_length , ob_space_Length , window )\n","Memoryin_window_length      = 20\n","window                      = 360\n","ob_space_Length             = 5\n","actions                     = 3\n","MemoryLimit                 = 200000\n","#-----------------------------------------------------------\n","fee                         = 0.0002\n","SL                          = 0.0020\n","TP                          = 0.0020\n","max_nb_trades               = 20\n","max_nb_steps                = 40"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJt5Q_44KQI7","outputId":"13245022-016d-4779-a6e8-4386493dbdc0"},"source":["#os.chdir(\"/content/drive/MyDrive\") \n","!ls\n","name = 'EURUSD.csv'\n","M1 = pd.read_csv(name, encoding='utf-16',header=None, names=['Date', 'Close'] )\n","data = M1['Close'].to_numpy()\n","len(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["EURUSD.csv  LiveTrading_uv23_4.ipynb  old  WSaved\r\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["7601433"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKcSwdiPKTh8","outputId":"2f1af636-9505-4c14-f578-17fb6e6412d1"},"source":["data = data[ 5000000:7000000  ]\n","len(data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000000"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7-om3J9ZKxIy"},"source":["def show(data):  # data is np\n","  len1 = len(data)\n","  plt.figure(figsize=(13, 6))\n","  x = data[:,0]\n","  y  = np.arange(start=0, stop=len(x) , step=1) \n","  plt.plot(  y ,  x    , label=\"EURUSD Line\" )\n","  plt.plot(  y ,  data[:,1]  , label=\"SMA5 Line\" )\n","  plt.plot(  y ,  data[:,2]  , label=\"SMA10 Line\" )\n","  plt.plot(  y ,  data[:,3]  , label=\"SMA20 Line\" )\n","  plt.plot(  y ,  data[:,4]  , label=\"SMA50 Line\" )\n","  plt.show()\n","  \n","def SMA(Data,Periodes):\n","    #print(' type : {} '.format(type(Data)))\n","    n = len(Data)-1 \n","    res = []\n","    s2 = Data[0]\n","    res.append(s2)\n","    for i in range(1,Periodes+1):\n","        s2 = s2 + Data[i]\n","        s2 = round( s2/2 ,5 )\n","        res.append(s2)\n","        \n","    for i in range(Periodes,n):\n","      iSMA = 0\n","      for j in range(i - Periodes ,i):\n","        if(j>=n):\n","          iSMA = iSMA + Data[n]\n","        else:\n","          iSMA = iSMA + Data[j]\n","      iSMA = iSMA/Periodes  \n","      res.append(iSMA)\n","    return np.array(res)  \n","def get_stats360(substats):\n","  length = len(substats)\n","  if (length!=(360+50)):\n","    print(' get360stats is not 360 + 50 data')\n","    return 0\n","  else : \n","    stats = np.transpose(  [substats , SMA(substats,5) , SMA(substats,10) , SMA(substats,20) , SMA(substats,49)     ]  )\n","    stats = stats[50:length]\n","    max = stats.max() \n","    min = stats.min()\n","    stats =   stats/(max-min) - min/(max-min) \n","    return stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMtw7bDwP7Wu"},"source":["def build_model( ob_space_Length,window , actions,Memoryin_window_length):\n","    model = Sequential()\n","    model.add(Dense(128,activation='relu', input_shape=(Memoryin_window_length,ob_space_Length,window) ) )\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(actions))\n","    return model    \n","def build_agent(model, actions,Memoryin_window_length):\n","    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n","    memory = SequentialMemory(limit= MemoryLimit , window_length=Memoryin_window_length)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n","                  enable_dueling_network=True, dueling_type='avg', \n","                   nb_actions=actions, nb_steps_warmup=1000 ,\n","                   gamma=0.95 \n","                  )\n","    return dqn  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9s6AoTOSP7nQ"},"source":["modelive = build_model(ob_space_Length, window, actions,Memoryin_window_length)\n","dqnlive = build_agent(modelive, actions,Memoryin_window_length)\n","dqnlive.compile( Adam(lr=1e-4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVPGrwD-P71O"},"source":["class testingenv(Env ):\n","\n","\n","        return self.state\n","#-----------------------------------------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgoHO8hxLWuo"},"source":["AllReward = []\n","Short = []\n","stats360 = []\n","\n","jTime= 1\n","for i in range(1,20):\n","  max_nb_steps = i\n","  \n","  jDATA=data[jTime:(jTime+360+50) ]\n","  jstats = get_stats360(jDATA) \n","  stats360.append( jstats)\n","  env = testingenv( np.array(Short) , stats360 )\n","  _ = dqnlive.test(env, nb_episodes=1, visualize=False  )\n","  action = env.action\n","  next_data = data[(jTime+360+50):(jTime+360+50+600) ]\n","  price = jDATA[len(jDATA)-1]\n","  reward , find_new_index,Short_ = reward_of_stats( price , action , next_data , SL, TP , fee )\n","  AllReward.append(reward)\n","  Short.append([Short_,find_new_index])\n","  jTime+=find_new_index\n","  #print('r : {} '.format(reward))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBsS_U9vV5M_","outputId":"5f0047c9-2dab-48ea-f447-f285d77cea00"},"source":["sum(AllReward[0:(len(AllReward)-1)])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-156.00000000000003"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"markdown","metadata":{"id":"jKbnr5kTU4B4"},"source":["# ***8M traning ***"]},{"cell_type":"code","metadata":{"id":"RjZKGyLPa7lp"},"source":["def reward_of_stats( price , action , next_data , SL, TP , fee ):\n","  reward = 0     # 0 is action == 2\n","  find_new_index = 1        # 1 is action == 2\n","  Short_ = -1\n","  x = 0\n","  if(action==0):\n","    index = 0\n","    while( index < len(next_data) ):\n","        newprice = next_data[index]\n","        if( newprice<= (price-TP) ):\n","          reward =  TP - fee\n","          find_new_index = index \n","          index = len(next_data) \n","          Short_ = 0\n","          break\n","        if( newprice>= (price+SL) ):\n","          reward = -1*SL #- fee\n","          find_new_index = index \n","          index = len(next_data) \n","          Short_ = 1\n","          break\n","        index +=1 \n","  #-------------------------------------    \n","  if(action==1):\n","    index = 0\n","    while( index < len(next_data)  ):           \n","      newprice = next_data[index]\n","      if( newprice >= (price+TP) ):\n","        reward =  TP -fee\n","        find_new_index = index \n","        index = len(next_data)\n","        Short_ = 2\n","        break\n","      if( newprice<=(price-SL) ):\n","        reward = -1*SL #- fee\n","        find_new_index = index \n","        index = len(next_data)\n","        Short_ = 3\n","        break\n","      index +=1 \n","  if(actions==2):\n","     reward=0\n","     Short_=6\n","     find_new_index = random.randint(2, 15)    \n","  if(  Short_==-1  ):\n","    find_new_index = len(next_data)  \n","    if( action==0 ):\n","      reward = price - next_data[len(next_data)-1]\n","      Short_ = 4\n","    if( action==1 ):\n","      reward = next_data[len(next_data)-1] - price\n","      Short_ = 5\n","\n","  reward = reward*10000  \n","  find_new_index += random.randint(2, 5) \n","  return reward , find_new_index , Short_ \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KjN2cHLVFeD"},"source":["class traning_8M(Env ):\n","    def __init__(self):\n","         #------------  observation_space  observation_space --------------------------------------------\n","        global ob_space_Length , window ,max_nb_trades  , TP , SL , fee  , data ,jTime , AllReward , max_nb_steps\n","        self.observation_space = Box(low= 0.0, high= 1.0, shape=( ob_space_Length ,window), dtype=np.float64 )\n","        self.action_space = Discrete(2)\n","        #------------  Parameters --------------------------------------------\n","        self.iTime_episode=0\n","        self.action = -1\n","        self.jDATA = 0\n","        self.nb_trade = 0\n","        self.nb_step  = 0\n","         \n","    def step(self, action):\n","        global ob_space_Length , window   , TP , SL , fee  , data ,jTime , max_nb_trades , max_nb_steps\n","        reward = 0\n","        self.action = action\n","        if(action==0 or action==1):\n","          self.nb_trade+=1\n","        \n","\n","        next_data = data[(jTime+360+50-1):(jTime+360+50+700-1) ]\n","        price = data[(jTime+360+50-1)]\n","        reward , find_new_index,Short_ = reward_of_stats( price , self.action , next_data , SL, TP , fee )\n","        #AllReward.append(reward)\n","\n","        \n","\n","        jTime+=find_new_index\n","\n","        info_ = {}\n","        \n","        if(  (jTime+2000 )>len(data) ):\n","          jTime = 2\n","        self.jDATA = data[jTime:(jTime+360+50) ]\n","        jstats = get_stats360(self.jDATA) \n","        self.state = jstats\n","        self.state = np.transpose(self.state)\n","\n","        self.iTime_episode+=1\n","        done = False\n","        self.nb_step+=1\n","        if( self.nb_trade>=max_nb_trades or self.nb_step>=max_nb_steps ):\n","          done = True\n","        return self.state, reward, done, info_\n","    def render(self):\n","        g=0\n","    def reset(self):\n","        global ob_space_Length , window   , TP , SL , fee , max_nb_steps , data ,jTime \n","        #print(' jTime :{} '.format(jTime))\n","        self.nb_trade=0\n","        self.nb_step  = 0\n","        if(  (jTime+2000 )>len(data) ):\n","          jTime = 2\n","        self.jDATA=data[jTime:(jTime+360+50) ]\n","        jstats = get_stats360(self.jDATA) \n","        self.state = jstats\n","        self.state = np.transpose(self.state)\n","        return self.state\n","#-----------------------------------------------------------------\n","env_8M = traning_8M(  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ATJUZBiVFtw","outputId":"c8997c0f-8644-484c-e54d-d3c085db64fe"},"source":["AllReward = 0\n","jTime = 1\n","FileVersion = \"WSaved/V12/\"\n","time = datetime.now()\n","time = time.strftime(\"%H-%M-%S\")\n","render = 0\n","steps = 2000000000\n","Looping = 1\n","NameSaving = \"testing\"\n","for i in range(0,Looping):\n","  dqnlive.fit( env_8M , nb_steps=steps, visualize=False, verbose=2)\n","  time_ = datetime.now()\n","  time_ = time_.strftime(\"%H-%M-%S\")\n","  NameSaving = FileVersion+time_+'/' +\"WSaveddqn_\"+str(time_)+\".h5f\"\n","  print('IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII') \n","  print(NameSaving)\n","  print('IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n","  dqnlive.save_weights(NameSaving, overwrite=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training for 2000000000 steps ...\n","         28/2000000000: episode: 1, duration: 1.008s, episode steps:  28, steps per second:  28, episode reward: -34.900, mean reward: -1.246 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","         53/2000000000: episode: 2, duration: 0.658s, episode steps:  25, steps per second:  38, episode reward: -71.100, mean reward: -2.844 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","         81/2000000000: episode: 3, duration: 0.674s, episode steps:  28, steps per second:  42, episode reward: -89.900, mean reward: -3.211 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        110/2000000000: episode: 4, duration: 0.679s, episode steps:  29, steps per second:  43, episode reward: 84.100, mean reward:  2.900 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        143/2000000000: episode: 5, duration: 0.734s, episode steps:  33, steps per second:  45, episode reward: 103.600, mean reward:  3.139 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        172/2000000000: episode: 6, duration: 0.718s, episode steps:  29, steps per second:  40, episode reward: -77.200, mean reward: -2.662 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        206/2000000000: episode: 7, duration: 0.736s, episode steps:  34, steps per second:  46, episode reward: -153.100, mean reward: -4.503 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        235/2000000000: episode: 8, duration: 0.724s, episode steps:  29, steps per second:  40, episode reward: 34.400, mean reward:  1.186 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        266/2000000000: episode: 9, duration: 0.823s, episode steps:  31, steps per second:  38, episode reward: -63.300, mean reward: -2.042 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        301/2000000000: episode: 10, duration: 0.794s, episode steps:  35, steps per second:  44, episode reward: -102.500, mean reward: -2.929 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        330/2000000000: episode: 11, duration: 0.630s, episode steps:  29, steps per second:  46, episode reward: -33.900, mean reward: -1.169 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        366/2000000000: episode: 12, duration: 0.840s, episode steps:  36, steps per second:  43, episode reward:  8.200, mean reward:  0.228 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        390/2000000000: episode: 13, duration: 0.525s, episode steps:  24, steps per second:  46, episode reward: -25.800, mean reward: -1.075 [-20.000, 18.000], mean action: 0.583 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        418/2000000000: episode: 14, duration: 0.670s, episode steps:  28, steps per second:  42, episode reward: 42.400, mean reward:  1.514 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        451/2000000000: episode: 15, duration: 0.695s, episode steps:  33, steps per second:  47, episode reward: -203.000, mean reward: -6.152 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        480/2000000000: episode: 16, duration: 0.612s, episode steps:  29, steps per second:  47, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        511/2000000000: episode: 17, duration: 0.687s, episode steps:  31, steps per second:  45, episode reward: 57.400, mean reward:  1.852 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        551/2000000000: episode: 18, duration: 0.894s, episode steps:  40, steps per second:  45, episode reward: 47.200, mean reward:  1.180 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        576/2000000000: episode: 19, duration: 0.575s, episode steps:  25, steps per second:  43, episode reward: 128.700, mean reward:  5.148 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        609/2000000000: episode: 20, duration: 0.713s, episode steps:  33, steps per second:  46, episode reward: -242.000, mean reward: -7.333 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        631/2000000000: episode: 21, duration: 0.583s, episode steps:  22, steps per second:  38, episode reward: 52.300, mean reward:  2.377 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        662/2000000000: episode: 22, duration: 0.689s, episode steps:  31, steps per second:  45, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        688/2000000000: episode: 23, duration: 0.624s, episode steps:  26, steps per second:  42, episode reward: 115.900, mean reward:  4.458 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        718/2000000000: episode: 24, duration: 0.710s, episode steps:  30, steps per second:  42, episode reward: 107.900, mean reward:  3.597 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        748/2000000000: episode: 25, duration: 0.645s, episode steps:  30, steps per second:  47, episode reward: -85.300, mean reward: -2.843 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        785/2000000000: episode: 26, duration: 0.792s, episode steps:  37, steps per second:  47, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        819/2000000000: episode: 27, duration: 0.707s, episode steps:  34, steps per second:  48, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        852/2000000000: episode: 28, duration: 0.727s, episode steps:  33, steps per second:  45, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        876/2000000000: episode: 29, duration: 0.524s, episode steps:  24, steps per second:  46, episode reward: -63.100, mean reward: -2.629 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        909/2000000000: episode: 30, duration: 0.716s, episode steps:  33, steps per second:  46, episode reward: -125.100, mean reward: -3.791 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        934/2000000000: episode: 31, duration: 0.521s, episode steps:  25, steps per second:  48, episode reward: -20.000, mean reward: -0.800 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        961/2000000000: episode: 32, duration: 0.573s, episode steps:  27, steps per second:  47, episode reward: -221.200, mean reward: -8.193 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","        988/2000000000: episode: 33, duration: 0.585s, episode steps:  27, steps per second:  46, episode reward: -134.000, mean reward: -4.963 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n","       1022/2000000000: episode: 34, duration: 4.015s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 101.627601, mean_q: 0.021656, mean_eps: 0.909010\n","       1048/2000000000: episode: 35, duration: 2.916s, episode steps:  26, steps per second:   9, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 102.781459, mean_q: -0.024809, mean_eps: 0.906895\n"],"name":"stdout"},{"output_type":"stream","text":["       1079/2000000000: episode: 36, duration: 3.816s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 104.668362, mean_q: 0.269531, mean_eps: 0.904330\n","       1109/2000000000: episode: 37, duration: 3.855s, episode steps:  30, steps per second:   8, episode reward: -134.000, mean reward: -4.467 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 102.263246, mean_q: 0.661261, mean_eps: 0.901585\n","       1142/2000000000: episode: 38, duration: 4.159s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 102.962614, mean_q: 0.103977, mean_eps: 0.898750\n","       1171/2000000000: episode: 39, duration: 3.607s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 109.218360, mean_q: 0.128110, mean_eps: 0.895960\n","       1198/2000000000: episode: 40, duration: 3.163s, episode steps:  27, steps per second:   9, episode reward: -20.000, mean reward: -0.741 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 108.305881, mean_q: 0.382443, mean_eps: 0.893440\n","       1233/2000000000: episode: 41, duration: 4.137s, episode steps:  35, steps per second:   8, episode reward: 154.400, mean reward:  4.411 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 106.634601, mean_q: 0.606365, mean_eps: 0.890650\n","       1258/2000000000: episode: 42, duration: 2.835s, episode steps:  25, steps per second:   9, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 102.795572, mean_q: 1.014509, mean_eps: 0.887950\n","       1288/2000000000: episode: 43, duration: 3.487s, episode steps:  30, steps per second:   9, episode reward:  4.900, mean reward:  0.163 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 98.401732, mean_q: 1.118705, mean_eps: 0.885475\n","       1315/2000000000: episode: 44, duration: 3.223s, episode steps:  27, steps per second:   8, episode reward: -58.000, mean reward: -2.148 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 100.575047, mean_q: 2.644999, mean_eps: 0.882910\n","       1348/2000000000: episode: 45, duration: 4.304s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 94.557168, mean_q: 2.018077, mean_eps: 0.880210\n","       1373/2000000000: episode: 46, duration: 3.552s, episode steps:  25, steps per second:   7, episode reward: -20.000, mean reward: -0.800 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 98.229054, mean_q: 2.518011, mean_eps: 0.877600\n","       1403/2000000000: episode: 47, duration: 4.057s, episode steps:  30, steps per second:   7, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 97.385345, mean_q: 2.753618, mean_eps: 0.875125\n","       1434/2000000000: episode: 48, duration: 3.799s, episode steps:  31, steps per second:   8, episode reward: 110.000, mean reward:  3.548 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 93.064309, mean_q: 2.480073, mean_eps: 0.872380\n","       1462/2000000000: episode: 49, duration: 4.057s, episode steps:  28, steps per second:   7, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 88.292756, mean_q: 4.023754, mean_eps: 0.869725\n","       1485/2000000000: episode: 50, duration: 3.193s, episode steps:  23, steps per second:   7, episode reward: -58.000, mean reward: -2.522 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 90.124098, mean_q: 4.045622, mean_eps: 0.867430\n","       1514/2000000000: episode: 51, duration: 3.512s, episode steps:  29, steps per second:   8, episode reward: -3.900, mean reward: -0.134 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 97.258989, mean_q: 4.368869, mean_eps: 0.865090\n","       1547/2000000000: episode: 52, duration: 4.265s, episode steps:  33, steps per second:   8, episode reward: -35.900, mean reward: -1.088 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 98.493080, mean_q: 3.734916, mean_eps: 0.862300\n","       1581/2000000000: episode: 53, duration: 4.050s, episode steps:  34, steps per second:   8, episode reward: 86.400, mean reward:  2.541 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 92.927204, mean_q: 4.141945, mean_eps: 0.859285\n","       1608/2000000000: episode: 54, duration: 3.333s, episode steps:  27, steps per second:   8, episode reward: -3.400, mean reward: -0.126 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 95.199012, mean_q: 3.727655, mean_eps: 0.856540\n","       1637/2000000000: episode: 55, duration: 3.758s, episode steps:  29, steps per second:   8, episode reward: -64.900, mean reward: -2.238 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.419717, mean_q: 3.778945, mean_eps: 0.854020\n","       1660/2000000000: episode: 56, duration: 2.700s, episode steps:  23, steps per second:   9, episode reward: -106.500, mean reward: -4.630 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 97.714686, mean_q: 4.068060, mean_eps: 0.851680\n","       1695/2000000000: episode: 57, duration: 3.897s, episode steps:  35, steps per second:   9, episode reward:  9.800, mean reward:  0.280 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 90.195448, mean_q: 3.565519, mean_eps: 0.849070\n","       1732/2000000000: episode: 58, duration: 4.277s, episode steps:  37, steps per second:   9, episode reward: 29.800, mean reward:  0.805 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 85.175296, mean_q: 3.972523, mean_eps: 0.845830\n","       1759/2000000000: episode: 59, duration: 3.235s, episode steps:  27, steps per second:   8, episode reward: -134.000, mean reward: -4.963 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 97.884595, mean_q: 4.065001, mean_eps: 0.842950\n","       1790/2000000000: episode: 60, duration: 3.636s, episode steps:  31, steps per second:   9, episode reward: 77.000, mean reward:  2.484 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 98.174605, mean_q: 3.950240, mean_eps: 0.840340\n","       1816/2000000000: episode: 61, duration: 3.128s, episode steps:  26, steps per second:   8, episode reward:  1.400, mean reward:  0.054 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 86.878926, mean_q: 3.977348, mean_eps: 0.837775\n","       1840/2000000000: episode: 62, duration: 3.073s, episode steps:  24, steps per second:   8, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.801229, mean_q: 5.405009, mean_eps: 0.835525\n","       1876/2000000000: episode: 63, duration: 4.453s, episode steps:  36, steps per second:   8, episode reward: -89.800, mean reward: -2.494 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 91.600211, mean_q: 4.503403, mean_eps: 0.832825\n","       1904/2000000000: episode: 64, duration: 3.410s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 90.963786, mean_q: 4.296675, mean_eps: 0.829945\n","       1928/2000000000: episode: 65, duration: 2.722s, episode steps:  24, steps per second:   9, episode reward: -58.000, mean reward: -2.417 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 88.930027, mean_q: 4.721166, mean_eps: 0.827605\n","       1957/2000000000: episode: 66, duration: 3.542s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 98.784829, mean_q: 4.329199, mean_eps: 0.825220\n","       1987/2000000000: episode: 67, duration: 3.986s, episode steps:  30, steps per second:   8, episode reward: -150.300, mean reward: -5.010 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 93.536759, mean_q: 4.148705, mean_eps: 0.822565\n","       2020/2000000000: episode: 68, duration: 3.823s, episode steps:  33, steps per second:   9, episode reward: -54.600, mean reward: -1.655 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 88.794039, mean_q: 3.835400, mean_eps: 0.819730\n"],"name":"stdout"},{"output_type":"stream","text":["       2043/2000000000: episode: 69, duration: 2.806s, episode steps:  23, steps per second:   8, episode reward: -32.800, mean reward: -1.426 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 92.030308, mean_q: 3.530421, mean_eps: 0.817210\n","       2074/2000000000: episode: 70, duration: 4.000s, episode steps:  31, steps per second:   8, episode reward: -176.000, mean reward: -5.677 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 93.416249, mean_q: 4.674639, mean_eps: 0.814780\n","       2106/2000000000: episode: 71, duration: 3.978s, episode steps:  32, steps per second:   8, episode reward:  3.800, mean reward:  0.119 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 87.407740, mean_q: 4.735533, mean_eps: 0.811945\n","       2131/2000000000: episode: 72, duration: 3.242s, episode steps:  25, steps per second:   8, episode reward: -197.900, mean reward: -7.916 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 91.936162, mean_q: 3.012743, mean_eps: 0.809380\n","       2153/2000000000: episode: 73, duration: 2.852s, episode steps:  22, steps per second:   8, episode reward: 77.000, mean reward:  3.500 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 86.932643, mean_q: 4.337125, mean_eps: 0.807265\n","       2180/2000000000: episode: 74, duration: 3.053s, episode steps:  27, steps per second:   9, episode reward: -120.700, mean reward: -4.470 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 85.179118, mean_q: 3.856018, mean_eps: 0.805060\n","       2209/2000000000: episode: 75, duration: 3.175s, episode steps:  29, steps per second:   9, episode reward: -61.900, mean reward: -2.134 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.501046, mean_q: 4.969304, mean_eps: 0.802540\n","       2239/2000000000: episode: 76, duration: 3.522s, episode steps:  30, steps per second:   9, episode reward: 87.000, mean reward:  2.900 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 90.867307, mean_q: 3.480745, mean_eps: 0.799885\n","       2271/2000000000: episode: 77, duration: 3.670s, episode steps:  32, steps per second:   9, episode reward:  1.000, mean reward:  0.031 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 91.384618, mean_q: 4.094034, mean_eps: 0.797095\n","       2301/2000000000: episode: 78, duration: 3.513s, episode steps:  30, steps per second:   9, episode reward: 49.600, mean reward:  1.653 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 84.416828, mean_q: 3.926767, mean_eps: 0.794305\n","       2327/2000000000: episode: 79, duration: 3.159s, episode steps:  26, steps per second:   8, episode reward: -20.000, mean reward: -0.769 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 88.333226, mean_q: 3.345042, mean_eps: 0.791785\n","       2356/2000000000: episode: 80, duration: 3.628s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 89.447800, mean_q: 4.434628, mean_eps: 0.789310\n","       2385/2000000000: episode: 81, duration: 3.600s, episode steps:  29, steps per second:   8, episode reward: -52.800, mean reward: -1.821 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 90.488036, mean_q: 4.615963, mean_eps: 0.786700\n","       2409/2000000000: episode: 82, duration: 2.894s, episode steps:  24, steps per second:   8, episode reward: 93.800, mean reward:  3.908 [-20.000, 18.000], mean action: 0.917 [0.000, 2.000],  loss: 85.738626, mean_q: 4.384104, mean_eps: 0.784315\n","       2433/2000000000: episode: 83, duration: 2.878s, episode steps:  24, steps per second:   8, episode reward: 129.100, mean reward:  5.379 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 91.114543, mean_q: 4.378907, mean_eps: 0.782155\n","       2459/2000000000: episode: 84, duration: 3.181s, episode steps:  26, steps per second:   8, episode reward: -88.700, mean reward: -3.412 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 85.150045, mean_q: 4.041814, mean_eps: 0.779905\n","       2484/2000000000: episode: 85, duration: 3.143s, episode steps:  25, steps per second:   8, episode reward: -4.100, mean reward: -0.164 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 87.771710, mean_q: 4.911812, mean_eps: 0.777610\n","       2513/2000000000: episode: 86, duration: 3.508s, episode steps:  29, steps per second:   8, episode reward: -111.700, mean reward: -3.852 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.048953, mean_q: 4.297479, mean_eps: 0.775180\n","       2545/2000000000: episode: 87, duration: 3.967s, episode steps:  32, steps per second:   8, episode reward: -35.700, mean reward: -1.116 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 91.982916, mean_q: 4.402908, mean_eps: 0.772435\n","       2585/2000000000: episode: 88, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: -96.300, mean reward: -2.407 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.231527, mean_q: 4.180369, mean_eps: 0.769195\n","       2611/2000000000: episode: 89, duration: 3.217s, episode steps:  26, steps per second:   8, episode reward: 53.800, mean reward:  2.069 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 87.464922, mean_q: 4.327486, mean_eps: 0.766225\n","       2645/2000000000: episode: 90, duration: 3.808s, episode steps:  34, steps per second:   9, episode reward: -35.300, mean reward: -1.038 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 87.805561, mean_q: 4.441747, mean_eps: 0.763525\n","       2673/2000000000: episode: 91, duration: 3.423s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.613745, mean_q: 3.052467, mean_eps: 0.760735\n","       2701/2000000000: episode: 92, duration: 3.696s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.119805, mean_q: 3.520432, mean_eps: 0.758215\n","       2734/2000000000: episode: 93, duration: 4.223s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 90.831399, mean_q: 4.312301, mean_eps: 0.755470\n","       2759/2000000000: episode: 94, duration: 3.286s, episode steps:  25, steps per second:   8, episode reward: -58.000, mean reward: -2.320 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 80.852056, mean_q: 3.605930, mean_eps: 0.752860\n","       2795/2000000000: episode: 95, duration: 4.232s, episode steps:  36, steps per second:   9, episode reward: -172.000, mean reward: -4.778 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 89.083778, mean_q: 4.120292, mean_eps: 0.750115\n","       2825/2000000000: episode: 96, duration: 3.543s, episode steps:  30, steps per second:   8, episode reward: 19.800, mean reward:  0.660 [-20.000, 19.800], mean action: 1.067 [0.000, 2.000],  loss: 89.320915, mean_q: 4.056302, mean_eps: 0.747145\n","       2853/2000000000: episode: 97, duration: 3.315s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 88.931703, mean_q: 3.508511, mean_eps: 0.744535\n","       2881/2000000000: episode: 98, duration: 3.504s, episode steps:  28, steps per second:   8, episode reward: -2.600, mean reward: -0.093 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 83.010396, mean_q: 4.283181, mean_eps: 0.742015\n","       2909/2000000000: episode: 99, duration: 3.430s, episode steps:  28, steps per second:   8, episode reward: -166.000, mean reward: -5.929 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 92.200865, mean_q: 3.382030, mean_eps: 0.739495\n","       2939/2000000000: episode: 100, duration: 3.691s, episode steps:  30, steps per second:   8, episode reward: -80.400, mean reward: -2.680 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 94.656994, mean_q: 4.043500, mean_eps: 0.736885\n","       2976/2000000000: episode: 101, duration: 4.286s, episode steps:  37, steps per second:   9, episode reward: 81.000, mean reward:  2.189 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 89.652431, mean_q: 3.550840, mean_eps: 0.733870\n"],"name":"stdout"},{"output_type":"stream","text":["       3006/2000000000: episode: 102, duration: 3.482s, episode steps:  30, steps per second:   9, episode reward: -143.500, mean reward: -4.783 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 93.710412, mean_q: 3.231948, mean_eps: 0.730855\n","       3039/2000000000: episode: 103, duration: 3.762s, episode steps:  33, steps per second:   9, episode reward: -90.400, mean reward: -2.739 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 87.175750, mean_q: 4.513366, mean_eps: 0.728020\n","       3067/2000000000: episode: 104, duration: 2.977s, episode steps:  28, steps per second:   9, episode reward: 33.300, mean reward:  1.189 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 85.516264, mean_q: 2.862664, mean_eps: 0.725275\n","       3093/2000000000: episode: 105, duration: 2.964s, episode steps:  26, steps per second:   9, episode reward: -51.300, mean reward: -1.973 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 92.034174, mean_q: 4.530453, mean_eps: 0.722845\n","       3123/2000000000: episode: 106, duration: 3.478s, episode steps:  30, steps per second:   9, episode reward: 101.100, mean reward:  3.370 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.938001, mean_q: 5.061242, mean_eps: 0.720325\n","       3153/2000000000: episode: 107, duration: 3.443s, episode steps:  30, steps per second:   9, episode reward: -91.900, mean reward: -3.063 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.524538, mean_q: 3.936161, mean_eps: 0.717625\n","       3186/2000000000: episode: 108, duration: 3.580s, episode steps:  33, steps per second:   9, episode reward: -28.700, mean reward: -0.870 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 92.671634, mean_q: 3.063200, mean_eps: 0.714790\n","       3214/2000000000: episode: 109, duration: 3.415s, episode steps:  28, steps per second:   8, episode reward: -103.600, mean reward: -3.700 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 97.100646, mean_q: 4.098920, mean_eps: 0.712045\n","       3248/2000000000: episode: 110, duration: 3.889s, episode steps:  34, steps per second:   9, episode reward: -68.900, mean reward: -2.026 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 92.933278, mean_q: 3.096318, mean_eps: 0.709255\n","       3279/2000000000: episode: 111, duration: 3.871s, episode steps:  31, steps per second:   8, episode reward: 11.200, mean reward:  0.361 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.906615, mean_q: 3.542224, mean_eps: 0.706330\n","       3316/2000000000: episode: 112, duration: 4.443s, episode steps:  37, steps per second:   8, episode reward: 84.800, mean reward:  2.292 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 95.800217, mean_q: 4.206100, mean_eps: 0.703270\n","       3341/2000000000: episode: 113, duration: 3.060s, episode steps:  25, steps per second:   8, episode reward: -58.000, mean reward: -2.320 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 84.579498, mean_q: 4.043280, mean_eps: 0.700480\n","       3372/2000000000: episode: 114, duration: 3.766s, episode steps:  31, steps per second:   8, episode reward: 55.200, mean reward:  1.781 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 89.747488, mean_q: 3.952073, mean_eps: 0.697960\n","       3403/2000000000: episode: 115, duration: 3.547s, episode steps:  31, steps per second:   9, episode reward: -73.300, mean reward: -2.365 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 88.219657, mean_q: 3.531755, mean_eps: 0.695170\n","       3435/2000000000: episode: 116, duration: 3.549s, episode steps:  32, steps per second:   9, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 92.844856, mean_q: 3.947904, mean_eps: 0.692335\n","       3464/2000000000: episode: 117, duration: 3.281s, episode steps:  29, steps per second:   9, episode reward: 15.200, mean reward:  0.524 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 94.508914, mean_q: 3.577285, mean_eps: 0.689590\n","       3492/2000000000: episode: 118, duration: 3.279s, episode steps:  28, steps per second:   9, episode reward: 72.200, mean reward:  2.579 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 89.617209, mean_q: 4.446935, mean_eps: 0.687025\n","       3518/2000000000: episode: 119, duration: 3.084s, episode steps:  26, steps per second:   8, episode reward: 53.800, mean reward:  2.069 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 81.878801, mean_q: 4.145691, mean_eps: 0.684595\n","       3544/2000000000: episode: 120, duration: 2.986s, episode steps:  26, steps per second:   9, episode reward: -100.200, mean reward: -3.854 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 92.376233, mean_q: 5.279861, mean_eps: 0.682255\n","       3576/2000000000: episode: 121, duration: 3.761s, episode steps:  32, steps per second:   9, episode reward: 76.200, mean reward:  2.381 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 90.613872, mean_q: 3.289940, mean_eps: 0.679645\n","       3610/2000000000: episode: 122, duration: 4.087s, episode steps:  34, steps per second:   8, episode reward: -31.600, mean reward: -0.929 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.219452, mean_q: 3.180247, mean_eps: 0.676675\n","       3642/2000000000: episode: 123, duration: 4.013s, episode steps:  32, steps per second:   8, episode reward: -174.300, mean reward: -5.447 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 88.612933, mean_q: 4.333358, mean_eps: 0.673705\n","       3673/2000000000: episode: 124, duration: 3.748s, episode steps:  31, steps per second:   8, episode reward:  5.300, mean reward:  0.171 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 90.401179, mean_q: 2.412622, mean_eps: 0.670870\n","       3707/2000000000: episode: 125, duration: 3.837s, episode steps:  34, steps per second:   9, episode reward:  9.200, mean reward:  0.271 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.586424, mean_q: 3.367992, mean_eps: 0.667945\n","       3735/2000000000: episode: 126, duration: 3.170s, episode steps:  28, steps per second:   9, episode reward: -134.000, mean reward: -4.786 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 94.518333, mean_q: 3.862345, mean_eps: 0.665155\n","       3766/2000000000: episode: 127, duration: 3.487s, episode steps:  31, steps per second:   9, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 87.042697, mean_q: 2.888957, mean_eps: 0.662500\n","       3791/2000000000: episode: 128, duration: 2.775s, episode steps:  25, steps per second:   9, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 89.090628, mean_q: 3.301387, mean_eps: 0.659980\n","       3830/2000000000: episode: 129, duration: 4.951s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 91.335333, mean_q: 3.898034, mean_eps: 0.657100\n","       3854/2000000000: episode: 130, duration: 3.214s, episode steps:  24, steps per second:   7, episode reward: 115.000, mean reward:  4.792 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 86.167764, mean_q: 3.703747, mean_eps: 0.654265\n","       3890/2000000000: episode: 131, duration: 4.359s, episode steps:  36, steps per second:   8, episode reward: 41.000, mean reward:  1.139 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 88.709940, mean_q: 4.295361, mean_eps: 0.651565\n","       3915/2000000000: episode: 132, duration: 2.906s, episode steps:  25, steps per second:   9, episode reward: 42.600, mean reward:  1.704 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 92.432896, mean_q: 4.251254, mean_eps: 0.648820\n","       3944/2000000000: episode: 133, duration: 3.508s, episode steps:  29, steps per second:   8, episode reward: 137.000, mean reward:  4.724 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 87.652546, mean_q: 4.089445, mean_eps: 0.646390\n","       3972/2000000000: episode: 134, duration: 3.356s, episode steps:  28, steps per second:   8, episode reward: -96.000, mean reward: -3.429 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.499465, mean_q: 4.278202, mean_eps: 0.643825\n"],"name":"stdout"},{"output_type":"stream","text":["       4006/2000000000: episode: 135, duration: 3.699s, episode steps:  34, steps per second:   9, episode reward: -76.600, mean reward: -2.253 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 88.827062, mean_q: 3.690927, mean_eps: 0.641035\n","       4041/2000000000: episode: 136, duration: 4.027s, episode steps:  35, steps per second:   9, episode reward: 38.100, mean reward:  1.089 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 84.902050, mean_q: 3.724181, mean_eps: 0.637930\n","       4079/2000000000: episode: 137, duration: 5.011s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 87.769823, mean_q: 3.710875, mean_eps: 0.634645\n","       4107/2000000000: episode: 138, duration: 4.079s, episode steps:  28, steps per second:   7, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 92.145719, mean_q: 4.021432, mean_eps: 0.631675\n","       4143/2000000000: episode: 139, duration: 4.503s, episode steps:  36, steps per second:   8, episode reward: 38.600, mean reward:  1.072 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 91.208015, mean_q: 2.994060, mean_eps: 0.628795\n","       4170/2000000000: episode: 140, duration: 3.232s, episode steps:  27, steps per second:   8, episode reward: -39.700, mean reward: -1.470 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 96.854032, mean_q: 3.737889, mean_eps: 0.625960\n","       4198/2000000000: episode: 141, duration: 3.402s, episode steps:  28, steps per second:   8, episode reward:  5.100, mean reward:  0.182 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 85.465568, mean_q: 4.225690, mean_eps: 0.623485\n","       4226/2000000000: episode: 142, duration: 3.400s, episode steps:  28, steps per second:   8, episode reward: 49.700, mean reward:  1.775 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 88.465719, mean_q: 2.842426, mean_eps: 0.620965\n","       4254/2000000000: episode: 143, duration: 3.435s, episode steps:  28, steps per second:   8, episode reward: 89.500, mean reward:  3.196 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 91.398324, mean_q: 4.320684, mean_eps: 0.618445\n","       4283/2000000000: episode: 144, duration: 3.569s, episode steps:  29, steps per second:   8, episode reward: -134.000, mean reward: -4.621 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 89.725598, mean_q: 3.513215, mean_eps: 0.615880\n","       4314/2000000000: episode: 145, duration: 3.813s, episode steps:  31, steps per second:   8, episode reward: -94.400, mean reward: -3.045 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 90.114949, mean_q: 3.647583, mean_eps: 0.613180\n","       4348/2000000000: episode: 146, duration: 4.333s, episode steps:  34, steps per second:   8, episode reward: -1.300, mean reward: -0.038 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.258753, mean_q: 3.061568, mean_eps: 0.610255\n","       4380/2000000000: episode: 147, duration: 4.489s, episode steps:  32, steps per second:   7, episode reward:  9.900, mean reward:  0.309 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 88.010347, mean_q: 3.953349, mean_eps: 0.607285\n","       4410/2000000000: episode: 148, duration: 4.449s, episode steps:  30, steps per second:   7, episode reward: -87.900, mean reward: -2.930 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 88.055628, mean_q: 3.781232, mean_eps: 0.604495\n","       4440/2000000000: episode: 149, duration: 3.713s, episode steps:  30, steps per second:   8, episode reward: 38.900, mean reward:  1.297 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 99.730599, mean_q: 3.687243, mean_eps: 0.601795\n","       4471/2000000000: episode: 150, duration: 3.820s, episode steps:  31, steps per second:   8, episode reward: -55.100, mean reward: -1.777 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 92.530874, mean_q: 3.181880, mean_eps: 0.599050\n","       4509/2000000000: episode: 151, duration: 4.602s, episode steps:  38, steps per second:   8, episode reward: -69.800, mean reward: -1.837 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 82.507850, mean_q: 3.142606, mean_eps: 0.595945\n","       4548/2000000000: episode: 152, duration: 5.091s, episode steps:  39, steps per second:   8, episode reward: -172.000, mean reward: -4.410 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 84.989242, mean_q: 4.134899, mean_eps: 0.592480\n","       4579/2000000000: episode: 153, duration: 3.715s, episode steps:  31, steps per second:   8, episode reward: -84.900, mean reward: -2.739 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 81.693330, mean_q: 3.111212, mean_eps: 0.589330\n","       4618/2000000000: episode: 154, duration: 4.553s, episode steps:  39, steps per second:   9, episode reward: 82.000, mean reward:  2.103 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 95.567026, mean_q: 3.553172, mean_eps: 0.586180\n","       4648/2000000000: episode: 155, duration: 3.791s, episode steps:  30, steps per second:   8, episode reward: -77.200, mean reward: -2.573 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 86.120209, mean_q: 3.686375, mean_eps: 0.583075\n","       4680/2000000000: episode: 156, duration: 4.005s, episode steps:  32, steps per second:   8, episode reward: 118.800, mean reward:  3.713 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 88.552861, mean_q: 3.780757, mean_eps: 0.580285\n","       4704/2000000000: episode: 157, duration: 2.995s, episode steps:  24, steps per second:   8, episode reward: 65.500, mean reward:  2.729 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 86.218174, mean_q: 3.664209, mean_eps: 0.577765\n","       4741/2000000000: episode: 158, duration: 4.302s, episode steps:  37, steps per second:   9, episode reward: -50.800, mean reward: -1.373 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 87.670243, mean_q: 3.545133, mean_eps: 0.575020\n","       4770/2000000000: episode: 159, duration: 3.345s, episode steps:  29, steps per second:   9, episode reward: 44.800, mean reward:  1.545 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.948109, mean_q: 4.712309, mean_eps: 0.572050\n","       4801/2000000000: episode: 160, duration: 3.822s, episode steps:  31, steps per second:   8, episode reward: -75.600, mean reward: -2.439 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 86.903345, mean_q: 2.864607, mean_eps: 0.569350\n","       4837/2000000000: episode: 161, duration: 4.411s, episode steps:  36, steps per second:   8, episode reward: -110.600, mean reward: -3.072 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.442636, mean_q: 3.206507, mean_eps: 0.566335\n","       4870/2000000000: episode: 162, duration: 4.395s, episode steps:  33, steps per second:   8, episode reward: -22.700, mean reward: -0.688 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 84.601583, mean_q: 3.135441, mean_eps: 0.563230\n","       4903/2000000000: episode: 163, duration: 4.229s, episode steps:  33, steps per second:   8, episode reward: -128.600, mean reward: -3.897 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.000569, mean_q: 3.985596, mean_eps: 0.560260\n","       4943/2000000000: episode: 164, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: 11.900, mean reward:  0.297 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 81.923201, mean_q: 4.134358, mean_eps: 0.556975\n","       4971/2000000000: episode: 165, duration: 3.228s, episode steps:  28, steps per second:   9, episode reward: -7.100, mean reward: -0.254 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.469499, mean_q: 4.621509, mean_eps: 0.553915\n","       5008/2000000000: episode: 166, duration: 4.218s, episode steps:  37, steps per second:   9, episode reward: 102.400, mean reward:  2.768 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 79.235382, mean_q: 3.586375, mean_eps: 0.550990\n","       5044/2000000000: episode: 167, duration: 4.101s, episode steps:  36, steps per second:   9, episode reward: -67.900, mean reward: -1.886 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.923281, mean_q: 3.653906, mean_eps: 0.547705\n"],"name":"stdout"},{"output_type":"stream","text":["       5075/2000000000: episode: 168, duration: 3.428s, episode steps:  31, steps per second:   9, episode reward: -108.100, mean reward: -3.487 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.705356, mean_q: 4.329818, mean_eps: 0.544690\n","       5100/2000000000: episode: 169, duration: 2.831s, episode steps:  25, steps per second:   9, episode reward:  9.900, mean reward:  0.396 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 87.783745, mean_q: 4.140422, mean_eps: 0.542170\n","       5125/2000000000: episode: 170, duration: 3.000s, episode steps:  25, steps per second:   8, episode reward: -17.400, mean reward: -0.696 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 81.683816, mean_q: 3.621598, mean_eps: 0.539920\n","       5159/2000000000: episode: 171, duration: 3.810s, episode steps:  34, steps per second:   9, episode reward: -81.600, mean reward: -2.400 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 85.672601, mean_q: 3.844288, mean_eps: 0.537265\n","       5192/2000000000: episode: 172, duration: 3.861s, episode steps:  33, steps per second:   9, episode reward: -53.800, mean reward: -1.630 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.654337, mean_q: 3.353955, mean_eps: 0.534250\n","       5232/2000000000: episode: 173, duration: 4.594s, episode steps:  40, steps per second:   9, episode reward: 25.800, mean reward:  0.645 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.077529, mean_q: 3.263261, mean_eps: 0.530965\n","       5263/2000000000: episode: 174, duration: 3.658s, episode steps:  31, steps per second:   8, episode reward: -81.500, mean reward: -2.629 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 79.925728, mean_q: 4.142097, mean_eps: 0.527770\n","       5296/2000000000: episode: 175, duration: 3.852s, episode steps:  33, steps per second:   9, episode reward: -7.200, mean reward: -0.218 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 80.919634, mean_q: 3.548916, mean_eps: 0.524890\n","       5325/2000000000: episode: 176, duration: 3.643s, episode steps:  29, steps per second:   8, episode reward: 38.600, mean reward:  1.331 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 84.769279, mean_q: 4.098206, mean_eps: 0.522100\n","       5355/2000000000: episode: 177, duration: 3.769s, episode steps:  30, steps per second:   8, episode reward: -17.300, mean reward: -0.577 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 82.455764, mean_q: 3.658750, mean_eps: 0.519445\n","       5386/2000000000: episode: 178, duration: 3.803s, episode steps:  31, steps per second:   8, episode reward: -99.800, mean reward: -3.219 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 83.437818, mean_q: 3.600820, mean_eps: 0.516700\n","       5418/2000000000: episode: 179, duration: 3.926s, episode steps:  32, steps per second:   8, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.914545, mean_q: 3.016234, mean_eps: 0.513865\n","       5449/2000000000: episode: 180, duration: 4.140s, episode steps:  31, steps per second:   7, episode reward: -7.600, mean reward: -0.245 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 85.914542, mean_q: 4.082052, mean_eps: 0.511030\n","       5481/2000000000: episode: 181, duration: 3.861s, episode steps:  32, steps per second:   8, episode reward: -41.400, mean reward: -1.294 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 81.851291, mean_q: 3.659326, mean_eps: 0.508195\n","       5510/2000000000: episode: 182, duration: 3.928s, episode steps:  29, steps per second:   7, episode reward: 33.300, mean reward:  1.148 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.741563, mean_q: 3.462970, mean_eps: 0.505450\n","       5541/2000000000: episode: 183, duration: 3.721s, episode steps:  31, steps per second:   8, episode reward: -39.500, mean reward: -1.274 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 77.247665, mean_q: 3.087126, mean_eps: 0.502750\n","       5581/2000000000: episode: 184, duration: 4.689s, episode steps:  40, steps per second:   9, episode reward: -87.300, mean reward: -2.183 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.714088, mean_q: 2.654226, mean_eps: 0.499555\n","       5614/2000000000: episode: 185, duration: 3.645s, episode steps:  33, steps per second:   9, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 80.441359, mean_q: 3.619368, mean_eps: 0.496270\n","       5645/2000000000: episode: 186, duration: 3.460s, episode steps:  31, steps per second:   9, episode reward: -85.000, mean reward: -2.742 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 78.706438, mean_q: 4.047665, mean_eps: 0.493390\n","       5680/2000000000: episode: 187, duration: 3.847s, episode steps:  35, steps per second:   9, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 80.969381, mean_q: 3.288182, mean_eps: 0.490420\n","       5714/2000000000: episode: 188, duration: 3.794s, episode steps:  34, steps per second:   9, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 77.549041, mean_q: 3.691741, mean_eps: 0.487315\n","       5744/2000000000: episode: 189, duration: 3.561s, episode steps:  30, steps per second:   8, episode reward: -73.700, mean reward: -2.457 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 86.530147, mean_q: 3.616608, mean_eps: 0.484435\n","       5774/2000000000: episode: 190, duration: 3.526s, episode steps:  30, steps per second:   9, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 79.670857, mean_q: 3.448253, mean_eps: 0.481735\n","       5798/2000000000: episode: 191, duration: 2.962s, episode steps:  24, steps per second:   8, episode reward: -20.000, mean reward: -0.833 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 85.206519, mean_q: 2.827505, mean_eps: 0.479305\n","       5836/2000000000: episode: 192, duration: 4.568s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 84.166321, mean_q: 3.363472, mean_eps: 0.476515\n","       5873/2000000000: episode: 193, duration: 4.372s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 81.190029, mean_q: 3.038170, mean_eps: 0.473140\n","       5910/2000000000: episode: 194, duration: 4.552s, episode steps:  37, steps per second:   8, episode reward: -134.000, mean reward: -3.622 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 80.815530, mean_q: 3.279129, mean_eps: 0.469810\n","       5940/2000000000: episode: 195, duration: 4.227s, episode steps:  30, steps per second:   7, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 84.062488, mean_q: 3.402172, mean_eps: 0.466795\n","       5978/2000000000: episode: 196, duration: 5.578s, episode steps:  38, steps per second:   7, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 79.041103, mean_q: 2.674063, mean_eps: 0.463735\n","       6007/2000000000: episode: 197, duration: 3.903s, episode steps:  29, steps per second:   7, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 81.954525, mean_q: 3.366827, mean_eps: 0.460720\n","       6032/2000000000: episode: 198, duration: 3.540s, episode steps:  25, steps per second:   7, episode reward: -96.000, mean reward: -3.840 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 73.706522, mean_q: 3.986132, mean_eps: 0.458290\n","       6060/2000000000: episode: 199, duration: 4.220s, episode steps:  28, steps per second:   7, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 84.127041, mean_q: 3.218378, mean_eps: 0.455905\n","       6088/2000000000: episode: 200, duration: 4.055s, episode steps:  28, steps per second:   7, episode reward: -89.000, mean reward: -3.179 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 78.465169, mean_q: 3.789797, mean_eps: 0.453385\n"],"name":"stdout"},{"output_type":"stream","text":["       6123/2000000000: episode: 201, duration: 4.450s, episode steps:  35, steps per second:   8, episode reward: -147.800, mean reward: -4.223 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 81.038420, mean_q: 2.546298, mean_eps: 0.450550\n","       6153/2000000000: episode: 202, duration: 3.800s, episode steps:  30, steps per second:   8, episode reward: 47.500, mean reward:  1.583 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.322287, mean_q: 3.674027, mean_eps: 0.447625\n","       6185/2000000000: episode: 203, duration: 4.104s, episode steps:  32, steps per second:   8, episode reward: 82.100, mean reward:  2.566 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.607501, mean_q: 3.089345, mean_eps: 0.444835\n","       6214/2000000000: episode: 204, duration: 3.503s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 1.172 [0.000, 2.000],  loss: 85.363644, mean_q: 3.601671, mean_eps: 0.442090\n","       6252/2000000000: episode: 205, duration: 4.909s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 85.262034, mean_q: 2.684711, mean_eps: 0.439075\n","       6287/2000000000: episode: 206, duration: 4.481s, episode steps:  35, steps per second:   8, episode reward:  2.000, mean reward:  0.057 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 78.103262, mean_q: 3.952539, mean_eps: 0.435790\n","       6323/2000000000: episode: 207, duration: 4.464s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 80.790808, mean_q: 3.724992, mean_eps: 0.432595\n","       6356/2000000000: episode: 208, duration: 4.088s, episode steps:  33, steps per second:   8, episode reward: -31.200, mean reward: -0.945 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 78.021580, mean_q: 3.287256, mean_eps: 0.429490\n","       6388/2000000000: episode: 209, duration: 3.888s, episode steps:  32, steps per second:   8, episode reward: -69.200, mean reward: -2.162 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 77.388576, mean_q: 3.819837, mean_eps: 0.426565\n","       6427/2000000000: episode: 210, duration: 4.832s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 82.835354, mean_q: 4.054669, mean_eps: 0.423370\n","       6457/2000000000: episode: 211, duration: 3.961s, episode steps:  30, steps per second:   8, episode reward: -114.400, mean reward: -3.813 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.135259, mean_q: 3.396432, mean_eps: 0.420265\n","       6481/2000000000: episode: 212, duration: 2.939s, episode steps:  24, steps per second:   8, episode reward: 32.600, mean reward:  1.358 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 86.279221, mean_q: 3.679398, mean_eps: 0.417835\n","       6510/2000000000: episode: 213, duration: 3.603s, episode steps:  29, steps per second:   8, episode reward: 151.300, mean reward:  5.217 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 78.499764, mean_q: 3.418126, mean_eps: 0.415450\n","       6544/2000000000: episode: 214, duration: 4.078s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.839881, mean_q: 3.912188, mean_eps: 0.412615\n","       6584/2000000000: episode: 215, duration: 4.569s, episode steps:  40, steps per second:   9, episode reward: -74.700, mean reward: -1.868 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 81.455679, mean_q: 3.473795, mean_eps: 0.409285\n","       6619/2000000000: episode: 216, duration: 4.296s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 79.739651, mean_q: 4.273172, mean_eps: 0.405910\n","       6655/2000000000: episode: 217, duration: 4.437s, episode steps:  36, steps per second:   8, episode reward: -44.000, mean reward: -1.222 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 83.228968, mean_q: 3.046335, mean_eps: 0.402715\n","       6693/2000000000: episode: 218, duration: 4.695s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 80.533925, mean_q: 3.713976, mean_eps: 0.399385\n","       6729/2000000000: episode: 219, duration: 4.138s, episode steps:  36, steps per second:   9, episode reward: -46.700, mean reward: -1.297 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.266082, mean_q: 3.342588, mean_eps: 0.396055\n","       6769/2000000000: episode: 220, duration: 4.858s, episode steps:  40, steps per second:   8, episode reward: -136.900, mean reward: -3.423 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.299384, mean_q: 3.681910, mean_eps: 0.392635\n","       6795/2000000000: episode: 221, duration: 3.286s, episode steps:  26, steps per second:   8, episode reward: -37.700, mean reward: -1.450 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.300177, mean_q: 2.821600, mean_eps: 0.389665\n","       6827/2000000000: episode: 222, duration: 4.149s, episode steps:  32, steps per second:   8, episode reward: 91.200, mean reward:  2.850 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.607321, mean_q: 3.267796, mean_eps: 0.387055\n","       6864/2000000000: episode: 223, duration: 4.947s, episode steps:  37, steps per second:   7, episode reward: -52.600, mean reward: -1.422 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 78.754573, mean_q: 3.405050, mean_eps: 0.383950\n","       6898/2000000000: episode: 224, duration: 4.189s, episode steps:  34, steps per second:   8, episode reward:  1.400, mean reward:  0.041 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 86.893267, mean_q: 2.810185, mean_eps: 0.380755\n","       6931/2000000000: episode: 225, duration: 4.158s, episode steps:  33, steps per second:   8, episode reward: -22.400, mean reward: -0.679 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 84.598074, mean_q: 3.675286, mean_eps: 0.377740\n","       6964/2000000000: episode: 226, duration: 4.371s, episode steps:  33, steps per second:   8, episode reward: -27.000, mean reward: -0.818 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 76.460209, mean_q: 3.686137, mean_eps: 0.374770\n","       6993/2000000000: episode: 227, duration: 3.965s, episode steps:  29, steps per second:   7, episode reward:  4.500, mean reward:  0.155 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 85.170311, mean_q: 3.816795, mean_eps: 0.371980\n","       7021/2000000000: episode: 228, duration: 3.555s, episode steps:  28, steps per second:   8, episode reward: 96.100, mean reward:  3.432 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 78.602715, mean_q: 2.890075, mean_eps: 0.369415\n","       7054/2000000000: episode: 229, duration: 4.212s, episode steps:  33, steps per second:   8, episode reward: -2.400, mean reward: -0.073 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.376591, mean_q: 2.848930, mean_eps: 0.366670\n","       7094/2000000000: episode: 230, duration: 4.943s, episode steps:  40, steps per second:   8, episode reward: -41.500, mean reward: -1.038 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.571616, mean_q: 3.474845, mean_eps: 0.363385\n","       7120/2000000000: episode: 231, duration: 3.365s, episode steps:  26, steps per second:   8, episode reward: -129.700, mean reward: -4.988 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 79.561392, mean_q: 3.853991, mean_eps: 0.360415\n","       7155/2000000000: episode: 232, duration: 4.650s, episode steps:  35, steps per second:   8, episode reward: -204.500, mean reward: -5.843 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 80.386142, mean_q: 3.688309, mean_eps: 0.357670\n","       7195/2000000000: episode: 233, duration: 5.404s, episode steps:  40, steps per second:   7, episode reward: 38.400, mean reward:  0.960 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.172116, mean_q: 3.006133, mean_eps: 0.354295\n"],"name":"stdout"},{"output_type":"stream","text":["       7226/2000000000: episode: 234, duration: 4.115s, episode steps:  31, steps per second:   8, episode reward: 44.300, mean reward:  1.429 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 78.522979, mean_q: 3.818050, mean_eps: 0.351100\n","       7266/2000000000: episode: 235, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: -148.700, mean reward: -3.718 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 84.486154, mean_q: 3.177444, mean_eps: 0.347905\n","       7306/2000000000: episode: 236, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.058431, mean_q: 2.959939, mean_eps: 0.344305\n","       7337/2000000000: episode: 237, duration: 3.926s, episode steps:  31, steps per second:   8, episode reward: -129.200, mean reward: -4.168 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 81.270616, mean_q: 2.647910, mean_eps: 0.341110\n","       7377/2000000000: episode: 238, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.606146, mean_q: 3.383282, mean_eps: 0.337915\n","       7413/2000000000: episode: 239, duration: 4.590s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 78.859506, mean_q: 2.630703, mean_eps: 0.334495\n","       7450/2000000000: episode: 240, duration: 4.919s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 77.218164, mean_q: 3.623199, mean_eps: 0.331210\n","       7479/2000000000: episode: 241, duration: 3.963s, episode steps:  29, steps per second:   7, episode reward: -68.700, mean reward: -2.369 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.112033, mean_q: 3.627006, mean_eps: 0.328240\n","       7510/2000000000: episode: 242, duration: 4.108s, episode steps:  31, steps per second:   8, episode reward: -28.500, mean reward: -0.919 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.355944, mean_q: 3.235658, mean_eps: 0.325540\n","       7543/2000000000: episode: 243, duration: 4.094s, episode steps:  33, steps per second:   8, episode reward: -27.400, mean reward: -0.830 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 80.118627, mean_q: 3.673447, mean_eps: 0.322660\n","       7574/2000000000: episode: 244, duration: 3.972s, episode steps:  31, steps per second:   8, episode reward: -135.500, mean reward: -4.371 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 71.779169, mean_q: 3.500993, mean_eps: 0.319780\n","       7614/2000000000: episode: 245, duration: 5.355s, episode steps:  40, steps per second:   7, episode reward: 155.500, mean reward:  3.888 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.179935, mean_q: 3.717675, mean_eps: 0.316585\n","       7642/2000000000: episode: 246, duration: 3.810s, episode steps:  28, steps per second:   7, episode reward: 87.800, mean reward:  3.136 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 81.810451, mean_q: 3.262669, mean_eps: 0.313525\n","       7669/2000000000: episode: 247, duration: 4.020s, episode steps:  27, steps per second:   7, episode reward: 43.300, mean reward:  1.604 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.591232, mean_q: 3.975290, mean_eps: 0.311050\n","       7705/2000000000: episode: 248, duration: 5.235s, episode steps:  36, steps per second:   7, episode reward: 115.100, mean reward:  3.197 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.681609, mean_q: 3.170519, mean_eps: 0.308215\n","       7735/2000000000: episode: 249, duration: 4.377s, episode steps:  30, steps per second:   7, episode reward: -7.600, mean reward: -0.253 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 76.501173, mean_q: 3.017225, mean_eps: 0.305245\n","       7768/2000000000: episode: 250, duration: 4.335s, episode steps:  33, steps per second:   8, episode reward: 79.900, mean reward:  2.421 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 71.177388, mean_q: 3.672098, mean_eps: 0.302410\n","       7804/2000000000: episode: 251, duration: 4.446s, episode steps:  36, steps per second:   8, episode reward: 52.000, mean reward:  1.444 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 72.368044, mean_q: 3.155896, mean_eps: 0.299305\n","       7835/2000000000: episode: 252, duration: 4.323s, episode steps:  31, steps per second:   7, episode reward: 78.300, mean reward:  2.526 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.584057, mean_q: 3.440163, mean_eps: 0.296290\n","       7875/2000000000: episode: 253, duration: 6.122s, episode steps:  40, steps per second:   7, episode reward: 39.800, mean reward:  0.995 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.658096, mean_q: 3.505851, mean_eps: 0.293095\n","       7915/2000000000: episode: 254, duration: 5.271s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.300], mean action: 1.400 [0.000, 2.000],  loss: 83.284842, mean_q: 3.172306, mean_eps: 0.289495\n","       7943/2000000000: episode: 255, duration: 3.753s, episode steps:  28, steps per second:   7, episode reward: 42.600, mean reward:  1.521 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 77.603304, mean_q: 3.071341, mean_eps: 0.286435\n","       7979/2000000000: episode: 256, duration: 4.823s, episode steps:  36, steps per second:   7, episode reward: 15.100, mean reward:  0.419 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.660922, mean_q: 3.438987, mean_eps: 0.283555\n","       8008/2000000000: episode: 257, duration: 3.963s, episode steps:  29, steps per second:   7, episode reward: -2.500, mean reward: -0.086 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 79.669606, mean_q: 3.693354, mean_eps: 0.280630\n","       8040/2000000000: episode: 258, duration: 4.003s, episode steps:  32, steps per second:   8, episode reward: -118.800, mean reward: -3.713 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 73.484457, mean_q: 3.555872, mean_eps: 0.277885\n","       8068/2000000000: episode: 259, duration: 3.339s, episode steps:  28, steps per second:   8, episode reward: -115.700, mean reward: -4.132 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 80.101942, mean_q: 3.794480, mean_eps: 0.275185\n","       8101/2000000000: episode: 260, duration: 4.003s, episode steps:  33, steps per second:   8, episode reward: -150.700, mean reward: -4.567 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 80.033481, mean_q: 4.117317, mean_eps: 0.272440\n","       8126/2000000000: episode: 261, duration: 3.446s, episode steps:  25, steps per second:   7, episode reward: 40.600, mean reward:  1.624 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 76.217893, mean_q: 4.294771, mean_eps: 0.269830\n","       8160/2000000000: episode: 262, duration: 4.606s, episode steps:  34, steps per second:   7, episode reward: 120.000, mean reward:  3.529 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.487725, mean_q: 3.243025, mean_eps: 0.267175\n","       8192/2000000000: episode: 263, duration: 4.243s, episode steps:  32, steps per second:   8, episode reward: 125.900, mean reward:  3.934 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 81.805683, mean_q: 3.318434, mean_eps: 0.264205\n","       8227/2000000000: episode: 264, duration: 4.347s, episode steps:  35, steps per second:   8, episode reward: 41.700, mean reward:  1.191 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 78.462371, mean_q: 3.114471, mean_eps: 0.261190\n","       8266/2000000000: episode: 265, duration: 4.971s, episode steps:  39, steps per second:   8, episode reward: -134.000, mean reward: -3.436 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 79.829503, mean_q: 3.163899, mean_eps: 0.257860\n","       8306/2000000000: episode: 266, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 79.870857, mean_q: 3.511985, mean_eps: 0.254305\n"],"name":"stdout"},{"output_type":"stream","text":["       8338/2000000000: episode: 267, duration: 3.755s, episode steps:  32, steps per second:   9, episode reward: -124.500, mean reward: -3.891 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.049585, mean_q: 4.157168, mean_eps: 0.251065\n","       8372/2000000000: episode: 268, duration: 4.192s, episode steps:  34, steps per second:   8, episode reward: 48.800, mean reward:  1.435 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.577519, mean_q: 3.546667, mean_eps: 0.248095\n","       8400/2000000000: episode: 269, duration: 3.489s, episode steps:  28, steps per second:   8, episode reward: -57.700, mean reward: -2.061 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 83.543767, mean_q: 3.604106, mean_eps: 0.245305\n","       8440/2000000000: episode: 270, duration: 5.201s, episode steps:  40, steps per second:   8, episode reward: -72.000, mean reward: -1.800 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.368677, mean_q: 4.268948, mean_eps: 0.242245\n","       8477/2000000000: episode: 271, duration: 5.027s, episode steps:  37, steps per second:   7, episode reward: -179.100, mean reward: -4.841 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 76.639933, mean_q: 3.064654, mean_eps: 0.238780\n","       8517/2000000000: episode: 272, duration: 4.762s, episode steps:  40, steps per second:   8, episode reward: -77.200, mean reward: -1.930 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.241529, mean_q: 3.218486, mean_eps: 0.235315\n","       8549/2000000000: episode: 273, duration: 3.832s, episode steps:  32, steps per second:   8, episode reward: 44.700, mean reward:  1.397 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 76.742421, mean_q: 3.701717, mean_eps: 0.232075\n","       8580/2000000000: episode: 274, duration: 3.840s, episode steps:  31, steps per second:   8, episode reward: -47.600, mean reward: -1.535 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 77.019595, mean_q: 3.345203, mean_eps: 0.229240\n","       8615/2000000000: episode: 275, duration: 4.509s, episode steps:  35, steps per second:   8, episode reward:  2.600, mean reward:  0.074 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 78.680836, mean_q: 3.331179, mean_eps: 0.226270\n","       8655/2000000000: episode: 276, duration: 4.912s, episode steps:  40, steps per second:   8, episode reward: -29.900, mean reward: -0.748 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.213381, mean_q: 2.884244, mean_eps: 0.222895\n","       8692/2000000000: episode: 277, duration: 4.124s, episode steps:  37, steps per second:   9, episode reward: -14.900, mean reward: -0.403 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 78.346341, mean_q: 3.482051, mean_eps: 0.219430\n","       8723/2000000000: episode: 278, duration: 3.543s, episode steps:  31, steps per second:   9, episode reward: -149.500, mean reward: -4.823 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 82.453259, mean_q: 3.950260, mean_eps: 0.216370\n","       8761/2000000000: episode: 279, duration: 4.334s, episode steps:  38, steps per second:   9, episode reward: 92.300, mean reward:  2.429 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 74.635947, mean_q: 3.776615, mean_eps: 0.213265\n","       8798/2000000000: episode: 280, duration: 4.345s, episode steps:  37, steps per second:   9, episode reward: -34.100, mean reward: -0.922 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 75.768777, mean_q: 2.783213, mean_eps: 0.209890\n","       8827/2000000000: episode: 281, duration: 3.522s, episode steps:  29, steps per second:   8, episode reward: 37.200, mean reward:  1.283 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 77.122291, mean_q: 4.071882, mean_eps: 0.206920\n","       8867/2000000000: episode: 282, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: -24.600, mean reward: -0.615 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 79.442438, mean_q: 3.471512, mean_eps: 0.203815\n","       8905/2000000000: episode: 283, duration: 4.502s, episode steps:  38, steps per second:   8, episode reward:  2.500, mean reward:  0.066 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 73.183830, mean_q: 3.342186, mean_eps: 0.200305\n","       8937/2000000000: episode: 284, duration: 4.022s, episode steps:  32, steps per second:   8, episode reward: -30.800, mean reward: -0.963 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 78.458275, mean_q: 3.377085, mean_eps: 0.197155\n","       8975/2000000000: episode: 285, duration: 4.839s, episode steps:  38, steps per second:   8, episode reward: 38.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 65.493999, mean_q: 3.398639, mean_eps: 0.194005\n","       9015/2000000000: episode: 286, duration: 5.647s, episode steps:  40, steps per second:   7, episode reward: 13.700, mean reward:  0.343 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.327204, mean_q: 3.499326, mean_eps: 0.190495\n","       9050/2000000000: episode: 287, duration: 4.486s, episode steps:  35, steps per second:   8, episode reward: -26.700, mean reward: -0.763 [-20.000, 18.900], mean action: 1.086 [0.000, 2.000],  loss: 75.173645, mean_q: 3.797358, mean_eps: 0.187120\n","       9089/2000000000: episode: 288, duration: 4.598s, episode steps:  39, steps per second:   8, episode reward:  6.200, mean reward:  0.159 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 72.854547, mean_q: 3.650968, mean_eps: 0.183790\n","       9121/2000000000: episode: 289, duration: 3.954s, episode steps:  32, steps per second:   8, episode reward: 44.900, mean reward:  1.403 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.425321, mean_q: 3.713932, mean_eps: 0.180595\n","       9151/2000000000: episode: 290, duration: 3.554s, episode steps:  30, steps per second:   8, episode reward: 90.000, mean reward:  3.000 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 71.990102, mean_q: 3.499142, mean_eps: 0.177805\n","       9181/2000000000: episode: 291, duration: 3.626s, episode steps:  30, steps per second:   8, episode reward: 41.800, mean reward:  1.393 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 81.693717, mean_q: 3.148151, mean_eps: 0.175105\n","       9213/2000000000: episode: 292, duration: 3.734s, episode steps:  32, steps per second:   9, episode reward: -3.700, mean reward: -0.116 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 76.081315, mean_q: 3.589221, mean_eps: 0.172315\n","       9251/2000000000: episode: 293, duration: 4.368s, episode steps:  38, steps per second:   9, episode reward: -59.200, mean reward: -1.558 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 77.921148, mean_q: 3.135700, mean_eps: 0.169165\n","       9288/2000000000: episode: 294, duration: 4.279s, episode steps:  37, steps per second:   9, episode reward: 113.100, mean reward:  3.057 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 73.945808, mean_q: 3.654782, mean_eps: 0.165790\n","       9328/2000000000: episode: 295, duration: 4.746s, episode steps:  40, steps per second:   8, episode reward: 21.500, mean reward:  0.538 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.703156, mean_q: 3.409000, mean_eps: 0.162325\n","       9355/2000000000: episode: 296, duration: 3.412s, episode steps:  27, steps per second:   8, episode reward: -7.800, mean reward: -0.289 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 75.536410, mean_q: 3.502998, mean_eps: 0.159310\n","       9394/2000000000: episode: 297, duration: 4.514s, episode steps:  39, steps per second:   9, episode reward: -123.800, mean reward: -3.174 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 76.945823, mean_q: 2.951676, mean_eps: 0.156340\n","       9431/2000000000: episode: 298, duration: 3.912s, episode steps:  37, steps per second:   9, episode reward: -62.500, mean reward: -1.689 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.578440, mean_q: 3.128770, mean_eps: 0.152920\n","       9460/2000000000: episode: 299, duration: 3.346s, episode steps:  29, steps per second:   9, episode reward:  3.800, mean reward:  0.131 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 71.516642, mean_q: 3.462973, mean_eps: 0.149950\n"],"name":"stdout"},{"output_type":"stream","text":["       9495/2000000000: episode: 300, duration: 4.265s, episode steps:  35, steps per second:   8, episode reward: -4.600, mean reward: -0.131 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 75.005914, mean_q: 3.339083, mean_eps: 0.147070\n","       9527/2000000000: episode: 301, duration: 3.971s, episode steps:  32, steps per second:   8, episode reward: -8.600, mean reward: -0.269 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 76.890741, mean_q: 3.379095, mean_eps: 0.144055\n","       9567/2000000000: episode: 302, duration: 4.573s, episode steps:  40, steps per second:   9, episode reward: -147.400, mean reward: -3.685 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.961345, mean_q: 3.622057, mean_eps: 0.140815\n","       9605/2000000000: episode: 303, duration: 4.431s, episode steps:  38, steps per second:   9, episode reward: -31.100, mean reward: -0.818 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 78.380732, mean_q: 3.609905, mean_eps: 0.137305\n","       9634/2000000000: episode: 304, duration: 3.492s, episode steps:  29, steps per second:   8, episode reward: 25.300, mean reward:  0.872 [-20.000, 19.400], mean action: 1.069 [0.000, 2.000],  loss: 82.213922, mean_q: 4.455747, mean_eps: 0.134290\n","       9667/2000000000: episode: 305, duration: 3.902s, episode steps:  33, steps per second:   8, episode reward: -65.500, mean reward: -1.985 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 77.409574, mean_q: 3.086573, mean_eps: 0.131500\n","       9702/2000000000: episode: 306, duration: 4.143s, episode steps:  35, steps per second:   8, episode reward: -0.600, mean reward: -0.017 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 76.504238, mean_q: 3.528514, mean_eps: 0.128440\n","       9735/2000000000: episode: 307, duration: 3.853s, episode steps:  33, steps per second:   9, episode reward: 73.900, mean reward:  2.239 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.410254, mean_q: 3.620454, mean_eps: 0.125380\n","       9774/2000000000: episode: 308, duration: 4.568s, episode steps:  39, steps per second:   9, episode reward: 113.200, mean reward:  2.903 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 77.012398, mean_q: 2.791174, mean_eps: 0.122140\n","       9804/2000000000: episode: 309, duration: 3.530s, episode steps:  30, steps per second:   8, episode reward: 48.400, mean reward:  1.613 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.192097, mean_q: 3.709493, mean_eps: 0.119035\n","       9842/2000000000: episode: 310, duration: 4.435s, episode steps:  38, steps per second:   9, episode reward:  3.900, mean reward:  0.103 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 71.862411, mean_q: 3.136840, mean_eps: 0.115975\n","       9880/2000000000: episode: 311, duration: 4.599s, episode steps:  38, steps per second:   8, episode reward: 140.300, mean reward:  3.692 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 77.505615, mean_q: 3.066651, mean_eps: 0.112555\n","       9916/2000000000: episode: 312, duration: 4.689s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 72.362055, mean_q: 2.897775, mean_eps: 0.109225\n","       9952/2000000000: episode: 313, duration: 4.290s, episode steps:  36, steps per second:   8, episode reward: 67.700, mean reward:  1.881 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 76.995335, mean_q: 3.488888, mean_eps: 0.105985\n","       9984/2000000000: episode: 314, duration: 3.873s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.672247, mean_q: 3.363306, mean_eps: 0.102925\n","      10018/2000000000: episode: 315, duration: 4.026s, episode steps:  34, steps per second:   8, episode reward: -43.300, mean reward: -1.274 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.987319, mean_q: 3.838757, mean_eps: 0.100360\n","      10045/2000000000: episode: 316, duration: 3.178s, episode steps:  27, steps per second:   8, episode reward: -96.000, mean reward: -3.556 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 82.403080, mean_q: 6.674968, mean_eps: 0.100000\n","      10079/2000000000: episode: 317, duration: 3.843s, episode steps:  34, steps per second:   9, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 83.452551, mean_q: 6.720240, mean_eps: 0.100000\n","      10118/2000000000: episode: 318, duration: 4.430s, episode steps:  39, steps per second:   9, episode reward: -134.000, mean reward: -3.436 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 81.680777, mean_q: 6.837612, mean_eps: 0.100000\n","      10150/2000000000: episode: 319, duration: 3.997s, episode steps:  32, steps per second:   8, episode reward: 125.000, mean reward:  3.906 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 85.540639, mean_q: 5.986200, mean_eps: 0.100000\n","      10182/2000000000: episode: 320, duration: 4.032s, episode steps:  32, steps per second:   8, episode reward: -35.000, mean reward: -1.094 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.410132, mean_q: 6.833503, mean_eps: 0.100000\n","      10209/2000000000: episode: 321, duration: 3.061s, episode steps:  27, steps per second:   9, episode reward: -58.000, mean reward: -2.148 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 88.049911, mean_q: 6.758230, mean_eps: 0.100000\n","      10243/2000000000: episode: 322, duration: 4.106s, episode steps:  34, steps per second:   8, episode reward: -43.700, mean reward: -1.285 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 84.763317, mean_q: 6.570592, mean_eps: 0.100000\n","      10276/2000000000: episode: 323, duration: 4.001s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.107029, mean_q: 6.941944, mean_eps: 0.100000\n","      10316/2000000000: episode: 324, duration: 4.971s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.465461, mean_q: 6.655698, mean_eps: 0.100000\n","      10347/2000000000: episode: 325, duration: 3.914s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 83.729165, mean_q: 6.860411, mean_eps: 0.100000\n","      10380/2000000000: episode: 326, duration: 4.200s, episode steps:  33, steps per second:   8, episode reward: -14.400, mean reward: -0.436 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 80.455598, mean_q: 6.373713, mean_eps: 0.100000\n","      10415/2000000000: episode: 327, duration: 4.200s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 0.943 [0.000, 2.000],  loss: 87.541385, mean_q: 5.981234, mean_eps: 0.100000\n","      10442/2000000000: episode: 328, duration: 3.355s, episode steps:  27, steps per second:   8, episode reward: -134.000, mean reward: -4.963 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 87.905252, mean_q: 7.420285, mean_eps: 0.100000\n","      10482/2000000000: episode: 329, duration: 4.578s, episode steps:  40, steps per second:   9, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 87.440907, mean_q: 6.556459, mean_eps: 0.100000\n","      10513/2000000000: episode: 330, duration: 3.658s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 83.542894, mean_q: 6.316004, mean_eps: 0.100000\n","      10546/2000000000: episode: 331, duration: 3.793s, episode steps:  33, steps per second:   9, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 81.250361, mean_q: 6.477924, mean_eps: 0.100000\n","      10578/2000000000: episode: 332, duration: 3.695s, episode steps:  32, steps per second:   9, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 80.527407, mean_q: 7.104070, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      10608/2000000000: episode: 333, duration: 3.453s, episode steps:  30, steps per second:   9, episode reward: -162.100, mean reward: -5.403 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.743809, mean_q: 6.917861, mean_eps: 0.100000\n","      10639/2000000000: episode: 334, duration: 3.539s, episode steps:  31, steps per second:   9, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 81.257183, mean_q: 6.443578, mean_eps: 0.100000\n","      10675/2000000000: episode: 335, duration: 4.087s, episode steps:  36, steps per second:   9, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 84.837684, mean_q: 6.830237, mean_eps: 0.100000\n","      10715/2000000000: episode: 336, duration: 4.492s, episode steps:  40, steps per second:   9, episode reward: -75.300, mean reward: -1.882 [-20.000, 18.700], mean action: 1.350 [0.000, 2.000],  loss: 81.577652, mean_q: 5.846953, mean_eps: 0.100000\n","      10753/2000000000: episode: 337, duration: 4.166s, episode steps:  38, steps per second:   9, episode reward: 28.800, mean reward:  0.758 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 71.074633, mean_q: 6.788832, mean_eps: 0.100000\n","      10793/2000000000: episode: 338, duration: 5.213s, episode steps:  40, steps per second:   8, episode reward: -126.200, mean reward: -3.155 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 75.754012, mean_q: 6.182359, mean_eps: 0.100000\n","      10824/2000000000: episode: 339, duration: 4.099s, episode steps:  31, steps per second:   8, episode reward: 106.200, mean reward:  3.426 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.013437, mean_q: 5.973646, mean_eps: 0.100000\n","      10864/2000000000: episode: 340, duration: 4.499s, episode steps:  40, steps per second:   9, episode reward: -183.700, mean reward: -4.593 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.363322, mean_q: 6.608647, mean_eps: 0.100000\n","      10901/2000000000: episode: 341, duration: 4.233s, episode steps:  37, steps per second:   9, episode reward: 132.300, mean reward:  3.576 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 83.099142, mean_q: 6.784378, mean_eps: 0.100000\n","      10941/2000000000: episode: 342, duration: 4.576s, episode steps:  40, steps per second:   9, episode reward: 80.200, mean reward:  2.005 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.419814, mean_q: 6.783490, mean_eps: 0.100000\n","      10981/2000000000: episode: 343, duration: 4.808s, episode steps:  40, steps per second:   8, episode reward: -95.600, mean reward: -2.390 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.457144, mean_q: 6.266809, mean_eps: 0.100000\n","      11021/2000000000: episode: 344, duration: 4.728s, episode steps:  40, steps per second:   8, episode reward: -8.600, mean reward: -0.215 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 77.711215, mean_q: 6.173959, mean_eps: 0.100000\n","      11061/2000000000: episode: 345, duration: 4.705s, episode steps:  40, steps per second:   9, episode reward: 76.200, mean reward:  1.905 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.934774, mean_q: 6.067239, mean_eps: 0.100000\n","      11089/2000000000: episode: 346, duration: 3.296s, episode steps:  28, steps per second:   8, episode reward: -0.800, mean reward: -0.029 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 82.530103, mean_q: 6.924965, mean_eps: 0.100000\n","      11119/2000000000: episode: 347, duration: 3.485s, episode steps:  30, steps per second:   9, episode reward: -75.200, mean reward: -2.507 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 78.828326, mean_q: 6.858664, mean_eps: 0.100000\n","      11159/2000000000: episode: 348, duration: 4.600s, episode steps:  40, steps per second:   9, episode reward: -36.300, mean reward: -0.908 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.449334, mean_q: 6.542068, mean_eps: 0.100000\n","      11196/2000000000: episode: 349, duration: 4.561s, episode steps:  37, steps per second:   8, episode reward: -161.700, mean reward: -4.370 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 85.064854, mean_q: 6.526987, mean_eps: 0.100000\n","      11230/2000000000: episode: 350, duration: 4.021s, episode steps:  34, steps per second:   8, episode reward: 49.100, mean reward:  1.444 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 83.472362, mean_q: 6.852372, mean_eps: 0.100000\n","      11260/2000000000: episode: 351, duration: 3.817s, episode steps:  30, steps per second:   8, episode reward: 73.300, mean reward:  2.443 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 80.574859, mean_q: 6.822829, mean_eps: 0.100000\n","      11299/2000000000: episode: 352, duration: 4.821s, episode steps:  39, steps per second:   8, episode reward: 81.400, mean reward:  2.087 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 77.426994, mean_q: 7.153454, mean_eps: 0.100000\n","      11333/2000000000: episode: 353, duration: 4.274s, episode steps:  34, steps per second:   8, episode reward: -134.000, mean reward: -3.941 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 80.694588, mean_q: 6.647974, mean_eps: 0.100000\n","      11373/2000000000: episode: 354, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: -19.200, mean reward: -0.480 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.666652, mean_q: 6.602260, mean_eps: 0.100000\n","      11407/2000000000: episode: 355, duration: 4.436s, episode steps:  34, steps per second:   8, episode reward: -18.800, mean reward: -0.553 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 74.834373, mean_q: 5.918814, mean_eps: 0.100000\n","      11445/2000000000: episode: 356, duration: 4.747s, episode steps:  38, steps per second:   8, episode reward: -66.200, mean reward: -1.742 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.843170, mean_q: 6.585415, mean_eps: 0.100000\n","      11476/2000000000: episode: 357, duration: 3.753s, episode steps:  31, steps per second:   8, episode reward: 33.200, mean reward:  1.071 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 87.085725, mean_q: 6.849365, mean_eps: 0.100000\n","      11513/2000000000: episode: 358, duration: 4.458s, episode steps:  37, steps per second:   8, episode reward: 101.100, mean reward:  2.732 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 81.784497, mean_q: 7.156400, mean_eps: 0.100000\n","      11546/2000000000: episode: 359, duration: 4.150s, episode steps:  33, steps per second:   8, episode reward: -160.100, mean reward: -4.852 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 82.324831, mean_q: 6.752689, mean_eps: 0.100000\n","      11586/2000000000: episode: 360, duration: 4.784s, episode steps:  40, steps per second:   8, episode reward: -145.900, mean reward: -3.648 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 81.903600, mean_q: 6.006578, mean_eps: 0.100000\n","      11622/2000000000: episode: 361, duration: 4.807s, episode steps:  36, steps per second:   7, episode reward: 208.000, mean reward:  5.778 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 75.277611, mean_q: 6.533942, mean_eps: 0.100000\n","      11661/2000000000: episode: 362, duration: 5.473s, episode steps:  39, steps per second:   7, episode reward: 112.600, mean reward:  2.887 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.404370, mean_q: 6.693866, mean_eps: 0.100000\n","      11694/2000000000: episode: 363, duration: 4.318s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 82.375682, mean_q: 6.365820, mean_eps: 0.100000\n","      11730/2000000000: episode: 364, duration: 4.639s, episode steps:  36, steps per second:   8, episode reward: 13.500, mean reward:  0.375 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 74.690688, mean_q: 6.486206, mean_eps: 0.100000\n","      11764/2000000000: episode: 365, duration: 4.127s, episode steps:  34, steps per second:   8, episode reward: 87.100, mean reward:  2.562 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 72.773957, mean_q: 7.176164, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      11798/2000000000: episode: 366, duration: 4.119s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 80.624576, mean_q: 7.873799, mean_eps: 0.100000\n","      11836/2000000000: episode: 367, duration: 4.793s, episode steps:  38, steps per second:   8, episode reward: -9.300, mean reward: -0.245 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 81.673033, mean_q: 6.669703, mean_eps: 0.100000\n","      11876/2000000000: episode: 368, duration: 4.655s, episode steps:  40, steps per second:   9, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.270120, mean_q: 6.580284, mean_eps: 0.100000\n","      11909/2000000000: episode: 369, duration: 4.294s, episode steps:  33, steps per second:   8, episode reward: -69.700, mean reward: -2.112 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 80.937062, mean_q: 7.324973, mean_eps: 0.100000\n","      11941/2000000000: episode: 370, duration: 3.844s, episode steps:  32, steps per second:   8, episode reward: -65.700, mean reward: -2.053 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 78.692589, mean_q: 6.691727, mean_eps: 0.100000\n","      11977/2000000000: episode: 371, duration: 4.741s, episode steps:  36, steps per second:   8, episode reward: 67.500, mean reward:  1.875 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.542755, mean_q: 6.698785, mean_eps: 0.100000\n","      12008/2000000000: episode: 372, duration: 3.892s, episode steps:  31, steps per second:   8, episode reward: 29.700, mean reward:  0.958 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 80.892690, mean_q: 6.300336, mean_eps: 0.100000\n","      12048/2000000000: episode: 373, duration: 5.087s, episode steps:  40, steps per second:   8, episode reward: 98.300, mean reward:  2.457 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.934994, mean_q: 5.993789, mean_eps: 0.100000\n","      12080/2000000000: episode: 374, duration: 4.472s, episode steps:  32, steps per second:   7, episode reward: 124.900, mean reward:  3.903 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 77.397143, mean_q: 7.431991, mean_eps: 0.100000\n","      12120/2000000000: episode: 375, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: -85.700, mean reward: -2.143 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.824198, mean_q: 6.274678, mean_eps: 0.100000\n","      12155/2000000000: episode: 376, duration: 4.742s, episode steps:  35, steps per second:   7, episode reward: -128.700, mean reward: -3.677 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 81.039484, mean_q: 5.995219, mean_eps: 0.100000\n","      12187/2000000000: episode: 377, duration: 3.902s, episode steps:  32, steps per second:   8, episode reward: 116.700, mean reward:  3.647 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 74.412871, mean_q: 6.979654, mean_eps: 0.100000\n","      12225/2000000000: episode: 378, duration: 4.435s, episode steps:  38, steps per second:   9, episode reward: -191.900, mean reward: -5.050 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 81.312530, mean_q: 6.457933, mean_eps: 0.100000\n","      12252/2000000000: episode: 379, duration: 3.133s, episode steps:  27, steps per second:   9, episode reward: -154.700, mean reward: -5.730 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 71.483436, mean_q: 6.887049, mean_eps: 0.100000\n","      12285/2000000000: episode: 380, duration: 3.739s, episode steps:  33, steps per second:   9, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.176600, mean_q: 6.832110, mean_eps: 0.100000\n","      12325/2000000000: episode: 381, duration: 4.685s, episode steps:  40, steps per second:   9, episode reward: -33.300, mean reward: -0.833 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.069098, mean_q: 6.609834, mean_eps: 0.100000\n","      12365/2000000000: episode: 382, duration: 4.428s, episode steps:  40, steps per second:   9, episode reward: -110.000, mean reward: -2.750 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.920902, mean_q: 7.305124, mean_eps: 0.100000\n","      12405/2000000000: episode: 383, duration: 4.625s, episode steps:  40, steps per second:   9, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.400020, mean_q: 6.888750, mean_eps: 0.100000\n","      12440/2000000000: episode: 384, duration: 4.395s, episode steps:  35, steps per second:   8, episode reward: -3.000, mean reward: -0.086 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 76.543086, mean_q: 6.520094, mean_eps: 0.100000\n","      12477/2000000000: episode: 385, duration: 4.067s, episode steps:  37, steps per second:   9, episode reward: 83.200, mean reward:  2.249 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 82.621052, mean_q: 6.382167, mean_eps: 0.100000\n","      12517/2000000000: episode: 386, duration: 4.619s, episode steps:  40, steps per second:   9, episode reward: 118.400, mean reward:  2.960 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.994569, mean_q: 6.703121, mean_eps: 0.100000\n","      12557/2000000000: episode: 387, duration: 4.548s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.519342, mean_q: 6.707830, mean_eps: 0.100000\n","      12597/2000000000: episode: 388, duration: 5.084s, episode steps:  40, steps per second:   8, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.518283, mean_q: 6.600571, mean_eps: 0.100000\n","      12625/2000000000: episode: 389, duration: 3.357s, episode steps:  28, steps per second:   8, episode reward:  9.800, mean reward:  0.350 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 72.612018, mean_q: 6.463667, mean_eps: 0.100000\n","      12658/2000000000: episode: 390, duration: 3.923s, episode steps:  33, steps per second:   8, episode reward: -134.000, mean reward: -4.061 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 73.993882, mean_q: 6.971588, mean_eps: 0.100000\n","      12698/2000000000: episode: 391, duration: 4.539s, episode steps:  40, steps per second:   9, episode reward: -17.100, mean reward: -0.428 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.016626, mean_q: 6.845841, mean_eps: 0.100000\n","      12738/2000000000: episode: 392, duration: 4.478s, episode steps:  40, steps per second:   9, episode reward: 62.600, mean reward:  1.565 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.043453, mean_q: 6.662304, mean_eps: 0.100000\n","      12770/2000000000: episode: 393, duration: 3.597s, episode steps:  32, steps per second:   9, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 85.896144, mean_q: 7.062844, mean_eps: 0.100000\n","      12810/2000000000: episode: 394, duration: 4.554s, episode steps:  40, steps per second:   9, episode reward: -17.900, mean reward: -0.447 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.981111, mean_q: 6.818584, mean_eps: 0.100000\n","      12850/2000000000: episode: 395, duration: 4.787s, episode steps:  40, steps per second:   8, episode reward: 33.200, mean reward:  0.830 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.598947, mean_q: 6.603925, mean_eps: 0.100000\n","      12890/2000000000: episode: 396, duration: 4.621s, episode steps:  40, steps per second:   9, episode reward: -98.100, mean reward: -2.452 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.622583, mean_q: 6.078835, mean_eps: 0.100000\n","      12926/2000000000: episode: 397, duration: 4.113s, episode steps:  36, steps per second:   9, episode reward: -15.700, mean reward: -0.436 [-20.000, 18.100], mean action: 1.278 [0.000, 2.000],  loss: 76.644626, mean_q: 7.020522, mean_eps: 0.100000\n","      12958/2000000000: episode: 398, duration: 3.834s, episode steps:  32, steps per second:   8, episode reward: -15.400, mean reward: -0.481 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.268205, mean_q: 6.709573, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      12986/2000000000: episode: 399, duration: 3.476s, episode steps:  28, steps per second:   8, episode reward: -58.000, mean reward: -2.071 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 77.776628, mean_q: 6.510135, mean_eps: 0.100000\n","      13020/2000000000: episode: 400, duration: 3.775s, episode steps:  34, steps per second:   9, episode reward: -63.000, mean reward: -1.853 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 76.559227, mean_q: 6.691649, mean_eps: 0.100000\n","      13060/2000000000: episode: 401, duration: 4.535s, episode steps:  40, steps per second:   9, episode reward: 12.600, mean reward:  0.315 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 74.374527, mean_q: 6.533164, mean_eps: 0.100000\n","      13092/2000000000: episode: 402, duration: 3.795s, episode steps:  32, steps per second:   8, episode reward: -100.800, mean reward: -3.150 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 75.533724, mean_q: 6.284727, mean_eps: 0.100000\n","      13132/2000000000: episode: 403, duration: 4.764s, episode steps:  40, steps per second:   8, episode reward: -16.100, mean reward: -0.403 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.712259, mean_q: 6.961176, mean_eps: 0.100000\n","      13169/2000000000: episode: 404, duration: 4.708s, episode steps:  37, steps per second:   8, episode reward: -32.300, mean reward: -0.873 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 77.451197, mean_q: 7.085875, mean_eps: 0.100000\n","      13209/2000000000: episode: 405, duration: 4.714s, episode steps:  40, steps per second:   8, episode reward: 33.800, mean reward:  0.845 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.067780, mean_q: 5.949454, mean_eps: 0.100000\n","      13249/2000000000: episode: 406, duration: 4.915s, episode steps:  40, steps per second:   8, episode reward: -63.800, mean reward: -1.595 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.786993, mean_q: 6.266308, mean_eps: 0.100000\n","      13289/2000000000: episode: 407, duration: 4.607s, episode steps:  40, steps per second:   9, episode reward: 114.200, mean reward:  2.855 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 77.426455, mean_q: 6.370395, mean_eps: 0.100000\n","      13323/2000000000: episode: 408, duration: 4.180s, episode steps:  34, steps per second:   8, episode reward:  4.800, mean reward:  0.141 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 72.384332, mean_q: 6.925364, mean_eps: 0.100000\n","      13363/2000000000: episode: 409, duration: 4.968s, episode steps:  40, steps per second:   8, episode reward: -30.800, mean reward: -0.770 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.906595, mean_q: 6.394738, mean_eps: 0.100000\n","      13402/2000000000: episode: 410, duration: 5.085s, episode steps:  39, steps per second:   8, episode reward: -78.200, mean reward: -2.005 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 84.302577, mean_q: 6.881064, mean_eps: 0.100000\n","      13440/2000000000: episode: 411, duration: 5.002s, episode steps:  38, steps per second:   8, episode reward: -107.100, mean reward: -2.818 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 78.940565, mean_q: 6.629887, mean_eps: 0.100000\n","      13480/2000000000: episode: 412, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: -101.600, mean reward: -2.540 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.271867, mean_q: 6.150867, mean_eps: 0.100000\n","      13520/2000000000: episode: 413, duration: 4.680s, episode steps:  40, steps per second:   9, episode reward: -18.200, mean reward: -0.455 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 77.647159, mean_q: 5.779248, mean_eps: 0.100000\n","      13560/2000000000: episode: 414, duration: 4.581s, episode steps:  40, steps per second:   9, episode reward: 46.200, mean reward:  1.155 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.389747, mean_q: 6.328523, mean_eps: 0.100000\n","      13591/2000000000: episode: 415, duration: 4.143s, episode steps:  31, steps per second:   7, episode reward: -96.500, mean reward: -3.113 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 67.968506, mean_q: 6.668213, mean_eps: 0.100000\n","      13631/2000000000: episode: 416, duration: 4.811s, episode steps:  40, steps per second:   8, episode reward: -88.900, mean reward: -2.222 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 72.854764, mean_q: 6.526821, mean_eps: 0.100000\n","      13671/2000000000: episode: 417, duration: 4.638s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.771650, mean_q: 6.867586, mean_eps: 0.100000\n","      13705/2000000000: episode: 418, duration: 4.090s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 69.358461, mean_q: 6.392391, mean_eps: 0.100000\n","      13745/2000000000: episode: 419, duration: 4.827s, episode steps:  40, steps per second:   8, episode reward: -60.100, mean reward: -1.502 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.517504, mean_q: 6.318248, mean_eps: 0.100000\n","      13785/2000000000: episode: 420, duration: 4.411s, episode steps:  40, steps per second:   9, episode reward: 53.400, mean reward:  1.335 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.507116, mean_q: 6.455184, mean_eps: 0.100000\n","      13818/2000000000: episode: 421, duration: 3.716s, episode steps:  33, steps per second:   9, episode reward: 12.000, mean reward:  0.364 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 70.269811, mean_q: 7.156448, mean_eps: 0.100000\n","      13853/2000000000: episode: 422, duration: 4.105s, episode steps:  35, steps per second:   9, episode reward: 121.000, mean reward:  3.457 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 70.846943, mean_q: 6.832472, mean_eps: 0.100000\n","      13893/2000000000: episode: 423, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: -2.100, mean reward: -0.053 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 74.211501, mean_q: 6.460578, mean_eps: 0.100000\n","      13933/2000000000: episode: 424, duration: 5.177s, episode steps:  40, steps per second:   8, episode reward: -23.300, mean reward: -0.582 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.279989, mean_q: 6.615290, mean_eps: 0.100000\n","      13968/2000000000: episode: 425, duration: 4.564s, episode steps:  35, steps per second:   8, episode reward: -159.000, mean reward: -4.543 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 74.915973, mean_q: 6.548297, mean_eps: 0.100000\n","      14008/2000000000: episode: 426, duration: 4.797s, episode steps:  40, steps per second:   8, episode reward: 69.000, mean reward:  1.725 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.237010, mean_q: 6.002580, mean_eps: 0.100000\n","      14048/2000000000: episode: 427, duration: 4.836s, episode steps:  40, steps per second:   8, episode reward: -130.000, mean reward: -3.250 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 66.405303, mean_q: 6.204703, mean_eps: 0.100000\n","      14088/2000000000: episode: 428, duration: 5.137s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.912676, mean_q: 6.038912, mean_eps: 0.100000\n","      14128/2000000000: episode: 429, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 49.900, mean reward:  1.248 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.246653, mean_q: 7.167719, mean_eps: 0.100000\n","      14154/2000000000: episode: 430, duration: 3.427s, episode steps:  26, steps per second:   8, episode reward: 65.200, mean reward:  2.508 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 71.887168, mean_q: 6.492804, mean_eps: 0.100000\n","      14191/2000000000: episode: 431, duration: 4.563s, episode steps:  37, steps per second:   8, episode reward: -101.300, mean reward: -2.738 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.095804, mean_q: 6.478390, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      14227/2000000000: episode: 432, duration: 4.775s, episode steps:  36, steps per second:   8, episode reward: -160.300, mean reward: -4.453 [-20.000, 19.900], mean action: 1.222 [0.000, 2.000],  loss: 72.397451, mean_q: 6.348955, mean_eps: 0.100000\n","      14264/2000000000: episode: 433, duration: 5.002s, episode steps:  37, steps per second:   7, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 68.772732, mean_q: 6.576081, mean_eps: 0.100000\n","      14297/2000000000: episode: 434, duration: 3.814s, episode steps:  33, steps per second:   9, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 79.484925, mean_q: 6.641076, mean_eps: 0.100000\n","      14335/2000000000: episode: 435, duration: 4.583s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 75.172177, mean_q: 6.148938, mean_eps: 0.100000\n","      14375/2000000000: episode: 436, duration: 4.763s, episode steps:  40, steps per second:   8, episode reward: 27.800, mean reward:  0.695 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.231465, mean_q: 5.853704, mean_eps: 0.100000\n","      14415/2000000000: episode: 437, duration: 4.771s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.369694, mean_q: 6.295430, mean_eps: 0.100000\n","      14452/2000000000: episode: 438, duration: 4.472s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 72.674214, mean_q: 6.555654, mean_eps: 0.100000\n","      14492/2000000000: episode: 439, duration: 4.523s, episode steps:  40, steps per second:   9, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.748807, mean_q: 6.804321, mean_eps: 0.100000\n","      14532/2000000000: episode: 440, duration: 4.517s, episode steps:  40, steps per second:   9, episode reward: -112.000, mean reward: -2.800 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.200842, mean_q: 6.822907, mean_eps: 0.100000\n","      14572/2000000000: episode: 441, duration: 4.634s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.383698, mean_q: 5.809089, mean_eps: 0.100000\n","      14608/2000000000: episode: 442, duration: 4.076s, episode steps:  36, steps per second:   9, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 76.746745, mean_q: 6.994720, mean_eps: 0.100000\n","      14647/2000000000: episode: 443, duration: 4.524s, episode steps:  39, steps per second:   9, episode reward: -185.800, mean reward: -4.764 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 67.994355, mean_q: 6.393333, mean_eps: 0.100000\n","      14677/2000000000: episode: 444, duration: 3.267s, episode steps:  30, steps per second:   9, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 71.958177, mean_q: 6.524582, mean_eps: 0.100000\n","      14708/2000000000: episode: 445, duration: 3.443s, episode steps:  31, steps per second:   9, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.764406, mean_q: 6.152193, mean_eps: 0.100000\n","      14738/2000000000: episode: 446, duration: 3.555s, episode steps:  30, steps per second:   8, episode reward: -21.900, mean reward: -0.730 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 70.828350, mean_q: 7.257792, mean_eps: 0.100000\n","      14778/2000000000: episode: 447, duration: 4.661s, episode steps:  40, steps per second:   9, episode reward: 53.000, mean reward:  1.325 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.967257, mean_q: 6.372432, mean_eps: 0.100000\n","      14814/2000000000: episode: 448, duration: 4.210s, episode steps:  36, steps per second:   9, episode reward: -124.300, mean reward: -3.453 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 70.198015, mean_q: 6.731308, mean_eps: 0.100000\n","      14852/2000000000: episode: 449, duration: 5.121s, episode steps:  38, steps per second:   7, episode reward: -2.800, mean reward: -0.074 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 73.553989, mean_q: 6.624024, mean_eps: 0.100000\n","      14887/2000000000: episode: 450, duration: 4.132s, episode steps:  35, steps per second:   8, episode reward: -203.200, mean reward: -5.806 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.969238, mean_q: 6.268016, mean_eps: 0.100000\n","      14926/2000000000: episode: 451, duration: 4.548s, episode steps:  39, steps per second:   9, episode reward: -36.600, mean reward: -0.938 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 71.875945, mean_q: 6.029778, mean_eps: 0.100000\n","      14966/2000000000: episode: 452, duration: 4.771s, episode steps:  40, steps per second:   8, episode reward: 78.200, mean reward:  1.955 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.223970, mean_q: 6.126736, mean_eps: 0.100000\n","      15001/2000000000: episode: 453, duration: 4.014s, episode steps:  35, steps per second:   9, episode reward: 13.400, mean reward:  0.383 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 74.368460, mean_q: 6.731136, mean_eps: 0.100000\n","      15041/2000000000: episode: 454, duration: 4.415s, episode steps:  40, steps per second:   9, episode reward: -11.800, mean reward: -0.295 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.970947, mean_q: 6.588469, mean_eps: 0.100000\n","      15081/2000000000: episode: 455, duration: 4.663s, episode steps:  40, steps per second:   9, episode reward: -11.100, mean reward: -0.277 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.881837, mean_q: 6.726317, mean_eps: 0.100000\n","      15120/2000000000: episode: 456, duration: 4.727s, episode steps:  39, steps per second:   8, episode reward: -81.500, mean reward: -2.090 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 68.103456, mean_q: 6.758992, mean_eps: 0.100000\n","      15160/2000000000: episode: 457, duration: 4.642s, episode steps:  40, steps per second:   9, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.192355, mean_q: 5.777101, mean_eps: 0.100000\n","      15193/2000000000: episode: 458, duration: 4.137s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 77.480945, mean_q: 6.469981, mean_eps: 0.100000\n","      15233/2000000000: episode: 459, duration: 5.119s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.181788, mean_q: 6.483124, mean_eps: 0.100000\n","      15272/2000000000: episode: 460, duration: 4.942s, episode steps:  39, steps per second:   8, episode reward: -190.000, mean reward: -4.872 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 77.312973, mean_q: 6.372090, mean_eps: 0.100000\n","      15308/2000000000: episode: 461, duration: 4.401s, episode steps:  36, steps per second:   8, episode reward: -70.500, mean reward: -1.958 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 71.241065, mean_q: 6.731038, mean_eps: 0.100000\n","      15348/2000000000: episode: 462, duration: 4.598s, episode steps:  40, steps per second:   9, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.131212, mean_q: 6.785077, mean_eps: 0.100000\n","      15388/2000000000: episode: 463, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: 134.600, mean reward:  3.365 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.569312, mean_q: 6.134317, mean_eps: 0.100000\n","      15428/2000000000: episode: 464, duration: 4.705s, episode steps:  40, steps per second:   9, episode reward: 74.600, mean reward:  1.865 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.650290, mean_q: 6.373994, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      15466/2000000000: episode: 465, duration: 4.753s, episode steps:  38, steps per second:   8, episode reward: -167.000, mean reward: -4.395 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 66.960014, mean_q: 6.490880, mean_eps: 0.100000\n","      15506/2000000000: episode: 466, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: -37.500, mean reward: -0.938 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.824507, mean_q: 6.287747, mean_eps: 0.100000\n","      15546/2000000000: episode: 467, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: -50.200, mean reward: -1.255 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.151543, mean_q: 6.234299, mean_eps: 0.100000\n","      15581/2000000000: episode: 468, duration: 4.225s, episode steps:  35, steps per second:   8, episode reward: -88.300, mean reward: -2.523 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.052867, mean_q: 6.963911, mean_eps: 0.100000\n","      15611/2000000000: episode: 469, duration: 3.571s, episode steps:  30, steps per second:   8, episode reward: -11.700, mean reward: -0.390 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 70.410745, mean_q: 6.596682, mean_eps: 0.100000\n","      15651/2000000000: episode: 470, duration: 4.557s, episode steps:  40, steps per second:   9, episode reward: -7.600, mean reward: -0.190 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.985067, mean_q: 5.789763, mean_eps: 0.100000\n","      15689/2000000000: episode: 471, duration: 4.397s, episode steps:  38, steps per second:   9, episode reward: 37.000, mean reward:  0.974 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 73.123875, mean_q: 6.472703, mean_eps: 0.100000\n","      15729/2000000000: episode: 472, duration: 4.549s, episode steps:  40, steps per second:   9, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.535433, mean_q: 6.122606, mean_eps: 0.100000\n","      15769/2000000000: episode: 473, duration: 4.601s, episode steps:  40, steps per second:   9, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 74.645772, mean_q: 6.692974, mean_eps: 0.100000\n","      15809/2000000000: episode: 474, duration: 4.601s, episode steps:  40, steps per second:   9, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.713746, mean_q: 6.269660, mean_eps: 0.100000\n","      15845/2000000000: episode: 475, duration: 4.369s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.423954, mean_q: 5.996273, mean_eps: 0.100000\n","      15878/2000000000: episode: 476, duration: 3.927s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 70.784832, mean_q: 6.459909, mean_eps: 0.100000\n","      15913/2000000000: episode: 477, duration: 4.259s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 79.137948, mean_q: 7.057755, mean_eps: 0.100000\n","      15945/2000000000: episode: 478, duration: 3.871s, episode steps:  32, steps per second:   8, episode reward: 38.000, mean reward:  1.188 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 76.738161, mean_q: 6.709209, mean_eps: 0.100000\n","      15985/2000000000: episode: 479, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: 16.000, mean reward:  0.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.428562, mean_q: 6.144295, mean_eps: 0.100000\n","      16016/2000000000: episode: 480, duration: 3.512s, episode steps:  31, steps per second:   9, episode reward: 65.000, mean reward:  2.097 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 75.104297, mean_q: 7.102669, mean_eps: 0.100000\n","      16056/2000000000: episode: 481, duration: 4.509s, episode steps:  40, steps per second:   9, episode reward:  1.200, mean reward:  0.030 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.700367, mean_q: 6.410038, mean_eps: 0.100000\n","      16088/2000000000: episode: 482, duration: 3.906s, episode steps:  32, steps per second:   8, episode reward: 25.100, mean reward:  0.784 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 67.041262, mean_q: 6.215735, mean_eps: 0.100000\n","      16128/2000000000: episode: 483, duration: 4.700s, episode steps:  40, steps per second:   9, episode reward: 79.300, mean reward:  1.983 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 81.737568, mean_q: 6.730364, mean_eps: 0.100000\n","      16164/2000000000: episode: 484, duration: 4.354s, episode steps:  36, steps per second:   8, episode reward: 12.600, mean reward:  0.350 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 72.661754, mean_q: 7.100388, mean_eps: 0.100000\n","      16196/2000000000: episode: 485, duration: 3.950s, episode steps:  32, steps per second:   8, episode reward: 90.300, mean reward:  2.822 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 78.726623, mean_q: 6.032436, mean_eps: 0.100000\n","      16227/2000000000: episode: 486, duration: 4.044s, episode steps:  31, steps per second:   8, episode reward: -20.200, mean reward: -0.652 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.353305, mean_q: 6.870927, mean_eps: 0.100000\n","      16262/2000000000: episode: 487, duration: 4.334s, episode steps:  35, steps per second:   8, episode reward: 53.800, mean reward:  1.537 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.517824, mean_q: 6.228586, mean_eps: 0.100000\n","      16302/2000000000: episode: 488, duration: 4.460s, episode steps:  40, steps per second:   9, episode reward: 27.900, mean reward:  0.698 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.052335, mean_q: 6.454680, mean_eps: 0.100000\n","      16342/2000000000: episode: 489, duration: 4.626s, episode steps:  40, steps per second:   9, episode reward: -11.000, mean reward: -0.275 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.368338, mean_q: 6.132070, mean_eps: 0.100000\n","      16368/2000000000: episode: 490, duration: 3.202s, episode steps:  26, steps per second:   8, episode reward: -96.000, mean reward: -3.692 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 65.369719, mean_q: 5.888665, mean_eps: 0.100000\n","      16408/2000000000: episode: 491, duration: 4.671s, episode steps:  40, steps per second:   9, episode reward: -25.600, mean reward: -0.640 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.954664, mean_q: 5.727207, mean_eps: 0.100000\n","      16446/2000000000: episode: 492, duration: 4.377s, episode steps:  38, steps per second:   9, episode reward: 34.500, mean reward:  0.908 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 71.111878, mean_q: 7.012145, mean_eps: 0.100000\n","      16479/2000000000: episode: 493, duration: 4.128s, episode steps:  33, steps per second:   8, episode reward: -80.500, mean reward: -2.439 [-20.000, 18.000], mean action: 1.303 [0.000, 2.000],  loss: 73.385599, mean_q: 6.638113, mean_eps: 0.100000\n","      16512/2000000000: episode: 494, duration: 3.879s, episode steps:  33, steps per second:   9, episode reward: 63.500, mean reward:  1.924 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 73.666035, mean_q: 6.803894, mean_eps: 0.100000\n","      16552/2000000000: episode: 495, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: -96.600, mean reward: -2.415 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.063073, mean_q: 6.470012, mean_eps: 0.100000\n","      16590/2000000000: episode: 496, duration: 4.382s, episode steps:  38, steps per second:   9, episode reward: -146.600, mean reward: -3.858 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 70.678497, mean_q: 6.338402, mean_eps: 0.100000\n","      16630/2000000000: episode: 497, duration: 4.797s, episode steps:  40, steps per second:   8, episode reward: -134.000, mean reward: -3.350 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.697660, mean_q: 6.535842, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      16663/2000000000: episode: 498, duration: 3.956s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 71.836058, mean_q: 6.713966, mean_eps: 0.100000\n","      16702/2000000000: episode: 499, duration: 4.462s, episode steps:  39, steps per second:   9, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 74.563281, mean_q: 6.248679, mean_eps: 0.100000\n","      16742/2000000000: episode: 500, duration: 4.496s, episode steps:  40, steps per second:   9, episode reward:  8.800, mean reward:  0.220 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.098630, mean_q: 6.319144, mean_eps: 0.100000\n","      16773/2000000000: episode: 501, duration: 3.592s, episode steps:  31, steps per second:   9, episode reward: 59.000, mean reward:  1.903 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 74.145210, mean_q: 6.257444, mean_eps: 0.100000\n","      16813/2000000000: episode: 502, duration: 4.512s, episode steps:  40, steps per second:   9, episode reward: 149.100, mean reward:  3.727 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.686343, mean_q: 6.703291, mean_eps: 0.100000\n","      16852/2000000000: episode: 503, duration: 5.025s, episode steps:  39, steps per second:   8, episode reward: 56.200, mean reward:  1.441 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 74.276838, mean_q: 6.556034, mean_eps: 0.100000\n","      16892/2000000000: episode: 504, duration: 4.816s, episode steps:  40, steps per second:   8, episode reward: -16.300, mean reward: -0.407 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.558488, mean_q: 6.349875, mean_eps: 0.100000\n","      16931/2000000000: episode: 505, duration: 4.436s, episode steps:  39, steps per second:   9, episode reward:  8.400, mean reward:  0.215 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 79.899435, mean_q: 6.369957, mean_eps: 0.100000\n","      16969/2000000000: episode: 506, duration: 4.604s, episode steps:  38, steps per second:   8, episode reward: 38.600, mean reward:  1.016 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 74.172471, mean_q: 6.178856, mean_eps: 0.100000\n","      17000/2000000000: episode: 507, duration: 3.665s, episode steps:  31, steps per second:   8, episode reward: 43.900, mean reward:  1.416 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.116120, mean_q: 6.631670, mean_eps: 0.100000\n","      17034/2000000000: episode: 508, duration: 4.202s, episode steps:  34, steps per second:   8, episode reward: 37.500, mean reward:  1.103 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 80.470376, mean_q: 6.193486, mean_eps: 0.100000\n","      17074/2000000000: episode: 509, duration: 4.834s, episode steps:  40, steps per second:   8, episode reward: -172.600, mean reward: -4.315 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.494079, mean_q: 5.837907, mean_eps: 0.100000\n","      17114/2000000000: episode: 510, duration: 4.549s, episode steps:  40, steps per second:   9, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.612979, mean_q: 6.806485, mean_eps: 0.100000\n","      17154/2000000000: episode: 511, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: -2.000, mean reward: -0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.406664, mean_q: 6.372446, mean_eps: 0.100000\n","      17194/2000000000: episode: 512, duration: 4.527s, episode steps:  40, steps per second:   9, episode reward: 83.600, mean reward:  2.090 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.125928, mean_q: 6.465342, mean_eps: 0.100000\n","      17231/2000000000: episode: 513, duration: 4.321s, episode steps:  37, steps per second:   9, episode reward: -76.600, mean reward: -2.070 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 71.861812, mean_q: 6.745116, mean_eps: 0.100000\n","      17269/2000000000: episode: 514, duration: 4.411s, episode steps:  38, steps per second:   9, episode reward: 21.000, mean reward:  0.553 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 69.246257, mean_q: 6.649133, mean_eps: 0.100000\n","      17309/2000000000: episode: 515, duration: 4.665s, episode steps:  40, steps per second:   9, episode reward: -45.200, mean reward: -1.130 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.425593, mean_q: 5.891729, mean_eps: 0.100000\n","      17343/2000000000: episode: 516, duration: 3.958s, episode steps:  34, steps per second:   9, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 72.747691, mean_q: 5.557467, mean_eps: 0.100000\n","      17379/2000000000: episode: 517, duration: 4.158s, episode steps:  36, steps per second:   9, episode reward: 99.100, mean reward:  2.753 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 65.302830, mean_q: 6.406055, mean_eps: 0.100000\n","      17419/2000000000: episode: 518, duration: 4.473s, episode steps:  40, steps per second:   9, episode reward: 177.700, mean reward:  4.443 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.102809, mean_q: 6.409011, mean_eps: 0.100000\n","      17459/2000000000: episode: 519, duration: 4.686s, episode steps:  40, steps per second:   9, episode reward: -134.600, mean reward: -3.365 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.752316, mean_q: 6.461225, mean_eps: 0.100000\n","      17499/2000000000: episode: 520, duration: 4.645s, episode steps:  40, steps per second:   9, episode reward: 28.700, mean reward:  0.717 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 69.337017, mean_q: 6.478016, mean_eps: 0.100000\n","      17539/2000000000: episode: 521, duration: 4.333s, episode steps:  40, steps per second:   9, episode reward: 13.200, mean reward:  0.330 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.937197, mean_q: 6.123326, mean_eps: 0.100000\n","      17579/2000000000: episode: 522, duration: 4.528s, episode steps:  40, steps per second:   9, episode reward:  3.700, mean reward:  0.093 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.380435, mean_q: 6.796753, mean_eps: 0.100000\n","      17617/2000000000: episode: 523, duration: 4.208s, episode steps:  38, steps per second:   9, episode reward: -70.600, mean reward: -1.858 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 67.369597, mean_q: 5.991048, mean_eps: 0.100000\n","      17657/2000000000: episode: 524, duration: 4.575s, episode steps:  40, steps per second:   9, episode reward: -21.100, mean reward: -0.528 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.638240, mean_q: 6.059824, mean_eps: 0.100000\n","      17694/2000000000: episode: 525, duration: 4.164s, episode steps:  37, steps per second:   9, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 63.448559, mean_q: 6.532185, mean_eps: 0.100000\n","      17734/2000000000: episode: 526, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: 28.300, mean reward:  0.708 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.943329, mean_q: 6.497385, mean_eps: 0.100000\n","      17774/2000000000: episode: 527, duration: 4.745s, episode steps:  40, steps per second:   8, episode reward: -129.600, mean reward: -3.240 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.402879, mean_q: 7.066958, mean_eps: 0.100000\n","      17813/2000000000: episode: 528, duration: 4.500s, episode steps:  39, steps per second:   9, episode reward: 59.100, mean reward:  1.515 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 67.017181, mean_q: 6.743182, mean_eps: 0.100000\n","      17848/2000000000: episode: 529, duration: 4.140s, episode steps:  35, steps per second:   8, episode reward: -44.600, mean reward: -1.274 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 73.228005, mean_q: 7.090368, mean_eps: 0.100000\n","      17888/2000000000: episode: 530, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: 89.300, mean reward:  2.233 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.000022, mean_q: 6.482553, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      17928/2000000000: episode: 531, duration: 4.646s, episode steps:  40, steps per second:   9, episode reward: 64.400, mean reward:  1.610 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.601736, mean_q: 7.083248, mean_eps: 0.100000\n","      17968/2000000000: episode: 532, duration: 4.399s, episode steps:  40, steps per second:   9, episode reward: 36.600, mean reward:  0.915 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.839709, mean_q: 6.067293, mean_eps: 0.100000\n","      18008/2000000000: episode: 533, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: -28.600, mean reward: -0.715 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.519817, mean_q: 6.186972, mean_eps: 0.100000\n","      18047/2000000000: episode: 534, duration: 4.890s, episode steps:  39, steps per second:   8, episode reward:  6.800, mean reward:  0.174 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 69.328706, mean_q: 6.046602, mean_eps: 0.100000\n","      18087/2000000000: episode: 535, duration: 4.608s, episode steps:  40, steps per second:   9, episode reward: -155.700, mean reward: -3.892 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.552944, mean_q: 6.339921, mean_eps: 0.100000\n","      18123/2000000000: episode: 536, duration: 4.183s, episode steps:  36, steps per second:   9, episode reward: 20.600, mean reward:  0.572 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 68.289688, mean_q: 6.328735, mean_eps: 0.100000\n","      18160/2000000000: episode: 537, duration: 4.258s, episode steps:  37, steps per second:   9, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 72.830339, mean_q: 6.053397, mean_eps: 0.100000\n","      18200/2000000000: episode: 538, duration: 4.615s, episode steps:  40, steps per second:   9, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.278972, mean_q: 6.192869, mean_eps: 0.100000\n","      18236/2000000000: episode: 539, duration: 4.499s, episode steps:  36, steps per second:   8, episode reward: -210.000, mean reward: -5.833 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 76.178261, mean_q: 6.321965, mean_eps: 0.100000\n","      18267/2000000000: episode: 540, duration: 3.852s, episode steps:  31, steps per second:   8, episode reward: -85.100, mean reward: -2.745 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 71.152790, mean_q: 6.505919, mean_eps: 0.100000\n","      18307/2000000000: episode: 541, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: 33.300, mean reward:  0.832 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.938301, mean_q: 7.035481, mean_eps: 0.100000\n","      18337/2000000000: episode: 542, duration: 3.676s, episode steps:  30, steps per second:   8, episode reward: 14.500, mean reward:  0.483 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 66.965665, mean_q: 7.505525, mean_eps: 0.100000\n","      18377/2000000000: episode: 543, duration: 4.916s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.174631, mean_q: 5.860795, mean_eps: 0.100000\n","      18417/2000000000: episode: 544, duration: 5.180s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 71.604791, mean_q: 6.712877, mean_eps: 0.100000\n","      18448/2000000000: episode: 545, duration: 3.796s, episode steps:  31, steps per second:   8, episode reward: -134.000, mean reward: -4.323 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 73.425515, mean_q: 6.837542, mean_eps: 0.100000\n","      18488/2000000000: episode: 546, duration: 4.878s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 71.646593, mean_q: 5.887487, mean_eps: 0.100000\n","      18523/2000000000: episode: 547, duration: 4.348s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 70.308642, mean_q: 6.078195, mean_eps: 0.100000\n","      18563/2000000000: episode: 548, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: -37.200, mean reward: -0.930 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.909664, mean_q: 6.945526, mean_eps: 0.100000\n","      18603/2000000000: episode: 549, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 15.000, mean reward:  0.375 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.902306, mean_q: 6.839298, mean_eps: 0.100000\n","      18643/2000000000: episode: 550, duration: 4.929s, episode steps:  40, steps per second:   8, episode reward: -55.200, mean reward: -1.380 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.330845, mean_q: 6.544718, mean_eps: 0.100000\n","      18678/2000000000: episode: 551, duration: 4.434s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 74.194937, mean_q: 6.337007, mean_eps: 0.100000\n","      18718/2000000000: episode: 552, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.150532, mean_q: 7.136601, mean_eps: 0.100000\n","      18758/2000000000: episode: 553, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.719804, mean_q: 6.532888, mean_eps: 0.100000\n","      18798/2000000000: episode: 554, duration: 4.605s, episode steps:  40, steps per second:   9, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.238838, mean_q: 6.795833, mean_eps: 0.100000\n","      18838/2000000000: episode: 555, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.496140, mean_q: 6.482364, mean_eps: 0.100000\n","      18878/2000000000: episode: 556, duration: 5.048s, episode steps:  40, steps per second:   8, episode reward: -64.400, mean reward: -1.610 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.535891, mean_q: 6.424128, mean_eps: 0.100000\n","      18911/2000000000: episode: 557, duration: 4.056s, episode steps:  33, steps per second:   8, episode reward:  4.000, mean reward:  0.121 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 69.194753, mean_q: 6.974699, mean_eps: 0.100000\n","      18951/2000000000: episode: 558, duration: 4.485s, episode steps:  40, steps per second:   9, episode reward: 72.500, mean reward:  1.813 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 66.456127, mean_q: 6.585403, mean_eps: 0.100000\n","      18982/2000000000: episode: 559, duration: 3.692s, episode steps:  31, steps per second:   8, episode reward: 44.200, mean reward:  1.426 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.023840, mean_q: 6.632072, mean_eps: 0.100000\n","      19022/2000000000: episode: 560, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -208.000, mean reward: -5.200 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.468311, mean_q: 6.373146, mean_eps: 0.100000\n","      19061/2000000000: episode: 561, duration: 4.498s, episode steps:  39, steps per second:   9, episode reward: -58.800, mean reward: -1.508 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 69.809922, mean_q: 6.371663, mean_eps: 0.100000\n","      19101/2000000000: episode: 562, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 131.400, mean reward:  3.285 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.125587, mean_q: 6.218118, mean_eps: 0.100000\n","      19141/2000000000: episode: 563, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: -148.000, mean reward: -3.700 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.013126, mean_q: 6.462079, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      19181/2000000000: episode: 564, duration: 4.604s, episode steps:  40, steps per second:   9, episode reward: 15.900, mean reward:  0.397 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.230854, mean_q: 6.118241, mean_eps: 0.100000\n","      19220/2000000000: episode: 565, duration: 4.547s, episode steps:  39, steps per second:   9, episode reward: 39.200, mean reward:  1.005 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 69.334790, mean_q: 6.315403, mean_eps: 0.100000\n","      19260/2000000000: episode: 566, duration: 4.551s, episode steps:  40, steps per second:   9, episode reward: -105.000, mean reward: -2.625 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 75.244049, mean_q: 5.740463, mean_eps: 0.100000\n","      19293/2000000000: episode: 567, duration: 3.786s, episode steps:  33, steps per second:   9, episode reward:  4.300, mean reward:  0.130 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 66.383389, mean_q: 6.097363, mean_eps: 0.100000\n","      19325/2000000000: episode: 568, duration: 4.079s, episode steps:  32, steps per second:   8, episode reward: -60.100, mean reward: -1.878 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 73.829739, mean_q: 6.334186, mean_eps: 0.100000\n","      19361/2000000000: episode: 569, duration: 4.132s, episode steps:  36, steps per second:   9, episode reward: 51.600, mean reward:  1.433 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 67.358490, mean_q: 6.190254, mean_eps: 0.100000\n","      19401/2000000000: episode: 570, duration: 4.692s, episode steps:  40, steps per second:   9, episode reward: 70.200, mean reward:  1.755 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 65.082551, mean_q: 6.547016, mean_eps: 0.100000\n","      19441/2000000000: episode: 571, duration: 4.566s, episode steps:  40, steps per second:   9, episode reward: -6.900, mean reward: -0.173 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.953122, mean_q: 6.081678, mean_eps: 0.100000\n","      19481/2000000000: episode: 572, duration: 4.636s, episode steps:  40, steps per second:   9, episode reward: -81.000, mean reward: -2.025 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.128475, mean_q: 6.604593, mean_eps: 0.100000\n","      19521/2000000000: episode: 573, duration: 4.636s, episode steps:  40, steps per second:   9, episode reward: -65.100, mean reward: -1.627 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.422670, mean_q: 6.799601, mean_eps: 0.100000\n","      19558/2000000000: episode: 574, duration: 4.386s, episode steps:  37, steps per second:   8, episode reward: 41.000, mean reward:  1.108 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 70.995489, mean_q: 5.880064, mean_eps: 0.100000\n","      19598/2000000000: episode: 575, duration: 4.616s, episode steps:  40, steps per second:   9, episode reward: 59.400, mean reward:  1.485 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.795281, mean_q: 6.233801, mean_eps: 0.100000\n","      19638/2000000000: episode: 576, duration: 4.679s, episode steps:  40, steps per second:   9, episode reward: 150.800, mean reward:  3.770 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.862153, mean_q: 6.449841, mean_eps: 0.100000\n","      19678/2000000000: episode: 577, duration: 4.883s, episode steps:  40, steps per second:   8, episode reward:  9.900, mean reward:  0.247 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.229613, mean_q: 6.419954, mean_eps: 0.100000\n","      19718/2000000000: episode: 578, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: -180.800, mean reward: -4.520 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.666369, mean_q: 5.732852, mean_eps: 0.100000\n","      19755/2000000000: episode: 579, duration: 4.185s, episode steps:  37, steps per second:   9, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 66.173054, mean_q: 6.337806, mean_eps: 0.100000\n","      19790/2000000000: episode: 580, duration: 4.070s, episode steps:  35, steps per second:   9, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 70.230499, mean_q: 6.097535, mean_eps: 0.100000\n","      19830/2000000000: episode: 581, duration: 4.536s, episode steps:  40, steps per second:   9, episode reward: 136.500, mean reward:  3.412 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.512211, mean_q: 6.139285, mean_eps: 0.100000\n","      19870/2000000000: episode: 582, duration: 4.520s, episode steps:  40, steps per second:   9, episode reward: -200.000, mean reward: -5.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.803401, mean_q: 6.829234, mean_eps: 0.100000\n","      19910/2000000000: episode: 583, duration: 4.677s, episode steps:  40, steps per second:   9, episode reward: -63.400, mean reward: -1.585 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.203315, mean_q: 5.973763, mean_eps: 0.100000\n","      19936/2000000000: episode: 584, duration: 3.024s, episode steps:  26, steps per second:   9, episode reward: -130.700, mean reward: -5.027 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 72.953708, mean_q: 6.050184, mean_eps: 0.100000\n","      19968/2000000000: episode: 585, duration: 3.702s, episode steps:  32, steps per second:   9, episode reward: 21.300, mean reward:  0.666 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 71.133111, mean_q: 6.516909, mean_eps: 0.100000\n","      20008/2000000000: episode: 586, duration: 4.465s, episode steps:  40, steps per second:   9, episode reward: 20.500, mean reward:  0.512 [-20.000, 18.600], mean action: 1.400 [0.000, 2.000],  loss: 69.494325, mean_q: 5.873936, mean_eps: 0.100000\n","      20043/2000000000: episode: 587, duration: 4.010s, episode steps:  35, steps per second:   9, episode reward: 117.800, mean reward:  3.366 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 82.519400, mean_q: 8.861846, mean_eps: 0.100000\n","      20083/2000000000: episode: 588, duration: 4.529s, episode steps:  40, steps per second:   9, episode reward: -36.700, mean reward: -0.917 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 78.949849, mean_q: 8.180137, mean_eps: 0.100000\n","      20123/2000000000: episode: 589, duration: 4.420s, episode steps:  40, steps per second:   9, episode reward: 59.000, mean reward:  1.475 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.521719, mean_q: 7.925321, mean_eps: 0.100000\n","      20163/2000000000: episode: 590, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward: -44.500, mean reward: -1.112 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.614488, mean_q: 8.491738, mean_eps: 0.100000\n","      20201/2000000000: episode: 591, duration: 4.473s, episode steps:  38, steps per second:   8, episode reward: 130.700, mean reward:  3.439 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 73.961176, mean_q: 7.517830, mean_eps: 0.100000\n","      20238/2000000000: episode: 592, duration: 4.414s, episode steps:  37, steps per second:   8, episode reward: 86.800, mean reward:  2.346 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 79.659878, mean_q: 9.028389, mean_eps: 0.100000\n","      20278/2000000000: episode: 593, duration: 4.701s, episode steps:  40, steps per second:   9, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.493234, mean_q: 8.172312, mean_eps: 0.100000\n","      20318/2000000000: episode: 594, duration: 4.857s, episode steps:  40, steps per second:   8, episode reward: -104.300, mean reward: -2.608 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.650835, mean_q: 7.445621, mean_eps: 0.100000\n","      20358/2000000000: episode: 595, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: -96.500, mean reward: -2.413 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 70.759906, mean_q: 8.006434, mean_eps: 0.100000\n","      20398/2000000000: episode: 596, duration: 4.688s, episode steps:  40, steps per second:   9, episode reward: 44.300, mean reward:  1.107 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.467044, mean_q: 8.146045, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      20434/2000000000: episode: 597, duration: 4.367s, episode steps:  36, steps per second:   8, episode reward:  5.600, mean reward:  0.156 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 74.971581, mean_q: 8.917518, mean_eps: 0.100000\n","      20472/2000000000: episode: 598, duration: 4.887s, episode steps:  38, steps per second:   8, episode reward: -15.000, mean reward: -0.395 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 74.389808, mean_q: 8.620903, mean_eps: 0.100000\n","      20508/2000000000: episode: 599, duration: 4.290s, episode steps:  36, steps per second:   8, episode reward: -45.800, mean reward: -1.272 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 73.073507, mean_q: 8.868523, mean_eps: 0.100000\n","      20541/2000000000: episode: 600, duration: 3.970s, episode steps:  33, steps per second:   8, episode reward: -105.100, mean reward: -3.185 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 69.033803, mean_q: 8.254366, mean_eps: 0.100000\n","      20580/2000000000: episode: 601, duration: 4.595s, episode steps:  39, steps per second:   8, episode reward:  9.900, mean reward:  0.254 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 71.636965, mean_q: 8.178433, mean_eps: 0.100000\n","      20620/2000000000: episode: 602, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward:  8.000, mean reward:  0.200 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 71.733036, mean_q: 7.912241, mean_eps: 0.100000\n","      20659/2000000000: episode: 603, duration: 5.145s, episode steps:  39, steps per second:   8, episode reward: 86.200, mean reward:  2.210 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 72.529816, mean_q: 8.255829, mean_eps: 0.100000\n","      20699/2000000000: episode: 604, duration: 4.893s, episode steps:  40, steps per second:   8, episode reward: 53.500, mean reward:  1.337 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.122127, mean_q: 8.593498, mean_eps: 0.100000\n","      20732/2000000000: episode: 605, duration: 3.892s, episode steps:  33, steps per second:   8, episode reward: -26.100, mean reward: -0.791 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 81.034749, mean_q: 8.715225, mean_eps: 0.100000\n","      20769/2000000000: episode: 606, duration: 4.649s, episode steps:  37, steps per second:   8, episode reward: -23.900, mean reward: -0.646 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.548257, mean_q: 8.667432, mean_eps: 0.100000\n","      20802/2000000000: episode: 607, duration: 4.219s, episode steps:  33, steps per second:   8, episode reward: -135.700, mean reward: -4.112 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 69.482268, mean_q: 9.073136, mean_eps: 0.100000\n","      20842/2000000000: episode: 608, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: -43.900, mean reward: -1.098 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.014412, mean_q: 9.306791, mean_eps: 0.100000\n","      20877/2000000000: episode: 609, duration: 4.438s, episode steps:  35, steps per second:   8, episode reward: -53.100, mean reward: -1.517 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 73.315726, mean_q: 8.521146, mean_eps: 0.100000\n","      20910/2000000000: episode: 610, duration: 4.144s, episode steps:  33, steps per second:   8, episode reward: 36.900, mean reward:  1.118 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 81.312075, mean_q: 8.346109, mean_eps: 0.100000\n","      20950/2000000000: episode: 611, duration: 4.773s, episode steps:  40, steps per second:   8, episode reward: -146.000, mean reward: -3.650 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 75.431877, mean_q: 8.010962, mean_eps: 0.100000\n","      20989/2000000000: episode: 612, duration: 4.553s, episode steps:  39, steps per second:   9, episode reward: 35.800, mean reward:  0.918 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 72.127250, mean_q: 8.530443, mean_eps: 0.100000\n","      21027/2000000000: episode: 613, duration: 4.864s, episode steps:  38, steps per second:   8, episode reward: -111.700, mean reward: -2.939 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 75.673908, mean_q: 8.534350, mean_eps: 0.100000\n","      21067/2000000000: episode: 614, duration: 4.967s, episode steps:  40, steps per second:   8, episode reward: 122.600, mean reward:  3.065 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.081720, mean_q: 8.536234, mean_eps: 0.100000\n","      21107/2000000000: episode: 615, duration: 5.251s, episode steps:  40, steps per second:   8, episode reward: -144.400, mean reward: -3.610 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.969557, mean_q: 8.462712, mean_eps: 0.100000\n","      21141/2000000000: episode: 616, duration: 4.214s, episode steps:  34, steps per second:   8, episode reward: 62.100, mean reward:  1.826 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 67.748420, mean_q: 7.935885, mean_eps: 0.100000\n","      21179/2000000000: episode: 617, duration: 4.704s, episode steps:  38, steps per second:   8, episode reward: -37.400, mean reward: -0.984 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 68.276435, mean_q: 8.762743, mean_eps: 0.100000\n","      21219/2000000000: episode: 618, duration: 4.824s, episode steps:  40, steps per second:   8, episode reward: 42.200, mean reward:  1.055 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.956564, mean_q: 8.325270, mean_eps: 0.100000\n","      21259/2000000000: episode: 619, duration: 4.809s, episode steps:  40, steps per second:   8, episode reward: 120.000, mean reward:  3.000 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 68.436629, mean_q: 8.142033, mean_eps: 0.100000\n","      21299/2000000000: episode: 620, duration: 4.810s, episode steps:  40, steps per second:   8, episode reward:  9.100, mean reward:  0.228 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.186986, mean_q: 8.387671, mean_eps: 0.100000\n","      21339/2000000000: episode: 621, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: -42.000, mean reward: -1.050 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.484900, mean_q: 8.279238, mean_eps: 0.100000\n","      21379/2000000000: episode: 622, duration: 5.248s, episode steps:  40, steps per second:   8, episode reward: 42.200, mean reward:  1.055 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.187963, mean_q: 8.122251, mean_eps: 0.100000\n","      21419/2000000000: episode: 623, duration: 4.948s, episode steps:  40, steps per second:   8, episode reward: -120.200, mean reward: -3.005 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.322841, mean_q: 8.892238, mean_eps: 0.100000\n","      21459/2000000000: episode: 624, duration: 4.955s, episode steps:  40, steps per second:   8, episode reward: -0.100, mean reward: -0.003 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.165301, mean_q: 8.491026, mean_eps: 0.100000\n","      21499/2000000000: episode: 625, duration: 5.135s, episode steps:  40, steps per second:   8, episode reward: 74.100, mean reward:  1.852 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.169629, mean_q: 8.721033, mean_eps: 0.100000\n","      21539/2000000000: episode: 626, duration: 4.696s, episode steps:  40, steps per second:   9, episode reward: -52.200, mean reward: -1.305 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.764842, mean_q: 7.929526, mean_eps: 0.100000\n","      21579/2000000000: episode: 627, duration: 5.014s, episode steps:  40, steps per second:   8, episode reward: -58.200, mean reward: -1.455 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.329697, mean_q: 8.189055, mean_eps: 0.100000\n","      21608/2000000000: episode: 628, duration: 3.523s, episode steps:  29, steps per second:   8, episode reward: 98.100, mean reward:  3.383 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 68.447490, mean_q: 8.502831, mean_eps: 0.100000\n","      21645/2000000000: episode: 629, duration: 4.438s, episode steps:  37, steps per second:   8, episode reward: 92.900, mean reward:  2.511 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 68.850180, mean_q: 8.155843, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      21684/2000000000: episode: 630, duration: 4.553s, episode steps:  39, steps per second:   9, episode reward: 49.400, mean reward:  1.267 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 75.541585, mean_q: 8.415297, mean_eps: 0.100000\n","      21724/2000000000: episode: 631, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 48.300, mean reward:  1.208 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.377618, mean_q: 8.912179, mean_eps: 0.100000\n","      21751/2000000000: episode: 632, duration: 3.437s, episode steps:  27, steps per second:   8, episode reward: 42.300, mean reward:  1.567 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 67.264329, mean_q: 8.697121, mean_eps: 0.100000\n","      21791/2000000000: episode: 633, duration: 4.966s, episode steps:  40, steps per second:   8, episode reward: -29.600, mean reward: -0.740 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.254950, mean_q: 8.237008, mean_eps: 0.100000\n","      21821/2000000000: episode: 634, duration: 3.646s, episode steps:  30, steps per second:   8, episode reward: -102.300, mean reward: -3.410 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 70.641361, mean_q: 8.033239, mean_eps: 0.100000\n","      21860/2000000000: episode: 635, duration: 4.898s, episode steps:  39, steps per second:   8, episode reward: -41.200, mean reward: -1.056 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 69.818917, mean_q: 7.950662, mean_eps: 0.100000\n","      21897/2000000000: episode: 636, duration: 4.680s, episode steps:  37, steps per second:   8, episode reward: -84.200, mean reward: -2.276 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 69.822352, mean_q: 8.436609, mean_eps: 0.100000\n","      21937/2000000000: episode: 637, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: 19.400, mean reward:  0.485 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.985425, mean_q: 8.561853, mean_eps: 0.100000\n","      21977/2000000000: episode: 638, duration: 5.170s, episode steps:  40, steps per second:   8, episode reward: -15.600, mean reward: -0.390 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.511409, mean_q: 7.917798, mean_eps: 0.100000\n","      22015/2000000000: episode: 639, duration: 5.058s, episode steps:  38, steps per second:   8, episode reward: -26.200, mean reward: -0.689 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 65.555789, mean_q: 8.361461, mean_eps: 0.100000\n","      22055/2000000000: episode: 640, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.113246, mean_q: 8.485043, mean_eps: 0.100000\n","      22092/2000000000: episode: 641, duration: 4.883s, episode steps:  37, steps per second:   8, episode reward: -81.500, mean reward: -2.203 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 68.521407, mean_q: 8.850530, mean_eps: 0.100000\n","      22132/2000000000: episode: 642, duration: 5.541s, episode steps:  40, steps per second:   7, episode reward:  8.100, mean reward:  0.202 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.112833, mean_q: 8.168229, mean_eps: 0.100000\n","      22172/2000000000: episode: 643, duration: 5.314s, episode steps:  40, steps per second:   8, episode reward: 144.300, mean reward:  3.608 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.164872, mean_q: 8.336345, mean_eps: 0.100000\n","      22211/2000000000: episode: 644, duration: 5.276s, episode steps:  39, steps per second:   7, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 66.093454, mean_q: 8.659929, mean_eps: 0.100000\n","      22249/2000000000: episode: 645, duration: 5.068s, episode steps:  38, steps per second:   7, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 70.983016, mean_q: 8.749653, mean_eps: 0.100000\n","      22281/2000000000: episode: 646, duration: 4.182s, episode steps:  32, steps per second:   8, episode reward: 86.100, mean reward:  2.691 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.451034, mean_q: 8.242828, mean_eps: 0.100000\n","      22318/2000000000: episode: 647, duration: 4.804s, episode steps:  37, steps per second:   8, episode reward: 55.200, mean reward:  1.492 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 67.446068, mean_q: 9.368175, mean_eps: 0.100000\n","      22358/2000000000: episode: 648, duration: 5.046s, episode steps:  40, steps per second:   8, episode reward:  0.500, mean reward:  0.013 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.889794, mean_q: 8.616740, mean_eps: 0.100000\n","      22398/2000000000: episode: 649, duration: 5.046s, episode steps:  40, steps per second:   8, episode reward: -47.700, mean reward: -1.193 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.950470, mean_q: 7.981692, mean_eps: 0.100000\n","      22438/2000000000: episode: 650, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: -66.600, mean reward: -1.665 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.254596, mean_q: 8.457880, mean_eps: 0.100000\n","      22478/2000000000: episode: 651, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.788759, mean_q: 8.687599, mean_eps: 0.100000\n","      22512/2000000000: episode: 652, duration: 4.326s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 72.179697, mean_q: 9.625167, mean_eps: 0.100000\n","      22552/2000000000: episode: 653, duration: 5.192s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.314391, mean_q: 8.577584, mean_eps: 0.100000\n","      22592/2000000000: episode: 654, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.173189, mean_q: 7.986416, mean_eps: 0.100000\n","      22620/2000000000: episode: 655, duration: 4.117s, episode steps:  28, steps per second:   7, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 61.363229, mean_q: 8.370095, mean_eps: 0.100000\n","      22660/2000000000: episode: 656, duration: 5.712s, episode steps:  40, steps per second:   7, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.453834, mean_q: 8.662167, mean_eps: 0.100000\n","      22700/2000000000: episode: 657, duration: 5.602s, episode steps:  40, steps per second:   7, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.569379, mean_q: 7.852883, mean_eps: 0.100000\n","      22731/2000000000: episode: 658, duration: 4.238s, episode steps:  31, steps per second:   7, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 70.934186, mean_q: 8.363130, mean_eps: 0.100000\n","      22771/2000000000: episode: 659, duration: 5.390s, episode steps:  40, steps per second:   7, episode reward: 32.300, mean reward:  0.807 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.101439, mean_q: 8.666274, mean_eps: 0.100000\n","      22811/2000000000: episode: 660, duration: 5.152s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.673635, mean_q: 8.301561, mean_eps: 0.100000\n","      22851/2000000000: episode: 661, duration: 5.326s, episode steps:  40, steps per second:   8, episode reward: -22.200, mean reward: -0.555 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.761131, mean_q: 8.639750, mean_eps: 0.100000\n","      22891/2000000000: episode: 662, duration: 5.495s, episode steps:  40, steps per second:   7, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.735861, mean_q: 8.163088, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      22931/2000000000: episode: 663, duration: 5.429s, episode steps:  40, steps per second:   7, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.221758, mean_q: 7.963668, mean_eps: 0.100000\n","      22967/2000000000: episode: 664, duration: 4.726s, episode steps:  36, steps per second:   8, episode reward: -118.400, mean reward: -3.289 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 62.113836, mean_q: 8.516223, mean_eps: 0.100000\n","      23007/2000000000: episode: 665, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward:  4.400, mean reward:  0.110 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.868490, mean_q: 8.410114, mean_eps: 0.100000\n","      23035/2000000000: episode: 666, duration: 3.660s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 59.702557, mean_q: 8.536950, mean_eps: 0.100000\n","      23069/2000000000: episode: 667, duration: 4.452s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 74.728847, mean_q: 7.967259, mean_eps: 0.100000\n","      23109/2000000000: episode: 668, duration: 5.160s, episode steps:  40, steps per second:   8, episode reward: 36.000, mean reward:  0.900 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.618582, mean_q: 8.220020, mean_eps: 0.100000\n","      23149/2000000000: episode: 669, duration: 4.851s, episode steps:  40, steps per second:   8, episode reward: -150.000, mean reward: -3.750 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.148118, mean_q: 8.533053, mean_eps: 0.100000\n","      23189/2000000000: episode: 670, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: -33.400, mean reward: -0.835 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.595731, mean_q: 8.231749, mean_eps: 0.100000\n","      23224/2000000000: episode: 671, duration: 4.539s, episode steps:  35, steps per second:   8, episode reward: -7.700, mean reward: -0.220 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 69.097707, mean_q: 8.560833, mean_eps: 0.100000\n","      23264/2000000000: episode: 672, duration: 5.259s, episode steps:  40, steps per second:   8, episode reward: -81.300, mean reward: -2.032 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.785713, mean_q: 7.956088, mean_eps: 0.100000\n","      23304/2000000000: episode: 673, duration: 4.795s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.007067, mean_q: 8.688300, mean_eps: 0.100000\n","      23344/2000000000: episode: 674, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.824658, mean_q: 8.377095, mean_eps: 0.100000\n","      23373/2000000000: episode: 675, duration: 3.914s, episode steps:  29, steps per second:   7, episode reward: 67.000, mean reward:  2.310 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 69.817878, mean_q: 8.365255, mean_eps: 0.100000\n","      23413/2000000000: episode: 676, duration: 5.463s, episode steps:  40, steps per second:   7, episode reward: 22.700, mean reward:  0.567 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.734658, mean_q: 8.635732, mean_eps: 0.100000\n","      23453/2000000000: episode: 677, duration: 5.610s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.567495, mean_q: 8.529435, mean_eps: 0.100000\n","      23493/2000000000: episode: 678, duration: 5.325s, episode steps:  40, steps per second:   8, episode reward:  6.000, mean reward:  0.150 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.712042, mean_q: 8.577942, mean_eps: 0.100000\n","      23533/2000000000: episode: 679, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: -96.400, mean reward: -2.410 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.891565, mean_q: 8.449205, mean_eps: 0.100000\n","      23568/2000000000: episode: 680, duration: 4.540s, episode steps:  35, steps per second:   8, episode reward: -79.000, mean reward: -2.257 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.648047, mean_q: 8.735486, mean_eps: 0.100000\n","      23608/2000000000: episode: 681, duration: 5.066s, episode steps:  40, steps per second:   8, episode reward: -30.500, mean reward: -0.762 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.128597, mean_q: 8.445946, mean_eps: 0.100000\n","      23648/2000000000: episode: 682, duration: 5.145s, episode steps:  40, steps per second:   8, episode reward: -101.200, mean reward: -2.530 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.137906, mean_q: 8.743644, mean_eps: 0.100000\n","      23675/2000000000: episode: 683, duration: 3.423s, episode steps:  27, steps per second:   8, episode reward: -1.600, mean reward: -0.059 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 66.627132, mean_q: 7.883229, mean_eps: 0.100000\n","      23715/2000000000: episode: 684, duration: 5.321s, episode steps:  40, steps per second:   8, episode reward: -62.500, mean reward: -1.562 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.466160, mean_q: 8.715772, mean_eps: 0.100000\n","      23755/2000000000: episode: 685, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.848842, mean_q: 8.615652, mean_eps: 0.100000\n","      23795/2000000000: episode: 686, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: -126.500, mean reward: -3.162 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.295064, mean_q: 8.527014, mean_eps: 0.100000\n","      23835/2000000000: episode: 687, duration: 5.645s, episode steps:  40, steps per second:   7, episode reward: -156.600, mean reward: -3.915 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.360615, mean_q: 8.345545, mean_eps: 0.100000\n","      23862/2000000000: episode: 688, duration: 3.558s, episode steps:  27, steps per second:   8, episode reward: -120.000, mean reward: -4.444 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 63.334236, mean_q: 9.408037, mean_eps: 0.100000\n","      23902/2000000000: episode: 689, duration: 4.886s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.890088, mean_q: 8.296610, mean_eps: 0.100000\n","      23937/2000000000: episode: 690, duration: 4.327s, episode steps:  35, steps per second:   8, episode reward: 48.700, mean reward:  1.391 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 68.026668, mean_q: 8.477723, mean_eps: 0.100000\n","      23977/2000000000: episode: 691, duration: 5.119s, episode steps:  40, steps per second:   8, episode reward: -32.900, mean reward: -0.822 [-20.000, 19.100], mean action: 1.325 [0.000, 2.000],  loss: 70.342293, mean_q: 8.498741, mean_eps: 0.100000\n","      24017/2000000000: episode: 692, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: -114.200, mean reward: -2.855 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.780865, mean_q: 8.467186, mean_eps: 0.100000\n","      24053/2000000000: episode: 693, duration: 4.775s, episode steps:  36, steps per second:   8, episode reward: -65.300, mean reward: -1.814 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 66.713189, mean_q: 8.286549, mean_eps: 0.100000\n","      24093/2000000000: episode: 694, duration: 4.801s, episode steps:  40, steps per second:   8, episode reward: -125.300, mean reward: -3.133 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.939014, mean_q: 8.374732, mean_eps: 0.100000\n","      24133/2000000000: episode: 695, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: -52.600, mean reward: -1.315 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 63.377032, mean_q: 7.816306, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      24173/2000000000: episode: 696, duration: 5.383s, episode steps:  40, steps per second:   7, episode reward: 11.800, mean reward:  0.295 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 66.280361, mean_q: 8.363116, mean_eps: 0.100000\n","      24213/2000000000: episode: 697, duration: 5.356s, episode steps:  40, steps per second:   7, episode reward: 37.000, mean reward:  0.925 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.838858, mean_q: 8.094694, mean_eps: 0.100000\n","      24246/2000000000: episode: 698, duration: 4.587s, episode steps:  33, steps per second:   7, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 62.533291, mean_q: 9.033125, mean_eps: 0.100000\n","      24286/2000000000: episode: 699, duration: 5.670s, episode steps:  40, steps per second:   7, episode reward: -158.800, mean reward: -3.970 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.040985, mean_q: 8.571621, mean_eps: 0.100000\n","      24325/2000000000: episode: 700, duration: 5.353s, episode steps:  39, steps per second:   7, episode reward: 101.200, mean reward:  2.595 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 68.253656, mean_q: 8.564783, mean_eps: 0.100000\n","      24365/2000000000: episode: 701, duration: 5.286s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.625423, mean_q: 8.200054, mean_eps: 0.100000\n","      24402/2000000000: episode: 702, duration: 4.840s, episode steps:  37, steps per second:   8, episode reward: -23.500, mean reward: -0.635 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 72.079358, mean_q: 8.086107, mean_eps: 0.100000\n","      24442/2000000000: episode: 703, duration: 5.525s, episode steps:  40, steps per second:   7, episode reward: -119.600, mean reward: -2.990 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.889338, mean_q: 8.340362, mean_eps: 0.100000\n","      24482/2000000000: episode: 704, duration: 4.995s, episode steps:  40, steps per second:   8, episode reward: -27.900, mean reward: -0.697 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.928514, mean_q: 8.609305, mean_eps: 0.100000\n","      24517/2000000000: episode: 705, duration: 4.655s, episode steps:  35, steps per second:   8, episode reward: -34.000, mean reward: -0.971 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 65.054027, mean_q: 8.669009, mean_eps: 0.100000\n","      24556/2000000000: episode: 706, duration: 5.317s, episode steps:  39, steps per second:   7, episode reward: -14.200, mean reward: -0.364 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 64.872215, mean_q: 8.716040, mean_eps: 0.100000\n","      24594/2000000000: episode: 707, duration: 5.070s, episode steps:  38, steps per second:   7, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 70.546088, mean_q: 8.416293, mean_eps: 0.100000\n","      24634/2000000000: episode: 708, duration: 5.378s, episode steps:  40, steps per second:   7, episode reward: 13.500, mean reward:  0.337 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.232344, mean_q: 8.000842, mean_eps: 0.100000\n","      24674/2000000000: episode: 709, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 61.421810, mean_q: 7.915023, mean_eps: 0.100000\n","      24714/2000000000: episode: 710, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: -87.600, mean reward: -2.190 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.210575, mean_q: 8.313739, mean_eps: 0.100000\n","      24754/2000000000: episode: 711, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: -3.800, mean reward: -0.095 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.028539, mean_q: 8.879367, mean_eps: 0.100000\n","      24793/2000000000: episode: 712, duration: 5.077s, episode steps:  39, steps per second:   8, episode reward: -29.000, mean reward: -0.744 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 64.736171, mean_q: 9.024992, mean_eps: 0.100000\n","      24833/2000000000: episode: 713, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: 105.100, mean reward:  2.628 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.679315, mean_q: 8.698322, mean_eps: 0.100000\n","      24873/2000000000: episode: 714, duration: 5.297s, episode steps:  40, steps per second:   8, episode reward: 13.000, mean reward:  0.325 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.350966, mean_q: 8.442255, mean_eps: 0.100000\n","      24906/2000000000: episode: 715, duration: 4.550s, episode steps:  33, steps per second:   7, episode reward: -111.000, mean reward: -3.364 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 60.027625, mean_q: 9.879992, mean_eps: 0.100000\n","      24946/2000000000: episode: 716, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: 58.200, mean reward:  1.455 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.842602, mean_q: 8.308403, mean_eps: 0.100000\n","      24986/2000000000: episode: 717, duration: 4.694s, episode steps:  40, steps per second:   9, episode reward: 41.700, mean reward:  1.043 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.269586, mean_q: 8.323300, mean_eps: 0.100000\n","      25023/2000000000: episode: 718, duration: 4.224s, episode steps:  37, steps per second:   9, episode reward: 24.500, mean reward:  0.662 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 66.478595, mean_q: 8.051118, mean_eps: 0.100000\n","      25060/2000000000: episode: 719, duration: 4.269s, episode steps:  37, steps per second:   9, episode reward: -76.300, mean reward: -2.062 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 68.555133, mean_q: 8.262526, mean_eps: 0.100000\n","      25100/2000000000: episode: 720, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.716195, mean_q: 8.448868, mean_eps: 0.100000\n","      25140/2000000000: episode: 721, duration: 4.779s, episode steps:  40, steps per second:   8, episode reward: -164.200, mean reward: -4.105 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.381156, mean_q: 8.979568, mean_eps: 0.100000\n","      25180/2000000000: episode: 722, duration: 5.078s, episode steps:  40, steps per second:   8, episode reward: 31.400, mean reward:  0.785 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.355209, mean_q: 8.332566, mean_eps: 0.100000\n","      25220/2000000000: episode: 723, duration: 5.108s, episode steps:  40, steps per second:   8, episode reward: -19.900, mean reward: -0.498 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.535618, mean_q: 8.647401, mean_eps: 0.100000\n","      25260/2000000000: episode: 724, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: -43.000, mean reward: -1.075 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.107122, mean_q: 8.535484, mean_eps: 0.100000\n","      25300/2000000000: episode: 725, duration: 4.554s, episode steps:  40, steps per second:   9, episode reward: -41.100, mean reward: -1.028 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.364641, mean_q: 7.832125, mean_eps: 0.100000\n","      25334/2000000000: episode: 726, duration: 4.636s, episode steps:  34, steps per second:   7, episode reward: -222.000, mean reward: -6.529 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 71.215547, mean_q: 7.941125, mean_eps: 0.100000\n","      25374/2000000000: episode: 727, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: -43.400, mean reward: -1.085 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 64.711246, mean_q: 8.294668, mean_eps: 0.100000\n","      25414/2000000000: episode: 728, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: -56.300, mean reward: -1.407 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.438769, mean_q: 8.484822, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      25454/2000000000: episode: 729, duration: 5.317s, episode steps:  40, steps per second:   8, episode reward: -14.900, mean reward: -0.373 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 61.575082, mean_q: 8.932411, mean_eps: 0.100000\n","      25494/2000000000: episode: 730, duration: 5.670s, episode steps:  40, steps per second:   7, episode reward: 13.700, mean reward:  0.343 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 62.475401, mean_q: 8.425245, mean_eps: 0.100000\n","      25534/2000000000: episode: 731, duration: 5.464s, episode steps:  40, steps per second:   7, episode reward: -35.600, mean reward: -0.890 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.466562, mean_q: 8.328815, mean_eps: 0.100000\n","      25574/2000000000: episode: 732, duration: 5.508s, episode steps:  40, steps per second:   7, episode reward: -75.900, mean reward: -1.897 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.916147, mean_q: 8.348135, mean_eps: 0.100000\n","      25614/2000000000: episode: 733, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward: 49.900, mean reward:  1.248 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.942634, mean_q: 8.262795, mean_eps: 0.100000\n","      25654/2000000000: episode: 734, duration: 4.210s, episode steps:  40, steps per second:  10, episode reward: -43.800, mean reward: -1.095 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 64.986889, mean_q: 8.113052, mean_eps: 0.100000\n","      25694/2000000000: episode: 735, duration: 4.320s, episode steps:  40, steps per second:   9, episode reward: -15.300, mean reward: -0.382 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.015245, mean_q: 8.312291, mean_eps: 0.100000\n","      25733/2000000000: episode: 736, duration: 4.346s, episode steps:  39, steps per second:   9, episode reward: -154.000, mean reward: -3.949 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 66.738579, mean_q: 8.676880, mean_eps: 0.100000\n","      25773/2000000000: episode: 737, duration: 4.699s, episode steps:  40, steps per second:   9, episode reward: 29.200, mean reward:  0.730 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.765527, mean_q: 8.075757, mean_eps: 0.100000\n","      25809/2000000000: episode: 738, duration: 4.117s, episode steps:  36, steps per second:   9, episode reward: -23.800, mean reward: -0.661 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.919365, mean_q: 8.445858, mean_eps: 0.100000\n","      25845/2000000000: episode: 739, duration: 4.039s, episode steps:  36, steps per second:   9, episode reward: 23.800, mean reward:  0.661 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 67.738731, mean_q: 7.866140, mean_eps: 0.100000\n","      25885/2000000000: episode: 740, duration: 4.704s, episode steps:  40, steps per second:   9, episode reward: -22.100, mean reward: -0.552 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.568163, mean_q: 8.446297, mean_eps: 0.100000\n","      25925/2000000000: episode: 741, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: -122.900, mean reward: -3.072 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.277727, mean_q: 8.675583, mean_eps: 0.100000\n","      25965/2000000000: episode: 742, duration: 5.405s, episode steps:  40, steps per second:   7, episode reward: -47.200, mean reward: -1.180 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 64.176591, mean_q: 8.495514, mean_eps: 0.100000\n","      26005/2000000000: episode: 743, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: -55.300, mean reward: -1.382 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.183441, mean_q: 8.431253, mean_eps: 0.100000\n","      26045/2000000000: episode: 744, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: -12.000, mean reward: -0.300 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 64.187441, mean_q: 7.813278, mean_eps: 0.100000\n","      26085/2000000000: episode: 745, duration: 5.213s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.979695, mean_q: 8.654451, mean_eps: 0.100000\n","      26124/2000000000: episode: 746, duration: 5.024s, episode steps:  39, steps per second:   8, episode reward: -81.800, mean reward: -2.097 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 65.986035, mean_q: 8.112642, mean_eps: 0.100000\n","      26160/2000000000: episode: 747, duration: 4.501s, episode steps:  36, steps per second:   8, episode reward: -39.400, mean reward: -1.094 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 64.480287, mean_q: 8.795393, mean_eps: 0.100000\n","      26200/2000000000: episode: 748, duration: 5.149s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.762529, mean_q: 8.736040, mean_eps: 0.100000\n","      26234/2000000000: episode: 749, duration: 4.251s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 69.718031, mean_q: 8.407681, mean_eps: 0.100000\n","      26272/2000000000: episode: 750, duration: 5.238s, episode steps:  38, steps per second:   7, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 68.166610, mean_q: 7.998716, mean_eps: 0.100000\n","      26312/2000000000: episode: 751, duration: 5.479s, episode steps:  40, steps per second:   7, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.450956, mean_q: 8.275149, mean_eps: 0.100000\n","      26352/2000000000: episode: 752, duration: 5.103s, episode steps:  40, steps per second:   8, episode reward: -59.900, mean reward: -1.497 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.507090, mean_q: 8.548168, mean_eps: 0.100000\n","      26392/2000000000: episode: 753, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: -117.800, mean reward: -2.945 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 62.237566, mean_q: 8.710030, mean_eps: 0.100000\n","      26432/2000000000: episode: 754, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: -152.700, mean reward: -3.818 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.869344, mean_q: 8.130065, mean_eps: 0.100000\n","      26472/2000000000: episode: 755, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.084118, mean_q: 8.867841, mean_eps: 0.100000\n","      26512/2000000000: episode: 756, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.425702, mean_q: 8.599275, mean_eps: 0.100000\n","      26548/2000000000: episode: 757, duration: 4.506s, episode steps:  36, steps per second:   8, episode reward: 22.500, mean reward:  0.625 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 68.976602, mean_q: 8.274584, mean_eps: 0.100000\n","      26581/2000000000: episode: 758, duration: 4.164s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 71.129119, mean_q: 8.358450, mean_eps: 0.100000\n","      26621/2000000000: episode: 759, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.667097, mean_q: 8.446497, mean_eps: 0.100000\n","      26661/2000000000: episode: 760, duration: 5.002s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.438888, mean_q: 8.079221, mean_eps: 0.100000\n","      26693/2000000000: episode: 761, duration: 4.117s, episode steps:  32, steps per second:   8, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 69.728649, mean_q: 8.067075, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      26733/2000000000: episode: 762, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: -64.200, mean reward: -1.605 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.479261, mean_q: 8.325316, mean_eps: 0.100000\n","      26768/2000000000: episode: 763, duration: 4.805s, episode steps:  35, steps per second:   7, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 69.477794, mean_q: 8.849327, mean_eps: 0.100000\n","      26804/2000000000: episode: 764, duration: 5.194s, episode steps:  36, steps per second:   7, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 69.174977, mean_q: 7.869831, mean_eps: 0.100000\n","      26837/2000000000: episode: 765, duration: 4.315s, episode steps:  33, steps per second:   8, episode reward: -86.600, mean reward: -2.624 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 64.044930, mean_q: 8.535974, mean_eps: 0.100000\n","      26877/2000000000: episode: 766, duration: 5.247s, episode steps:  40, steps per second:   8, episode reward: -86.300, mean reward: -2.158 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.649030, mean_q: 8.064415, mean_eps: 0.100000\n","      26910/2000000000: episode: 767, duration: 4.257s, episode steps:  33, steps per second:   8, episode reward: -117.800, mean reward: -3.570 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 71.243773, mean_q: 8.148424, mean_eps: 0.100000\n","      26950/2000000000: episode: 768, duration: 5.444s, episode steps:  40, steps per second:   7, episode reward: 86.900, mean reward:  2.172 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 62.484742, mean_q: 8.442924, mean_eps: 0.100000\n","      26990/2000000000: episode: 769, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.007066, mean_q: 8.574420, mean_eps: 0.100000\n","      27030/2000000000: episode: 770, duration: 4.898s, episode steps:  40, steps per second:   8, episode reward: 123.200, mean reward:  3.080 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.429925, mean_q: 8.298450, mean_eps: 0.100000\n","      27066/2000000000: episode: 771, duration: 4.542s, episode steps:  36, steps per second:   8, episode reward: -2.300, mean reward: -0.064 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 69.079863, mean_q: 8.138421, mean_eps: 0.100000\n","      27106/2000000000: episode: 772, duration: 5.504s, episode steps:  40, steps per second:   7, episode reward: -13.200, mean reward: -0.330 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.791380, mean_q: 8.252888, mean_eps: 0.100000\n","      27136/2000000000: episode: 773, duration: 3.844s, episode steps:  30, steps per second:   8, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 64.445055, mean_q: 8.284818, mean_eps: 0.100000\n","      27176/2000000000: episode: 774, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: -38.700, mean reward: -0.967 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.038260, mean_q: 8.554627, mean_eps: 0.100000\n","      27212/2000000000: episode: 775, duration: 4.655s, episode steps:  36, steps per second:   8, episode reward: -21.800, mean reward: -0.606 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 66.770900, mean_q: 8.185211, mean_eps: 0.100000\n","      27252/2000000000: episode: 776, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: 46.100, mean reward:  1.152 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.656409, mean_q: 8.098332, mean_eps: 0.100000\n","      27292/2000000000: episode: 777, duration: 4.978s, episode steps:  40, steps per second:   8, episode reward: 34.400, mean reward:  0.860 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.646355, mean_q: 8.437201, mean_eps: 0.100000\n","      27331/2000000000: episode: 778, duration: 5.376s, episode steps:  39, steps per second:   7, episode reward: 56.500, mean reward:  1.449 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 66.946847, mean_q: 8.674257, mean_eps: 0.100000\n","      27371/2000000000: episode: 779, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward: -103.100, mean reward: -2.578 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.918572, mean_q: 8.390703, mean_eps: 0.100000\n","      27411/2000000000: episode: 780, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: -104.000, mean reward: -2.600 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.459022, mean_q: 8.104363, mean_eps: 0.100000\n","      27451/2000000000: episode: 781, duration: 5.486s, episode steps:  40, steps per second:   7, episode reward: 18.500, mean reward:  0.462 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.357522, mean_q: 8.172850, mean_eps: 0.100000\n","      27489/2000000000: episode: 782, duration: 5.245s, episode steps:  38, steps per second:   7, episode reward: -25.500, mean reward: -0.671 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 68.154404, mean_q: 8.923405, mean_eps: 0.100000\n","      27529/2000000000: episode: 783, duration: 5.402s, episode steps:  40, steps per second:   7, episode reward: -29.600, mean reward: -0.740 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.425101, mean_q: 8.278053, mean_eps: 0.100000\n","      27569/2000000000: episode: 784, duration: 5.426s, episode steps:  40, steps per second:   7, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.928062, mean_q: 8.289438, mean_eps: 0.100000\n","      27609/2000000000: episode: 785, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: -97.500, mean reward: -2.438 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.253259, mean_q: 8.164422, mean_eps: 0.100000\n","      27649/2000000000: episode: 786, duration: 5.392s, episode steps:  40, steps per second:   7, episode reward: -72.600, mean reward: -1.815 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.365457, mean_q: 8.533862, mean_eps: 0.100000\n","      27688/2000000000: episode: 787, duration: 4.852s, episode steps:  39, steps per second:   8, episode reward: -172.000, mean reward: -4.410 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 63.464816, mean_q: 8.369366, mean_eps: 0.100000\n","      27727/2000000000: episode: 788, duration: 5.082s, episode steps:  39, steps per second:   8, episode reward: 25.500, mean reward:  0.654 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 62.744328, mean_q: 8.514809, mean_eps: 0.100000\n","      27767/2000000000: episode: 789, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: -77.100, mean reward: -1.927 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.197983, mean_q: 8.035082, mean_eps: 0.100000\n","      27804/2000000000: episode: 790, duration: 5.230s, episode steps:  37, steps per second:   7, episode reward: 47.700, mean reward:  1.289 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 68.154090, mean_q: 8.647436, mean_eps: 0.100000\n","      27844/2000000000: episode: 791, duration: 5.510s, episode steps:  40, steps per second:   7, episode reward: 12.300, mean reward:  0.307 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.449308, mean_q: 8.109729, mean_eps: 0.100000\n","      27884/2000000000: episode: 792, duration: 5.897s, episode steps:  40, steps per second:   7, episode reward: 55.100, mean reward:  1.378 [-20.000, 19.900], mean action: 1.525 [0.000, 2.000],  loss: 62.739965, mean_q: 8.185396, mean_eps: 0.100000\n","      27924/2000000000: episode: 793, duration: 5.531s, episode steps:  40, steps per second:   7, episode reward: 47.200, mean reward:  1.180 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.000179, mean_q: 8.121583, mean_eps: 0.100000\n","      27958/2000000000: episode: 794, duration: 4.634s, episode steps:  34, steps per second:   7, episode reward: -74.200, mean reward: -2.182 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 63.797691, mean_q: 8.567816, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      27997/2000000000: episode: 795, duration: 5.225s, episode steps:  39, steps per second:   7, episode reward: -49.800, mean reward: -1.277 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 63.083879, mean_q: 8.665232, mean_eps: 0.100000\n","      28036/2000000000: episode: 796, duration: 5.216s, episode steps:  39, steps per second:   7, episode reward: -128.600, mean reward: -3.297 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 62.878978, mean_q: 8.574161, mean_eps: 0.100000\n","      28076/2000000000: episode: 797, duration: 5.346s, episode steps:  40, steps per second:   7, episode reward: -29.900, mean reward: -0.747 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 60.794261, mean_q: 8.264672, mean_eps: 0.100000\n","      28110/2000000000: episode: 798, duration: 4.577s, episode steps:  34, steps per second:   7, episode reward: -143.200, mean reward: -4.212 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 62.655604, mean_q: 7.950474, mean_eps: 0.100000\n","      28147/2000000000: episode: 799, duration: 4.706s, episode steps:  37, steps per second:   8, episode reward: -66.700, mean reward: -1.803 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 67.191282, mean_q: 8.571574, mean_eps: 0.100000\n","      28182/2000000000: episode: 800, duration: 4.508s, episode steps:  35, steps per second:   8, episode reward: 29.600, mean reward:  0.846 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 64.474402, mean_q: 8.110863, mean_eps: 0.100000\n","      28222/2000000000: episode: 801, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.230530, mean_q: 8.394850, mean_eps: 0.100000\n","      28262/2000000000: episode: 802, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 76.955725, mean_q: 8.104367, mean_eps: 0.100000\n","      28302/2000000000: episode: 803, duration: 5.204s, episode steps:  40, steps per second:   8, episode reward: -80.400, mean reward: -2.010 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.478922, mean_q: 8.718073, mean_eps: 0.100000\n","      28342/2000000000: episode: 804, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: 54.400, mean reward:  1.360 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.660026, mean_q: 8.615667, mean_eps: 0.100000\n","      28382/2000000000: episode: 805, duration: 5.358s, episode steps:  40, steps per second:   7, episode reward:  9.000, mean reward:  0.225 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.311072, mean_q: 8.350793, mean_eps: 0.100000\n","      28422/2000000000: episode: 806, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward: 99.700, mean reward:  2.493 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.206986, mean_q: 8.361286, mean_eps: 0.100000\n","      28461/2000000000: episode: 807, duration: 5.344s, episode steps:  39, steps per second:   7, episode reward: -96.100, mean reward: -2.464 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 62.721910, mean_q: 8.321128, mean_eps: 0.100000\n","      28501/2000000000: episode: 808, duration: 5.436s, episode steps:  40, steps per second:   7, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 59.835618, mean_q: 8.899286, mean_eps: 0.100000\n","      28535/2000000000: episode: 809, duration: 5.007s, episode steps:  34, steps per second:   7, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 68.481590, mean_q: 8.590097, mean_eps: 0.100000\n","      28566/2000000000: episode: 810, duration: 4.064s, episode steps:  31, steps per second:   8, episode reward: 45.300, mean reward:  1.461 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 61.741409, mean_q: 8.390978, mean_eps: 0.100000\n","      28606/2000000000: episode: 811, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: -61.900, mean reward: -1.547 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.460671, mean_q: 8.251620, mean_eps: 0.100000\n","      28646/2000000000: episode: 812, duration: 5.441s, episode steps:  40, steps per second:   7, episode reward: -63.700, mean reward: -1.593 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.476269, mean_q: 8.935089, mean_eps: 0.100000\n","      28686/2000000000: episode: 813, duration: 5.697s, episode steps:  40, steps per second:   7, episode reward:  4.300, mean reward:  0.107 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 68.941729, mean_q: 7.932432, mean_eps: 0.100000\n","      28726/2000000000: episode: 814, duration: 5.255s, episode steps:  40, steps per second:   8, episode reward: 87.700, mean reward:  2.192 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.763319, mean_q: 8.714616, mean_eps: 0.100000\n","      28762/2000000000: episode: 815, duration: 4.655s, episode steps:  36, steps per second:   8, episode reward: 88.900, mean reward:  2.469 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 61.721389, mean_q: 8.720580, mean_eps: 0.100000\n","      28799/2000000000: episode: 816, duration: 4.641s, episode steps:  37, steps per second:   8, episode reward: -154.000, mean reward: -4.162 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 69.406539, mean_q: 8.812178, mean_eps: 0.100000\n","      28839/2000000000: episode: 817, duration: 5.396s, episode steps:  40, steps per second:   7, episode reward: -147.100, mean reward: -3.677 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 61.489065, mean_q: 8.859769, mean_eps: 0.100000\n","      28879/2000000000: episode: 818, duration: 5.302s, episode steps:  40, steps per second:   8, episode reward: -50.000, mean reward: -1.250 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 65.342466, mean_q: 8.091303, mean_eps: 0.100000\n","      28919/2000000000: episode: 819, duration: 5.207s, episode steps:  40, steps per second:   8, episode reward: -37.000, mean reward: -0.925 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.249918, mean_q: 8.418064, mean_eps: 0.100000\n","      28959/2000000000: episode: 820, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: -8.500, mean reward: -0.212 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.511260, mean_q: 7.954133, mean_eps: 0.100000\n","      28999/2000000000: episode: 821, duration: 5.791s, episode steps:  40, steps per second:   7, episode reward: -59.300, mean reward: -1.483 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.533861, mean_q: 8.281378, mean_eps: 0.100000\n","      29039/2000000000: episode: 822, duration: 5.648s, episode steps:  40, steps per second:   7, episode reward: -163.600, mean reward: -4.090 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.072283, mean_q: 8.534050, mean_eps: 0.100000\n","      29079/2000000000: episode: 823, duration: 5.613s, episode steps:  40, steps per second:   7, episode reward: 129.000, mean reward:  3.225 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.375938, mean_q: 8.659797, mean_eps: 0.100000\n","      29119/2000000000: episode: 824, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.550882, mean_q: 8.501010, mean_eps: 0.100000\n","      29154/2000000000: episode: 825, duration: 4.781s, episode steps:  35, steps per second:   7, episode reward: 71.800, mean reward:  2.051 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 66.862834, mean_q: 8.205330, mean_eps: 0.100000\n","      29191/2000000000: episode: 826, duration: 4.954s, episode steps:  37, steps per second:   7, episode reward: 33.700, mean reward:  0.911 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 67.351898, mean_q: 7.679316, mean_eps: 0.100000\n","      29231/2000000000: episode: 827, duration: 5.386s, episode steps:  40, steps per second:   7, episode reward: 24.400, mean reward:  0.610 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.891298, mean_q: 9.075135, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      29263/2000000000: episode: 828, duration: 4.234s, episode steps:  32, steps per second:   8, episode reward: -39.200, mean reward: -1.225 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 64.173315, mean_q: 8.591198, mean_eps: 0.100000\n","      29303/2000000000: episode: 829, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: -19.400, mean reward: -0.485 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 62.654076, mean_q: 8.685927, mean_eps: 0.100000\n","      29339/2000000000: episode: 830, duration: 4.862s, episode steps:  36, steps per second:   7, episode reward: 37.300, mean reward:  1.036 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 66.728426, mean_q: 8.205326, mean_eps: 0.100000\n","      29379/2000000000: episode: 831, duration: 5.266s, episode steps:  40, steps per second:   8, episode reward: 81.600, mean reward:  2.040 [-11.400, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.884655, mean_q: 8.277436, mean_eps: 0.100000\n","      29408/2000000000: episode: 832, duration: 3.809s, episode steps:  29, steps per second:   8, episode reward: 98.800, mean reward:  3.407 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 64.389437, mean_q: 8.734351, mean_eps: 0.100000\n","      29448/2000000000: episode: 833, duration: 5.394s, episode steps:  40, steps per second:   7, episode reward:  9.900, mean reward:  0.247 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 65.503315, mean_q: 8.300120, mean_eps: 0.100000\n","      29488/2000000000: episode: 834, duration: 5.121s, episode steps:  40, steps per second:   8, episode reward: 57.500, mean reward:  1.438 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.892523, mean_q: 8.059850, mean_eps: 0.100000\n","      29520/2000000000: episode: 835, duration: 4.420s, episode steps:  32, steps per second:   7, episode reward: 28.000, mean reward:  0.875 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.918660, mean_q: 8.092172, mean_eps: 0.100000\n","      29560/2000000000: episode: 836, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: 48.500, mean reward:  1.213 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.447089, mean_q: 8.267980, mean_eps: 0.100000\n","      29600/2000000000: episode: 837, duration: 5.479s, episode steps:  40, steps per second:   7, episode reward: -36.100, mean reward: -0.902 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.724802, mean_q: 8.403337, mean_eps: 0.100000\n","      29634/2000000000: episode: 838, duration: 4.567s, episode steps:  34, steps per second:   7, episode reward: 12.600, mean reward:  0.371 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 66.734023, mean_q: 9.386993, mean_eps: 0.100000\n","      29668/2000000000: episode: 839, duration: 4.938s, episode steps:  34, steps per second:   7, episode reward: 26.400, mean reward:  0.776 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 57.226343, mean_q: 8.814731, mean_eps: 0.100000\n","      29708/2000000000: episode: 840, duration: 5.812s, episode steps:  40, steps per second:   7, episode reward: -2.400, mean reward: -0.060 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.208804, mean_q: 8.559652, mean_eps: 0.100000\n","      29744/2000000000: episode: 841, duration: 5.155s, episode steps:  36, steps per second:   7, episode reward: 125.900, mean reward:  3.497 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 57.366234, mean_q: 7.697181, mean_eps: 0.100000\n","      29784/2000000000: episode: 842, duration: 5.351s, episode steps:  40, steps per second:   7, episode reward: 62.600, mean reward:  1.565 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.733718, mean_q: 8.819798, mean_eps: 0.100000\n","      29821/2000000000: episode: 843, duration: 4.947s, episode steps:  37, steps per second:   7, episode reward: 51.700, mean reward:  1.397 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 64.629513, mean_q: 7.999794, mean_eps: 0.100000\n","      29861/2000000000: episode: 844, duration: 5.467s, episode steps:  40, steps per second:   7, episode reward:  4.900, mean reward:  0.122 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 62.070630, mean_q: 8.118883, mean_eps: 0.100000\n","      29894/2000000000: episode: 845, duration: 4.406s, episode steps:  33, steps per second:   7, episode reward: -132.700, mean reward: -4.021 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 64.424481, mean_q: 8.899666, mean_eps: 0.100000\n","      29934/2000000000: episode: 846, duration: 5.457s, episode steps:  40, steps per second:   7, episode reward: 111.500, mean reward:  2.787 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.777901, mean_q: 8.259057, mean_eps: 0.100000\n","      29974/2000000000: episode: 847, duration: 4.941s, episode steps:  40, steps per second:   8, episode reward: -139.000, mean reward: -3.475 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 64.905848, mean_q: 8.472604, mean_eps: 0.100000\n","      30011/2000000000: episode: 848, duration: 4.281s, episode steps:  37, steps per second:   9, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 69.556826, mean_q: 8.626935, mean_eps: 0.100000\n","      30049/2000000000: episode: 849, duration: 4.471s, episode steps:  38, steps per second:   8, episode reward: -42.000, mean reward: -1.105 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 82.164002, mean_q: 11.732533, mean_eps: 0.100000\n","      30089/2000000000: episode: 850, duration: 4.622s, episode steps:  40, steps per second:   9, episode reward: 16.900, mean reward:  0.423 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.880029, mean_q: 11.568672, mean_eps: 0.100000\n","      30129/2000000000: episode: 851, duration: 4.816s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 79.460203, mean_q: 11.599335, mean_eps: 0.100000\n","      30169/2000000000: episode: 852, duration: 4.752s, episode steps:  40, steps per second:   8, episode reward: 95.900, mean reward:  2.398 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.545832, mean_q: 11.883938, mean_eps: 0.100000\n","      30208/2000000000: episode: 853, duration: 4.787s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 75.535012, mean_q: 10.703722, mean_eps: 0.100000\n","      30248/2000000000: episode: 854, duration: 4.643s, episode steps:  40, steps per second:   9, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 74.797585, mean_q: 11.993819, mean_eps: 0.100000\n","      30288/2000000000: episode: 855, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: 19.700, mean reward:  0.493 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.098410, mean_q: 11.030796, mean_eps: 0.100000\n","      30317/2000000000: episode: 856, duration: 3.254s, episode steps:  29, steps per second:   9, episode reward: 83.100, mean reward:  2.866 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 75.202994, mean_q: 12.054013, mean_eps: 0.100000\n","      30352/2000000000: episode: 857, duration: 4.023s, episode steps:  35, steps per second:   9, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 80.648978, mean_q: 11.925431, mean_eps: 0.100000\n","      30392/2000000000: episode: 858, duration: 4.899s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.437986, mean_q: 11.698590, mean_eps: 0.100000\n","      30429/2000000000: episode: 859, duration: 4.393s, episode steps:  37, steps per second:   8, episode reward: 10.100, mean reward:  0.273 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.529813, mean_q: 11.882940, mean_eps: 0.100000\n","      30469/2000000000: episode: 860, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: 192.000, mean reward:  4.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.013118, mean_q: 11.704293, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      30509/2000000000: episode: 861, duration: 4.744s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 71.581686, mean_q: 11.823887, mean_eps: 0.100000\n","      30549/2000000000: episode: 862, duration: 4.769s, episode steps:  40, steps per second:   8, episode reward: 64.900, mean reward:  1.623 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.902787, mean_q: 11.640695, mean_eps: 0.100000\n","      30589/2000000000: episode: 863, duration: 4.706s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.687862, mean_q: 11.452506, mean_eps: 0.100000\n","      30629/2000000000: episode: 864, duration: 4.567s, episode steps:  40, steps per second:   9, episode reward: -21.900, mean reward: -0.548 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.808590, mean_q: 11.618478, mean_eps: 0.100000\n","      30665/2000000000: episode: 865, duration: 4.387s, episode steps:  36, steps per second:   8, episode reward: -108.800, mean reward: -3.022 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 76.796418, mean_q: 11.717624, mean_eps: 0.100000\n","      30705/2000000000: episode: 866, duration: 4.926s, episode steps:  40, steps per second:   8, episode reward:  6.300, mean reward:  0.157 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.987419, mean_q: 11.399008, mean_eps: 0.100000\n","      30745/2000000000: episode: 867, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: -108.000, mean reward: -2.700 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.294607, mean_q: 11.701739, mean_eps: 0.100000\n","      30785/2000000000: episode: 868, duration: 4.847s, episode steps:  40, steps per second:   8, episode reward:  3.700, mean reward:  0.092 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.762454, mean_q: 11.841443, mean_eps: 0.100000\n","      30821/2000000000: episode: 869, duration: 4.262s, episode steps:  36, steps per second:   8, episode reward: 42.900, mean reward:  1.192 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 70.506558, mean_q: 11.866148, mean_eps: 0.100000\n","      30861/2000000000: episode: 870, duration: 4.652s, episode steps:  40, steps per second:   9, episode reward:  0.700, mean reward:  0.017 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.057735, mean_q: 11.859422, mean_eps: 0.100000\n","      30900/2000000000: episode: 871, duration: 4.365s, episode steps:  39, steps per second:   9, episode reward: -6.500, mean reward: -0.167 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 73.319810, mean_q: 11.628249, mean_eps: 0.100000\n","      30940/2000000000: episode: 872, duration: 4.696s, episode steps:  40, steps per second:   9, episode reward: -30.800, mean reward: -0.770 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.540875, mean_q: 11.519990, mean_eps: 0.100000\n","      30980/2000000000: episode: 873, duration: 4.675s, episode steps:  40, steps per second:   9, episode reward: 14.500, mean reward:  0.363 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 71.862615, mean_q: 12.100820, mean_eps: 0.100000\n","      31020/2000000000: episode: 874, duration: 4.655s, episode steps:  40, steps per second:   9, episode reward: -107.500, mean reward: -2.687 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.899192, mean_q: 11.683655, mean_eps: 0.100000\n","      31058/2000000000: episode: 875, duration: 4.587s, episode steps:  38, steps per second:   8, episode reward: -71.400, mean reward: -1.879 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 70.194379, mean_q: 11.797296, mean_eps: 0.100000\n","      31094/2000000000: episode: 876, duration: 4.353s, episode steps:  36, steps per second:   8, episode reward: 51.800, mean reward:  1.439 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 61.830872, mean_q: 12.016825, mean_eps: 0.100000\n","      31128/2000000000: episode: 877, duration: 4.057s, episode steps:  34, steps per second:   8, episode reward: -9.200, mean reward: -0.271 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 68.426229, mean_q: 11.823612, mean_eps: 0.100000\n","      31164/2000000000: episode: 878, duration: 4.145s, episode steps:  36, steps per second:   9, episode reward: 16.900, mean reward:  0.469 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 71.204236, mean_q: 11.539077, mean_eps: 0.100000\n","      31201/2000000000: episode: 879, duration: 4.331s, episode steps:  37, steps per second:   9, episode reward: -107.100, mean reward: -2.895 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 58.144417, mean_q: 11.707638, mean_eps: 0.100000\n","      31232/2000000000: episode: 880, duration: 3.853s, episode steps:  31, steps per second:   8, episode reward: 10.300, mean reward:  0.332 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 69.103663, mean_q: 12.439466, mean_eps: 0.100000\n","      31270/2000000000: episode: 881, duration: 4.650s, episode steps:  38, steps per second:   8, episode reward: 64.100, mean reward:  1.687 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 67.719933, mean_q: 12.073555, mean_eps: 0.100000\n","      31310/2000000000: episode: 882, duration: 4.767s, episode steps:  40, steps per second:   8, episode reward: 24.200, mean reward:  0.605 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 66.152787, mean_q: 11.714945, mean_eps: 0.100000\n","      31350/2000000000: episode: 883, duration: 5.372s, episode steps:  40, steps per second:   7, episode reward: -185.500, mean reward: -4.638 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 81.116741, mean_q: 12.225835, mean_eps: 0.100000\n","      31382/2000000000: episode: 884, duration: 3.857s, episode steps:  32, steps per second:   8, episode reward: 133.000, mean reward:  4.156 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 68.274568, mean_q: 12.934676, mean_eps: 0.100000\n","      31420/2000000000: episode: 885, duration: 4.879s, episode steps:  38, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 64.914093, mean_q: 11.394447, mean_eps: 0.100000\n","      31454/2000000000: episode: 886, duration: 4.303s, episode steps:  34, steps per second:   8, episode reward: 44.900, mean reward:  1.321 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 70.290685, mean_q: 11.911237, mean_eps: 0.100000\n","      31494/2000000000: episode: 887, duration: 4.662s, episode steps:  40, steps per second:   9, episode reward:  1.800, mean reward:  0.045 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 68.083919, mean_q: 11.096355, mean_eps: 0.100000\n","      31534/2000000000: episode: 888, duration: 5.234s, episode steps:  40, steps per second:   8, episode reward: -80.900, mean reward: -2.023 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.421585, mean_q: 11.371190, mean_eps: 0.100000\n","      31574/2000000000: episode: 889, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.907552, mean_q: 12.066798, mean_eps: 0.100000\n","      31614/2000000000: episode: 890, duration: 5.232s, episode steps:  40, steps per second:   8, episode reward: 68.400, mean reward:  1.710 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.204971, mean_q: 12.027807, mean_eps: 0.100000\n","      31654/2000000000: episode: 891, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward:  7.300, mean reward:  0.182 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.840470, mean_q: 11.257715, mean_eps: 0.100000\n","      31693/2000000000: episode: 892, duration: 5.534s, episode steps:  39, steps per second:   7, episode reward: 73.300, mean reward:  1.879 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 66.018725, mean_q: 11.040867, mean_eps: 0.100000\n","      31733/2000000000: episode: 893, duration: 5.891s, episode steps:  40, steps per second:   7, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.141903, mean_q: 11.957365, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      31768/2000000000: episode: 894, duration: 4.805s, episode steps:  35, steps per second:   7, episode reward: -37.100, mean reward: -1.060 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 71.012559, mean_q: 11.977555, mean_eps: 0.100000\n","      31806/2000000000: episode: 895, duration: 5.286s, episode steps:  38, steps per second:   7, episode reward: 65.300, mean reward:  1.718 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 72.210487, mean_q: 11.854925, mean_eps: 0.100000\n","      31841/2000000000: episode: 896, duration: 4.833s, episode steps:  35, steps per second:   7, episode reward: -186.700, mean reward: -5.334 [-20.000, 19.400], mean action: 1.143 [0.000, 2.000],  loss: 67.846952, mean_q: 11.265210, mean_eps: 0.100000\n","      31873/2000000000: episode: 897, duration: 4.068s, episode steps:  32, steps per second:   8, episode reward: -101.200, mean reward: -3.162 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 67.227555, mean_q: 12.075488, mean_eps: 0.100000\n","      31908/2000000000: episode: 898, duration: 4.889s, episode steps:  35, steps per second:   7, episode reward:  6.200, mean reward:  0.177 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 64.851683, mean_q: 11.529009, mean_eps: 0.100000\n","      31948/2000000000: episode: 899, duration: 5.696s, episode steps:  40, steps per second:   7, episode reward: -15.700, mean reward: -0.393 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.880387, mean_q: 11.651889, mean_eps: 0.100000\n","      31988/2000000000: episode: 900, duration: 5.866s, episode steps:  40, steps per second:   7, episode reward: -8.400, mean reward: -0.210 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.078070, mean_q: 11.474446, mean_eps: 0.100000\n","      32022/2000000000: episode: 901, duration: 4.802s, episode steps:  34, steps per second:   7, episode reward: 20.600, mean reward:  0.606 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 70.242170, mean_q: 12.850549, mean_eps: 0.100000\n","      32062/2000000000: episode: 902, duration: 5.454s, episode steps:  40, steps per second:   7, episode reward: -116.500, mean reward: -2.913 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.841114, mean_q: 11.699722, mean_eps: 0.100000\n","      32102/2000000000: episode: 903, duration: 5.390s, episode steps:  40, steps per second:   7, episode reward: -24.700, mean reward: -0.617 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.739932, mean_q: 12.216565, mean_eps: 0.100000\n","      32142/2000000000: episode: 904, duration: 4.790s, episode steps:  40, steps per second:   8, episode reward: -3.200, mean reward: -0.080 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.443632, mean_q: 11.656109, mean_eps: 0.100000\n","      32177/2000000000: episode: 905, duration: 4.283s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 62.315027, mean_q: 11.553529, mean_eps: 0.100000\n","      32217/2000000000: episode: 906, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: -11.200, mean reward: -0.280 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.450713, mean_q: 11.793229, mean_eps: 0.100000\n","      32254/2000000000: episode: 907, duration: 4.025s, episode steps:  37, steps per second:   9, episode reward: -101.800, mean reward: -2.751 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 73.726717, mean_q: 11.972840, mean_eps: 0.100000\n","      32294/2000000000: episode: 908, duration: 4.701s, episode steps:  40, steps per second:   9, episode reward: -100.200, mean reward: -2.505 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.586716, mean_q: 12.279823, mean_eps: 0.100000\n","      32331/2000000000: episode: 909, duration: 4.374s, episode steps:  37, steps per second:   8, episode reward: 110.600, mean reward:  2.989 [-20.000, 18.000], mean action: 1.351 [0.000, 2.000],  loss: 74.903742, mean_q: 12.040600, mean_eps: 0.100000\n","      32371/2000000000: episode: 910, duration: 4.601s, episode steps:  40, steps per second:   9, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.690496, mean_q: 12.168779, mean_eps: 0.100000\n","      32411/2000000000: episode: 911, duration: 4.704s, episode steps:  40, steps per second:   9, episode reward: 20.700, mean reward:  0.518 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.254565, mean_q: 11.749964, mean_eps: 0.100000\n","      32451/2000000000: episode: 912, duration: 4.898s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.565071, mean_q: 11.723897, mean_eps: 0.100000\n","      32491/2000000000: episode: 913, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.774907, mean_q: 11.674912, mean_eps: 0.100000\n","      32531/2000000000: episode: 914, duration: 4.788s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.889207, mean_q: 12.036800, mean_eps: 0.100000\n","      32569/2000000000: episode: 915, duration: 4.585s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 62.840645, mean_q: 12.033114, mean_eps: 0.100000\n","      32609/2000000000: episode: 916, duration: 4.806s, episode steps:  40, steps per second:   8, episode reward: 28.700, mean reward:  0.717 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.812535, mean_q: 11.817815, mean_eps: 0.100000\n","      32648/2000000000: episode: 917, duration: 4.620s, episode steps:  39, steps per second:   8, episode reward: -72.800, mean reward: -1.867 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 59.485321, mean_q: 11.558645, mean_eps: 0.100000\n","      32688/2000000000: episode: 918, duration: 5.141s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.332820, mean_q: 11.646768, mean_eps: 0.100000\n","      32728/2000000000: episode: 919, duration: 5.531s, episode steps:  40, steps per second:   7, episode reward: -119.400, mean reward: -2.985 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.058830, mean_q: 11.715532, mean_eps: 0.100000\n","      32768/2000000000: episode: 920, duration: 5.160s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.551205, mean_q: 11.212052, mean_eps: 0.100000\n","      32808/2000000000: episode: 921, duration: 5.514s, episode steps:  40, steps per second:   7, episode reward: -174.900, mean reward: -4.373 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.381342, mean_q: 11.703990, mean_eps: 0.100000\n","      32848/2000000000: episode: 922, duration: 5.741s, episode steps:  40, steps per second:   7, episode reward: -92.900, mean reward: -2.322 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.581988, mean_q: 11.955475, mean_eps: 0.100000\n","      32882/2000000000: episode: 923, duration: 4.586s, episode steps:  34, steps per second:   7, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 62.318876, mean_q: 11.539381, mean_eps: 0.100000\n","      32922/2000000000: episode: 924, duration: 5.792s, episode steps:  40, steps per second:   7, episode reward: 62.200, mean reward:  1.555 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.624114, mean_q: 11.551281, mean_eps: 0.100000\n","      32960/2000000000: episode: 925, duration: 5.101s, episode steps:  38, steps per second:   7, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 65.390717, mean_q: 11.509294, mean_eps: 0.100000\n","      32999/2000000000: episode: 926, duration: 5.372s, episode steps:  39, steps per second:   7, episode reward: -14.800, mean reward: -0.379 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 63.349680, mean_q: 11.920913, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      33039/2000000000: episode: 927, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: 63.000, mean reward:  1.575 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.872647, mean_q: 12.379322, mean_eps: 0.100000\n","      33079/2000000000: episode: 928, duration: 5.257s, episode steps:  40, steps per second:   8, episode reward: -35.000, mean reward: -0.875 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 67.352842, mean_q: 11.452358, mean_eps: 0.100000\n","      33119/2000000000: episode: 929, duration: 5.041s, episode steps:  40, steps per second:   8, episode reward: -0.900, mean reward: -0.023 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 70.832533, mean_q: 11.170558, mean_eps: 0.100000\n","      33150/2000000000: episode: 930, duration: 4.137s, episode steps:  31, steps per second:   7, episode reward: 83.000, mean reward:  2.677 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 70.017685, mean_q: 12.070335, mean_eps: 0.100000\n","      33190/2000000000: episode: 931, duration: 4.982s, episode steps:  40, steps per second:   8, episode reward: -1.200, mean reward: -0.030 [-20.000, 19.200], mean action: 1.275 [0.000, 2.000],  loss: 71.626699, mean_q: 11.656740, mean_eps: 0.100000\n","      33230/2000000000: episode: 932, duration: 5.296s, episode steps:  40, steps per second:   8, episode reward: 159.600, mean reward:  3.990 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.819747, mean_q: 11.717362, mean_eps: 0.100000\n","      33270/2000000000: episode: 933, duration: 5.025s, episode steps:  40, steps per second:   8, episode reward: 89.700, mean reward:  2.243 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 59.467216, mean_q: 11.105114, mean_eps: 0.100000\n","      33309/2000000000: episode: 934, duration: 4.783s, episode steps:  39, steps per second:   8, episode reward: -82.700, mean reward: -2.121 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 70.653882, mean_q: 11.698432, mean_eps: 0.100000\n","      33349/2000000000: episode: 935, duration: 5.161s, episode steps:  40, steps per second:   8, episode reward: -22.000, mean reward: -0.550 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.250558, mean_q: 11.704886, mean_eps: 0.100000\n","      33389/2000000000: episode: 936, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: -15.400, mean reward: -0.385 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.036001, mean_q: 11.907424, mean_eps: 0.100000\n","      33429/2000000000: episode: 937, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: -92.900, mean reward: -2.323 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.519879, mean_q: 11.599136, mean_eps: 0.100000\n","      33469/2000000000: episode: 938, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: 111.500, mean reward:  2.787 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.247984, mean_q: 11.696613, mean_eps: 0.100000\n","      33509/2000000000: episode: 939, duration: 5.158s, episode steps:  40, steps per second:   8, episode reward:  1.600, mean reward:  0.040 [-20.000, 18.700], mean action: 1.375 [0.000, 2.000],  loss: 66.682694, mean_q: 11.200602, mean_eps: 0.100000\n","      33548/2000000000: episode: 940, duration: 4.951s, episode steps:  39, steps per second:   8, episode reward: -80.700, mean reward: -2.069 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 68.837944, mean_q: 11.501069, mean_eps: 0.100000\n","      33588/2000000000: episode: 941, duration: 4.983s, episode steps:  40, steps per second:   8, episode reward: 66.500, mean reward:  1.662 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.096156, mean_q: 11.809955, mean_eps: 0.100000\n","      33628/2000000000: episode: 942, duration: 5.127s, episode steps:  40, steps per second:   8, episode reward: -60.400, mean reward: -1.510 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.147421, mean_q: 11.724889, mean_eps: 0.100000\n","      33668/2000000000: episode: 943, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: 32.900, mean reward:  0.823 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.330155, mean_q: 11.790976, mean_eps: 0.100000\n","      33708/2000000000: episode: 944, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: -25.900, mean reward: -0.648 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.487871, mean_q: 11.766099, mean_eps: 0.100000\n","      33748/2000000000: episode: 945, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: 21.200, mean reward:  0.530 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.782157, mean_q: 11.848383, mean_eps: 0.100000\n","      33786/2000000000: episode: 946, duration: 4.822s, episode steps:  38, steps per second:   8, episode reward: -9.000, mean reward: -0.237 [-20.000, 18.000], mean action: 1.053 [0.000, 2.000],  loss: 67.673264, mean_q: 12.186896, mean_eps: 0.100000\n","      33826/2000000000: episode: 947, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: 25.300, mean reward:  0.633 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.448653, mean_q: 11.379655, mean_eps: 0.100000\n","      33866/2000000000: episode: 948, duration: 5.015s, episode steps:  40, steps per second:   8, episode reward: 40.200, mean reward:  1.005 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.176711, mean_q: 12.049161, mean_eps: 0.100000\n","      33906/2000000000: episode: 949, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.432656, mean_q: 11.286127, mean_eps: 0.100000\n","      33946/2000000000: episode: 950, duration: 4.928s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.400850, mean_q: 11.266043, mean_eps: 0.100000\n","      33986/2000000000: episode: 951, duration: 5.173s, episode steps:  40, steps per second:   8, episode reward: -96.100, mean reward: -2.403 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.383277, mean_q: 11.797871, mean_eps: 0.100000\n","      34022/2000000000: episode: 952, duration: 4.651s, episode steps:  36, steps per second:   8, episode reward: -84.500, mean reward: -2.347 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 66.197852, mean_q: 12.416253, mean_eps: 0.100000\n","      34062/2000000000: episode: 953, duration: 5.149s, episode steps:  40, steps per second:   8, episode reward: -104.900, mean reward: -2.622 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 64.562158, mean_q: 12.426267, mean_eps: 0.100000\n","      34099/2000000000: episode: 954, duration: 4.837s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 65.760350, mean_q: 11.602847, mean_eps: 0.100000\n","      34139/2000000000: episode: 955, duration: 5.348s, episode steps:  40, steps per second:   7, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.577262, mean_q: 11.210218, mean_eps: 0.100000\n","      34175/2000000000: episode: 956, duration: 5.034s, episode steps:  36, steps per second:   7, episode reward: -77.600, mean reward: -2.156 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 72.550093, mean_q: 12.241265, mean_eps: 0.100000\n","      34202/2000000000: episode: 957, duration: 3.693s, episode steps:  27, steps per second:   7, episode reward: -42.400, mean reward: -1.570 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 64.678642, mean_q: 12.181476, mean_eps: 0.100000\n","      34241/2000000000: episode: 958, duration: 5.245s, episode steps:  39, steps per second:   7, episode reward: -185.300, mean reward: -4.751 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 69.163817, mean_q: 12.341584, mean_eps: 0.100000\n","      34279/2000000000: episode: 959, duration: 5.129s, episode steps:  38, steps per second:   7, episode reward: -172.000, mean reward: -4.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 65.086097, mean_q: 12.060707, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      34319/2000000000: episode: 960, duration: 5.348s, episode steps:  40, steps per second:   7, episode reward: -83.400, mean reward: -2.085 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.417008, mean_q: 11.602551, mean_eps: 0.100000\n","      34359/2000000000: episode: 961, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: -30.000, mean reward: -0.750 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 61.178404, mean_q: 12.047023, mean_eps: 0.100000\n","      34395/2000000000: episode: 962, duration: 4.459s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 60.142586, mean_q: 10.677359, mean_eps: 0.100000\n","      34435/2000000000: episode: 963, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: -161.800, mean reward: -4.045 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.505193, mean_q: 11.466934, mean_eps: 0.100000\n","      34475/2000000000: episode: 964, duration: 5.703s, episode steps:  40, steps per second:   7, episode reward: -100.300, mean reward: -2.508 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.970975, mean_q: 12.048619, mean_eps: 0.100000\n","      34513/2000000000: episode: 965, duration: 5.280s, episode steps:  38, steps per second:   7, episode reward: -87.500, mean reward: -2.303 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 64.048216, mean_q: 11.983547, mean_eps: 0.100000\n","      34553/2000000000: episode: 966, duration: 5.395s, episode steps:  40, steps per second:   7, episode reward: -102.400, mean reward: -2.560 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.068823, mean_q: 11.567205, mean_eps: 0.100000\n","      34593/2000000000: episode: 967, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.803727, mean_q: 11.343541, mean_eps: 0.100000\n","      34633/2000000000: episode: 968, duration: 5.055s, episode steps:  40, steps per second:   8, episode reward:  1.300, mean reward:  0.033 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.016396, mean_q: 10.828423, mean_eps: 0.100000\n","      34673/2000000000: episode: 969, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: 61.000, mean reward:  1.525 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.164567, mean_q: 11.564835, mean_eps: 0.100000\n","      34709/2000000000: episode: 970, duration: 4.881s, episode steps:  36, steps per second:   7, episode reward: 79.300, mean reward:  2.203 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 68.605471, mean_q: 11.685046, mean_eps: 0.100000\n","      34749/2000000000: episode: 971, duration: 5.571s, episode steps:  40, steps per second:   7, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.650281, mean_q: 11.057786, mean_eps: 0.100000\n","      34785/2000000000: episode: 972, duration: 4.452s, episode steps:  36, steps per second:   8, episode reward: -84.100, mean reward: -2.336 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 65.253211, mean_q: 12.310587, mean_eps: 0.100000\n","      34825/2000000000: episode: 973, duration: 5.163s, episode steps:  40, steps per second:   8, episode reward: 14.500, mean reward:  0.363 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 68.528293, mean_q: 12.155802, mean_eps: 0.100000\n","      34860/2000000000: episode: 974, duration: 4.250s, episode steps:  35, steps per second:   8, episode reward: -115.300, mean reward: -3.294 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 57.895838, mean_q: 11.817360, mean_eps: 0.100000\n","      34895/2000000000: episode: 975, duration: 4.116s, episode steps:  35, steps per second:   9, episode reward: -12.100, mean reward: -0.346 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 64.259740, mean_q: 11.858449, mean_eps: 0.100000\n","      34935/2000000000: episode: 976, duration: 4.844s, episode steps:  40, steps per second:   8, episode reward: 45.200, mean reward:  1.130 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.635487, mean_q: 11.257376, mean_eps: 0.100000\n","      34975/2000000000: episode: 977, duration: 4.821s, episode steps:  40, steps per second:   8, episode reward: 18.900, mean reward:  0.472 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.387136, mean_q: 11.820269, mean_eps: 0.100000\n","      35012/2000000000: episode: 978, duration: 4.410s, episode steps:  37, steps per second:   8, episode reward: -0.100, mean reward: -0.003 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 64.599575, mean_q: 11.768589, mean_eps: 0.100000\n","      35052/2000000000: episode: 979, duration: 4.618s, episode steps:  40, steps per second:   9, episode reward: -88.400, mean reward: -2.210 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 71.411843, mean_q: 11.792157, mean_eps: 0.100000\n","      35092/2000000000: episode: 980, duration: 4.579s, episode steps:  40, steps per second:   9, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 61.443471, mean_q: 12.316052, mean_eps: 0.100000\n","      35132/2000000000: episode: 981, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: 35.400, mean reward:  0.885 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.010392, mean_q: 11.295015, mean_eps: 0.100000\n","      35172/2000000000: episode: 982, duration: 4.895s, episode steps:  40, steps per second:   8, episode reward: 102.000, mean reward:  2.550 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 70.347468, mean_q: 11.702364, mean_eps: 0.100000\n","      35212/2000000000: episode: 983, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward: 181.200, mean reward:  4.530 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.377981, mean_q: 11.651080, mean_eps: 0.100000\n","      35252/2000000000: episode: 984, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward: 153.000, mean reward:  3.825 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.773361, mean_q: 11.594818, mean_eps: 0.100000\n","      35286/2000000000: episode: 985, duration: 4.458s, episode steps:  34, steps per second:   8, episode reward: -52.500, mean reward: -1.544 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 65.086718, mean_q: 11.763839, mean_eps: 0.100000\n","      35326/2000000000: episode: 986, duration: 4.696s, episode steps:  40, steps per second:   9, episode reward: 69.900, mean reward:  1.748 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.422614, mean_q: 11.645950, mean_eps: 0.100000\n","      35366/2000000000: episode: 987, duration: 4.645s, episode steps:  40, steps per second:   9, episode reward: -4.500, mean reward: -0.112 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.993481, mean_q: 11.329679, mean_eps: 0.100000\n","      35406/2000000000: episode: 988, duration: 4.690s, episode steps:  40, steps per second:   9, episode reward: -11.400, mean reward: -0.285 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 61.960122, mean_q: 11.830271, mean_eps: 0.100000\n","      35446/2000000000: episode: 989, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: -140.800, mean reward: -3.520 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 63.270068, mean_q: 11.561113, mean_eps: 0.100000\n","      35485/2000000000: episode: 990, duration: 4.451s, episode steps:  39, steps per second:   9, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 68.786694, mean_q: 11.289799, mean_eps: 0.100000\n","      35524/2000000000: episode: 991, duration: 4.724s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 65.828804, mean_q: 11.740829, mean_eps: 0.100000\n","      35564/2000000000: episode: 992, duration: 5.038s, episode steps:  40, steps per second:   8, episode reward: -60.400, mean reward: -1.510 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.326466, mean_q: 10.802759, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      35604/2000000000: episode: 993, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: -75.900, mean reward: -1.897 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.848670, mean_q: 12.248146, mean_eps: 0.100000\n","      35644/2000000000: episode: 994, duration: 5.251s, episode steps:  40, steps per second:   8, episode reward: -31.000, mean reward: -0.775 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.340800, mean_q: 11.328294, mean_eps: 0.100000\n","      35681/2000000000: episode: 995, duration: 4.841s, episode steps:  37, steps per second:   8, episode reward: -108.800, mean reward: -2.941 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 66.493531, mean_q: 11.747766, mean_eps: 0.100000\n","      35721/2000000000: episode: 996, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: 117.300, mean reward:  2.932 [-20.000, 18.400], mean action: 1.175 [0.000, 2.000],  loss: 73.939467, mean_q: 11.406118, mean_eps: 0.100000\n","      35756/2000000000: episode: 997, duration: 4.699s, episode steps:  35, steps per second:   7, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 65.271044, mean_q: 11.749502, mean_eps: 0.100000\n","      35796/2000000000: episode: 998, duration: 5.346s, episode steps:  40, steps per second:   7, episode reward: -101.300, mean reward: -2.533 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.995621, mean_q: 11.624686, mean_eps: 0.100000\n","      35836/2000000000: episode: 999, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: 19.500, mean reward:  0.487 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.380542, mean_q: 11.947791, mean_eps: 0.100000\n","      35875/2000000000: episode: 1000, duration: 4.816s, episode steps:  39, steps per second:   8, episode reward:  8.200, mean reward:  0.210 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 68.008259, mean_q: 11.184483, mean_eps: 0.100000\n","      35915/2000000000: episode: 1001, duration: 4.934s, episode steps:  40, steps per second:   8, episode reward: 25.200, mean reward:  0.630 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 60.650596, mean_q: 11.464668, mean_eps: 0.100000\n","      35955/2000000000: episode: 1002, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: -50.000, mean reward: -1.250 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.319572, mean_q: 11.771109, mean_eps: 0.100000\n","      35995/2000000000: episode: 1003, duration: 4.959s, episode steps:  40, steps per second:   8, episode reward: -110.300, mean reward: -2.757 [-20.000, 18.000], mean action: 1.725 [0.000, 2.000],  loss: 63.649913, mean_q: 11.287272, mean_eps: 0.100000\n","      36035/2000000000: episode: 1004, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: 40.300, mean reward:  1.008 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 66.807834, mean_q: 12.021570, mean_eps: 0.100000\n","      36068/2000000000: episode: 1005, duration: 4.330s, episode steps:  33, steps per second:   8, episode reward: 33.300, mean reward:  1.009 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 64.004985, mean_q: 11.828513, mean_eps: 0.100000\n","      36108/2000000000: episode: 1006, duration: 5.113s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.752419, mean_q: 11.448124, mean_eps: 0.100000\n","      36145/2000000000: episode: 1007, duration: 4.717s, episode steps:  37, steps per second:   8, episode reward: -88.800, mean reward: -2.400 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 68.384689, mean_q: 11.973351, mean_eps: 0.100000\n","      36184/2000000000: episode: 1008, duration: 4.849s, episode steps:  39, steps per second:   8, episode reward: -41.300, mean reward: -1.059 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 64.729147, mean_q: 11.966631, mean_eps: 0.100000\n","      36224/2000000000: episode: 1009, duration: 4.997s, episode steps:  40, steps per second:   8, episode reward: -57.500, mean reward: -1.438 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.992037, mean_q: 11.761244, mean_eps: 0.100000\n","      36263/2000000000: episode: 1010, duration: 4.964s, episode steps:  39, steps per second:   8, episode reward: 17.100, mean reward:  0.438 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 65.490536, mean_q: 11.556698, mean_eps: 0.100000\n","      36303/2000000000: episode: 1011, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: 79.600, mean reward:  1.990 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.946316, mean_q: 12.238057, mean_eps: 0.100000\n","      36343/2000000000: episode: 1012, duration: 5.166s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.629804, mean_q: 11.435560, mean_eps: 0.100000\n","      36383/2000000000: episode: 1013, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.156233, mean_q: 11.562836, mean_eps: 0.100000\n","      36423/2000000000: episode: 1014, duration: 4.690s, episode steps:  40, steps per second:   9, episode reward: -128.000, mean reward: -3.200 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 68.871080, mean_q: 11.117331, mean_eps: 0.100000\n","      36463/2000000000: episode: 1015, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: 44.800, mean reward:  1.120 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.209131, mean_q: 11.241803, mean_eps: 0.100000\n","      36503/2000000000: episode: 1016, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: 190.600, mean reward:  4.765 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.863493, mean_q: 11.762889, mean_eps: 0.100000\n","      36533/2000000000: episode: 1017, duration: 3.806s, episode steps:  30, steps per second:   8, episode reward: -3.000, mean reward: -0.100 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 72.647698, mean_q: 11.837203, mean_eps: 0.100000\n","      36573/2000000000: episode: 1018, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: 38.100, mean reward:  0.953 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.706540, mean_q: 12.126455, mean_eps: 0.100000\n","      36613/2000000000: episode: 1019, duration: 5.112s, episode steps:  40, steps per second:   8, episode reward: -48.000, mean reward: -1.200 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.127499, mean_q: 12.067908, mean_eps: 0.100000\n","      36645/2000000000: episode: 1020, duration: 4.082s, episode steps:  32, steps per second:   8, episode reward: -23.200, mean reward: -0.725 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 66.886243, mean_q: 12.000116, mean_eps: 0.100000\n","      36683/2000000000: episode: 1021, duration: 4.674s, episode steps:  38, steps per second:   8, episode reward: -28.700, mean reward: -0.755 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 63.337687, mean_q: 11.357107, mean_eps: 0.100000\n","      36715/2000000000: episode: 1022, duration: 4.001s, episode steps:  32, steps per second:   8, episode reward: -32.300, mean reward: -1.009 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 66.085619, mean_q: 11.720950, mean_eps: 0.100000\n","      36755/2000000000: episode: 1023, duration: 4.984s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.454377, mean_q: 11.579801, mean_eps: 0.100000\n","      36795/2000000000: episode: 1024, duration: 5.127s, episode steps:  40, steps per second:   8, episode reward: -191.100, mean reward: -4.778 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 60.683817, mean_q: 11.340681, mean_eps: 0.100000\n","      36831/2000000000: episode: 1025, duration: 4.252s, episode steps:  36, steps per second:   8, episode reward: -13.400, mean reward: -0.372 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 71.639382, mean_q: 11.296159, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      36871/2000000000: episode: 1026, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.639686, mean_q: 12.005996, mean_eps: 0.100000\n","      36911/2000000000: episode: 1027, duration: 4.962s, episode steps:  40, steps per second:   8, episode reward: 31.200, mean reward:  0.780 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 61.781671, mean_q: 11.115309, mean_eps: 0.100000\n","      36951/2000000000: episode: 1028, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward: -27.700, mean reward: -0.693 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.973564, mean_q: 11.855868, mean_eps: 0.100000\n","      36990/2000000000: episode: 1029, duration: 5.126s, episode steps:  39, steps per second:   8, episode reward: -34.500, mean reward: -0.885 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 68.122628, mean_q: 11.657332, mean_eps: 0.100000\n","      37030/2000000000: episode: 1030, duration: 4.852s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.124188, mean_q: 11.152006, mean_eps: 0.100000\n","      37070/2000000000: episode: 1031, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: 85.400, mean reward:  2.135 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 60.851167, mean_q: 11.634804, mean_eps: 0.100000\n","      37110/2000000000: episode: 1032, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: 25.400, mean reward:  0.635 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 59.438320, mean_q: 11.832628, mean_eps: 0.100000\n","      37150/2000000000: episode: 1033, duration: 5.260s, episode steps:  40, steps per second:   8, episode reward: -158.900, mean reward: -3.973 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.097010, mean_q: 11.496387, mean_eps: 0.100000\n","      37190/2000000000: episode: 1034, duration: 5.583s, episode steps:  40, steps per second:   7, episode reward: -39.200, mean reward: -0.980 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.727573, mean_q: 11.900436, mean_eps: 0.100000\n","      37229/2000000000: episode: 1035, duration: 5.011s, episode steps:  39, steps per second:   8, episode reward: 91.600, mean reward:  2.349 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.763107, mean_q: 11.643526, mean_eps: 0.100000\n","      37264/2000000000: episode: 1036, duration: 4.683s, episode steps:  35, steps per second:   7, episode reward: -25.900, mean reward: -0.740 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 74.344010, mean_q: 11.522883, mean_eps: 0.100000\n","      37304/2000000000: episode: 1037, duration: 5.096s, episode steps:  40, steps per second:   8, episode reward: 52.000, mean reward:  1.300 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 62.142935, mean_q: 10.964017, mean_eps: 0.100000\n","      37343/2000000000: episode: 1038, duration: 4.903s, episode steps:  39, steps per second:   8, episode reward: 36.200, mean reward:  0.928 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 64.777143, mean_q: 11.482606, mean_eps: 0.100000\n","      37383/2000000000: episode: 1039, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: 99.700, mean reward:  2.493 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.755435, mean_q: 11.438943, mean_eps: 0.100000\n","      37423/2000000000: episode: 1040, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: -95.200, mean reward: -2.380 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.534843, mean_q: 11.647259, mean_eps: 0.100000\n","      37463/2000000000: episode: 1041, duration: 4.836s, episode steps:  40, steps per second:   8, episode reward: 78.100, mean reward:  1.952 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 64.404110, mean_q: 11.543072, mean_eps: 0.100000\n","      37503/2000000000: episode: 1042, duration: 4.952s, episode steps:  40, steps per second:   8, episode reward: -46.900, mean reward: -1.172 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.963268, mean_q: 12.035775, mean_eps: 0.100000\n","      37538/2000000000: episode: 1043, duration: 4.360s, episode steps:  35, steps per second:   8, episode reward: -137.600, mean reward: -3.931 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 68.745819, mean_q: 12.096004, mean_eps: 0.100000\n","      37578/2000000000: episode: 1044, duration: 4.852s, episode steps:  40, steps per second:   8, episode reward: -21.500, mean reward: -0.537 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 66.071877, mean_q: 11.519923, mean_eps: 0.100000\n","      37616/2000000000: episode: 1045, duration: 4.620s, episode steps:  38, steps per second:   8, episode reward: 72.300, mean reward:  1.903 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 67.475933, mean_q: 11.513472, mean_eps: 0.100000\n","      37651/2000000000: episode: 1046, duration: 4.254s, episode steps:  35, steps per second:   8, episode reward: 43.300, mean reward:  1.237 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 64.308287, mean_q: 11.612372, mean_eps: 0.100000\n","      37691/2000000000: episode: 1047, duration: 4.616s, episode steps:  40, steps per second:   9, episode reward: -10.400, mean reward: -0.260 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.791900, mean_q: 11.442482, mean_eps: 0.100000\n","      37725/2000000000: episode: 1048, duration: 4.034s, episode steps:  34, steps per second:   8, episode reward: 57.800, mean reward:  1.700 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 59.949330, mean_q: 12.317731, mean_eps: 0.100000\n","      37765/2000000000: episode: 1049, duration: 4.659s, episode steps:  40, steps per second:   9, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 60.513352, mean_q: 11.729255, mean_eps: 0.100000\n","      37805/2000000000: episode: 1050, duration: 4.733s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.025617, mean_q: 11.778253, mean_eps: 0.100000\n","      37845/2000000000: episode: 1051, duration: 4.579s, episode steps:  40, steps per second:   9, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.550967, mean_q: 11.921713, mean_eps: 0.100000\n","      37885/2000000000: episode: 1052, duration: 5.003s, episode steps:  40, steps per second:   8, episode reward:  7.800, mean reward:  0.195 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.451536, mean_q: 12.171439, mean_eps: 0.100000\n","      37925/2000000000: episode: 1053, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.175211, mean_q: 12.153441, mean_eps: 0.100000\n","      37965/2000000000: episode: 1054, duration: 4.747s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.636940, mean_q: 11.342299, mean_eps: 0.100000\n","      37999/2000000000: episode: 1055, duration: 4.433s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 64.982924, mean_q: 11.385073, mean_eps: 0.100000\n","      38039/2000000000: episode: 1056, duration: 4.797s, episode steps:  40, steps per second:   8, episode reward: 41.800, mean reward:  1.045 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.902644, mean_q: 11.487981, mean_eps: 0.100000\n","      38079/2000000000: episode: 1057, duration: 4.796s, episode steps:  40, steps per second:   8, episode reward: -186.900, mean reward: -4.673 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 67.450157, mean_q: 11.322678, mean_eps: 0.100000\n","      38119/2000000000: episode: 1058, duration: 4.875s, episode steps:  40, steps per second:   8, episode reward: -73.400, mean reward: -1.835 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 62.746868, mean_q: 11.634418, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      38159/2000000000: episode: 1059, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.984374, mean_q: 10.827933, mean_eps: 0.100000\n","      38192/2000000000: episode: 1060, duration: 3.924s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 59.600666, mean_q: 11.909001, mean_eps: 0.100000\n","      38232/2000000000: episode: 1061, duration: 4.861s, episode steps:  40, steps per second:   8, episode reward: 22.900, mean reward:  0.572 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 62.406016, mean_q: 11.163791, mean_eps: 0.100000\n","      38271/2000000000: episode: 1062, duration: 4.584s, episode steps:  39, steps per second:   9, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 63.669978, mean_q: 11.099952, mean_eps: 0.100000\n","      38311/2000000000: episode: 1063, duration: 4.653s, episode steps:  40, steps per second:   9, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.631160, mean_q: 11.579705, mean_eps: 0.100000\n","      38343/2000000000: episode: 1064, duration: 3.693s, episode steps:  32, steps per second:   9, episode reward: -199.300, mean reward: -6.228 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 65.268579, mean_q: 11.616193, mean_eps: 0.100000\n","      38383/2000000000: episode: 1065, duration: 5.049s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 61.909865, mean_q: 11.771014, mean_eps: 0.100000\n","      38413/2000000000: episode: 1066, duration: 3.557s, episode steps:  30, steps per second:   8, episode reward: -96.000, mean reward: -3.200 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 67.192184, mean_q: 12.100801, mean_eps: 0.100000\n","      38453/2000000000: episode: 1067, duration: 4.560s, episode steps:  40, steps per second:   9, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.711113, mean_q: 11.648969, mean_eps: 0.100000\n","      38493/2000000000: episode: 1068, duration: 4.915s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.111123, mean_q: 11.140949, mean_eps: 0.100000\n","      38533/2000000000: episode: 1069, duration: 4.728s, episode steps:  40, steps per second:   8, episode reward: -2.700, mean reward: -0.067 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.913418, mean_q: 11.533055, mean_eps: 0.100000\n","      38573/2000000000: episode: 1070, duration: 4.629s, episode steps:  40, steps per second:   9, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 59.988421, mean_q: 11.510402, mean_eps: 0.100000\n","      38609/2000000000: episode: 1071, duration: 4.193s, episode steps:  36, steps per second:   9, episode reward: 118.500, mean reward:  3.292 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 64.856189, mean_q: 12.016591, mean_eps: 0.100000\n","      38649/2000000000: episode: 1072, duration: 4.652s, episode steps:  40, steps per second:   9, episode reward: 36.500, mean reward:  0.912 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.417278, mean_q: 11.377306, mean_eps: 0.100000\n","      38689/2000000000: episode: 1073, duration: 4.506s, episode steps:  40, steps per second:   9, episode reward: 13.100, mean reward:  0.328 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 63.191250, mean_q: 11.699473, mean_eps: 0.100000\n","      38727/2000000000: episode: 1074, duration: 4.572s, episode steps:  38, steps per second:   8, episode reward: -70.700, mean reward: -1.861 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 63.545424, mean_q: 11.220789, mean_eps: 0.100000\n","      38767/2000000000: episode: 1075, duration: 4.618s, episode steps:  40, steps per second:   9, episode reward: -81.300, mean reward: -2.033 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.058314, mean_q: 11.427517, mean_eps: 0.100000\n","      38807/2000000000: episode: 1076, duration: 4.767s, episode steps:  40, steps per second:   8, episode reward: -91.800, mean reward: -2.295 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 65.147924, mean_q: 12.266692, mean_eps: 0.100000\n","      38847/2000000000: episode: 1077, duration: 4.717s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 60.570747, mean_q: 11.885137, mean_eps: 0.100000\n","      38884/2000000000: episode: 1078, duration: 4.465s, episode steps:  37, steps per second:   8, episode reward: 22.200, mean reward:  0.600 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 65.235598, mean_q: 11.381870, mean_eps: 0.100000\n","      38919/2000000000: episode: 1079, duration: 4.270s, episode steps:  35, steps per second:   8, episode reward: 90.400, mean reward:  2.583 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 61.762120, mean_q: 11.800299, mean_eps: 0.100000\n","      38955/2000000000: episode: 1080, duration: 4.196s, episode steps:  36, steps per second:   9, episode reward: -41.800, mean reward: -1.161 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 64.327912, mean_q: 11.749260, mean_eps: 0.100000\n","      38993/2000000000: episode: 1081, duration: 4.768s, episode steps:  38, steps per second:   8, episode reward: 78.900, mean reward:  2.076 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 72.144612, mean_q: 11.269349, mean_eps: 0.100000\n","      39033/2000000000: episode: 1082, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: 26.100, mean reward:  0.653 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 65.887799, mean_q: 12.180937, mean_eps: 0.100000\n","      39073/2000000000: episode: 1083, duration: 5.210s, episode steps:  40, steps per second:   8, episode reward: 47.700, mean reward:  1.192 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.114929, mean_q: 11.844520, mean_eps: 0.100000\n","      39113/2000000000: episode: 1084, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.608758, mean_q: 11.023983, mean_eps: 0.100000\n","      39152/2000000000: episode: 1085, duration: 4.819s, episode steps:  39, steps per second:   8, episode reward: -53.200, mean reward: -1.364 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 64.562460, mean_q: 12.282758, mean_eps: 0.100000\n","      39192/2000000000: episode: 1086, duration: 4.847s, episode steps:  40, steps per second:   8, episode reward: 87.600, mean reward:  2.190 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 65.061662, mean_q: 11.461761, mean_eps: 0.100000\n","      39232/2000000000: episode: 1087, duration: 5.081s, episode steps:  40, steps per second:   8, episode reward: -50.500, mean reward: -1.262 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.553540, mean_q: 11.518531, mean_eps: 0.100000\n","      39272/2000000000: episode: 1088, duration: 4.748s, episode steps:  40, steps per second:   8, episode reward: -93.300, mean reward: -2.333 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.827720, mean_q: 11.717387, mean_eps: 0.100000\n","      39306/2000000000: episode: 1089, duration: 4.009s, episode steps:  34, steps per second:   8, episode reward: 20.700, mean reward:  0.609 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 64.766182, mean_q: 12.146839, mean_eps: 0.100000\n","      39346/2000000000: episode: 1090, duration: 4.505s, episode steps:  40, steps per second:   9, episode reward: 28.500, mean reward:  0.712 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.339121, mean_q: 11.651300, mean_eps: 0.100000\n","      39382/2000000000: episode: 1091, duration: 4.439s, episode steps:  36, steps per second:   8, episode reward: -40.700, mean reward: -1.131 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 65.074210, mean_q: 11.625798, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      39422/2000000000: episode: 1092, duration: 4.735s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.759601, mean_q: 11.577328, mean_eps: 0.100000\n","      39462/2000000000: episode: 1093, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.036141, mean_q: 11.609548, mean_eps: 0.100000\n","      39502/2000000000: episode: 1094, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: -28.300, mean reward: -0.708 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.443494, mean_q: 11.192682, mean_eps: 0.100000\n","      39542/2000000000: episode: 1095, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: 66.000, mean reward:  1.650 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 61.844141, mean_q: 11.326738, mean_eps: 0.100000\n","      39582/2000000000: episode: 1096, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: 76.400, mean reward:  1.910 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 62.052084, mean_q: 11.552500, mean_eps: 0.100000\n","      39622/2000000000: episode: 1097, duration: 4.711s, episode steps:  40, steps per second:   8, episode reward: 16.200, mean reward:  0.405 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.965844, mean_q: 11.558812, mean_eps: 0.100000\n","      39662/2000000000: episode: 1098, duration: 4.776s, episode steps:  40, steps per second:   8, episode reward: -36.200, mean reward: -0.905 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.105562, mean_q: 11.526497, mean_eps: 0.100000\n","      39702/2000000000: episode: 1099, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: 31.200, mean reward:  0.780 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.944520, mean_q: 11.874515, mean_eps: 0.100000\n","      39742/2000000000: episode: 1100, duration: 4.749s, episode steps:  40, steps per second:   8, episode reward: -42.900, mean reward: -1.073 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.653312, mean_q: 11.528072, mean_eps: 0.100000\n","      39782/2000000000: episode: 1101, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: 166.100, mean reward:  4.152 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.528852, mean_q: 11.871056, mean_eps: 0.100000\n","      39822/2000000000: episode: 1102, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: 76.100, mean reward:  1.903 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 58.897722, mean_q: 11.456618, mean_eps: 0.100000\n","      39862/2000000000: episode: 1103, duration: 4.725s, episode steps:  40, steps per second:   8, episode reward: -77.800, mean reward: -1.945 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.503164, mean_q: 11.280039, mean_eps: 0.100000\n","      39900/2000000000: episode: 1104, duration: 4.331s, episode steps:  38, steps per second:   9, episode reward: -12.700, mean reward: -0.334 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 62.312364, mean_q: 11.821062, mean_eps: 0.100000\n","      39940/2000000000: episode: 1105, duration: 5.398s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.186560, mean_q: 12.258373, mean_eps: 0.100000\n","      39980/2000000000: episode: 1106, duration: 4.872s, episode steps:  40, steps per second:   8, episode reward: 97.500, mean reward:  2.437 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 58.909638, mean_q: 11.265816, mean_eps: 0.100000\n","      40020/2000000000: episode: 1107, duration: 4.385s, episode steps:  40, steps per second:   9, episode reward:  3.300, mean reward:  0.082 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.790310, mean_q: 11.698234, mean_eps: 0.100000\n","      40060/2000000000: episode: 1108, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: 23.200, mean reward:  0.580 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.432470, mean_q: 12.926318, mean_eps: 0.100000\n","      40100/2000000000: episode: 1109, duration: 4.694s, episode steps:  40, steps per second:   9, episode reward: 144.500, mean reward:  3.613 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.350855, mean_q: 13.015144, mean_eps: 0.100000\n","      40138/2000000000: episode: 1110, duration: 4.624s, episode steps:  38, steps per second:   8, episode reward: 130.100, mean reward:  3.424 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 74.483445, mean_q: 12.377002, mean_eps: 0.100000\n","      40178/2000000000: episode: 1111, duration: 4.673s, episode steps:  40, steps per second:   9, episode reward: 34.000, mean reward:  0.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.168391, mean_q: 12.608064, mean_eps: 0.100000\n","      40212/2000000000: episode: 1112, duration: 4.341s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 74.681290, mean_q: 12.683936, mean_eps: 0.100000\n","      40252/2000000000: episode: 1113, duration: 5.681s, episode steps:  40, steps per second:   7, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.174323, mean_q: 12.460727, mean_eps: 0.100000\n","      40292/2000000000: episode: 1114, duration: 5.411s, episode steps:  40, steps per second:   7, episode reward: -7.300, mean reward: -0.183 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.290677, mean_q: 12.799064, mean_eps: 0.100000\n","      40330/2000000000: episode: 1115, duration: 4.912s, episode steps:  38, steps per second:   8, episode reward: 64.100, mean reward:  1.687 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 68.016485, mean_q: 13.374236, mean_eps: 0.100000\n","      40370/2000000000: episode: 1116, duration: 4.918s, episode steps:  40, steps per second:   8, episode reward: -70.400, mean reward: -1.760 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.080846, mean_q: 12.848055, mean_eps: 0.100000\n","      40410/2000000000: episode: 1117, duration: 4.730s, episode steps:  40, steps per second:   8, episode reward: -54.100, mean reward: -1.353 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.510137, mean_q: 13.186789, mean_eps: 0.100000\n","      40450/2000000000: episode: 1118, duration: 4.608s, episode steps:  40, steps per second:   9, episode reward: -10.100, mean reward: -0.252 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.365870, mean_q: 12.449345, mean_eps: 0.100000\n","      40490/2000000000: episode: 1119, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward: -58.800, mean reward: -1.470 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.704676, mean_q: 13.168435, mean_eps: 0.100000\n","      40518/2000000000: episode: 1120, duration: 3.682s, episode steps:  28, steps per second:   8, episode reward: -124.000, mean reward: -4.429 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 67.282343, mean_q: 13.205981, mean_eps: 0.100000\n","      40558/2000000000: episode: 1121, duration: 4.782s, episode steps:  40, steps per second:   8, episode reward: -3.300, mean reward: -0.083 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.852731, mean_q: 12.259049, mean_eps: 0.100000\n","      40596/2000000000: episode: 1122, duration: 4.393s, episode steps:  38, steps per second:   9, episode reward: -84.600, mean reward: -2.226 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 70.152315, mean_q: 12.453673, mean_eps: 0.100000\n","      40634/2000000000: episode: 1123, duration: 4.613s, episode steps:  38, steps per second:   8, episode reward: 48.600, mean reward:  1.279 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 69.388957, mean_q: 13.373937, mean_eps: 0.100000\n","      40674/2000000000: episode: 1124, duration: 4.858s, episode steps:  40, steps per second:   8, episode reward:  0.400, mean reward:  0.010 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.592206, mean_q: 12.800830, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      40710/2000000000: episode: 1125, duration: 4.376s, episode steps:  36, steps per second:   8, episode reward: 48.700, mean reward:  1.353 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 70.698372, mean_q: 13.004827, mean_eps: 0.100000\n","      40750/2000000000: episode: 1126, duration: 5.106s, episode steps:  40, steps per second:   8, episode reward: -13.900, mean reward: -0.347 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.872110, mean_q: 12.678926, mean_eps: 0.100000\n","      40790/2000000000: episode: 1127, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward:  2.300, mean reward:  0.058 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.239120, mean_q: 13.097220, mean_eps: 0.100000\n","      40830/2000000000: episode: 1128, duration: 4.723s, episode steps:  40, steps per second:   8, episode reward: 14.900, mean reward:  0.373 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.068854, mean_q: 13.241497, mean_eps: 0.100000\n","      40864/2000000000: episode: 1129, duration: 4.139s, episode steps:  34, steps per second:   8, episode reward: 34.900, mean reward:  1.026 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 68.973691, mean_q: 12.468522, mean_eps: 0.100000\n","      40896/2000000000: episode: 1130, duration: 4.211s, episode steps:  32, steps per second:   8, episode reward: 72.000, mean reward:  2.250 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 66.593435, mean_q: 12.645786, mean_eps: 0.100000\n","      40924/2000000000: episode: 1131, duration: 3.504s, episode steps:  28, steps per second:   8, episode reward: -108.300, mean reward: -3.868 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 72.175353, mean_q: 13.121331, mean_eps: 0.100000\n","      40964/2000000000: episode: 1132, duration: 5.072s, episode steps:  40, steps per second:   8, episode reward: -38.300, mean reward: -0.957 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.254998, mean_q: 12.816548, mean_eps: 0.100000\n","      41004/2000000000: episode: 1133, duration: 4.981s, episode steps:  40, steps per second:   8, episode reward: 69.000, mean reward:  1.725 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 67.221694, mean_q: 13.058109, mean_eps: 0.100000\n","      41039/2000000000: episode: 1134, duration: 4.397s, episode steps:  35, steps per second:   8, episode reward: 183.900, mean reward:  5.254 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.313447, mean_q: 13.667918, mean_eps: 0.100000\n","      41074/2000000000: episode: 1135, duration: 4.295s, episode steps:  35, steps per second:   8, episode reward: 79.600, mean reward:  2.274 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 73.158495, mean_q: 13.101479, mean_eps: 0.100000\n","      41114/2000000000: episode: 1136, duration: 5.487s, episode steps:  40, steps per second:   7, episode reward: 34.700, mean reward:  0.868 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.277734, mean_q: 12.613269, mean_eps: 0.100000\n","      41151/2000000000: episode: 1137, duration: 4.187s, episode steps:  37, steps per second:   9, episode reward: -83.400, mean reward: -2.254 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 73.078552, mean_q: 12.116767, mean_eps: 0.100000\n","      41186/2000000000: episode: 1138, duration: 4.096s, episode steps:  35, steps per second:   9, episode reward: -121.700, mean reward: -3.477 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 69.797003, mean_q: 12.406795, mean_eps: 0.100000\n","      41223/2000000000: episode: 1139, duration: 4.186s, episode steps:  37, steps per second:   9, episode reward: -22.900, mean reward: -0.619 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 64.106811, mean_q: 12.609012, mean_eps: 0.100000\n","      41263/2000000000: episode: 1140, duration: 4.475s, episode steps:  40, steps per second:   9, episode reward: -72.500, mean reward: -1.812 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 61.789215, mean_q: 12.765647, mean_eps: 0.100000\n","      41295/2000000000: episode: 1141, duration: 3.582s, episode steps:  32, steps per second:   9, episode reward: 71.700, mean reward:  2.241 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 63.298253, mean_q: 13.967455, mean_eps: 0.100000\n","      41326/2000000000: episode: 1142, duration: 3.405s, episode steps:  31, steps per second:   9, episode reward: 14.100, mean reward:  0.455 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 68.814914, mean_q: 12.840251, mean_eps: 0.100000\n","      41363/2000000000: episode: 1143, duration: 4.298s, episode steps:  37, steps per second:   9, episode reward: -37.000, mean reward: -1.000 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 65.962530, mean_q: 12.793984, mean_eps: 0.100000\n","      41403/2000000000: episode: 1144, duration: 4.505s, episode steps:  40, steps per second:   9, episode reward: 113.400, mean reward:  2.835 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.271636, mean_q: 12.785917, mean_eps: 0.100000\n","      41436/2000000000: episode: 1145, duration: 3.784s, episode steps:  33, steps per second:   9, episode reward: 15.500, mean reward:  0.470 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 70.628859, mean_q: 12.912246, mean_eps: 0.100000\n","      41467/2000000000: episode: 1146, duration: 3.829s, episode steps:  31, steps per second:   8, episode reward:  0.500, mean reward:  0.016 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 63.922343, mean_q: 13.137654, mean_eps: 0.100000\n","      41507/2000000000: episode: 1147, duration: 4.609s, episode steps:  40, steps per second:   9, episode reward: 83.900, mean reward:  2.097 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.451893, mean_q: 13.337974, mean_eps: 0.100000\n","      41546/2000000000: episode: 1148, duration: 4.940s, episode steps:  39, steps per second:   8, episode reward: -80.700, mean reward: -2.069 [-20.000, 19.600], mean action: 1.231 [0.000, 2.000],  loss: 66.755621, mean_q: 12.996029, mean_eps: 0.100000\n","      41585/2000000000: episode: 1149, duration: 4.641s, episode steps:  39, steps per second:   8, episode reward: -30.900, mean reward: -0.792 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 65.566191, mean_q: 12.479387, mean_eps: 0.100000\n","      41625/2000000000: episode: 1150, duration: 5.071s, episode steps:  40, steps per second:   8, episode reward: 48.200, mean reward:  1.205 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 67.785637, mean_q: 13.250781, mean_eps: 0.100000\n","      41662/2000000000: episode: 1151, duration: 4.125s, episode steps:  37, steps per second:   9, episode reward:  9.200, mean reward:  0.249 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 63.493722, mean_q: 13.069802, mean_eps: 0.100000\n","      41702/2000000000: episode: 1152, duration: 4.474s, episode steps:  40, steps per second:   9, episode reward: 99.900, mean reward:  2.497 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.881947, mean_q: 12.155095, mean_eps: 0.100000\n","      41740/2000000000: episode: 1153, duration: 4.537s, episode steps:  38, steps per second:   8, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 67.730297, mean_q: 12.661680, mean_eps: 0.100000\n","      41780/2000000000: episode: 1154, duration: 4.782s, episode steps:  40, steps per second:   8, episode reward: -7.900, mean reward: -0.198 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 66.710039, mean_q: 12.283634, mean_eps: 0.100000\n","      41820/2000000000: episode: 1155, duration: 4.764s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 65.693239, mean_q: 12.595355, mean_eps: 0.100000\n","      41860/2000000000: episode: 1156, duration: 4.739s, episode steps:  40, steps per second:   8, episode reward: -95.100, mean reward: -2.377 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 63.492012, mean_q: 13.082033, mean_eps: 0.100000\n","      41900/2000000000: episode: 1157, duration: 4.691s, episode steps:  40, steps per second:   9, episode reward: -148.000, mean reward: -3.700 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 66.378912, mean_q: 12.807852, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      41940/2000000000: episode: 1158, duration: 4.962s, episode steps:  40, steps per second:   8, episode reward: -35.800, mean reward: -0.895 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.639824, mean_q: 13.181332, mean_eps: 0.100000\n","      41980/2000000000: episode: 1159, duration: 4.804s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.137648, mean_q: 12.844297, mean_eps: 0.100000\n","      42018/2000000000: episode: 1160, duration: 4.933s, episode steps:  38, steps per second:   8, episode reward: 264.200, mean reward:  6.953 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 68.603538, mean_q: 13.133892, mean_eps: 0.100000\n","      42058/2000000000: episode: 1161, duration: 4.967s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 61.275707, mean_q: 12.893897, mean_eps: 0.100000\n","      42098/2000000000: episode: 1162, duration: 4.705s, episode steps:  40, steps per second:   9, episode reward: 102.000, mean reward:  2.550 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 63.476565, mean_q: 12.566342, mean_eps: 0.100000\n","      42134/2000000000: episode: 1163, duration: 4.332s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 69.543426, mean_q: 13.977847, mean_eps: 0.100000\n","      42169/2000000000: episode: 1164, duration: 4.058s, episode steps:  35, steps per second:   9, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 62.366757, mean_q: 13.071682, mean_eps: 0.100000\n","      42209/2000000000: episode: 1165, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.703618, mean_q: 12.248065, mean_eps: 0.100000\n","      42240/2000000000: episode: 1166, duration: 3.651s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 68.012408, mean_q: 14.001352, mean_eps: 0.100000\n","      42279/2000000000: episode: 1167, duration: 4.712s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 70.235327, mean_q: 13.216435, mean_eps: 0.100000\n","      42316/2000000000: episode: 1168, duration: 4.562s, episode steps:  37, steps per second:   8, episode reward: -134.000, mean reward: -3.622 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 64.044433, mean_q: 12.638256, mean_eps: 0.100000\n","      42356/2000000000: episode: 1169, duration: 4.872s, episode steps:  40, steps per second:   8, episode reward: -16.700, mean reward: -0.418 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.400379, mean_q: 13.013172, mean_eps: 0.100000\n","      42388/2000000000: episode: 1170, duration: 3.729s, episode steps:  32, steps per second:   9, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 65.329831, mean_q: 13.024827, mean_eps: 0.100000\n","      42428/2000000000: episode: 1171, duration: 4.602s, episode steps:  40, steps per second:   9, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 65.796052, mean_q: 13.581508, mean_eps: 0.100000\n","      42468/2000000000: episode: 1172, duration: 4.653s, episode steps:  40, steps per second:   9, episode reward: 37.300, mean reward:  0.933 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.008498, mean_q: 13.250889, mean_eps: 0.100000\n","      42508/2000000000: episode: 1173, duration: 4.706s, episode steps:  40, steps per second:   9, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.419111, mean_q: 12.437551, mean_eps: 0.100000\n","      42541/2000000000: episode: 1174, duration: 3.896s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 64.897244, mean_q: 13.162005, mean_eps: 0.100000\n","      42571/2000000000: episode: 1175, duration: 3.483s, episode steps:  30, steps per second:   9, episode reward: 107.200, mean reward:  3.573 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 67.694250, mean_q: 12.613461, mean_eps: 0.100000\n","      42611/2000000000: episode: 1176, duration: 4.702s, episode steps:  40, steps per second:   9, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.518706, mean_q: 12.756005, mean_eps: 0.100000\n","      42651/2000000000: episode: 1177, duration: 4.565s, episode steps:  40, steps per second:   9, episode reward: 48.800, mean reward:  1.220 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.502089, mean_q: 12.688631, mean_eps: 0.100000\n","      42691/2000000000: episode: 1178, duration: 4.585s, episode steps:  40, steps per second:   9, episode reward: -80.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.293165, mean_q: 12.778602, mean_eps: 0.100000\n","      42724/2000000000: episode: 1179, duration: 3.721s, episode steps:  33, steps per second:   9, episode reward: -74.800, mean reward: -2.267 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 63.236989, mean_q: 13.258490, mean_eps: 0.100000\n","      42764/2000000000: episode: 1180, duration: 4.551s, episode steps:  40, steps per second:   9, episode reward: 82.000, mean reward:  2.050 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.151725, mean_q: 12.363192, mean_eps: 0.100000\n","      42802/2000000000: episode: 1181, duration: 4.337s, episode steps:  38, steps per second:   9, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 66.391538, mean_q: 12.971884, mean_eps: 0.100000\n","      42842/2000000000: episode: 1182, duration: 4.716s, episode steps:  40, steps per second:   8, episode reward: 102.900, mean reward:  2.573 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 63.054531, mean_q: 12.733573, mean_eps: 0.100000\n","      42882/2000000000: episode: 1183, duration: 4.465s, episode steps:  40, steps per second:   9, episode reward: -32.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.810810, mean_q: 12.599596, mean_eps: 0.100000\n","      42921/2000000000: episode: 1184, duration: 4.354s, episode steps:  39, steps per second:   9, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 65.126201, mean_q: 13.271964, mean_eps: 0.100000\n","      42961/2000000000: episode: 1185, duration: 4.665s, episode steps:  40, steps per second:   9, episode reward: -11.300, mean reward: -0.282 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 62.669306, mean_q: 12.480239, mean_eps: 0.100000\n","      43001/2000000000: episode: 1186, duration: 4.475s, episode steps:  40, steps per second:   9, episode reward: -12.000, mean reward: -0.300 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.294420, mean_q: 12.923615, mean_eps: 0.100000\n","      43041/2000000000: episode: 1187, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: -25.800, mean reward: -0.645 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.439474, mean_q: 13.169569, mean_eps: 0.100000\n","      43081/2000000000: episode: 1188, duration: 5.098s, episode steps:  40, steps per second:   8, episode reward: 50.400, mean reward:  1.260 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.118835, mean_q: 12.703568, mean_eps: 0.100000\n","      43118/2000000000: episode: 1189, duration: 4.514s, episode steps:  37, steps per second:   8, episode reward: -88.700, mean reward: -2.397 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 64.994004, mean_q: 12.580334, mean_eps: 0.100000\n","      43158/2000000000: episode: 1190, duration: 4.744s, episode steps:  40, steps per second:   8, episode reward: 47.500, mean reward:  1.187 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 61.205731, mean_q: 12.921976, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      43198/2000000000: episode: 1191, duration: 4.656s, episode steps:  40, steps per second:   9, episode reward: 27.400, mean reward:  0.685 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.287386, mean_q: 12.797218, mean_eps: 0.100000\n","      43238/2000000000: episode: 1192, duration: 4.950s, episode steps:  40, steps per second:   8, episode reward: 99.500, mean reward:  2.487 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.251287, mean_q: 12.805773, mean_eps: 0.100000\n","      43271/2000000000: episode: 1193, duration: 3.968s, episode steps:  33, steps per second:   8, episode reward: -50.100, mean reward: -1.518 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 65.493658, mean_q: 13.401594, mean_eps: 0.100000\n","      43305/2000000000: episode: 1194, duration: 4.156s, episode steps:  34, steps per second:   8, episode reward: -103.800, mean reward: -3.053 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 64.542503, mean_q: 13.206414, mean_eps: 0.100000\n","      43345/2000000000: episode: 1195, duration: 4.599s, episode steps:  40, steps per second:   9, episode reward: 39.400, mean reward:  0.985 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.147529, mean_q: 13.272554, mean_eps: 0.100000\n","      43385/2000000000: episode: 1196, duration: 4.653s, episode steps:  40, steps per second:   9, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.379725, mean_q: 12.737500, mean_eps: 0.100000\n","      43425/2000000000: episode: 1197, duration: 4.723s, episode steps:  40, steps per second:   8, episode reward: 10.900, mean reward:  0.273 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.670015, mean_q: 12.755153, mean_eps: 0.100000\n","      43458/2000000000: episode: 1198, duration: 3.867s, episode steps:  33, steps per second:   9, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 63.434660, mean_q: 13.119950, mean_eps: 0.100000\n","      43498/2000000000: episode: 1199, duration: 4.650s, episode steps:  40, steps per second:   9, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.532364, mean_q: 13.241140, mean_eps: 0.100000\n","      43538/2000000000: episode: 1200, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: 98.100, mean reward:  2.452 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.533073, mean_q: 12.894696, mean_eps: 0.100000\n","      43578/2000000000: episode: 1201, duration: 4.766s, episode steps:  40, steps per second:   8, episode reward: 89.400, mean reward:  2.235 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.091351, mean_q: 12.531359, mean_eps: 0.100000\n","      43613/2000000000: episode: 1202, duration: 4.102s, episode steps:  35, steps per second:   9, episode reward: 46.500, mean reward:  1.329 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 65.728361, mean_q: 13.694415, mean_eps: 0.100000\n","      43653/2000000000: episode: 1203, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: -147.200, mean reward: -3.680 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.842682, mean_q: 12.581726, mean_eps: 0.100000\n","      43688/2000000000: episode: 1204, duration: 4.200s, episode steps:  35, steps per second:   8, episode reward: -16.900, mean reward: -0.483 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 62.920260, mean_q: 13.330924, mean_eps: 0.100000\n","      43728/2000000000: episode: 1205, duration: 4.599s, episode steps:  40, steps per second:   9, episode reward: -99.700, mean reward: -2.493 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.736991, mean_q: 12.465243, mean_eps: 0.100000\n","      43768/2000000000: episode: 1206, duration: 4.719s, episode steps:  40, steps per second:   8, episode reward: -5.800, mean reward: -0.145 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.732252, mean_q: 12.954472, mean_eps: 0.100000\n","      43808/2000000000: episode: 1207, duration: 4.732s, episode steps:  40, steps per second:   8, episode reward: -70.300, mean reward: -1.757 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 58.965633, mean_q: 12.986352, mean_eps: 0.100000\n","      43842/2000000000: episode: 1208, duration: 4.186s, episode steps:  34, steps per second:   8, episode reward: 192.000, mean reward:  5.647 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 65.287539, mean_q: 12.792388, mean_eps: 0.100000\n","      43882/2000000000: episode: 1209, duration: 4.868s, episode steps:  40, steps per second:   8, episode reward: -30.500, mean reward: -0.763 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.518937, mean_q: 12.638628, mean_eps: 0.100000\n","      43922/2000000000: episode: 1210, duration: 5.081s, episode steps:  40, steps per second:   8, episode reward: -101.900, mean reward: -2.547 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 65.017468, mean_q: 12.730055, mean_eps: 0.100000\n","      43962/2000000000: episode: 1211, duration: 5.852s, episode steps:  40, steps per second:   7, episode reward: -10.400, mean reward: -0.260 [-20.000, 18.000], mean action: 1.675 [0.000, 2.000],  loss: 61.808053, mean_q: 12.616285, mean_eps: 0.100000\n","      44002/2000000000: episode: 1212, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward: -1.500, mean reward: -0.038 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.435881, mean_q: 12.871441, mean_eps: 0.100000\n","      44033/2000000000: episode: 1213, duration: 3.862s, episode steps:  31, steps per second:   8, episode reward: 59.000, mean reward:  1.903 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 69.376226, mean_q: 13.293984, mean_eps: 0.100000\n","      44073/2000000000: episode: 1214, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: -23.300, mean reward: -0.582 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.857941, mean_q: 13.485803, mean_eps: 0.100000\n","      44113/2000000000: episode: 1215, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward: -70.100, mean reward: -1.752 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 69.905796, mean_q: 13.117464, mean_eps: 0.100000\n","      44153/2000000000: episode: 1216, duration: 4.745s, episode steps:  40, steps per second:   8, episode reward: -53.600, mean reward: -1.340 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 72.591224, mean_q: 12.533568, mean_eps: 0.100000\n","      44193/2000000000: episode: 1217, duration: 5.375s, episode steps:  40, steps per second:   7, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.020387, mean_q: 13.037302, mean_eps: 0.100000\n","      44233/2000000000: episode: 1218, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.326139, mean_q: 13.650442, mean_eps: 0.100000\n","      44266/2000000000: episode: 1219, duration: 4.137s, episode steps:  33, steps per second:   8, episode reward: 56.200, mean reward:  1.703 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 62.718963, mean_q: 12.487554, mean_eps: 0.100000\n","      44301/2000000000: episode: 1220, duration: 4.294s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 68.159571, mean_q: 13.272548, mean_eps: 0.100000\n","      44336/2000000000: episode: 1221, duration: 4.154s, episode steps:  35, steps per second:   8, episode reward: 65.700, mean reward:  1.877 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 65.886564, mean_q: 13.191354, mean_eps: 0.100000\n","      44376/2000000000: episode: 1222, duration: 5.086s, episode steps:  40, steps per second:   8, episode reward: 73.200, mean reward:  1.830 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.720497, mean_q: 13.028299, mean_eps: 0.100000\n","      44416/2000000000: episode: 1223, duration: 4.866s, episode steps:  40, steps per second:   8, episode reward: -18.100, mean reward: -0.452 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 70.793091, mean_q: 12.850234, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      44452/2000000000: episode: 1224, duration: 4.391s, episode steps:  36, steps per second:   8, episode reward: 34.200, mean reward:  0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.366731, mean_q: 12.526249, mean_eps: 0.100000\n","      44492/2000000000: episode: 1225, duration: 4.251s, episode steps:  40, steps per second:   9, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.268696, mean_q: 12.344157, mean_eps: 0.100000\n","      44532/2000000000: episode: 1226, duration: 4.338s, episode steps:  40, steps per second:   9, episode reward: 107.400, mean reward:  2.685 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 62.603834, mean_q: 12.751988, mean_eps: 0.100000\n","      44572/2000000000: episode: 1227, duration: 4.513s, episode steps:  40, steps per second:   9, episode reward: 42.600, mean reward:  1.065 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.913168, mean_q: 12.724718, mean_eps: 0.100000\n","      44604/2000000000: episode: 1228, duration: 3.811s, episode steps:  32, steps per second:   8, episode reward: -13.200, mean reward: -0.412 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 64.107685, mean_q: 13.137037, mean_eps: 0.100000\n","      44644/2000000000: episode: 1229, duration: 4.849s, episode steps:  40, steps per second:   8, episode reward: 70.500, mean reward:  1.762 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.546540, mean_q: 13.127030, mean_eps: 0.100000\n","      44682/2000000000: episode: 1230, duration: 4.743s, episode steps:  38, steps per second:   8, episode reward: 53.400, mean reward:  1.405 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 63.344721, mean_q: 13.237556, mean_eps: 0.100000\n","      44719/2000000000: episode: 1231, duration: 4.570s, episode steps:  37, steps per second:   8, episode reward: 83.200, mean reward:  2.249 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 65.816646, mean_q: 13.831093, mean_eps: 0.100000\n","      44759/2000000000: episode: 1232, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: 36.300, mean reward:  0.907 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.217326, mean_q: 12.951279, mean_eps: 0.100000\n","      44797/2000000000: episode: 1233, duration: 4.924s, episode steps:  38, steps per second:   8, episode reward: -49.000, mean reward: -1.289 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 61.561588, mean_q: 12.549544, mean_eps: 0.100000\n","      44837/2000000000: episode: 1234, duration: 5.174s, episode steps:  40, steps per second:   8, episode reward: -48.000, mean reward: -1.200 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.430827, mean_q: 13.122486, mean_eps: 0.100000\n","      44877/2000000000: episode: 1235, duration: 5.194s, episode steps:  40, steps per second:   8, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.594598, mean_q: 12.416213, mean_eps: 0.100000\n","      44914/2000000000: episode: 1236, duration: 4.750s, episode steps:  37, steps per second:   8, episode reward: -96.300, mean reward: -2.603 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 65.761102, mean_q: 13.095993, mean_eps: 0.100000\n","      44952/2000000000: episode: 1237, duration: 4.847s, episode steps:  38, steps per second:   8, episode reward: -92.000, mean reward: -2.421 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 72.020985, mean_q: 13.559949, mean_eps: 0.100000\n","      44992/2000000000: episode: 1238, duration: 5.091s, episode steps:  40, steps per second:   8, episode reward: -15.700, mean reward: -0.392 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 62.571744, mean_q: 12.562103, mean_eps: 0.100000\n","      45028/2000000000: episode: 1239, duration: 5.207s, episode steps:  36, steps per second:   7, episode reward: 10.100, mean reward:  0.281 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 65.029277, mean_q: 13.439682, mean_eps: 0.100000\n","      45068/2000000000: episode: 1240, duration: 4.965s, episode steps:  40, steps per second:   8, episode reward: 17.300, mean reward:  0.433 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 60.785813, mean_q: 13.242787, mean_eps: 0.100000\n","      45102/2000000000: episode: 1241, duration: 4.497s, episode steps:  34, steps per second:   8, episode reward: -3.200, mean reward: -0.094 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 66.871392, mean_q: 12.795571, mean_eps: 0.100000\n","      45142/2000000000: episode: 1242, duration: 4.743s, episode steps:  40, steps per second:   8, episode reward: -57.400, mean reward: -1.435 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.299790, mean_q: 12.825505, mean_eps: 0.100000\n","      45182/2000000000: episode: 1243, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: 161.200, mean reward:  4.030 [-20.000, 19.700], mean action: 1.350 [0.000, 2.000],  loss: 65.577320, mean_q: 13.402931, mean_eps: 0.100000\n","      45222/2000000000: episode: 1244, duration: 4.698s, episode steps:  40, steps per second:   9, episode reward: 174.700, mean reward:  4.368 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.354308, mean_q: 12.963458, mean_eps: 0.100000\n","      45255/2000000000: episode: 1245, duration: 3.865s, episode steps:  33, steps per second:   9, episode reward: 164.300, mean reward:  4.979 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 65.963901, mean_q: 13.030002, mean_eps: 0.100000\n","      45295/2000000000: episode: 1246, duration: 4.683s, episode steps:  40, steps per second:   9, episode reward: -58.200, mean reward: -1.455 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.286148, mean_q: 12.406449, mean_eps: 0.100000\n","      45330/2000000000: episode: 1247, duration: 4.367s, episode steps:  35, steps per second:   8, episode reward: -154.300, mean reward: -4.409 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 65.589553, mean_q: 13.410585, mean_eps: 0.100000\n","      45370/2000000000: episode: 1248, duration: 4.777s, episode steps:  40, steps per second:   8, episode reward: -38.200, mean reward: -0.955 [-20.000, 18.200], mean action: 1.475 [0.000, 2.000],  loss: 64.560717, mean_q: 13.333214, mean_eps: 0.100000\n","      45410/2000000000: episode: 1249, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: -1.700, mean reward: -0.043 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 57.652256, mean_q: 12.461043, mean_eps: 0.100000\n","      45449/2000000000: episode: 1250, duration: 4.814s, episode steps:  39, steps per second:   8, episode reward: -33.700, mean reward: -0.864 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 62.667871, mean_q: 13.498012, mean_eps: 0.100000\n","      45489/2000000000: episode: 1251, duration: 5.378s, episode steps:  40, steps per second:   7, episode reward: 119.500, mean reward:  2.987 [-8.300, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.560850, mean_q: 13.255648, mean_eps: 0.100000\n","      45524/2000000000: episode: 1252, duration: 4.234s, episode steps:  35, steps per second:   8, episode reward: -4.800, mean reward: -0.137 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 66.081483, mean_q: 12.701393, mean_eps: 0.100000\n","      45564/2000000000: episode: 1253, duration: 4.751s, episode steps:  40, steps per second:   8, episode reward: 31.100, mean reward:  0.778 [-20.000, 19.100], mean action: 1.425 [0.000, 2.000],  loss: 62.485663, mean_q: 13.244793, mean_eps: 0.100000\n","      45593/2000000000: episode: 1254, duration: 3.434s, episode steps:  29, steps per second:   8, episode reward: 111.800, mean reward:  3.855 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 64.249191, mean_q: 13.440414, mean_eps: 0.100000\n","      45633/2000000000: episode: 1255, duration: 4.455s, episode steps:  40, steps per second:   9, episode reward: -6.100, mean reward: -0.152 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.362470, mean_q: 13.720395, mean_eps: 0.100000\n","      45673/2000000000: episode: 1256, duration: 4.658s, episode steps:  40, steps per second:   9, episode reward: 30.900, mean reward:  0.772 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 60.697864, mean_q: 13.000309, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      45713/2000000000: episode: 1257, duration: 4.616s, episode steps:  40, steps per second:   9, episode reward: -23.300, mean reward: -0.582 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 60.772927, mean_q: 12.951222, mean_eps: 0.100000\n","      45749/2000000000: episode: 1258, duration: 4.342s, episode steps:  36, steps per second:   8, episode reward: 16.900, mean reward:  0.469 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 67.894854, mean_q: 13.289685, mean_eps: 0.100000\n","      45783/2000000000: episode: 1259, duration: 4.131s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 63.916319, mean_q: 13.876947, mean_eps: 0.100000\n","      45823/2000000000: episode: 1260, duration: 4.785s, episode steps:  40, steps per second:   8, episode reward: 54.300, mean reward:  1.357 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.397521, mean_q: 12.488163, mean_eps: 0.100000\n","      45863/2000000000: episode: 1261, duration: 4.832s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.753477, mean_q: 13.079158, mean_eps: 0.100000\n","      45903/2000000000: episode: 1262, duration: 4.893s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 60.850418, mean_q: 13.077555, mean_eps: 0.100000\n","      45937/2000000000: episode: 1263, duration: 4.200s, episode steps:  34, steps per second:   8, episode reward: -176.500, mean reward: -5.191 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 62.050482, mean_q: 12.951978, mean_eps: 0.100000\n","      45973/2000000000: episode: 1264, duration: 4.136s, episode steps:  36, steps per second:   9, episode reward: 24.000, mean reward:  0.667 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 62.818462, mean_q: 13.214942, mean_eps: 0.100000\n","      46011/2000000000: episode: 1265, duration: 4.396s, episode steps:  38, steps per second:   9, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 59.925739, mean_q: 12.892859, mean_eps: 0.100000\n","      46051/2000000000: episode: 1266, duration: 4.497s, episode steps:  40, steps per second:   9, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.525139, mean_q: 12.866125, mean_eps: 0.100000\n","      46088/2000000000: episode: 1267, duration: 4.941s, episode steps:  37, steps per second:   7, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 61.849097, mean_q: 12.843565, mean_eps: 0.100000\n","      46127/2000000000: episode: 1268, duration: 5.030s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 64.882921, mean_q: 12.608027, mean_eps: 0.100000\n","      46167/2000000000: episode: 1269, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward: -32.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 71.510256, mean_q: 13.296897, mean_eps: 0.100000\n","      46200/2000000000: episode: 1270, duration: 4.343s, episode steps:  33, steps per second:   8, episode reward: 24.000, mean reward:  0.727 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 64.319553, mean_q: 13.082440, mean_eps: 0.100000\n","      46240/2000000000: episode: 1271, duration: 5.007s, episode steps:  40, steps per second:   8, episode reward: -9.600, mean reward: -0.240 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.799371, mean_q: 12.731201, mean_eps: 0.100000\n","      46274/2000000000: episode: 1272, duration: 4.413s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 69.731093, mean_q: 12.834888, mean_eps: 0.100000\n","      46314/2000000000: episode: 1273, duration: 5.120s, episode steps:  40, steps per second:   8, episode reward: 62.200, mean reward:  1.555 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.672141, mean_q: 13.499751, mean_eps: 0.100000\n","      46354/2000000000: episode: 1274, duration: 5.028s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 60.072808, mean_q: 12.704079, mean_eps: 0.100000\n","      46394/2000000000: episode: 1275, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.483564, mean_q: 13.320209, mean_eps: 0.100000\n","      46434/2000000000: episode: 1276, duration: 5.101s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.613849, mean_q: 12.973761, mean_eps: 0.100000\n","      46474/2000000000: episode: 1277, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: 12.100, mean reward:  0.303 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 63.349326, mean_q: 13.353416, mean_eps: 0.100000\n","      46514/2000000000: episode: 1278, duration: 5.113s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.939924, mean_q: 13.563463, mean_eps: 0.100000\n","      46554/2000000000: episode: 1279, duration: 5.302s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 63.908901, mean_q: 12.752714, mean_eps: 0.100000\n","      46594/2000000000: episode: 1280, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: 50.100, mean reward:  1.252 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.425256, mean_q: 12.672721, mean_eps: 0.100000\n","      46629/2000000000: episode: 1281, duration: 4.367s, episode steps:  35, steps per second:   8, episode reward: -116.400, mean reward: -3.326 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.547538, mean_q: 13.075698, mean_eps: 0.100000\n","      46669/2000000000: episode: 1282, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: 24.100, mean reward:  0.603 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.605212, mean_q: 13.513547, mean_eps: 0.100000\n","      46703/2000000000: episode: 1283, duration: 4.209s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 66.437663, mean_q: 12.944848, mean_eps: 0.100000\n","      46743/2000000000: episode: 1284, duration: 4.966s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.050932, mean_q: 13.140105, mean_eps: 0.100000\n","      46782/2000000000: episode: 1285, duration: 4.603s, episode steps:  39, steps per second:   8, episode reward:  3.200, mean reward:  0.082 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 64.272890, mean_q: 12.218143, mean_eps: 0.100000\n","      46822/2000000000: episode: 1286, duration: 4.878s, episode steps:  40, steps per second:   8, episode reward: -93.600, mean reward: -2.340 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.492265, mean_q: 13.120723, mean_eps: 0.100000\n","      46862/2000000000: episode: 1287, duration: 4.666s, episode steps:  40, steps per second:   9, episode reward: 14.300, mean reward:  0.358 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.791050, mean_q: 12.859114, mean_eps: 0.100000\n","      46896/2000000000: episode: 1288, duration: 4.402s, episode steps:  34, steps per second:   8, episode reward: -77.800, mean reward: -2.288 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 67.410949, mean_q: 13.206222, mean_eps: 0.100000\n","      46936/2000000000: episode: 1289, duration: 5.390s, episode steps:  40, steps per second:   7, episode reward: 87.600, mean reward:  2.190 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.482268, mean_q: 13.081689, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      46970/2000000000: episode: 1290, duration: 4.472s, episode steps:  34, steps per second:   8, episode reward: -52.300, mean reward: -1.538 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 65.289375, mean_q: 12.983322, mean_eps: 0.100000\n","      47010/2000000000: episode: 1291, duration: 5.028s, episode steps:  40, steps per second:   8, episode reward: -3.800, mean reward: -0.095 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 57.717289, mean_q: 12.792865, mean_eps: 0.100000\n","      47047/2000000000: episode: 1292, duration: 5.010s, episode steps:  37, steps per second:   7, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 61.091177, mean_q: 12.755345, mean_eps: 0.100000\n","      47081/2000000000: episode: 1293, duration: 4.516s, episode steps:  34, steps per second:   8, episode reward: -53.300, mean reward: -1.568 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 65.661746, mean_q: 13.238016, mean_eps: 0.100000\n","      47118/2000000000: episode: 1294, duration: 4.543s, episode steps:  37, steps per second:   8, episode reward: 89.000, mean reward:  2.405 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 62.126413, mean_q: 13.248002, mean_eps: 0.100000\n","      47158/2000000000: episode: 1295, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: -20.400, mean reward: -0.510 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 62.593823, mean_q: 12.866633, mean_eps: 0.100000\n","      47198/2000000000: episode: 1296, duration: 4.889s, episode steps:  40, steps per second:   8, episode reward: -112.800, mean reward: -2.820 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.908466, mean_q: 12.826797, mean_eps: 0.100000\n","      47235/2000000000: episode: 1297, duration: 4.732s, episode steps:  37, steps per second:   8, episode reward: 85.900, mean reward:  2.322 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 62.999625, mean_q: 13.017514, mean_eps: 0.100000\n","      47272/2000000000: episode: 1298, duration: 4.628s, episode steps:  37, steps per second:   8, episode reward: -66.500, mean reward: -1.797 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 66.272354, mean_q: 13.020417, mean_eps: 0.100000\n","      47305/2000000000: episode: 1299, duration: 4.107s, episode steps:  33, steps per second:   8, episode reward: -134.000, mean reward: -4.061 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 65.167905, mean_q: 13.815561, mean_eps: 0.100000\n","      47345/2000000000: episode: 1300, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: -41.800, mean reward: -1.045 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.719190, mean_q: 13.326640, mean_eps: 0.100000\n","      47385/2000000000: episode: 1301, duration: 4.965s, episode steps:  40, steps per second:   8, episode reward: 72.300, mean reward:  1.807 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 61.375981, mean_q: 12.619952, mean_eps: 0.100000\n","      47425/2000000000: episode: 1302, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward: -122.400, mean reward: -3.060 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.539741, mean_q: 13.302640, mean_eps: 0.100000\n","      47465/2000000000: episode: 1303, duration: 5.361s, episode steps:  40, steps per second:   7, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.064268, mean_q: 13.274755, mean_eps: 0.100000\n","      47503/2000000000: episode: 1304, duration: 5.585s, episode steps:  38, steps per second:   7, episode reward: -60.300, mean reward: -1.587 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 68.288770, mean_q: 13.181626, mean_eps: 0.100000\n","      47543/2000000000: episode: 1305, duration: 5.322s, episode steps:  40, steps per second:   8, episode reward: 17.300, mean reward:  0.433 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 67.132609, mean_q: 13.241289, mean_eps: 0.100000\n","      47583/2000000000: episode: 1306, duration: 5.116s, episode steps:  40, steps per second:   8, episode reward: 87.200, mean reward:  2.180 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 59.817229, mean_q: 13.285582, mean_eps: 0.100000\n","      47623/2000000000: episode: 1307, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward: -32.600, mean reward: -0.815 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.124184, mean_q: 13.103018, mean_eps: 0.100000\n","      47663/2000000000: episode: 1308, duration: 5.366s, episode steps:  40, steps per second:   7, episode reward: -25.900, mean reward: -0.648 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.235297, mean_q: 13.383937, mean_eps: 0.100000\n","      47697/2000000000: episode: 1309, duration: 4.569s, episode steps:  34, steps per second:   7, episode reward: 69.500, mean reward:  2.044 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 59.853279, mean_q: 12.828575, mean_eps: 0.100000\n","      47737/2000000000: episode: 1310, duration: 5.371s, episode steps:  40, steps per second:   7, episode reward: 101.400, mean reward:  2.535 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.859810, mean_q: 13.415363, mean_eps: 0.100000\n","      47774/2000000000: episode: 1311, duration: 4.980s, episode steps:  37, steps per second:   7, episode reward: 20.900, mean reward:  0.565 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 64.516642, mean_q: 12.582110, mean_eps: 0.100000\n","      47814/2000000000: episode: 1312, duration: 5.327s, episode steps:  40, steps per second:   8, episode reward: -39.700, mean reward: -0.992 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.821257, mean_q: 12.572569, mean_eps: 0.100000\n","      47849/2000000000: episode: 1313, duration: 5.107s, episode steps:  35, steps per second:   7, episode reward: 110.600, mean reward:  3.160 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 61.677698, mean_q: 12.839120, mean_eps: 0.100000\n","      47889/2000000000: episode: 1314, duration: 5.305s, episode steps:  40, steps per second:   8, episode reward: 92.900, mean reward:  2.322 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 61.212611, mean_q: 12.605533, mean_eps: 0.100000\n","      47929/2000000000: episode: 1315, duration: 5.156s, episode steps:  40, steps per second:   8, episode reward: -91.800, mean reward: -2.295 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.624863, mean_q: 12.397872, mean_eps: 0.100000\n","      47969/2000000000: episode: 1316, duration: 5.211s, episode steps:  40, steps per second:   8, episode reward: -19.000, mean reward: -0.475 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.028377, mean_q: 12.762038, mean_eps: 0.100000\n","      47999/2000000000: episode: 1317, duration: 4.015s, episode steps:  30, steps per second:   7, episode reward: -57.600, mean reward: -1.920 [-20.000, 18.400], mean action: 0.933 [0.000, 2.000],  loss: 62.193157, mean_q: 13.090899, mean_eps: 0.100000\n","      48031/2000000000: episode: 1318, duration: 4.272s, episode steps:  32, steps per second:   7, episode reward: -11.400, mean reward: -0.356 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 69.252024, mean_q: 13.884565, mean_eps: 0.100000\n","      48068/2000000000: episode: 1319, duration: 4.931s, episode steps:  37, steps per second:   8, episode reward: -143.800, mean reward: -3.886 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 66.913586, mean_q: 13.446426, mean_eps: 0.100000\n","      48108/2000000000: episode: 1320, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: -11.000, mean reward: -0.275 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 62.867941, mean_q: 12.433602, mean_eps: 0.100000\n","      48148/2000000000: episode: 1321, duration: 5.539s, episode steps:  40, steps per second:   7, episode reward: -24.400, mean reward: -0.610 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.697710, mean_q: 12.640227, mean_eps: 0.100000\n","      48186/2000000000: episode: 1322, duration: 5.344s, episode steps:  38, steps per second:   7, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 72.691984, mean_q: 12.790431, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      48226/2000000000: episode: 1323, duration: 5.508s, episode steps:  40, steps per second:   7, episode reward:  6.000, mean reward:  0.150 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 62.866831, mean_q: 12.671768, mean_eps: 0.100000\n","      48258/2000000000: episode: 1324, duration: 4.582s, episode steps:  32, steps per second:   7, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 68.554822, mean_q: 12.874931, mean_eps: 0.100000\n","      48298/2000000000: episode: 1325, duration: 5.396s, episode steps:  40, steps per second:   7, episode reward: 38.200, mean reward:  0.955 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.341267, mean_q: 13.275503, mean_eps: 0.100000\n","      48338/2000000000: episode: 1326, duration: 5.378s, episode steps:  40, steps per second:   7, episode reward: -37.200, mean reward: -0.930 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.315955, mean_q: 13.378541, mean_eps: 0.100000\n","      48378/2000000000: episode: 1327, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: 57.000, mean reward:  1.425 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.992478, mean_q: 13.286254, mean_eps: 0.100000\n","      48418/2000000000: episode: 1328, duration: 5.656s, episode steps:  40, steps per second:   7, episode reward: -23.600, mean reward: -0.590 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.543851, mean_q: 12.900208, mean_eps: 0.100000\n","      48458/2000000000: episode: 1329, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: 73.400, mean reward:  1.835 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.800306, mean_q: 12.820219, mean_eps: 0.100000\n","      48498/2000000000: episode: 1330, duration: 6.011s, episode steps:  40, steps per second:   7, episode reward: -86.500, mean reward: -2.162 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.051437, mean_q: 12.396310, mean_eps: 0.100000\n","      48538/2000000000: episode: 1331, duration: 5.169s, episode steps:  40, steps per second:   8, episode reward: 100.000, mean reward:  2.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.776495, mean_q: 13.055242, mean_eps: 0.100000\n","      48578/2000000000: episode: 1332, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: -23.600, mean reward: -0.590 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 60.997483, mean_q: 12.357182, mean_eps: 0.100000\n","      48618/2000000000: episode: 1333, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: 82.100, mean reward:  2.053 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 71.840909, mean_q: 12.208764, mean_eps: 0.100000\n","      48658/2000000000: episode: 1334, duration: 5.489s, episode steps:  40, steps per second:   7, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.775201, mean_q: 13.022201, mean_eps: 0.100000\n","      48698/2000000000: episode: 1335, duration: 5.324s, episode steps:  40, steps per second:   8, episode reward: 59.000, mean reward:  1.475 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.528297, mean_q: 13.246890, mean_eps: 0.100000\n","      48738/2000000000: episode: 1336, duration: 5.360s, episode steps:  40, steps per second:   7, episode reward: -73.100, mean reward: -1.828 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.969934, mean_q: 12.905919, mean_eps: 0.100000\n","      48778/2000000000: episode: 1337, duration: 5.684s, episode steps:  40, steps per second:   7, episode reward: -41.200, mean reward: -1.030 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 64.864204, mean_q: 12.510676, mean_eps: 0.100000\n","      48818/2000000000: episode: 1338, duration: 5.882s, episode steps:  40, steps per second:   7, episode reward: 146.400, mean reward:  3.660 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 61.907021, mean_q: 12.752768, mean_eps: 0.100000\n","      48858/2000000000: episode: 1339, duration: 5.673s, episode steps:  40, steps per second:   7, episode reward: -103.800, mean reward: -2.595 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.577445, mean_q: 12.979462, mean_eps: 0.100000\n","      48898/2000000000: episode: 1340, duration: 5.278s, episode steps:  40, steps per second:   8, episode reward: -10.300, mean reward: -0.258 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.391266, mean_q: 12.813393, mean_eps: 0.100000\n","      48938/2000000000: episode: 1341, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward: -111.900, mean reward: -2.797 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.531878, mean_q: 13.420047, mean_eps: 0.100000\n","      48978/2000000000: episode: 1342, duration: 5.320s, episode steps:  40, steps per second:   8, episode reward: -18.100, mean reward: -0.452 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.750278, mean_q: 12.658499, mean_eps: 0.100000\n","      49018/2000000000: episode: 1343, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: 104.100, mean reward:  2.602 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.733980, mean_q: 12.834002, mean_eps: 0.100000\n","      49058/2000000000: episode: 1344, duration: 5.456s, episode steps:  40, steps per second:   7, episode reward: 65.200, mean reward:  1.630 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 62.952345, mean_q: 12.602335, mean_eps: 0.100000\n","      49090/2000000000: episode: 1345, duration: 4.297s, episode steps:  32, steps per second:   7, episode reward: 93.500, mean reward:  2.922 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 65.356530, mean_q: 13.378962, mean_eps: 0.100000\n","      49129/2000000000: episode: 1346, duration: 5.263s, episode steps:  39, steps per second:   7, episode reward: 72.700, mean reward:  1.864 [-20.000, 18.400], mean action: 1.205 [0.000, 2.000],  loss: 64.609375, mean_q: 12.855510, mean_eps: 0.100000\n","      49169/2000000000: episode: 1347, duration: 5.503s, episode steps:  40, steps per second:   7, episode reward: -45.700, mean reward: -1.143 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.171387, mean_q: 13.107338, mean_eps: 0.100000\n","      49209/2000000000: episode: 1348, duration: 5.560s, episode steps:  40, steps per second:   7, episode reward: 79.400, mean reward:  1.985 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.644788, mean_q: 13.240002, mean_eps: 0.100000\n","      49249/2000000000: episode: 1349, duration: 5.798s, episode steps:  40, steps per second:   7, episode reward: -74.200, mean reward: -1.855 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.831093, mean_q: 13.218367, mean_eps: 0.100000\n","      49283/2000000000: episode: 1350, duration: 4.701s, episode steps:  34, steps per second:   7, episode reward: 99.700, mean reward:  2.932 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 63.161878, mean_q: 13.363477, mean_eps: 0.100000\n","      49323/2000000000: episode: 1351, duration: 5.096s, episode steps:  40, steps per second:   8, episode reward: -64.900, mean reward: -1.622 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 62.401172, mean_q: 13.407142, mean_eps: 0.100000\n","      49363/2000000000: episode: 1352, duration: 5.154s, episode steps:  40, steps per second:   8, episode reward: 78.600, mean reward:  1.965 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.695972, mean_q: 12.652898, mean_eps: 0.100000\n","      49398/2000000000: episode: 1353, duration: 4.484s, episode steps:  35, steps per second:   8, episode reward: 129.900, mean reward:  3.711 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 67.130866, mean_q: 13.718356, mean_eps: 0.100000\n","      49438/2000000000: episode: 1354, duration: 5.115s, episode steps:  40, steps per second:   8, episode reward: -15.300, mean reward: -0.383 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 58.882200, mean_q: 13.002097, mean_eps: 0.100000\n","      49478/2000000000: episode: 1355, duration: 5.286s, episode steps:  40, steps per second:   8, episode reward: -37.100, mean reward: -0.927 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.010979, mean_q: 12.929345, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      49518/2000000000: episode: 1356, duration: 5.118s, episode steps:  40, steps per second:   8, episode reward: 148.000, mean reward:  3.700 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.680232, mean_q: 12.718704, mean_eps: 0.100000\n","      49550/2000000000: episode: 1357, duration: 4.376s, episode steps:  32, steps per second:   7, episode reward:  1.900, mean reward:  0.059 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 64.603147, mean_q: 13.662828, mean_eps: 0.100000\n","      49590/2000000000: episode: 1358, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward: 67.200, mean reward:  1.680 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.287971, mean_q: 12.579152, mean_eps: 0.100000\n","      49630/2000000000: episode: 1359, duration: 5.424s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 60.860781, mean_q: 12.769171, mean_eps: 0.100000\n","      49665/2000000000: episode: 1360, duration: 4.743s, episode steps:  35, steps per second:   7, episode reward: -62.100, mean reward: -1.774 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 60.368089, mean_q: 13.345526, mean_eps: 0.100000\n","      49705/2000000000: episode: 1361, duration: 5.512s, episode steps:  40, steps per second:   7, episode reward: -109.400, mean reward: -2.735 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 62.242581, mean_q: 12.635090, mean_eps: 0.100000\n","      49741/2000000000: episode: 1362, duration: 4.507s, episode steps:  36, steps per second:   8, episode reward: -72.700, mean reward: -2.019 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 56.951605, mean_q: 13.169598, mean_eps: 0.100000\n","      49781/2000000000: episode: 1363, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward:  2.400, mean reward:  0.060 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.815954, mean_q: 13.048117, mean_eps: 0.100000\n","      49821/2000000000: episode: 1364, duration: 5.449s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 61.965186, mean_q: 13.460369, mean_eps: 0.100000\n","      49861/2000000000: episode: 1365, duration: 5.289s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.454056, mean_q: 13.416076, mean_eps: 0.100000\n","      49901/2000000000: episode: 1366, duration: 5.116s, episode steps:  40, steps per second:   8, episode reward: 55.000, mean reward:  1.375 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 61.581466, mean_q: 12.498869, mean_eps: 0.100000\n","      49939/2000000000: episode: 1367, duration: 5.170s, episode steps:  38, steps per second:   7, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.368 [0.000, 2.000],  loss: 64.392406, mean_q: 12.996296, mean_eps: 0.100000\n","      49979/2000000000: episode: 1368, duration: 5.210s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.993759, mean_q: 13.270359, mean_eps: 0.100000\n","      50019/2000000000: episode: 1369, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.351394, mean_q: 12.954107, mean_eps: 0.100000\n","      50052/2000000000: episode: 1370, duration: 3.911s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 77.957832, mean_q: 14.784597, mean_eps: 0.100000\n","      50092/2000000000: episode: 1371, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.770392, mean_q: 14.433838, mean_eps: 0.100000\n","      50132/2000000000: episode: 1372, duration: 4.926s, episode steps:  40, steps per second:   8, episode reward: -33.500, mean reward: -0.837 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 77.477449, mean_q: 14.410127, mean_eps: 0.100000\n","      50172/2000000000: episode: 1373, duration: 4.802s, episode steps:  40, steps per second:   8, episode reward: 25.900, mean reward:  0.648 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.256768, mean_q: 14.353099, mean_eps: 0.100000\n","      50210/2000000000: episode: 1374, duration: 4.652s, episode steps:  38, steps per second:   8, episode reward: 75.100, mean reward:  1.976 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 68.745472, mean_q: 15.590559, mean_eps: 0.100000\n","      50250/2000000000: episode: 1375, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: 82.000, mean reward:  2.050 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 67.623189, mean_q: 14.844801, mean_eps: 0.100000\n","      50290/2000000000: episode: 1376, duration: 5.003s, episode steps:  40, steps per second:   8, episode reward:  3.600, mean reward:  0.090 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.580164, mean_q: 14.917211, mean_eps: 0.100000\n","      50330/2000000000: episode: 1377, duration: 4.520s, episode steps:  40, steps per second:   9, episode reward: 142.100, mean reward:  3.552 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.266932, mean_q: 14.520206, mean_eps: 0.100000\n","      50370/2000000000: episode: 1378, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: -12.000, mean reward: -0.300 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 70.317610, mean_q: 14.353059, mean_eps: 0.100000\n","      50410/2000000000: episode: 1379, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: 125.500, mean reward:  3.138 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.942204, mean_q: 15.259627, mean_eps: 0.100000\n","      50442/2000000000: episode: 1380, duration: 4.042s, episode steps:  32, steps per second:   8, episode reward: -104.100, mean reward: -3.253 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 75.466396, mean_q: 15.009552, mean_eps: 0.100000\n","      50482/2000000000: episode: 1381, duration: 4.848s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.583549, mean_q: 14.930835, mean_eps: 0.100000\n","      50522/2000000000: episode: 1382, duration: 4.695s, episode steps:  40, steps per second:   9, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.028948, mean_q: 14.939957, mean_eps: 0.100000\n","      50562/2000000000: episode: 1383, duration: 4.663s, episode steps:  40, steps per second:   9, episode reward: -27.600, mean reward: -0.690 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.849772, mean_q: 14.694093, mean_eps: 0.100000\n","      50602/2000000000: episode: 1384, duration: 4.886s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.597075, mean_q: 14.449564, mean_eps: 0.100000\n","      50642/2000000000: episode: 1385, duration: 4.828s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.554198, mean_q: 14.287351, mean_eps: 0.100000\n","      50682/2000000000: episode: 1386, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: -29.900, mean reward: -0.748 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.909005, mean_q: 15.063361, mean_eps: 0.100000\n","      50717/2000000000: episode: 1387, duration: 4.394s, episode steps:  35, steps per second:   8, episode reward: -0.900, mean reward: -0.026 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 65.882332, mean_q: 14.502581, mean_eps: 0.100000\n","      50757/2000000000: episode: 1388, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: -126.100, mean reward: -3.153 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.507933, mean_q: 14.014377, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      50795/2000000000: episode: 1389, duration: 4.616s, episode steps:  38, steps per second:   8, episode reward: -16.800, mean reward: -0.442 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 73.636567, mean_q: 14.260309, mean_eps: 0.100000\n","      50827/2000000000: episode: 1390, duration: 3.938s, episode steps:  32, steps per second:   8, episode reward: 54.100, mean reward:  1.691 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 65.728710, mean_q: 15.401776, mean_eps: 0.100000\n","      50867/2000000000: episode: 1391, duration: 5.167s, episode steps:  40, steps per second:   8, episode reward: 78.400, mean reward:  1.960 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.143467, mean_q: 14.425857, mean_eps: 0.100000\n","      50907/2000000000: episode: 1392, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: -118.700, mean reward: -2.967 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 63.481796, mean_q: 15.075420, mean_eps: 0.100000\n","      50944/2000000000: episode: 1393, duration: 4.852s, episode steps:  37, steps per second:   8, episode reward: 170.000, mean reward:  4.595 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.389388, mean_q: 15.102496, mean_eps: 0.100000\n","      50984/2000000000: episode: 1394, duration: 4.847s, episode steps:  40, steps per second:   8, episode reward: -87.800, mean reward: -2.195 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.442799, mean_q: 14.868050, mean_eps: 0.100000\n","      51024/2000000000: episode: 1395, duration: 4.899s, episode steps:  40, steps per second:   8, episode reward: -154.000, mean reward: -3.850 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.661863, mean_q: 14.405181, mean_eps: 0.100000\n","      51064/2000000000: episode: 1396, duration: 4.584s, episode steps:  40, steps per second:   9, episode reward: -50.600, mean reward: -1.265 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.475568, mean_q: 14.271796, mean_eps: 0.100000\n","      51104/2000000000: episode: 1397, duration: 4.761s, episode steps:  40, steps per second:   8, episode reward: -78.400, mean reward: -1.960 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.351930, mean_q: 15.061359, mean_eps: 0.100000\n","      51141/2000000000: episode: 1398, duration: 4.544s, episode steps:  37, steps per second:   8, episode reward: 107.800, mean reward:  2.914 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 69.350976, mean_q: 14.577447, mean_eps: 0.100000\n","      51181/2000000000: episode: 1399, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.626000, mean_q: 15.440911, mean_eps: 0.100000\n","      51217/2000000000: episode: 1400, duration: 4.418s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 68.234637, mean_q: 14.578228, mean_eps: 0.100000\n","      51257/2000000000: episode: 1401, duration: 4.918s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.665093, mean_q: 14.992581, mean_eps: 0.100000\n","      51297/2000000000: episode: 1402, duration: 4.833s, episode steps:  40, steps per second:   8, episode reward: 59.300, mean reward:  1.483 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.385136, mean_q: 14.754765, mean_eps: 0.100000\n","      51337/2000000000: episode: 1403, duration: 4.864s, episode steps:  40, steps per second:   8, episode reward: -103.800, mean reward: -2.595 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.732602, mean_q: 15.214113, mean_eps: 0.100000\n","      51377/2000000000: episode: 1404, duration: 4.986s, episode steps:  40, steps per second:   8, episode reward: -20.500, mean reward: -0.513 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.218838, mean_q: 14.442445, mean_eps: 0.100000\n","      51417/2000000000: episode: 1405, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: -35.700, mean reward: -0.893 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.942947, mean_q: 14.380873, mean_eps: 0.100000\n","      51457/2000000000: episode: 1406, duration: 4.585s, episode steps:  40, steps per second:   9, episode reward: 55.900, mean reward:  1.397 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.636567, mean_q: 14.815818, mean_eps: 0.100000\n","      51497/2000000000: episode: 1407, duration: 4.578s, episode steps:  40, steps per second:   9, episode reward: 61.600, mean reward:  1.540 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.539228, mean_q: 14.434133, mean_eps: 0.100000\n","      51537/2000000000: episode: 1408, duration: 4.650s, episode steps:  40, steps per second:   9, episode reward: 34.400, mean reward:  0.860 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 71.480342, mean_q: 15.174658, mean_eps: 0.100000\n","      51574/2000000000: episode: 1409, duration: 4.220s, episode steps:  37, steps per second:   9, episode reward: 35.500, mean reward:  0.959 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 62.214172, mean_q: 14.668749, mean_eps: 0.100000\n","      51614/2000000000: episode: 1410, duration: 4.454s, episode steps:  40, steps per second:   9, episode reward: 75.400, mean reward:  1.885 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.179991, mean_q: 15.139962, mean_eps: 0.100000\n","      51654/2000000000: episode: 1411, duration: 4.417s, episode steps:  40, steps per second:   9, episode reward: -106.900, mean reward: -2.673 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.907620, mean_q: 13.923510, mean_eps: 0.100000\n","      51687/2000000000: episode: 1412, duration: 3.881s, episode steps:  33, steps per second:   9, episode reward: -38.700, mean reward: -1.173 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 67.408668, mean_q: 15.556926, mean_eps: 0.100000\n","      51727/2000000000: episode: 1413, duration: 4.513s, episode steps:  40, steps per second:   9, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.491360, mean_q: 14.446756, mean_eps: 0.100000\n","      51767/2000000000: episode: 1414, duration: 4.371s, episode steps:  40, steps per second:   9, episode reward: 146.100, mean reward:  3.653 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.795374, mean_q: 14.987198, mean_eps: 0.100000\n","      51807/2000000000: episode: 1415, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: 21.900, mean reward:  0.548 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.354116, mean_q: 14.700834, mean_eps: 0.100000\n","      51847/2000000000: episode: 1416, duration: 4.751s, episode steps:  40, steps per second:   8, episode reward: 12.000, mean reward:  0.300 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.816567, mean_q: 14.241563, mean_eps: 0.100000\n","      51881/2000000000: episode: 1417, duration: 4.242s, episode steps:  34, steps per second:   8, episode reward: 27.000, mean reward:  0.794 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 67.918763, mean_q: 15.111661, mean_eps: 0.100000\n","      51918/2000000000: episode: 1418, duration: 4.507s, episode steps:  37, steps per second:   8, episode reward: -128.800, mean reward: -3.481 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 71.736225, mean_q: 14.914626, mean_eps: 0.100000\n","      51958/2000000000: episode: 1419, duration: 4.769s, episode steps:  40, steps per second:   8, episode reward: -36.600, mean reward: -0.915 [-20.000, 18.000], mean action: 1.700 [0.000, 2.000],  loss: 68.020864, mean_q: 13.984043, mean_eps: 0.100000\n","      51998/2000000000: episode: 1420, duration: 4.803s, episode steps:  40, steps per second:   8, episode reward: 175.300, mean reward:  4.383 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.171381, mean_q: 14.888535, mean_eps: 0.100000\n","      52032/2000000000: episode: 1421, duration: 4.017s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 68.168458, mean_q: 15.101321, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      52072/2000000000: episode: 1422, duration: 4.656s, episode steps:  40, steps per second:   9, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.189749, mean_q: 14.929289, mean_eps: 0.100000\n","      52109/2000000000: episode: 1423, duration: 4.289s, episode steps:  37, steps per second:   9, episode reward: -50.500, mean reward: -1.365 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 67.934814, mean_q: 15.165137, mean_eps: 0.100000\n","      52149/2000000000: episode: 1424, duration: 4.803s, episode steps:  40, steps per second:   8, episode reward:  3.600, mean reward:  0.090 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.699513, mean_q: 14.471482, mean_eps: 0.100000\n","      52189/2000000000: episode: 1425, duration: 4.660s, episode steps:  40, steps per second:   9, episode reward: -57.400, mean reward: -1.435 [-20.000, 18.000], mean action: 1.075 [0.000, 2.000],  loss: 65.543209, mean_q: 14.408205, mean_eps: 0.100000\n","      52229/2000000000: episode: 1426, duration: 4.940s, episode steps:  40, steps per second:   8, episode reward: -1.300, mean reward: -0.032 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.612905, mean_q: 15.174095, mean_eps: 0.100000\n","      52263/2000000000: episode: 1427, duration: 4.096s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 65.150513, mean_q: 14.150247, mean_eps: 0.100000\n","      52303/2000000000: episode: 1428, duration: 4.820s, episode steps:  40, steps per second:   8, episode reward: 14.800, mean reward:  0.370 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 64.296084, mean_q: 15.341289, mean_eps: 0.100000\n","      52333/2000000000: episode: 1429, duration: 3.536s, episode steps:  30, steps per second:   8, episode reward: -45.000, mean reward: -1.500 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 69.250388, mean_q: 14.782804, mean_eps: 0.100000\n","      52373/2000000000: episode: 1430, duration: 4.750s, episode steps:  40, steps per second:   8, episode reward: -119.100, mean reward: -2.977 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.768778, mean_q: 14.783839, mean_eps: 0.100000\n","      52413/2000000000: episode: 1431, duration: 4.947s, episode steps:  40, steps per second:   8, episode reward: 102.600, mean reward:  2.565 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.000292, mean_q: 15.077794, mean_eps: 0.100000\n","      52453/2000000000: episode: 1432, duration: 4.626s, episode steps:  40, steps per second:   9, episode reward: 77.800, mean reward:  1.945 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.837706, mean_q: 15.037345, mean_eps: 0.100000\n","      52493/2000000000: episode: 1433, duration: 4.512s, episode steps:  40, steps per second:   9, episode reward: -31.800, mean reward: -0.795 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.011969, mean_q: 15.006627, mean_eps: 0.100000\n","      52533/2000000000: episode: 1434, duration: 4.389s, episode steps:  40, steps per second:   9, episode reward: -47.000, mean reward: -1.175 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 64.082573, mean_q: 14.536823, mean_eps: 0.100000\n","      52568/2000000000: episode: 1435, duration: 4.132s, episode steps:  35, steps per second:   8, episode reward: 32.200, mean reward:  0.920 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 61.593337, mean_q: 15.134427, mean_eps: 0.100000\n","      52608/2000000000: episode: 1436, duration: 4.877s, episode steps:  40, steps per second:   8, episode reward: 20.200, mean reward:  0.505 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.878062, mean_q: 14.724088, mean_eps: 0.100000\n","      52644/2000000000: episode: 1437, duration: 4.544s, episode steps:  36, steps per second:   8, episode reward: -65.000, mean reward: -1.806 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 68.516254, mean_q: 14.449934, mean_eps: 0.100000\n","      52684/2000000000: episode: 1438, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 93.800, mean reward:  2.345 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.016244, mean_q: 14.389273, mean_eps: 0.100000\n","      52724/2000000000: episode: 1439, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 49.800, mean reward:  1.245 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 71.591303, mean_q: 14.116258, mean_eps: 0.100000\n","      52755/2000000000: episode: 1440, duration: 3.714s, episode steps:  31, steps per second:   8, episode reward: 21.500, mean reward:  0.694 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 71.121306, mean_q: 15.374330, mean_eps: 0.100000\n","      52795/2000000000: episode: 1441, duration: 4.632s, episode steps:  40, steps per second:   9, episode reward: -36.100, mean reward: -0.903 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 60.231364, mean_q: 15.170843, mean_eps: 0.100000\n","      52828/2000000000: episode: 1442, duration: 4.102s, episode steps:  33, steps per second:   8, episode reward: -124.600, mean reward: -3.776 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.196597, mean_q: 15.319026, mean_eps: 0.100000\n","      52863/2000000000: episode: 1443, duration: 4.323s, episode steps:  35, steps per second:   8, episode reward: 130.800, mean reward:  3.737 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.010480, mean_q: 15.276866, mean_eps: 0.100000\n","      52903/2000000000: episode: 1444, duration: 4.736s, episode steps:  40, steps per second:   8, episode reward: 38.900, mean reward:  0.972 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.854580, mean_q: 15.266158, mean_eps: 0.100000\n","      52943/2000000000: episode: 1445, duration: 4.902s, episode steps:  40, steps per second:   8, episode reward: 65.900, mean reward:  1.648 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.265183, mean_q: 14.492376, mean_eps: 0.100000\n","      52983/2000000000: episode: 1446, duration: 4.827s, episode steps:  40, steps per second:   8, episode reward: -44.800, mean reward: -1.120 [-20.000, 19.600], mean action: 1.375 [0.000, 2.000],  loss: 67.665760, mean_q: 14.621291, mean_eps: 0.100000\n","      53022/2000000000: episode: 1447, duration: 4.599s, episode steps:  39, steps per second:   8, episode reward: 66.200, mean reward:  1.697 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 70.948932, mean_q: 15.657516, mean_eps: 0.100000\n","      53055/2000000000: episode: 1448, duration: 3.802s, episode steps:  33, steps per second:   9, episode reward: 26.100, mean reward:  0.791 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 70.747777, mean_q: 15.183334, mean_eps: 0.100000\n","      53095/2000000000: episode: 1449, duration: 4.707s, episode steps:  40, steps per second:   8, episode reward: 65.700, mean reward:  1.643 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.159880, mean_q: 14.700323, mean_eps: 0.100000\n","      53133/2000000000: episode: 1450, duration: 4.522s, episode steps:  38, steps per second:   8, episode reward: -3.300, mean reward: -0.087 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 63.076325, mean_q: 14.582991, mean_eps: 0.100000\n","      53170/2000000000: episode: 1451, duration: 4.434s, episode steps:  37, steps per second:   8, episode reward: 78.800, mean reward:  2.130 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.973003, mean_q: 15.176794, mean_eps: 0.100000\n","      53207/2000000000: episode: 1452, duration: 4.557s, episode steps:  37, steps per second:   8, episode reward: -145.200, mean reward: -3.924 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 64.878964, mean_q: 14.755335, mean_eps: 0.100000\n","      53247/2000000000: episode: 1453, duration: 4.616s, episode steps:  40, steps per second:   9, episode reward: -121.500, mean reward: -3.038 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.075130, mean_q: 14.840412, mean_eps: 0.100000\n","      53287/2000000000: episode: 1454, duration: 4.439s, episode steps:  40, steps per second:   9, episode reward: -26.600, mean reward: -0.665 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.449601, mean_q: 14.897137, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      53320/2000000000: episode: 1455, duration: 3.933s, episode steps:  33, steps per second:   8, episode reward: 108.100, mean reward:  3.276 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 70.653987, mean_q: 15.144149, mean_eps: 0.100000\n","      53354/2000000000: episode: 1456, duration: 3.933s, episode steps:  34, steps per second:   9, episode reward: 40.500, mean reward:  1.191 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 63.250443, mean_q: 15.094939, mean_eps: 0.100000\n","      53394/2000000000: episode: 1457, duration: 4.681s, episode steps:  40, steps per second:   9, episode reward: 86.700, mean reward:  2.167 [-8.700, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.851447, mean_q: 14.437621, mean_eps: 0.100000\n","      53434/2000000000: episode: 1458, duration: 4.699s, episode steps:  40, steps per second:   9, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.930253, mean_q: 15.065245, mean_eps: 0.100000\n","      53474/2000000000: episode: 1459, duration: 4.714s, episode steps:  40, steps per second:   8, episode reward:  0.900, mean reward:  0.023 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.267148, mean_q: 14.781969, mean_eps: 0.100000\n","      53514/2000000000: episode: 1460, duration: 4.668s, episode steps:  40, steps per second:   9, episode reward: -5.700, mean reward: -0.142 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 61.938947, mean_q: 14.492699, mean_eps: 0.100000\n","      53550/2000000000: episode: 1461, duration: 4.194s, episode steps:  36, steps per second:   9, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 63.911522, mean_q: 14.867367, mean_eps: 0.100000\n","      53590/2000000000: episode: 1462, duration: 4.844s, episode steps:  40, steps per second:   8, episode reward: 185.400, mean reward:  4.635 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.146731, mean_q: 14.812797, mean_eps: 0.100000\n","      53630/2000000000: episode: 1463, duration: 4.775s, episode steps:  40, steps per second:   8, episode reward: -45.900, mean reward: -1.148 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.323167, mean_q: 15.176870, mean_eps: 0.100000\n","      53670/2000000000: episode: 1464, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward: -104.800, mean reward: -2.620 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.712301, mean_q: 15.311130, mean_eps: 0.100000\n","      53706/2000000000: episode: 1465, duration: 4.896s, episode steps:  36, steps per second:   7, episode reward: -153.800, mean reward: -4.272 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 67.279401, mean_q: 15.847686, mean_eps: 0.100000\n","      53746/2000000000: episode: 1466, duration: 5.180s, episode steps:  40, steps per second:   8, episode reward: -20.300, mean reward: -0.508 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.670906, mean_q: 14.317288, mean_eps: 0.100000\n","      53782/2000000000: episode: 1467, duration: 4.453s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 64.310653, mean_q: 14.820903, mean_eps: 0.100000\n","      53822/2000000000: episode: 1468, duration: 4.751s, episode steps:  40, steps per second:   8, episode reward: 40.900, mean reward:  1.022 [-20.000, 18.900], mean action: 1.250 [0.000, 2.000],  loss: 68.379112, mean_q: 15.367289, mean_eps: 0.100000\n","      53862/2000000000: episode: 1469, duration: 4.619s, episode steps:  40, steps per second:   9, episode reward: -43.700, mean reward: -1.093 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 60.406165, mean_q: 14.714118, mean_eps: 0.100000\n","      53902/2000000000: episode: 1470, duration: 4.689s, episode steps:  40, steps per second:   9, episode reward: -65.100, mean reward: -1.627 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.299196, mean_q: 14.810472, mean_eps: 0.100000\n","      53942/2000000000: episode: 1471, duration: 4.944s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 61.457614, mean_q: 14.505218, mean_eps: 0.100000\n","      53982/2000000000: episode: 1472, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 105.300, mean reward:  2.633 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.176878, mean_q: 14.113958, mean_eps: 0.100000\n","      54020/2000000000: episode: 1473, duration: 4.482s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 66.217471, mean_q: 14.802532, mean_eps: 0.100000\n","      54060/2000000000: episode: 1474, duration: 4.716s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.299411, mean_q: 15.156336, mean_eps: 0.100000\n","      54091/2000000000: episode: 1475, duration: 3.944s, episode steps:  31, steps per second:   8, episode reward: -46.300, mean reward: -1.494 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 65.175490, mean_q: 15.661227, mean_eps: 0.100000\n","      54131/2000000000: episode: 1476, duration: 4.866s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.274611, mean_q: 14.916903, mean_eps: 0.100000\n","      54171/2000000000: episode: 1477, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 28.000, mean reward:  0.700 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 69.808205, mean_q: 15.236257, mean_eps: 0.100000\n","      54211/2000000000: episode: 1478, duration: 4.970s, episode steps:  40, steps per second:   8, episode reward: -51.000, mean reward: -1.275 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 61.593268, mean_q: 15.271559, mean_eps: 0.100000\n","      54251/2000000000: episode: 1479, duration: 5.293s, episode steps:  40, steps per second:   8, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.236712, mean_q: 14.151673, mean_eps: 0.100000\n","      54291/2000000000: episode: 1480, duration: 5.172s, episode steps:  40, steps per second:   8, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.833941, mean_q: 14.794560, mean_eps: 0.100000\n","      54331/2000000000: episode: 1481, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: -13.900, mean reward: -0.347 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.368233, mean_q: 15.149532, mean_eps: 0.100000\n","      54371/2000000000: episode: 1482, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 39.700, mean reward:  0.993 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.347036, mean_q: 15.154891, mean_eps: 0.100000\n","      54410/2000000000: episode: 1483, duration: 4.781s, episode steps:  39, steps per second:   8, episode reward: -3.200, mean reward: -0.082 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 74.955765, mean_q: 14.656625, mean_eps: 0.100000\n","      54448/2000000000: episode: 1484, duration: 4.778s, episode steps:  38, steps per second:   8, episode reward: -14.400, mean reward: -0.379 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 65.482642, mean_q: 14.486573, mean_eps: 0.100000\n","      54488/2000000000: episode: 1485, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.075998, mean_q: 14.387868, mean_eps: 0.100000\n","      54521/2000000000: episode: 1486, duration: 4.320s, episode steps:  33, steps per second:   8, episode reward: 170.000, mean reward:  5.152 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 66.421751, mean_q: 15.170960, mean_eps: 0.100000\n","      54559/2000000000: episode: 1487, duration: 4.962s, episode steps:  38, steps per second:   8, episode reward: 17.600, mean reward:  0.463 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 71.958679, mean_q: 14.883907, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      54599/2000000000: episode: 1488, duration: 5.230s, episode steps:  40, steps per second:   8, episode reward: -150.000, mean reward: -3.750 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.764051, mean_q: 14.238087, mean_eps: 0.100000\n","      54639/2000000000: episode: 1489, duration: 5.166s, episode steps:  40, steps per second:   8, episode reward: -188.100, mean reward: -4.703 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.394043, mean_q: 14.794572, mean_eps: 0.100000\n","      54679/2000000000: episode: 1490, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: -194.400, mean reward: -4.860 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.543380, mean_q: 14.713274, mean_eps: 0.100000\n","      54719/2000000000: episode: 1491, duration: 5.200s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.884136, mean_q: 14.968338, mean_eps: 0.100000\n","      54751/2000000000: episode: 1492, duration: 4.370s, episode steps:  32, steps per second:   7, episode reward: 115.200, mean reward:  3.600 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 69.740180, mean_q: 14.472254, mean_eps: 0.100000\n","      54783/2000000000: episode: 1493, duration: 4.187s, episode steps:  32, steps per second:   8, episode reward: -13.400, mean reward: -0.419 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 71.011606, mean_q: 15.095034, mean_eps: 0.100000\n","      54818/2000000000: episode: 1494, duration: 4.531s, episode steps:  35, steps per second:   8, episode reward: -10.400, mean reward: -0.297 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 68.843004, mean_q: 14.370228, mean_eps: 0.100000\n","      54854/2000000000: episode: 1495, duration: 4.195s, episode steps:  36, steps per second:   9, episode reward: 192.600, mean reward:  5.350 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 56.426301, mean_q: 15.001643, mean_eps: 0.100000\n","      54894/2000000000: episode: 1496, duration: 4.960s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.771436, mean_q: 14.896737, mean_eps: 0.100000\n","      54931/2000000000: episode: 1497, duration: 4.528s, episode steps:  37, steps per second:   8, episode reward: 77.500, mean reward:  2.095 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 66.010560, mean_q: 15.167266, mean_eps: 0.100000\n","      54971/2000000000: episode: 1498, duration: 4.597s, episode steps:  40, steps per second:   9, episode reward: 69.800, mean reward:  1.745 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 69.826483, mean_q: 14.549633, mean_eps: 0.100000\n","      55011/2000000000: episode: 1499, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward: 76.100, mean reward:  1.903 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.926693, mean_q: 15.340295, mean_eps: 0.100000\n","      55051/2000000000: episode: 1500, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: -39.500, mean reward: -0.987 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.120200, mean_q: 14.731652, mean_eps: 0.100000\n","      55084/2000000000: episode: 1501, duration: 4.065s, episode steps:  33, steps per second:   8, episode reward: 82.300, mean reward:  2.494 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 63.950353, mean_q: 14.592874, mean_eps: 0.100000\n","      55119/2000000000: episode: 1502, duration: 4.194s, episode steps:  35, steps per second:   8, episode reward:  4.500, mean reward:  0.129 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 67.466381, mean_q: 15.189107, mean_eps: 0.100000\n","      55159/2000000000: episode: 1503, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.495702, mean_q: 14.834557, mean_eps: 0.100000\n","      55199/2000000000: episode: 1504, duration: 4.897s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.841825, mean_q: 15.057678, mean_eps: 0.100000\n","      55238/2000000000: episode: 1505, duration: 4.600s, episode steps:  39, steps per second:   8, episode reward: 12.000, mean reward:  0.308 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 68.063848, mean_q: 15.375049, mean_eps: 0.100000\n","      55278/2000000000: episode: 1506, duration: 5.031s, episode steps:  40, steps per second:   8, episode reward: -150.000, mean reward: -3.750 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 61.586476, mean_q: 14.422838, mean_eps: 0.100000\n","      55318/2000000000: episode: 1507, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.987455, mean_q: 14.666452, mean_eps: 0.100000\n","      55358/2000000000: episode: 1508, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 11.700, mean reward:  0.292 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.642075, mean_q: 14.948336, mean_eps: 0.100000\n","      55398/2000000000: episode: 1509, duration: 5.382s, episode steps:  40, steps per second:   7, episode reward: 49.900, mean reward:  1.248 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.547313, mean_q: 15.484256, mean_eps: 0.100000\n","      55436/2000000000: episode: 1510, duration: 5.000s, episode steps:  38, steps per second:   8, episode reward: -100.000, mean reward: -2.632 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 65.500255, mean_q: 14.898180, mean_eps: 0.100000\n","      55471/2000000000: episode: 1511, duration: 4.816s, episode steps:  35, steps per second:   7, episode reward: 143.000, mean reward:  4.086 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 70.605070, mean_q: 16.162921, mean_eps: 0.100000\n","      55507/2000000000: episode: 1512, duration: 4.715s, episode steps:  36, steps per second:   8, episode reward:  9.400, mean reward:  0.261 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 67.136185, mean_q: 14.683832, mean_eps: 0.100000\n","      55547/2000000000: episode: 1513, duration: 5.590s, episode steps:  40, steps per second:   7, episode reward: 49.900, mean reward:  1.248 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.709088, mean_q: 14.964256, mean_eps: 0.100000\n","      55587/2000000000: episode: 1514, duration: 5.556s, episode steps:  40, steps per second:   7, episode reward: -79.500, mean reward: -1.988 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.408291, mean_q: 14.970568, mean_eps: 0.100000\n","      55627/2000000000: episode: 1515, duration: 5.386s, episode steps:  40, steps per second:   7, episode reward: 36.000, mean reward:  0.900 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 57.136690, mean_q: 14.616542, mean_eps: 0.100000\n","      55667/2000000000: episode: 1516, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: 17.800, mean reward:  0.445 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 60.664326, mean_q: 14.515155, mean_eps: 0.100000\n","      55701/2000000000: episode: 1517, duration: 4.527s, episode steps:  34, steps per second:   8, episode reward: 77.400, mean reward:  2.276 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 66.782707, mean_q: 14.588898, mean_eps: 0.100000\n","      55738/2000000000: episode: 1518, duration: 4.971s, episode steps:  37, steps per second:   7, episode reward: 86.400, mean reward:  2.335 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 61.482927, mean_q: 15.208749, mean_eps: 0.100000\n","      55775/2000000000: episode: 1519, duration: 4.905s, episode steps:  37, steps per second:   8, episode reward: 77.000, mean reward:  2.081 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 60.401962, mean_q: 14.934893, mean_eps: 0.100000\n","      55815/2000000000: episode: 1520, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: -109.400, mean reward: -2.735 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.319378, mean_q: 15.065149, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      55855/2000000000: episode: 1521, duration: 5.310s, episode steps:  40, steps per second:   8, episode reward: -41.600, mean reward: -1.040 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 60.381920, mean_q: 15.129795, mean_eps: 0.100000\n","      55895/2000000000: episode: 1522, duration: 5.192s, episode steps:  40, steps per second:   8, episode reward: -64.800, mean reward: -1.620 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.260927, mean_q: 15.236360, mean_eps: 0.100000\n","      55935/2000000000: episode: 1523, duration: 5.796s, episode steps:  40, steps per second:   7, episode reward: -121.700, mean reward: -3.043 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 65.966773, mean_q: 14.126174, mean_eps: 0.100000\n","      55975/2000000000: episode: 1524, duration: 5.553s, episode steps:  40, steps per second:   7, episode reward: -31.500, mean reward: -0.787 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.910600, mean_q: 14.524645, mean_eps: 0.100000\n","      56015/2000000000: episode: 1525, duration: 5.473s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.088108, mean_q: 14.375133, mean_eps: 0.100000\n","      56055/2000000000: episode: 1526, duration: 4.929s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 59.474503, mean_q: 14.756292, mean_eps: 0.100000\n","      56095/2000000000: episode: 1527, duration: 5.613s, episode steps:  40, steps per second:   7, episode reward: -174.000, mean reward: -4.350 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 64.616769, mean_q: 14.519779, mean_eps: 0.100000\n","      56133/2000000000: episode: 1528, duration: 5.146s, episode steps:  38, steps per second:   7, episode reward: -47.100, mean reward: -1.239 [-20.000, 18.000], mean action: 1.053 [0.000, 2.000],  loss: 70.086879, mean_q: 14.712162, mean_eps: 0.100000\n","      56173/2000000000: episode: 1529, duration: 5.452s, episode steps:  40, steps per second:   7, episode reward: -104.500, mean reward: -2.612 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.536539, mean_q: 14.402011, mean_eps: 0.100000\n","      56206/2000000000: episode: 1530, duration: 4.739s, episode steps:  33, steps per second:   7, episode reward: 52.600, mean reward:  1.594 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 58.973925, mean_q: 15.624610, mean_eps: 0.100000\n","      56246/2000000000: episode: 1531, duration: 5.550s, episode steps:  40, steps per second:   7, episode reward: -110.200, mean reward: -2.755 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 63.521539, mean_q: 15.500980, mean_eps: 0.100000\n","      56286/2000000000: episode: 1532, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: -23.800, mean reward: -0.595 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.382392, mean_q: 14.577031, mean_eps: 0.100000\n","      56326/2000000000: episode: 1533, duration: 5.451s, episode steps:  40, steps per second:   7, episode reward: 62.500, mean reward:  1.562 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.092032, mean_q: 14.270138, mean_eps: 0.100000\n","      56366/2000000000: episode: 1534, duration: 5.874s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.792657, mean_q: 15.384995, mean_eps: 0.100000\n","      56405/2000000000: episode: 1535, duration: 5.366s, episode steps:  39, steps per second:   7, episode reward: -10.200, mean reward: -0.262 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 66.292478, mean_q: 14.603103, mean_eps: 0.100000\n","      56445/2000000000: episode: 1536, duration: 5.464s, episode steps:  40, steps per second:   7, episode reward: -97.400, mean reward: -2.435 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.702186, mean_q: 14.783222, mean_eps: 0.100000\n","      56485/2000000000: episode: 1537, duration: 5.627s, episode steps:  40, steps per second:   7, episode reward: 15.200, mean reward:  0.380 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.640370, mean_q: 14.810814, mean_eps: 0.100000\n","      56525/2000000000: episode: 1538, duration: 5.962s, episode steps:  40, steps per second:   7, episode reward: -5.600, mean reward: -0.140 [-20.000, 19.900], mean action: 1.275 [0.000, 2.000],  loss: 60.237175, mean_q: 14.516040, mean_eps: 0.100000\n","      56559/2000000000: episode: 1539, duration: 4.724s, episode steps:  34, steps per second:   7, episode reward: 71.900, mean reward:  2.115 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 62.016372, mean_q: 14.857464, mean_eps: 0.100000\n","      56599/2000000000: episode: 1540, duration: 5.306s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.372271, mean_q: 14.462685, mean_eps: 0.100000\n","      56639/2000000000: episode: 1541, duration: 5.451s, episode steps:  40, steps per second:   7, episode reward: 22.600, mean reward:  0.565 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.350085, mean_q: 14.618523, mean_eps: 0.100000\n","      56670/2000000000: episode: 1542, duration: 4.341s, episode steps:  31, steps per second:   7, episode reward: -62.900, mean reward: -2.029 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 61.531016, mean_q: 14.724494, mean_eps: 0.100000\n","      56708/2000000000: episode: 1543, duration: 5.445s, episode steps:  38, steps per second:   7, episode reward: -94.300, mean reward: -2.482 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 60.207114, mean_q: 15.098642, mean_eps: 0.100000\n","      56748/2000000000: episode: 1544, duration: 5.657s, episode steps:  40, steps per second:   7, episode reward: -75.300, mean reward: -1.883 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.790529, mean_q: 14.390852, mean_eps: 0.100000\n","      56788/2000000000: episode: 1545, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: -8.000, mean reward: -0.200 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 61.432991, mean_q: 15.009430, mean_eps: 0.100000\n","      56825/2000000000: episode: 1546, duration: 4.703s, episode steps:  37, steps per second:   8, episode reward: -29.500, mean reward: -0.797 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 70.035756, mean_q: 14.772310, mean_eps: 0.100000\n","      56852/2000000000: episode: 1547, duration: 3.419s, episode steps:  27, steps per second:   8, episode reward: 65.400, mean reward:  2.422 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 66.522920, mean_q: 14.805295, mean_eps: 0.100000\n","      56888/2000000000: episode: 1548, duration: 4.684s, episode steps:  36, steps per second:   8, episode reward: -84.100, mean reward: -2.336 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 62.969491, mean_q: 15.479976, mean_eps: 0.100000\n","      56924/2000000000: episode: 1549, duration: 4.565s, episode steps:  36, steps per second:   8, episode reward: -3.100, mean reward: -0.086 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 65.128684, mean_q: 15.147106, mean_eps: 0.100000\n","      56962/2000000000: episode: 1550, duration: 4.830s, episode steps:  38, steps per second:   8, episode reward: 19.200, mean reward:  0.505 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 64.222878, mean_q: 14.891567, mean_eps: 0.100000\n","      56997/2000000000: episode: 1551, duration: 4.310s, episode steps:  35, steps per second:   8, episode reward: -157.700, mean reward: -4.506 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 64.819003, mean_q: 14.734031, mean_eps: 0.100000\n","      57037/2000000000: episode: 1552, duration: 4.960s, episode steps:  40, steps per second:   8, episode reward:  7.900, mean reward:  0.198 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.593473, mean_q: 15.053853, mean_eps: 0.100000\n","      57077/2000000000: episode: 1553, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: -44.500, mean reward: -1.113 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 69.116750, mean_q: 15.370806, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      57111/2000000000: episode: 1554, duration: 4.159s, episode steps:  34, steps per second:   8, episode reward: 15.800, mean reward:  0.465 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 72.535399, mean_q: 14.503228, mean_eps: 0.100000\n","      57151/2000000000: episode: 1555, duration: 4.945s, episode steps:  40, steps per second:   8, episode reward: 86.800, mean reward:  2.170 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.393327, mean_q: 15.254323, mean_eps: 0.100000\n","      57191/2000000000: episode: 1556, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: -111.700, mean reward: -2.793 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.911313, mean_q: 14.845970, mean_eps: 0.100000\n","      57229/2000000000: episode: 1557, duration: 4.821s, episode steps:  38, steps per second:   8, episode reward: -9.900, mean reward: -0.261 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 60.034216, mean_q: 15.213376, mean_eps: 0.100000\n","      57269/2000000000: episode: 1558, duration: 4.674s, episode steps:  40, steps per second:   9, episode reward: -102.700, mean reward: -2.568 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.364019, mean_q: 14.919290, mean_eps: 0.100000\n","      57309/2000000000: episode: 1559, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward: -58.200, mean reward: -1.455 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.190146, mean_q: 14.940513, mean_eps: 0.100000\n","      57349/2000000000: episode: 1560, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: 23.600, mean reward:  0.590 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 63.655951, mean_q: 15.532914, mean_eps: 0.100000\n","      57384/2000000000: episode: 1561, duration: 4.101s, episode steps:  35, steps per second:   9, episode reward: 106.200, mean reward:  3.034 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.241159, mean_q: 14.589307, mean_eps: 0.100000\n","      57424/2000000000: episode: 1562, duration: 4.892s, episode steps:  40, steps per second:   8, episode reward: -22.200, mean reward: -0.555 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.307440, mean_q: 14.438502, mean_eps: 0.100000\n","      57464/2000000000: episode: 1563, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward: -3.700, mean reward: -0.092 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.560953, mean_q: 15.130778, mean_eps: 0.100000\n","      57504/2000000000: episode: 1564, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 63.214356, mean_q: 14.913021, mean_eps: 0.100000\n","      57543/2000000000: episode: 1565, duration: 4.688s, episode steps:  39, steps per second:   8, episode reward: 180.400, mean reward:  4.626 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 68.433874, mean_q: 14.461516, mean_eps: 0.100000\n","      57583/2000000000: episode: 1566, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.726974, mean_q: 14.528387, mean_eps: 0.100000\n","      57623/2000000000: episode: 1567, duration: 4.895s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.306751, mean_q: 15.063787, mean_eps: 0.100000\n","      57657/2000000000: episode: 1568, duration: 4.041s, episode steps:  34, steps per second:   8, episode reward: -83.600, mean reward: -2.459 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 65.086151, mean_q: 15.073475, mean_eps: 0.100000\n","      57697/2000000000: episode: 1569, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: -69.200, mean reward: -1.730 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.841190, mean_q: 14.849083, mean_eps: 0.100000\n","      57736/2000000000: episode: 1570, duration: 4.594s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 59.090408, mean_q: 15.293730, mean_eps: 0.100000\n","      57776/2000000000: episode: 1571, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.116863, mean_q: 15.164031, mean_eps: 0.100000\n","      57816/2000000000: episode: 1572, duration: 4.614s, episode steps:  40, steps per second:   9, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.871058, mean_q: 14.865831, mean_eps: 0.100000\n","      57856/2000000000: episode: 1573, duration: 4.902s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.811946, mean_q: 14.080119, mean_eps: 0.100000\n","      57896/2000000000: episode: 1574, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.008628, mean_q: 14.605896, mean_eps: 0.100000\n","      57936/2000000000: episode: 1575, duration: 5.045s, episode steps:  40, steps per second:   8, episode reward: -116.300, mean reward: -2.907 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.061180, mean_q: 14.250097, mean_eps: 0.100000\n","      57976/2000000000: episode: 1576, duration: 4.778s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.349449, mean_q: 14.835477, mean_eps: 0.100000\n","      58016/2000000000: episode: 1577, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 63.045781, mean_q: 13.993426, mean_eps: 0.100000\n","      58056/2000000000: episode: 1578, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.127369, mean_q: 14.443914, mean_eps: 0.100000\n","      58093/2000000000: episode: 1579, duration: 4.421s, episode steps:  37, steps per second:   8, episode reward: -164.900, mean reward: -4.457 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 77.824150, mean_q: 14.538682, mean_eps: 0.100000\n","      58133/2000000000: episode: 1580, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 62.243167, mean_q: 14.870450, mean_eps: 0.100000\n","      58173/2000000000: episode: 1581, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 65.509479, mean_q: 14.075130, mean_eps: 0.100000\n","      58213/2000000000: episode: 1582, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward:  7.100, mean reward:  0.177 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.393312, mean_q: 14.280006, mean_eps: 0.100000\n","      58253/2000000000: episode: 1583, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: -130.000, mean reward: -3.250 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.103635, mean_q: 15.150426, mean_eps: 0.100000\n","      58293/2000000000: episode: 1584, duration: 5.004s, episode steps:  40, steps per second:   8, episode reward: 32.500, mean reward:  0.812 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 61.965542, mean_q: 15.401304, mean_eps: 0.100000\n","      58333/2000000000: episode: 1585, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: -9.500, mean reward: -0.237 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.715581, mean_q: 15.458771, mean_eps: 0.100000\n","      58373/2000000000: episode: 1586, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 57.700, mean reward:  1.443 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 63.278289, mean_q: 15.163449, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      58413/2000000000: episode: 1587, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: 41.100, mean reward:  1.027 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.076360, mean_q: 15.261782, mean_eps: 0.100000\n","      58453/2000000000: episode: 1588, duration: 4.881s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.306108, mean_q: 14.747904, mean_eps: 0.100000\n","      58493/2000000000: episode: 1589, duration: 5.152s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 61.803588, mean_q: 14.526522, mean_eps: 0.100000\n","      58520/2000000000: episode: 1590, duration: 3.515s, episode steps:  27, steps per second:   8, episode reward: -172.000, mean reward: -6.370 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 66.209494, mean_q: 15.656936, mean_eps: 0.100000\n","      58555/2000000000: episode: 1591, duration: 4.139s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 59.883314, mean_q: 14.665862, mean_eps: 0.100000\n","      58589/2000000000: episode: 1592, duration: 4.110s, episode steps:  34, steps per second:   8, episode reward: 13.900, mean reward:  0.409 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 65.130365, mean_q: 14.663241, mean_eps: 0.100000\n","      58628/2000000000: episode: 1593, duration: 4.817s, episode steps:  39, steps per second:   8, episode reward: -67.900, mean reward: -1.741 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 62.152122, mean_q: 14.269688, mean_eps: 0.100000\n","      58668/2000000000: episode: 1594, duration: 4.887s, episode steps:  40, steps per second:   8, episode reward: 125.700, mean reward:  3.142 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.670760, mean_q: 14.578169, mean_eps: 0.100000\n","      58708/2000000000: episode: 1595, duration: 4.912s, episode steps:  40, steps per second:   8, episode reward: -21.100, mean reward: -0.527 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.677038, mean_q: 14.356068, mean_eps: 0.100000\n","      58748/2000000000: episode: 1596, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: -22.800, mean reward: -0.570 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.710441, mean_q: 14.977182, mean_eps: 0.100000\n","      58784/2000000000: episode: 1597, duration: 4.457s, episode steps:  36, steps per second:   8, episode reward: -32.600, mean reward: -0.906 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 61.697870, mean_q: 15.209956, mean_eps: 0.100000\n","      58824/2000000000: episode: 1598, duration: 4.841s, episode steps:  40, steps per second:   8, episode reward: -148.000, mean reward: -3.700 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.940697, mean_q: 14.246084, mean_eps: 0.100000\n","      58864/2000000000: episode: 1599, duration: 5.134s, episode steps:  40, steps per second:   8, episode reward: -1.300, mean reward: -0.032 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.418893, mean_q: 14.383762, mean_eps: 0.100000\n","      58904/2000000000: episode: 1600, duration: 5.173s, episode steps:  40, steps per second:   8, episode reward: 60.800, mean reward:  1.520 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.814838, mean_q: 14.644714, mean_eps: 0.100000\n","      58944/2000000000: episode: 1601, duration: 5.133s, episode steps:  40, steps per second:   8, episode reward: -117.800, mean reward: -2.945 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 61.269509, mean_q: 15.032599, mean_eps: 0.100000\n","      58984/2000000000: episode: 1602, duration: 4.759s, episode steps:  40, steps per second:   8, episode reward: 53.200, mean reward:  1.330 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.280449, mean_q: 14.881385, mean_eps: 0.100000\n","      59021/2000000000: episode: 1603, duration: 4.527s, episode steps:  37, steps per second:   8, episode reward: -94.000, mean reward: -2.541 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 64.212267, mean_q: 14.785409, mean_eps: 0.100000\n","      59061/2000000000: episode: 1604, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -38.300, mean reward: -0.957 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.727022, mean_q: 14.715323, mean_eps: 0.100000\n","      59096/2000000000: episode: 1605, duration: 4.312s, episode steps:  35, steps per second:   8, episode reward: -45.000, mean reward: -1.286 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 63.984668, mean_q: 14.463784, mean_eps: 0.100000\n","      59136/2000000000: episode: 1606, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: 84.600, mean reward:  2.115 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 62.393236, mean_q: 14.296807, mean_eps: 0.100000\n","      59176/2000000000: episode: 1607, duration: 4.844s, episode steps:  40, steps per second:   8, episode reward: -25.100, mean reward: -0.627 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 61.750451, mean_q: 15.044749, mean_eps: 0.100000\n","      59213/2000000000: episode: 1608, duration: 4.566s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 65.815807, mean_q: 15.065753, mean_eps: 0.100000\n","      59253/2000000000: episode: 1609, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: -96.100, mean reward: -2.403 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.759723, mean_q: 15.068571, mean_eps: 0.100000\n","      59293/2000000000: episode: 1610, duration: 4.851s, episode steps:  40, steps per second:   8, episode reward:  3.200, mean reward:  0.080 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.376556, mean_q: 15.192280, mean_eps: 0.100000\n","      59333/2000000000: episode: 1611, duration: 4.805s, episode steps:  40, steps per second:   8, episode reward:  5.400, mean reward:  0.135 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.166107, mean_q: 14.687619, mean_eps: 0.100000\n","      59373/2000000000: episode: 1612, duration: 4.760s, episode steps:  40, steps per second:   8, episode reward: 30.300, mean reward:  0.757 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.562921, mean_q: 14.658950, mean_eps: 0.100000\n","      59413/2000000000: episode: 1613, duration: 4.786s, episode steps:  40, steps per second:   8, episode reward: 73.100, mean reward:  1.827 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 66.076840, mean_q: 14.368231, mean_eps: 0.100000\n","      59453/2000000000: episode: 1614, duration: 4.685s, episode steps:  40, steps per second:   9, episode reward: 112.700, mean reward:  2.818 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.651907, mean_q: 14.687003, mean_eps: 0.100000\n","      59493/2000000000: episode: 1615, duration: 4.883s, episode steps:  40, steps per second:   8, episode reward: 102.700, mean reward:  2.568 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.161209, mean_q: 14.523238, mean_eps: 0.100000\n","      59530/2000000000: episode: 1616, duration: 4.500s, episode steps:  37, steps per second:   8, episode reward: 165.500, mean reward:  4.473 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.525116, mean_q: 14.562949, mean_eps: 0.100000\n","      59567/2000000000: episode: 1617, duration: 4.394s, episode steps:  37, steps per second:   8, episode reward: 52.800, mean reward:  1.427 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 61.512991, mean_q: 15.202506, mean_eps: 0.100000\n","      59604/2000000000: episode: 1618, duration: 4.331s, episode steps:  37, steps per second:   9, episode reward: -58.800, mean reward: -1.589 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 69.042116, mean_q: 15.063138, mean_eps: 0.100000\n","      59642/2000000000: episode: 1619, duration: 4.442s, episode steps:  38, steps per second:   9, episode reward: 26.800, mean reward:  0.705 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 65.704541, mean_q: 14.975717, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      59682/2000000000: episode: 1620, duration: 4.816s, episode steps:  40, steps per second:   8, episode reward: -91.200, mean reward: -2.280 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.891318, mean_q: 14.347080, mean_eps: 0.100000\n","      59722/2000000000: episode: 1621, duration: 4.597s, episode steps:  40, steps per second:   9, episode reward: -17.700, mean reward: -0.442 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.168491, mean_q: 15.289368, mean_eps: 0.100000\n","      59762/2000000000: episode: 1622, duration: 4.661s, episode steps:  40, steps per second:   9, episode reward: 25.600, mean reward:  0.640 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.336519, mean_q: 14.406807, mean_eps: 0.100000\n","      59793/2000000000: episode: 1623, duration: 3.672s, episode steps:  31, steps per second:   8, episode reward: -17.300, mean reward: -0.558 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 67.188429, mean_q: 15.223810, mean_eps: 0.100000\n","      59833/2000000000: episode: 1624, duration: 4.693s, episode steps:  40, steps per second:   9, episode reward: -124.500, mean reward: -3.112 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.481476, mean_q: 14.793222, mean_eps: 0.100000\n","      59873/2000000000: episode: 1625, duration: 4.933s, episode steps:  40, steps per second:   8, episode reward: -65.100, mean reward: -1.627 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 65.309093, mean_q: 14.502296, mean_eps: 0.100000\n","      59911/2000000000: episode: 1626, duration: 4.632s, episode steps:  38, steps per second:   8, episode reward: 108.800, mean reward:  2.863 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 59.511532, mean_q: 15.150888, mean_eps: 0.100000\n","      59951/2000000000: episode: 1627, duration: 4.725s, episode steps:  40, steps per second:   8, episode reward: 26.800, mean reward:  0.670 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 61.184329, mean_q: 14.539994, mean_eps: 0.100000\n","      59991/2000000000: episode: 1628, duration: 5.189s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.613262, mean_q: 15.034597, mean_eps: 0.100000\n","      60031/2000000000: episode: 1629, duration: 4.708s, episode steps:  40, steps per second:   8, episode reward: 43.400, mean reward:  1.085 [-20.000, 19.600], mean action: 1.250 [0.000, 2.000],  loss: 70.102631, mean_q: 16.055355, mean_eps: 0.100000\n","      60071/2000000000: episode: 1630, duration: 4.409s, episode steps:  40, steps per second:   9, episode reward: 28.300, mean reward:  0.708 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.079411, mean_q: 16.977948, mean_eps: 0.100000\n","      60107/2000000000: episode: 1631, duration: 4.394s, episode steps:  36, steps per second:   8, episode reward: 71.500, mean reward:  1.986 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 69.687488, mean_q: 17.690596, mean_eps: 0.100000\n","      60147/2000000000: episode: 1632, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: -5.700, mean reward: -0.143 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.746229, mean_q: 16.885533, mean_eps: 0.100000\n","      60182/2000000000: episode: 1633, duration: 4.088s, episode steps:  35, steps per second:   9, episode reward: -7.500, mean reward: -0.214 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.470730, mean_q: 16.775206, mean_eps: 0.100000\n","      60222/2000000000: episode: 1634, duration: 4.574s, episode steps:  40, steps per second:   9, episode reward: 15.200, mean reward:  0.380 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.102736, mean_q: 17.120089, mean_eps: 0.100000\n","      60262/2000000000: episode: 1635, duration: 4.592s, episode steps:  40, steps per second:   9, episode reward: -50.200, mean reward: -1.255 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.485503, mean_q: 16.510295, mean_eps: 0.100000\n","      60302/2000000000: episode: 1636, duration: 4.589s, episode steps:  40, steps per second:   9, episode reward: -48.900, mean reward: -1.222 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.651006, mean_q: 16.242234, mean_eps: 0.100000\n","      60342/2000000000: episode: 1637, duration: 4.723s, episode steps:  40, steps per second:   8, episode reward: 37.700, mean reward:  0.942 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.566366, mean_q: 16.913329, mean_eps: 0.100000\n","      60382/2000000000: episode: 1638, duration: 4.620s, episode steps:  40, steps per second:   9, episode reward: -8.200, mean reward: -0.205 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.360330, mean_q: 16.805717, mean_eps: 0.100000\n","      60422/2000000000: episode: 1639, duration: 4.558s, episode steps:  40, steps per second:   9, episode reward: 38.900, mean reward:  0.972 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.812798, mean_q: 17.285722, mean_eps: 0.100000\n","      60459/2000000000: episode: 1640, duration: 4.499s, episode steps:  37, steps per second:   8, episode reward: 76.000, mean reward:  2.054 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 74.950137, mean_q: 16.748665, mean_eps: 0.100000\n","      60499/2000000000: episode: 1641, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: -47.400, mean reward: -1.185 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.630144, mean_q: 16.673158, mean_eps: 0.100000\n","      60539/2000000000: episode: 1642, duration: 4.881s, episode steps:  40, steps per second:   8, episode reward: -29.300, mean reward: -0.732 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.000855, mean_q: 17.198631, mean_eps: 0.100000\n","      60579/2000000000: episode: 1643, duration: 4.994s, episode steps:  40, steps per second:   8, episode reward: -48.200, mean reward: -1.205 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.739293, mean_q: 16.625652, mean_eps: 0.100000\n","      60619/2000000000: episode: 1644, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.779737, mean_q: 16.639446, mean_eps: 0.100000\n","      60657/2000000000: episode: 1645, duration: 4.649s, episode steps:  38, steps per second:   8, episode reward: -95.000, mean reward: -2.500 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 66.950219, mean_q: 16.953003, mean_eps: 0.100000\n","      60693/2000000000: episode: 1646, duration: 4.512s, episode steps:  36, steps per second:   8, episode reward: -127.600, mean reward: -3.544 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 64.898224, mean_q: 16.282087, mean_eps: 0.100000\n","      60733/2000000000: episode: 1647, duration: 5.078s, episode steps:  40, steps per second:   8, episode reward: 17.600, mean reward:  0.440 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.740263, mean_q: 17.441668, mean_eps: 0.100000\n","      60773/2000000000: episode: 1648, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: -4.300, mean reward: -0.107 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.111962, mean_q: 17.290287, mean_eps: 0.100000\n","      60813/2000000000: episode: 1649, duration: 5.080s, episode steps:  40, steps per second:   8, episode reward: -119.100, mean reward: -2.978 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.618815, mean_q: 16.281272, mean_eps: 0.100000\n","      60853/2000000000: episode: 1650, duration: 5.372s, episode steps:  40, steps per second:   7, episode reward: 39.500, mean reward:  0.988 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.309619, mean_q: 17.099307, mean_eps: 0.100000\n","      60887/2000000000: episode: 1651, duration: 4.437s, episode steps:  34, steps per second:   8, episode reward: 34.700, mean reward:  1.021 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 73.820104, mean_q: 16.570400, mean_eps: 0.100000\n","      60927/2000000000: episode: 1652, duration: 5.353s, episode steps:  40, steps per second:   7, episode reward: 180.500, mean reward:  4.513 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.323644, mean_q: 16.564741, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      60967/2000000000: episode: 1653, duration: 5.200s, episode steps:  40, steps per second:   8, episode reward: 121.700, mean reward:  3.042 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.458029, mean_q: 16.139442, mean_eps: 0.100000\n","      61007/2000000000: episode: 1654, duration: 5.110s, episode steps:  40, steps per second:   8, episode reward:  6.100, mean reward:  0.153 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.041731, mean_q: 16.201787, mean_eps: 0.100000\n","      61047/2000000000: episode: 1655, duration: 5.140s, episode steps:  40, steps per second:   8, episode reward: -26.600, mean reward: -0.665 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.209398, mean_q: 16.312710, mean_eps: 0.100000\n","      61087/2000000000: episode: 1656, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: -3.200, mean reward: -0.080 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.661522, mean_q: 17.302328, mean_eps: 0.100000\n","      61127/2000000000: episode: 1657, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: -71.400, mean reward: -1.785 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.036009, mean_q: 16.718712, mean_eps: 0.100000\n","      61167/2000000000: episode: 1658, duration: 4.809s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.475520, mean_q: 16.372079, mean_eps: 0.100000\n","      61207/2000000000: episode: 1659, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward: 19.000, mean reward:  0.475 [-20.000, 18.000], mean action: 1.675 [0.000, 2.000],  loss: 71.852001, mean_q: 17.202391, mean_eps: 0.100000\n","      61244/2000000000: episode: 1660, duration: 4.448s, episode steps:  37, steps per second:   8, episode reward: 82.400, mean reward:  2.227 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 68.410354, mean_q: 16.317695, mean_eps: 0.100000\n","      61284/2000000000: episode: 1661, duration: 4.875s, episode steps:  40, steps per second:   8, episode reward: 17.300, mean reward:  0.432 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.850825, mean_q: 16.734374, mean_eps: 0.100000\n","      61321/2000000000: episode: 1662, duration: 4.542s, episode steps:  37, steps per second:   8, episode reward: 22.200, mean reward:  0.600 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 65.214177, mean_q: 16.414919, mean_eps: 0.100000\n","      61361/2000000000: episode: 1663, duration: 4.915s, episode steps:  40, steps per second:   8, episode reward: -71.000, mean reward: -1.775 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.902527, mean_q: 16.616672, mean_eps: 0.100000\n","      61401/2000000000: episode: 1664, duration: 4.881s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.997121, mean_q: 17.024068, mean_eps: 0.100000\n","      61440/2000000000: episode: 1665, duration: 4.858s, episode steps:  39, steps per second:   8, episode reward: 75.600, mean reward:  1.938 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 64.871139, mean_q: 16.314272, mean_eps: 0.100000\n","      61479/2000000000: episode: 1666, duration: 5.016s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 73.940541, mean_q: 17.276971, mean_eps: 0.100000\n","      61516/2000000000: episode: 1667, duration: 4.644s, episode steps:  37, steps per second:   8, episode reward: 155.200, mean reward:  4.195 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 70.229577, mean_q: 17.066340, mean_eps: 0.100000\n","      61556/2000000000: episode: 1668, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: 11.700, mean reward:  0.292 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.277551, mean_q: 16.551936, mean_eps: 0.100000\n","      61596/2000000000: episode: 1669, duration: 5.177s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 72.633347, mean_q: 16.356689, mean_eps: 0.100000\n","      61636/2000000000: episode: 1670, duration: 5.475s, episode steps:  40, steps per second:   7, episode reward: 98.700, mean reward:  2.467 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.464674, mean_q: 16.571495, mean_eps: 0.100000\n","      61676/2000000000: episode: 1671, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: -50.800, mean reward: -1.270 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.417892, mean_q: 16.895225, mean_eps: 0.100000\n","      61707/2000000000: episode: 1672, duration: 4.028s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 66.651537, mean_q: 17.166312, mean_eps: 0.100000\n","      61743/2000000000: episode: 1673, duration: 4.776s, episode steps:  36, steps per second:   8, episode reward: -134.000, mean reward: -3.722 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 68.268226, mean_q: 16.757120, mean_eps: 0.100000\n","      61781/2000000000: episode: 1674, duration: 4.818s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 67.768312, mean_q: 17.645298, mean_eps: 0.100000\n","      61818/2000000000: episode: 1675, duration: 4.610s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 65.204681, mean_q: 17.267836, mean_eps: 0.100000\n","      61858/2000000000: episode: 1676, duration: 4.585s, episode steps:  40, steps per second:   9, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.822450, mean_q: 17.037412, mean_eps: 0.100000\n","      61896/2000000000: episode: 1677, duration: 4.332s, episode steps:  38, steps per second:   9, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 68.852216, mean_q: 16.980770, mean_eps: 0.100000\n","      61936/2000000000: episode: 1678, duration: 4.866s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.588601, mean_q: 17.373636, mean_eps: 0.100000\n","      61973/2000000000: episode: 1679, duration: 4.362s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 73.318249, mean_q: 16.680106, mean_eps: 0.100000\n","      62013/2000000000: episode: 1680, duration: 4.715s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.622747, mean_q: 16.456153, mean_eps: 0.100000\n","      62053/2000000000: episode: 1681, duration: 4.771s, episode steps:  40, steps per second:   8, episode reward: 81.900, mean reward:  2.047 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.747938, mean_q: 16.337902, mean_eps: 0.100000\n","      62088/2000000000: episode: 1682, duration: 4.271s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 66.288524, mean_q: 16.735789, mean_eps: 0.100000\n","      62114/2000000000: episode: 1683, duration: 3.127s, episode steps:  26, steps per second:   8, episode reward: -139.600, mean reward: -5.369 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 66.936290, mean_q: 17.258520, mean_eps: 0.100000\n","      62154/2000000000: episode: 1684, duration: 4.678s, episode steps:  40, steps per second:   9, episode reward: 137.400, mean reward:  3.435 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.467172, mean_q: 16.007398, mean_eps: 0.100000\n","      62191/2000000000: episode: 1685, duration: 4.366s, episode steps:  37, steps per second:   8, episode reward: 59.900, mean reward:  1.619 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 61.389400, mean_q: 17.198147, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      62231/2000000000: episode: 1686, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: -22.300, mean reward: -0.558 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.238904, mean_q: 17.022074, mean_eps: 0.100000\n","      62261/2000000000: episode: 1687, duration: 3.592s, episode steps:  30, steps per second:   8, episode reward:  4.800, mean reward:  0.160 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 63.181572, mean_q: 16.616563, mean_eps: 0.100000\n","      62301/2000000000: episode: 1688, duration: 4.758s, episode steps:  40, steps per second:   8, episode reward: -52.100, mean reward: -1.303 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.061491, mean_q: 16.897300, mean_eps: 0.100000\n","      62341/2000000000: episode: 1689, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.692472, mean_q: 17.075230, mean_eps: 0.100000\n","      62381/2000000000: episode: 1690, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: 127.900, mean reward:  3.198 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.292646, mean_q: 17.049716, mean_eps: 0.100000\n","      62421/2000000000: episode: 1691, duration: 4.678s, episode steps:  40, steps per second:   9, episode reward: 53.200, mean reward:  1.330 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.741958, mean_q: 16.923322, mean_eps: 0.100000\n","      62461/2000000000: episode: 1692, duration: 4.715s, episode steps:  40, steps per second:   8, episode reward:  7.100, mean reward:  0.178 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.489320, mean_q: 17.535460, mean_eps: 0.100000\n","      62501/2000000000: episode: 1693, duration: 4.673s, episode steps:  40, steps per second:   9, episode reward: -35.600, mean reward: -0.890 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.620581, mean_q: 16.637758, mean_eps: 0.100000\n","      62539/2000000000: episode: 1694, duration: 4.541s, episode steps:  38, steps per second:   8, episode reward: 17.600, mean reward:  0.463 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 68.777191, mean_q: 17.259808, mean_eps: 0.100000\n","      62577/2000000000: episode: 1695, duration: 4.527s, episode steps:  38, steps per second:   8, episode reward: -74.200, mean reward: -1.953 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 66.478302, mean_q: 16.948015, mean_eps: 0.100000\n","      62617/2000000000: episode: 1696, duration: 4.536s, episode steps:  40, steps per second:   9, episode reward: 17.000, mean reward:  0.425 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.855438, mean_q: 16.668108, mean_eps: 0.100000\n","      62648/2000000000: episode: 1697, duration: 3.553s, episode steps:  31, steps per second:   9, episode reward: 90.600, mean reward:  2.923 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 64.430752, mean_q: 17.134422, mean_eps: 0.100000\n","      62688/2000000000: episode: 1698, duration: 4.767s, episode steps:  40, steps per second:   8, episode reward: 122.100, mean reward:  3.053 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.387635, mean_q: 16.897365, mean_eps: 0.100000\n","      62728/2000000000: episode: 1699, duration: 4.841s, episode steps:  40, steps per second:   8, episode reward: -31.400, mean reward: -0.785 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 64.255005, mean_q: 16.907367, mean_eps: 0.100000\n","      62768/2000000000: episode: 1700, duration: 4.883s, episode steps:  40, steps per second:   8, episode reward: 61.200, mean reward:  1.530 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.436448, mean_q: 17.051511, mean_eps: 0.100000\n","      62806/2000000000: episode: 1701, duration: 4.746s, episode steps:  38, steps per second:   8, episode reward: -20.600, mean reward: -0.542 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 66.699627, mean_q: 16.876624, mean_eps: 0.100000\n","      62846/2000000000: episode: 1702, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 18.800, mean reward:  0.470 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.299584, mean_q: 16.502090, mean_eps: 0.100000\n","      62886/2000000000: episode: 1703, duration: 5.071s, episode steps:  40, steps per second:   8, episode reward: 114.400, mean reward:  2.860 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.252818, mean_q: 17.358547, mean_eps: 0.100000\n","      62926/2000000000: episode: 1704, duration: 5.080s, episode steps:  40, steps per second:   8, episode reward: -64.400, mean reward: -1.610 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.095528, mean_q: 16.862048, mean_eps: 0.100000\n","      62966/2000000000: episode: 1705, duration: 4.875s, episode steps:  40, steps per second:   8, episode reward: -23.500, mean reward: -0.588 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.324355, mean_q: 17.346614, mean_eps: 0.100000\n","      63000/2000000000: episode: 1706, duration: 4.273s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 64.806198, mean_q: 17.090065, mean_eps: 0.100000\n","      63040/2000000000: episode: 1707, duration: 4.870s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.221074, mean_q: 17.329564, mean_eps: 0.100000\n","      63074/2000000000: episode: 1708, duration: 3.971s, episode steps:  34, steps per second:   9, episode reward: -4.200, mean reward: -0.124 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 69.339076, mean_q: 17.323636, mean_eps: 0.100000\n","      63114/2000000000: episode: 1709, duration: 4.484s, episode steps:  40, steps per second:   9, episode reward: 172.000, mean reward:  4.300 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.410196, mean_q: 17.135185, mean_eps: 0.100000\n","      63154/2000000000: episode: 1710, duration: 4.653s, episode steps:  40, steps per second:   9, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 63.689687, mean_q: 16.701756, mean_eps: 0.100000\n","      63186/2000000000: episode: 1711, duration: 4.281s, episode steps:  32, steps per second:   7, episode reward: 37.300, mean reward:  1.166 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 62.184458, mean_q: 16.155010, mean_eps: 0.100000\n","      63226/2000000000: episode: 1712, duration: 4.683s, episode steps:  40, steps per second:   9, episode reward: 34.800, mean reward:  0.870 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.158174, mean_q: 16.573838, mean_eps: 0.100000\n","      63266/2000000000: episode: 1713, duration: 4.858s, episode steps:  40, steps per second:   8, episode reward: 81.900, mean reward:  2.047 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.625243, mean_q: 17.473562, mean_eps: 0.100000\n","      63306/2000000000: episode: 1714, duration: 5.287s, episode steps:  40, steps per second:   8, episode reward: -122.600, mean reward: -3.065 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.674755, mean_q: 16.739333, mean_eps: 0.100000\n","      63346/2000000000: episode: 1715, duration: 4.931s, episode steps:  40, steps per second:   8, episode reward: 96.900, mean reward:  2.423 [-20.000, 19.100], mean action: 1.425 [0.000, 2.000],  loss: 69.509425, mean_q: 16.959787, mean_eps: 0.100000\n","      63382/2000000000: episode: 1716, duration: 4.444s, episode steps:  36, steps per second:   8, episode reward: -30.600, mean reward: -0.850 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 67.217159, mean_q: 16.941385, mean_eps: 0.100000\n","      63422/2000000000: episode: 1717, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 21.900, mean reward:  0.547 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.156957, mean_q: 16.975170, mean_eps: 0.100000\n","      63458/2000000000: episode: 1718, duration: 4.378s, episode steps:  36, steps per second:   8, episode reward: 101.800, mean reward:  2.828 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 65.179014, mean_q: 16.753261, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      63498/2000000000: episode: 1719, duration: 4.716s, episode steps:  40, steps per second:   8, episode reward: -151.900, mean reward: -3.798 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.026360, mean_q: 17.314406, mean_eps: 0.100000\n","      63537/2000000000: episode: 1720, duration: 4.652s, episode steps:  39, steps per second:   8, episode reward: 76.500, mean reward:  1.962 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 64.310675, mean_q: 16.851330, mean_eps: 0.100000\n","      63574/2000000000: episode: 1721, duration: 4.547s, episode steps:  37, steps per second:   8, episode reward: -24.800, mean reward: -0.670 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.122765, mean_q: 16.446384, mean_eps: 0.100000\n","      63611/2000000000: episode: 1722, duration: 4.326s, episode steps:  37, steps per second:   9, episode reward: 41.300, mean reward:  1.116 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 64.927724, mean_q: 16.697735, mean_eps: 0.100000\n","      63649/2000000000: episode: 1723, duration: 4.541s, episode steps:  38, steps per second:   8, episode reward: -0.700, mean reward: -0.018 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 67.060774, mean_q: 17.120084, mean_eps: 0.100000\n","      63686/2000000000: episode: 1724, duration: 4.261s, episode steps:  37, steps per second:   9, episode reward: -37.200, mean reward: -1.005 [-20.000, 18.000], mean action: 1.027 [0.000, 2.000],  loss: 67.313966, mean_q: 17.250285, mean_eps: 0.100000\n","      63726/2000000000: episode: 1725, duration: 4.739s, episode steps:  40, steps per second:   8, episode reward: 59.600, mean reward:  1.490 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.742654, mean_q: 16.654793, mean_eps: 0.100000\n","      63765/2000000000: episode: 1726, duration: 4.721s, episode steps:  39, steps per second:   8, episode reward: 162.700, mean reward:  4.172 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 64.766122, mean_q: 17.142694, mean_eps: 0.100000\n","      63793/2000000000: episode: 1727, duration: 3.379s, episode steps:  28, steps per second:   8, episode reward: 85.800, mean reward:  3.064 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 72.190130, mean_q: 17.170621, mean_eps: 0.100000\n","      63831/2000000000: episode: 1728, duration: 4.571s, episode steps:  38, steps per second:   8, episode reward: -109.900, mean reward: -2.892 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 64.021418, mean_q: 17.255362, mean_eps: 0.100000\n","      63871/2000000000: episode: 1729, duration: 4.733s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.381554, mean_q: 16.655952, mean_eps: 0.100000\n","      63911/2000000000: episode: 1730, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.713920, mean_q: 16.873967, mean_eps: 0.100000\n","      63951/2000000000: episode: 1731, duration: 4.739s, episode steps:  40, steps per second:   8, episode reward: -31.100, mean reward: -0.778 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 65.057768, mean_q: 17.227459, mean_eps: 0.100000\n","      63991/2000000000: episode: 1732, duration: 4.728s, episode steps:  40, steps per second:   8, episode reward: -99.600, mean reward: -2.490 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.407286, mean_q: 16.879929, mean_eps: 0.100000\n","      64031/2000000000: episode: 1733, duration: 4.643s, episode steps:  40, steps per second:   9, episode reward: -19.200, mean reward: -0.480 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.646013, mean_q: 16.785395, mean_eps: 0.100000\n","      64071/2000000000: episode: 1734, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 58.800, mean reward:  1.470 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 70.182371, mean_q: 17.390754, mean_eps: 0.100000\n","      64108/2000000000: episode: 1735, duration: 4.355s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 65.892159, mean_q: 16.931726, mean_eps: 0.100000\n","      64148/2000000000: episode: 1736, duration: 4.663s, episode steps:  40, steps per second:   9, episode reward: 86.500, mean reward:  2.163 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 59.124600, mean_q: 17.148227, mean_eps: 0.100000\n","      64186/2000000000: episode: 1737, duration: 4.420s, episode steps:  38, steps per second:   9, episode reward: -46.400, mean reward: -1.221 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 63.716658, mean_q: 17.179335, mean_eps: 0.100000\n","      64221/2000000000: episode: 1738, duration: 4.382s, episode steps:  35, steps per second:   8, episode reward: 19.900, mean reward:  0.569 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 62.810700, mean_q: 17.318590, mean_eps: 0.100000\n","      64261/2000000000: episode: 1739, duration: 5.165s, episode steps:  40, steps per second:   8, episode reward: 67.800, mean reward:  1.695 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.348545, mean_q: 17.122808, mean_eps: 0.100000\n","      64290/2000000000: episode: 1740, duration: 3.496s, episode steps:  29, steps per second:   8, episode reward: 145.300, mean reward:  5.010 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 72.346898, mean_q: 16.819712, mean_eps: 0.100000\n","      64330/2000000000: episode: 1741, duration: 4.683s, episode steps:  40, steps per second:   9, episode reward: -48.500, mean reward: -1.212 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.645401, mean_q: 17.519434, mean_eps: 0.100000\n","      64368/2000000000: episode: 1742, duration: 4.449s, episode steps:  38, steps per second:   9, episode reward: 72.100, mean reward:  1.897 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 65.470075, mean_q: 16.782484, mean_eps: 0.100000\n","      64408/2000000000: episode: 1743, duration: 4.911s, episode steps:  40, steps per second:   8, episode reward: -22.000, mean reward: -0.550 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.557607, mean_q: 16.871656, mean_eps: 0.100000\n","      64437/2000000000: episode: 1744, duration: 3.628s, episode steps:  29, steps per second:   8, episode reward: 91.500, mean reward:  3.155 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 71.042006, mean_q: 17.380580, mean_eps: 0.100000\n","      64473/2000000000: episode: 1745, duration: 4.413s, episode steps:  36, steps per second:   8, episode reward: -61.000, mean reward: -1.694 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 69.211916, mean_q: 17.673757, mean_eps: 0.100000\n","      64513/2000000000: episode: 1746, duration: 4.586s, episode steps:  40, steps per second:   9, episode reward: -117.000, mean reward: -2.925 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.553669, mean_q: 16.741849, mean_eps: 0.100000\n","      64553/2000000000: episode: 1747, duration: 4.773s, episode steps:  40, steps per second:   8, episode reward: -12.800, mean reward: -0.320 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 62.066380, mean_q: 17.664749, mean_eps: 0.100000\n","      64593/2000000000: episode: 1748, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward:  8.400, mean reward:  0.210 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.343493, mean_q: 16.827466, mean_eps: 0.100000\n","      64633/2000000000: episode: 1749, duration: 4.656s, episode steps:  40, steps per second:   9, episode reward: 62.800, mean reward:  1.570 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.903547, mean_q: 17.067465, mean_eps: 0.100000\n","      64665/2000000000: episode: 1750, duration: 4.034s, episode steps:  32, steps per second:   8, episode reward: -36.600, mean reward: -1.144 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 68.575027, mean_q: 17.536335, mean_eps: 0.100000\n","      64705/2000000000: episode: 1751, duration: 5.189s, episode steps:  40, steps per second:   8, episode reward: 47.600, mean reward:  1.190 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.418971, mean_q: 17.491830, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      64745/2000000000: episode: 1752, duration: 4.787s, episode steps:  40, steps per second:   8, episode reward: 111.400, mean reward:  2.785 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.153862, mean_q: 17.367680, mean_eps: 0.100000\n","      64785/2000000000: episode: 1753, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: -64.900, mean reward: -1.622 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.755988, mean_q: 16.902997, mean_eps: 0.100000\n","      64821/2000000000: episode: 1754, duration: 4.576s, episode steps:  36, steps per second:   8, episode reward: -63.400, mean reward: -1.761 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 63.490316, mean_q: 17.486705, mean_eps: 0.100000\n","      64861/2000000000: episode: 1755, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward: 20.900, mean reward:  0.522 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.507891, mean_q: 16.347561, mean_eps: 0.100000\n","      64901/2000000000: episode: 1756, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward: -19.500, mean reward: -0.488 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.131645, mean_q: 16.384751, mean_eps: 0.100000\n","      64941/2000000000: episode: 1757, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 30.100, mean reward:  0.753 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.282211, mean_q: 16.404495, mean_eps: 0.100000\n","      64978/2000000000: episode: 1758, duration: 4.611s, episode steps:  37, steps per second:   8, episode reward: -61.100, mean reward: -1.651 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 64.347134, mean_q: 17.662530, mean_eps: 0.100000\n","      65018/2000000000: episode: 1759, duration: 4.784s, episode steps:  40, steps per second:   8, episode reward: -49.400, mean reward: -1.235 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.463902, mean_q: 16.719300, mean_eps: 0.100000\n","      65058/2000000000: episode: 1760, duration: 4.888s, episode steps:  40, steps per second:   8, episode reward: 77.100, mean reward:  1.928 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.101844, mean_q: 17.189157, mean_eps: 0.100000\n","      65098/2000000000: episode: 1761, duration: 4.815s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 72.960517, mean_q: 17.246856, mean_eps: 0.100000\n","      65138/2000000000: episode: 1762, duration: 4.920s, episode steps:  40, steps per second:   8, episode reward: -104.900, mean reward: -2.622 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.925133, mean_q: 16.730620, mean_eps: 0.100000\n","      65178/2000000000: episode: 1763, duration: 4.824s, episode steps:  40, steps per second:   8, episode reward: 65.900, mean reward:  1.647 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.719943, mean_q: 17.617969, mean_eps: 0.100000\n","      65214/2000000000: episode: 1764, duration: 4.137s, episode steps:  36, steps per second:   9, episode reward: 28.400, mean reward:  0.789 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.823187, mean_q: 16.843700, mean_eps: 0.100000\n","      65254/2000000000: episode: 1765, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: -172.500, mean reward: -4.313 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.403358, mean_q: 16.626105, mean_eps: 0.100000\n","      65291/2000000000: episode: 1766, duration: 4.266s, episode steps:  37, steps per second:   9, episode reward: -129.100, mean reward: -3.489 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 66.537834, mean_q: 17.045019, mean_eps: 0.100000\n","      65331/2000000000: episode: 1767, duration: 4.840s, episode steps:  40, steps per second:   8, episode reward: -11.100, mean reward: -0.278 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.247571, mean_q: 16.828776, mean_eps: 0.100000\n","      65365/2000000000: episode: 1768, duration: 4.031s, episode steps:  34, steps per second:   8, episode reward: -111.600, mean reward: -3.282 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 57.972553, mean_q: 16.306256, mean_eps: 0.100000\n","      65390/2000000000: episode: 1769, duration: 3.030s, episode steps:  25, steps per second:   8, episode reward: -157.000, mean reward: -6.280 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 66.771508, mean_q: 16.719656, mean_eps: 0.100000\n","      65430/2000000000: episode: 1770, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: -59.000, mean reward: -1.475 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.064655, mean_q: 16.493165, mean_eps: 0.100000\n","      65467/2000000000: episode: 1771, duration: 4.453s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 61.036192, mean_q: 16.906122, mean_eps: 0.100000\n","      65507/2000000000: episode: 1772, duration: 4.623s, episode steps:  40, steps per second:   9, episode reward: -69.400, mean reward: -1.735 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.517038, mean_q: 16.952306, mean_eps: 0.100000\n","      65547/2000000000: episode: 1773, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: -208.000, mean reward: -5.200 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.280398, mean_q: 16.561610, mean_eps: 0.100000\n","      65587/2000000000: episode: 1774, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.230967, mean_q: 16.921782, mean_eps: 0.100000\n","      65624/2000000000: episode: 1775, duration: 4.753s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 67.940594, mean_q: 16.849726, mean_eps: 0.100000\n","      65664/2000000000: episode: 1776, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 55.400, mean reward:  1.385 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.338254, mean_q: 17.243226, mean_eps: 0.100000\n","      65699/2000000000: episode: 1777, duration: 4.282s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 63.337889, mean_q: 16.769434, mean_eps: 0.100000\n","      65739/2000000000: episode: 1778, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.323957, mean_q: 16.525100, mean_eps: 0.100000\n","      65779/2000000000: episode: 1779, duration: 4.971s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.691864, mean_q: 17.098388, mean_eps: 0.100000\n","      65819/2000000000: episode: 1780, duration: 5.030s, episode steps:  40, steps per second:   8, episode reward: -8.200, mean reward: -0.205 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.223207, mean_q: 16.593324, mean_eps: 0.100000\n","      65859/2000000000: episode: 1781, duration: 4.597s, episode steps:  40, steps per second:   9, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.803670, mean_q: 17.283551, mean_eps: 0.100000\n","      65899/2000000000: episode: 1782, duration: 4.809s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.594818, mean_q: 17.383128, mean_eps: 0.100000\n","      65939/2000000000: episode: 1783, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.705610, mean_q: 16.895622, mean_eps: 0.100000\n","      65977/2000000000: episode: 1784, duration: 4.529s, episode steps:  38, steps per second:   8, episode reward: -89.600, mean reward: -2.358 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 58.404232, mean_q: 17.192612, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      66017/2000000000: episode: 1785, duration: 4.664s, episode steps:  40, steps per second:   9, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.505776, mean_q: 16.840874, mean_eps: 0.100000\n","      66057/2000000000: episode: 1786, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 51.200, mean reward:  1.280 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.379885, mean_q: 16.751763, mean_eps: 0.100000\n","      66097/2000000000: episode: 1787, duration: 4.687s, episode steps:  40, steps per second:   9, episode reward:  3.700, mean reward:  0.093 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 67.807612, mean_q: 16.828479, mean_eps: 0.100000\n","      66137/2000000000: episode: 1788, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: 49.900, mean reward:  1.247 [-20.000, 18.000], mean action: 1.725 [1.000, 2.000],  loss: 65.399474, mean_q: 16.521018, mean_eps: 0.100000\n","      66177/2000000000: episode: 1789, duration: 4.718s, episode steps:  40, steps per second:   8, episode reward: -8.300, mean reward: -0.208 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.251411, mean_q: 16.822472, mean_eps: 0.100000\n","      66217/2000000000: episode: 1790, duration: 4.726s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.436897, mean_q: 17.001196, mean_eps: 0.100000\n","      66257/2000000000: episode: 1791, duration: 4.761s, episode steps:  40, steps per second:   8, episode reward: -110.000, mean reward: -2.750 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 69.796523, mean_q: 16.992576, mean_eps: 0.100000\n","      66291/2000000000: episode: 1792, duration: 4.092s, episode steps:  34, steps per second:   8, episode reward: -53.700, mean reward: -1.579 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 64.685158, mean_q: 17.079807, mean_eps: 0.100000\n","      66331/2000000000: episode: 1793, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.405271, mean_q: 16.572305, mean_eps: 0.100000\n","      66371/2000000000: episode: 1794, duration: 4.703s, episode steps:  40, steps per second:   9, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.479770, mean_q: 16.949140, mean_eps: 0.100000\n","      66411/2000000000: episode: 1795, duration: 4.702s, episode steps:  40, steps per second:   9, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.118121, mean_q: 17.001106, mean_eps: 0.100000\n","      66450/2000000000: episode: 1796, duration: 4.775s, episode steps:  39, steps per second:   8, episode reward: 57.400, mean reward:  1.472 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 67.631522, mean_q: 17.386816, mean_eps: 0.100000\n","      66490/2000000000: episode: 1797, duration: 4.749s, episode steps:  40, steps per second:   8, episode reward: -24.600, mean reward: -0.615 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 62.298804, mean_q: 16.364068, mean_eps: 0.100000\n","      66530/2000000000: episode: 1798, duration: 4.614s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.723098, mean_q: 16.961815, mean_eps: 0.100000\n","      66570/2000000000: episode: 1799, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: -6.000, mean reward: -0.150 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.908546, mean_q: 17.007970, mean_eps: 0.100000\n","      66607/2000000000: episode: 1800, duration: 4.309s, episode steps:  37, steps per second:   9, episode reward:  9.500, mean reward:  0.257 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 74.818394, mean_q: 17.368350, mean_eps: 0.100000\n","      66647/2000000000: episode: 1801, duration: 4.775s, episode steps:  40, steps per second:   8, episode reward: 29.500, mean reward:  0.738 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.576639, mean_q: 16.494456, mean_eps: 0.100000\n","      66687/2000000000: episode: 1802, duration: 4.620s, episode steps:  40, steps per second:   9, episode reward: 29.600, mean reward:  0.740 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.835895, mean_q: 17.300998, mean_eps: 0.100000\n","      66721/2000000000: episode: 1803, duration: 3.826s, episode steps:  34, steps per second:   9, episode reward: -58.300, mean reward: -1.715 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 64.276234, mean_q: 17.677489, mean_eps: 0.100000\n","      66761/2000000000: episode: 1804, duration: 4.486s, episode steps:  40, steps per second:   9, episode reward: 49.100, mean reward:  1.228 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 63.433917, mean_q: 16.647794, mean_eps: 0.100000\n","      66801/2000000000: episode: 1805, duration: 4.503s, episode steps:  40, steps per second:   9, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 62.070250, mean_q: 16.773175, mean_eps: 0.100000\n","      66829/2000000000: episode: 1806, duration: 3.590s, episode steps:  28, steps per second:   8, episode reward: -42.600, mean reward: -1.521 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 63.460762, mean_q: 16.544753, mean_eps: 0.100000\n","      66869/2000000000: episode: 1807, duration: 4.891s, episode steps:  40, steps per second:   8, episode reward: -30.700, mean reward: -0.768 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 67.745908, mean_q: 16.959899, mean_eps: 0.100000\n","      66909/2000000000: episode: 1808, duration: 5.173s, episode steps:  40, steps per second:   8, episode reward: 19.900, mean reward:  0.497 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 59.711268, mean_q: 16.779537, mean_eps: 0.100000\n","      66949/2000000000: episode: 1809, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.863989, mean_q: 17.351144, mean_eps: 0.100000\n","      66979/2000000000: episode: 1810, duration: 3.775s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 61.824509, mean_q: 17.573305, mean_eps: 0.100000\n","      67014/2000000000: episode: 1811, duration: 4.041s, episode steps:  35, steps per second:   9, episode reward: 47.600, mean reward:  1.360 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 69.346258, mean_q: 17.205294, mean_eps: 0.100000\n","      67054/2000000000: episode: 1812, duration: 4.647s, episode steps:  40, steps per second:   9, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.066075, mean_q: 17.280287, mean_eps: 0.100000\n","      67094/2000000000: episode: 1813, duration: 4.857s, episode steps:  40, steps per second:   8, episode reward: -111.900, mean reward: -2.798 [-20.000, 18.100], mean action: 1.325 [0.000, 2.000],  loss: 73.858032, mean_q: 16.140911, mean_eps: 0.100000\n","      67123/2000000000: episode: 1814, duration: 3.238s, episode steps:  29, steps per second:   9, episode reward: -53.300, mean reward: -1.838 [-20.000, 18.000], mean action: 0.724 [0.000, 2.000],  loss: 63.892921, mean_q: 17.225241, mean_eps: 0.100000\n","      67162/2000000000: episode: 1815, duration: 4.668s, episode steps:  39, steps per second:   8, episode reward: 51.200, mean reward:  1.313 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 70.542279, mean_q: 16.861693, mean_eps: 0.100000\n","      67202/2000000000: episode: 1816, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward: 38.800, mean reward:  0.970 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 61.063768, mean_q: 17.050984, mean_eps: 0.100000\n","      67242/2000000000: episode: 1817, duration: 5.191s, episode steps:  40, steps per second:   8, episode reward:  0.900, mean reward:  0.022 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.209102, mean_q: 17.273843, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      67280/2000000000: episode: 1818, duration: 4.629s, episode steps:  38, steps per second:   8, episode reward: -2.100, mean reward: -0.055 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 63.897309, mean_q: 17.070504, mean_eps: 0.100000\n","      67320/2000000000: episode: 1819, duration: 4.781s, episode steps:  40, steps per second:   8, episode reward: 34.700, mean reward:  0.867 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.383685, mean_q: 16.868515, mean_eps: 0.100000\n","      67354/2000000000: episode: 1820, duration: 3.973s, episode steps:  34, steps per second:   9, episode reward: 26.300, mean reward:  0.774 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 61.946995, mean_q: 17.182000, mean_eps: 0.100000\n","      67392/2000000000: episode: 1821, duration: 4.549s, episode steps:  38, steps per second:   8, episode reward: -56.000, mean reward: -1.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 70.309683, mean_q: 16.629983, mean_eps: 0.100000\n","      67427/2000000000: episode: 1822, duration: 4.102s, episode steps:  35, steps per second:   9, episode reward: 126.200, mean reward:  3.606 [-20.000, 18.000], mean action: 1.343 [0.000, 2.000],  loss: 65.466444, mean_q: 17.392020, mean_eps: 0.100000\n","      67466/2000000000: episode: 1823, duration: 4.499s, episode steps:  39, steps per second:   9, episode reward: -18.900, mean reward: -0.485 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 68.978200, mean_q: 17.062431, mean_eps: 0.100000\n","      67502/2000000000: episode: 1824, duration: 4.258s, episode steps:  36, steps per second:   8, episode reward: 162.500, mean reward:  4.514 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 63.860641, mean_q: 16.965037, mean_eps: 0.100000\n","      67542/2000000000: episode: 1825, duration: 4.775s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.408686, mean_q: 17.366453, mean_eps: 0.100000\n","      67582/2000000000: episode: 1826, duration: 4.714s, episode steps:  40, steps per second:   8, episode reward: -21.900, mean reward: -0.548 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.830801, mean_q: 16.847928, mean_eps: 0.100000\n","      67621/2000000000: episode: 1827, duration: 4.712s, episode steps:  39, steps per second:   8, episode reward: -60.100, mean reward: -1.541 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 66.366242, mean_q: 17.300499, mean_eps: 0.100000\n","      67661/2000000000: episode: 1828, duration: 4.748s, episode steps:  40, steps per second:   8, episode reward: 48.700, mean reward:  1.218 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.912657, mean_q: 16.867838, mean_eps: 0.100000\n","      67700/2000000000: episode: 1829, duration: 4.719s, episode steps:  39, steps per second:   8, episode reward: 23.200, mean reward:  0.595 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 67.023182, mean_q: 16.924035, mean_eps: 0.100000\n","      67735/2000000000: episode: 1830, duration: 4.077s, episode steps:  35, steps per second:   9, episode reward: -16.200, mean reward: -0.463 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 65.666928, mean_q: 17.199759, mean_eps: 0.100000\n","      67775/2000000000: episode: 1831, duration: 4.821s, episode steps:  40, steps per second:   8, episode reward:  2.800, mean reward:  0.070 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 62.151455, mean_q: 16.894630, mean_eps: 0.100000\n","      67815/2000000000: episode: 1832, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.962617, mean_q: 16.646822, mean_eps: 0.100000\n","      67855/2000000000: episode: 1833, duration: 4.845s, episode steps:  40, steps per second:   8, episode reward: -72.200, mean reward: -1.805 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.635978, mean_q: 16.776976, mean_eps: 0.100000\n","      67895/2000000000: episode: 1834, duration: 4.575s, episode steps:  40, steps per second:   9, episode reward: 63.000, mean reward:  1.575 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 60.419862, mean_q: 16.697753, mean_eps: 0.100000\n","      67927/2000000000: episode: 1835, duration: 4.002s, episode steps:  32, steps per second:   8, episode reward: -121.400, mean reward: -3.794 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 67.744324, mean_q: 16.908853, mean_eps: 0.100000\n","      67964/2000000000: episode: 1836, duration: 4.435s, episode steps:  37, steps per second:   8, episode reward: 11.300, mean reward:  0.305 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 64.658197, mean_q: 17.047500, mean_eps: 0.100000\n","      68004/2000000000: episode: 1837, duration: 4.783s, episode steps:  40, steps per second:   8, episode reward: 131.200, mean reward:  3.280 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.512779, mean_q: 17.052994, mean_eps: 0.100000\n","      68035/2000000000: episode: 1838, duration: 3.610s, episode steps:  31, steps per second:   9, episode reward: -46.900, mean reward: -1.513 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 62.247345, mean_q: 17.448772, mean_eps: 0.100000\n","      68070/2000000000: episode: 1839, duration: 4.284s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 0.943 [0.000, 2.000],  loss: 65.542198, mean_q: 17.364896, mean_eps: 0.100000\n","      68109/2000000000: episode: 1840, duration: 4.798s, episode steps:  39, steps per second:   8, episode reward: 78.600, mean reward:  2.015 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 70.390639, mean_q: 16.280991, mean_eps: 0.100000\n","      68149/2000000000: episode: 1841, duration: 4.717s, episode steps:  40, steps per second:   8, episode reward: -7.200, mean reward: -0.180 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 63.748392, mean_q: 16.298419, mean_eps: 0.100000\n","      68184/2000000000: episode: 1842, duration: 4.455s, episode steps:  35, steps per second:   8, episode reward: -60.500, mean reward: -1.729 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 63.622386, mean_q: 17.318497, mean_eps: 0.100000\n","      68224/2000000000: episode: 1843, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward: -115.800, mean reward: -2.895 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 61.963177, mean_q: 16.918253, mean_eps: 0.100000\n","      68263/2000000000: episode: 1844, duration: 4.946s, episode steps:  39, steps per second:   8, episode reward:  7.800, mean reward:  0.200 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 68.071830, mean_q: 17.143952, mean_eps: 0.100000\n","      68303/2000000000: episode: 1845, duration: 4.839s, episode steps:  40, steps per second:   8, episode reward: 38.900, mean reward:  0.972 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.082339, mean_q: 16.714422, mean_eps: 0.100000\n","      68336/2000000000: episode: 1846, duration: 4.187s, episode steps:  33, steps per second:   8, episode reward: -76.200, mean reward: -2.309 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 71.347309, mean_q: 16.736961, mean_eps: 0.100000\n","      68376/2000000000: episode: 1847, duration: 5.206s, episode steps:  40, steps per second:   8, episode reward: -11.200, mean reward: -0.280 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.525059, mean_q: 17.307922, mean_eps: 0.100000\n","      68412/2000000000: episode: 1848, duration: 4.627s, episode steps:  36, steps per second:   8, episode reward: -3.700, mean reward: -0.103 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 63.216010, mean_q: 16.769927, mean_eps: 0.100000\n","      68449/2000000000: episode: 1849, duration: 4.654s, episode steps:  37, steps per second:   8, episode reward: -1.800, mean reward: -0.049 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 63.948654, mean_q: 17.386830, mean_eps: 0.100000\n","      68489/2000000000: episode: 1850, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: -117.400, mean reward: -2.935 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.530362, mean_q: 16.511329, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      68529/2000000000: episode: 1851, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: 46.600, mean reward:  1.165 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.455251, mean_q: 17.010295, mean_eps: 0.100000\n","      68569/2000000000: episode: 1852, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: 68.100, mean reward:  1.702 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.491672, mean_q: 17.052429, mean_eps: 0.100000\n","      68609/2000000000: episode: 1853, duration: 4.672s, episode steps:  40, steps per second:   9, episode reward: 11.900, mean reward:  0.297 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 61.336881, mean_q: 16.752983, mean_eps: 0.100000\n","      68649/2000000000: episode: 1854, duration: 4.666s, episode steps:  40, steps per second:   9, episode reward: 30.900, mean reward:  0.772 [-20.000, 19.800], mean action: 1.500 [0.000, 2.000],  loss: 63.855818, mean_q: 17.543137, mean_eps: 0.100000\n","      68689/2000000000: episode: 1855, duration: 4.512s, episode steps:  40, steps per second:   9, episode reward: -0.000, mean reward: -0.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.843479, mean_q: 16.373659, mean_eps: 0.100000\n","      68728/2000000000: episode: 1856, duration: 4.701s, episode steps:  39, steps per second:   8, episode reward: 57.100, mean reward:  1.464 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 67.764953, mean_q: 16.529540, mean_eps: 0.100000\n","      68765/2000000000: episode: 1857, duration: 4.579s, episode steps:  37, steps per second:   8, episode reward: 94.900, mean reward:  2.565 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 65.522793, mean_q: 16.611644, mean_eps: 0.100000\n","      68803/2000000000: episode: 1858, duration: 5.048s, episode steps:  38, steps per second:   8, episode reward: 96.000, mean reward:  2.526 [-20.000, 19.400], mean action: 1.211 [0.000, 2.000],  loss: 65.974765, mean_q: 17.373439, mean_eps: 0.100000\n","      68837/2000000000: episode: 1859, duration: 4.005s, episode steps:  34, steps per second:   8, episode reward:  9.100, mean reward:  0.268 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 63.116402, mean_q: 16.870112, mean_eps: 0.100000\n","      68875/2000000000: episode: 1860, duration: 4.406s, episode steps:  38, steps per second:   9, episode reward: -58.700, mean reward: -1.545 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 65.418315, mean_q: 16.673812, mean_eps: 0.100000\n","      68915/2000000000: episode: 1861, duration: 5.025s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 67.823273, mean_q: 16.991425, mean_eps: 0.100000\n","      68955/2000000000: episode: 1862, duration: 5.094s, episode steps:  40, steps per second:   8, episode reward: 15.700, mean reward:  0.392 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.077626, mean_q: 16.981614, mean_eps: 0.100000\n","      68995/2000000000: episode: 1863, duration: 4.822s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 63.780394, mean_q: 17.249676, mean_eps: 0.100000\n","      69025/2000000000: episode: 1864, duration: 3.592s, episode steps:  30, steps per second:   8, episode reward: 209.900, mean reward:  6.997 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 62.539463, mean_q: 16.544561, mean_eps: 0.100000\n","      69056/2000000000: episode: 1865, duration: 3.736s, episode steps:  31, steps per second:   8, episode reward: -67.100, mean reward: -2.165 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 68.776790, mean_q: 16.574963, mean_eps: 0.100000\n","      69093/2000000000: episode: 1866, duration: 4.456s, episode steps:  37, steps per second:   8, episode reward: 75.100, mean reward:  2.030 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 64.910561, mean_q: 16.602992, mean_eps: 0.100000\n","      69133/2000000000: episode: 1867, duration: 4.847s, episode steps:  40, steps per second:   8, episode reward: -15.100, mean reward: -0.378 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 59.725225, mean_q: 17.241625, mean_eps: 0.100000\n","      69172/2000000000: episode: 1868, duration: 4.824s, episode steps:  39, steps per second:   8, episode reward: 162.700, mean reward:  4.172 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 67.876423, mean_q: 17.007851, mean_eps: 0.100000\n","      69212/2000000000: episode: 1869, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 120.100, mean reward:  3.003 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.969546, mean_q: 17.154503, mean_eps: 0.100000\n","      69252/2000000000: episode: 1870, duration: 5.121s, episode steps:  40, steps per second:   8, episode reward: -49.800, mean reward: -1.245 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 60.343791, mean_q: 17.668954, mean_eps: 0.100000\n","      69292/2000000000: episode: 1871, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.630693, mean_q: 16.864217, mean_eps: 0.100000\n","      69332/2000000000: episode: 1872, duration: 4.868s, episode steps:  40, steps per second:   8, episode reward: -52.100, mean reward: -1.303 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 60.746524, mean_q: 16.462695, mean_eps: 0.100000\n","      69372/2000000000: episode: 1873, duration: 5.336s, episode steps:  40, steps per second:   7, episode reward: -135.100, mean reward: -3.378 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.200943, mean_q: 16.837350, mean_eps: 0.100000\n","      69406/2000000000: episode: 1874, duration: 4.181s, episode steps:  34, steps per second:   8, episode reward: 36.800, mean reward:  1.082 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 65.740215, mean_q: 17.256500, mean_eps: 0.100000\n","      69446/2000000000: episode: 1875, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.502009, mean_q: 16.773034, mean_eps: 0.100000\n","      69486/2000000000: episode: 1876, duration: 4.756s, episode steps:  40, steps per second:   8, episode reward: 149.000, mean reward:  3.725 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.605353, mean_q: 16.721224, mean_eps: 0.100000\n","      69519/2000000000: episode: 1877, duration: 3.990s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 65.715371, mean_q: 17.001949, mean_eps: 0.100000\n","      69559/2000000000: episode: 1878, duration: 4.736s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.091601, mean_q: 16.688470, mean_eps: 0.100000\n","      69599/2000000000: episode: 1879, duration: 4.768s, episode steps:  40, steps per second:   8, episode reward: 109.900, mean reward:  2.748 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.340895, mean_q: 16.885497, mean_eps: 0.100000\n","      69639/2000000000: episode: 1880, duration: 4.931s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.787139, mean_q: 17.175501, mean_eps: 0.100000\n","      69679/2000000000: episode: 1881, duration: 4.662s, episode steps:  40, steps per second:   9, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 61.978594, mean_q: 16.882263, mean_eps: 0.100000\n","      69719/2000000000: episode: 1882, duration: 4.834s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 69.143402, mean_q: 16.245939, mean_eps: 0.100000\n","      69759/2000000000: episode: 1883, duration: 4.915s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.452998, mean_q: 17.489722, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      69799/2000000000: episode: 1884, duration: 4.951s, episode steps:  40, steps per second:   8, episode reward: -32.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.198307, mean_q: 16.750552, mean_eps: 0.100000\n","      69829/2000000000: episode: 1885, duration: 3.507s, episode steps:  30, steps per second:   9, episode reward: 31.800, mean reward:  1.060 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 69.356395, mean_q: 17.745119, mean_eps: 0.100000\n","      69869/2000000000: episode: 1886, duration: 5.075s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.262324, mean_q: 17.102231, mean_eps: 0.100000\n","      69909/2000000000: episode: 1887, duration: 5.204s, episode steps:  40, steps per second:   8, episode reward: -18.900, mean reward: -0.473 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.036581, mean_q: 17.928048, mean_eps: 0.100000\n","      69944/2000000000: episode: 1888, duration: 4.289s, episode steps:  35, steps per second:   8, episode reward: 26.300, mean reward:  0.751 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 63.496114, mean_q: 17.450413, mean_eps: 0.100000\n","      69981/2000000000: episode: 1889, duration: 4.423s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 71.283705, mean_q: 17.768203, mean_eps: 0.100000\n","      70021/2000000000: episode: 1890, duration: 4.662s, episode steps:  40, steps per second:   9, episode reward: -72.000, mean reward: -1.800 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.789926, mean_q: 16.868790, mean_eps: 0.100000\n","      70055/2000000000: episode: 1891, duration: 4.194s, episode steps:  34, steps per second:   8, episode reward: 116.600, mean reward:  3.429 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 70.583126, mean_q: 18.048233, mean_eps: 0.100000\n","      70086/2000000000: episode: 1892, duration: 3.655s, episode steps:  31, steps per second:   8, episode reward: 105.800, mean reward:  3.413 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 64.444656, mean_q: 18.738188, mean_eps: 0.100000\n","      70126/2000000000: episode: 1893, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: -18.500, mean reward: -0.462 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.531100, mean_q: 18.244278, mean_eps: 0.100000\n","      70166/2000000000: episode: 1894, duration: 4.982s, episode steps:  40, steps per second:   8, episode reward: -31.300, mean reward: -0.783 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.690384, mean_q: 17.467934, mean_eps: 0.100000\n","      70206/2000000000: episode: 1895, duration: 4.744s, episode steps:  40, steps per second:   8, episode reward: -23.400, mean reward: -0.585 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.773795, mean_q: 17.573008, mean_eps: 0.100000\n","      70246/2000000000: episode: 1896, duration: 4.995s, episode steps:  40, steps per second:   8, episode reward: -7.600, mean reward: -0.190 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.862061, mean_q: 18.073277, mean_eps: 0.100000\n","      70286/2000000000: episode: 1897, duration: 4.641s, episode steps:  40, steps per second:   9, episode reward: -150.000, mean reward: -3.750 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.541945, mean_q: 17.662936, mean_eps: 0.100000\n","      70318/2000000000: episode: 1898, duration: 3.805s, episode steps:  32, steps per second:   8, episode reward: -1.200, mean reward: -0.037 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.965882, mean_q: 18.162157, mean_eps: 0.100000\n","      70358/2000000000: episode: 1899, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.826128, mean_q: 17.808691, mean_eps: 0.100000\n","      70398/2000000000: episode: 1900, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.648743, mean_q: 17.421175, mean_eps: 0.100000\n","      70438/2000000000: episode: 1901, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: -17.100, mean reward: -0.428 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.083395, mean_q: 17.921267, mean_eps: 0.100000\n","      70477/2000000000: episode: 1902, duration: 4.621s, episode steps:  39, steps per second:   8, episode reward: 82.100, mean reward:  2.105 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 70.276098, mean_q: 17.865233, mean_eps: 0.100000\n","      70512/2000000000: episode: 1903, duration: 4.259s, episode steps:  35, steps per second:   8, episode reward: -34.100, mean reward: -0.974 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 66.877791, mean_q: 18.140551, mean_eps: 0.100000\n","      70552/2000000000: episode: 1904, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward:  1.100, mean reward:  0.027 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.650936, mean_q: 18.336049, mean_eps: 0.100000\n","      70592/2000000000: episode: 1905, duration: 4.898s, episode steps:  40, steps per second:   8, episode reward: -73.100, mean reward: -1.828 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.709543, mean_q: 17.678903, mean_eps: 0.100000\n","      70630/2000000000: episode: 1906, duration: 4.565s, episode steps:  38, steps per second:   8, episode reward: -134.000, mean reward: -3.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 70.916393, mean_q: 18.158993, mean_eps: 0.100000\n","      70665/2000000000: episode: 1907, duration: 3.898s, episode steps:  35, steps per second:   9, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.599672, mean_q: 18.330163, mean_eps: 0.100000\n","      70695/2000000000: episode: 1908, duration: 3.436s, episode steps:  30, steps per second:   9, episode reward: 58.400, mean reward:  1.947 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 62.383761, mean_q: 17.396401, mean_eps: 0.100000\n","      70735/2000000000: episode: 1909, duration: 4.510s, episode steps:  40, steps per second:   9, episode reward: -64.300, mean reward: -1.607 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.354112, mean_q: 18.389988, mean_eps: 0.100000\n","      70775/2000000000: episode: 1910, duration: 4.698s, episode steps:  40, steps per second:   9, episode reward: 56.900, mean reward:  1.423 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.808638, mean_q: 18.203582, mean_eps: 0.100000\n","      70815/2000000000: episode: 1911, duration: 4.528s, episode steps:  40, steps per second:   9, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.983891, mean_q: 17.889759, mean_eps: 0.100000\n","      70847/2000000000: episode: 1912, duration: 3.852s, episode steps:  32, steps per second:   8, episode reward: -6.400, mean reward: -0.200 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 65.490700, mean_q: 17.871427, mean_eps: 0.100000\n","      70887/2000000000: episode: 1913, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: -6.000, mean reward: -0.150 [-20.000, 18.000], mean action: 1.800 [0.000, 2.000],  loss: 72.032570, mean_q: 17.874882, mean_eps: 0.100000\n","      70927/2000000000: episode: 1914, duration: 4.628s, episode steps:  40, steps per second:   9, episode reward: 83.200, mean reward:  2.080 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.479588, mean_q: 17.883542, mean_eps: 0.100000\n","      70967/2000000000: episode: 1915, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: -17.800, mean reward: -0.445 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.676120, mean_q: 17.635800, mean_eps: 0.100000\n","      71007/2000000000: episode: 1916, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: -3.100, mean reward: -0.077 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 74.797601, mean_q: 18.304936, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      71047/2000000000: episode: 1917, duration: 4.686s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.799146, mean_q: 18.749658, mean_eps: 0.100000\n","      71084/2000000000: episode: 1918, duration: 4.398s, episode steps:  37, steps per second:   8, episode reward:  2.200, mean reward:  0.059 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 64.689339, mean_q: 17.733711, mean_eps: 0.100000\n","      71124/2000000000: episode: 1919, duration: 5.230s, episode steps:  40, steps per second:   8, episode reward: -29.200, mean reward: -0.730 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.472231, mean_q: 17.877914, mean_eps: 0.100000\n","      71164/2000000000: episode: 1920, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: 48.400, mean reward:  1.210 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.625338, mean_q: 17.615003, mean_eps: 0.100000\n","      71201/2000000000: episode: 1921, duration: 4.357s, episode steps:  37, steps per second:   8, episode reward: -41.900, mean reward: -1.132 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 69.082521, mean_q: 17.496362, mean_eps: 0.100000\n","      71241/2000000000: episode: 1922, duration: 4.520s, episode steps:  40, steps per second:   9, episode reward: 26.300, mean reward:  0.658 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 68.429005, mean_q: 17.691600, mean_eps: 0.100000\n","      71281/2000000000: episode: 1923, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: 13.800, mean reward:  0.345 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.069455, mean_q: 17.721676, mean_eps: 0.100000\n","      71317/2000000000: episode: 1924, duration: 4.008s, episode steps:  36, steps per second:   9, episode reward: 113.300, mean reward:  3.147 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 67.513084, mean_q: 17.930191, mean_eps: 0.100000\n","      71357/2000000000: episode: 1925, duration: 4.670s, episode steps:  40, steps per second:   9, episode reward: 15.700, mean reward:  0.392 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.767295, mean_q: 18.003965, mean_eps: 0.100000\n","      71397/2000000000: episode: 1926, duration: 4.391s, episode steps:  40, steps per second:   9, episode reward: -40.500, mean reward: -1.012 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.689822, mean_q: 18.031060, mean_eps: 0.100000\n","      71437/2000000000: episode: 1927, duration: 4.582s, episode steps:  40, steps per second:   9, episode reward: 32.000, mean reward:  0.800 [-20.000, 18.000], mean action: 1.800 [0.000, 2.000],  loss: 64.327155, mean_q: 17.175171, mean_eps: 0.100000\n","      71477/2000000000: episode: 1928, duration: 4.671s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.880267, mean_q: 17.702003, mean_eps: 0.100000\n","      71517/2000000000: episode: 1929, duration: 4.657s, episode steps:  40, steps per second:   9, episode reward: 10.000, mean reward:  0.250 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.070895, mean_q: 17.727751, mean_eps: 0.100000\n","      71557/2000000000: episode: 1930, duration: 4.739s, episode steps:  40, steps per second:   8, episode reward: -52.200, mean reward: -1.305 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.419195, mean_q: 17.862948, mean_eps: 0.100000\n","      71597/2000000000: episode: 1931, duration: 5.070s, episode steps:  40, steps per second:   8, episode reward: -12.900, mean reward: -0.323 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 67.711588, mean_q: 17.664628, mean_eps: 0.100000\n","      71637/2000000000: episode: 1932, duration: 4.792s, episode steps:  40, steps per second:   8, episode reward: 133.800, mean reward:  3.345 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.135198, mean_q: 18.284344, mean_eps: 0.100000\n","      71676/2000000000: episode: 1933, duration: 4.600s, episode steps:  39, steps per second:   8, episode reward: 115.000, mean reward:  2.949 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 66.624479, mean_q: 17.994921, mean_eps: 0.100000\n","      71710/2000000000: episode: 1934, duration: 4.150s, episode steps:  34, steps per second:   8, episode reward: -118.600, mean reward: -3.488 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 61.767250, mean_q: 18.191174, mean_eps: 0.100000\n","      71750/2000000000: episode: 1935, duration: 4.627s, episode steps:  40, steps per second:   9, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.303478, mean_q: 18.168604, mean_eps: 0.100000\n","      71785/2000000000: episode: 1936, duration: 4.094s, episode steps:  35, steps per second:   9, episode reward: 80.900, mean reward:  2.311 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.707453, mean_q: 18.962938, mean_eps: 0.100000\n","      71820/2000000000: episode: 1937, duration: 3.918s, episode steps:  35, steps per second:   9, episode reward: 26.700, mean reward:  0.763 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 65.395969, mean_q: 18.205545, mean_eps: 0.100000\n","      71860/2000000000: episode: 1938, duration: 4.678s, episode steps:  40, steps per second:   9, episode reward: -120.500, mean reward: -3.012 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.623962, mean_q: 18.860659, mean_eps: 0.100000\n","      71900/2000000000: episode: 1939, duration: 4.866s, episode steps:  40, steps per second:   8, episode reward:  6.400, mean reward:  0.160 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 60.848766, mean_q: 17.514736, mean_eps: 0.100000\n","      71929/2000000000: episode: 1940, duration: 3.478s, episode steps:  29, steps per second:   8, episode reward: -14.500, mean reward: -0.500 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 62.849933, mean_q: 18.496526, mean_eps: 0.100000\n","      71967/2000000000: episode: 1941, duration: 4.663s, episode steps:  38, steps per second:   8, episode reward: 92.500, mean reward:  2.434 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 71.321473, mean_q: 17.402981, mean_eps: 0.100000\n","      72007/2000000000: episode: 1942, duration: 4.656s, episode steps:  40, steps per second:   9, episode reward: -9.000, mean reward: -0.225 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.831120, mean_q: 18.097339, mean_eps: 0.100000\n","      72038/2000000000: episode: 1943, duration: 3.718s, episode steps:  31, steps per second:   8, episode reward: 246.000, mean reward:  7.935 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 67.995207, mean_q: 18.102304, mean_eps: 0.100000\n","      72078/2000000000: episode: 1944, duration: 4.747s, episode steps:  40, steps per second:   8, episode reward: -213.900, mean reward: -5.348 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.333312, mean_q: 17.258511, mean_eps: 0.100000\n","      72118/2000000000: episode: 1945, duration: 4.698s, episode steps:  40, steps per second:   9, episode reward: -140.800, mean reward: -3.520 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.493231, mean_q: 18.163937, mean_eps: 0.100000\n","      72158/2000000000: episode: 1946, duration: 4.860s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.533059, mean_q: 17.971763, mean_eps: 0.100000\n","      72196/2000000000: episode: 1947, duration: 4.669s, episode steps:  38, steps per second:   8, episode reward: 113.600, mean reward:  2.989 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 72.496340, mean_q: 17.653667, mean_eps: 0.100000\n","      72231/2000000000: episode: 1948, duration: 4.105s, episode steps:  35, steps per second:   9, episode reward: 46.000, mean reward:  1.314 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 70.183058, mean_q: 17.406900, mean_eps: 0.100000\n","      72271/2000000000: episode: 1949, duration: 4.648s, episode steps:  40, steps per second:   9, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 61.688642, mean_q: 18.485123, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      72311/2000000000: episode: 1950, duration: 4.839s, episode steps:  40, steps per second:   8, episode reward: 161.600, mean reward:  4.040 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.508520, mean_q: 17.479973, mean_eps: 0.100000\n","      72347/2000000000: episode: 1951, duration: 4.255s, episode steps:  36, steps per second:   8, episode reward: 94.100, mean reward:  2.614 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 71.979811, mean_q: 18.059491, mean_eps: 0.100000\n","      72387/2000000000: episode: 1952, duration: 4.774s, episode steps:  40, steps per second:   8, episode reward: -11.600, mean reward: -0.290 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.625046, mean_q: 18.223948, mean_eps: 0.100000\n","      72427/2000000000: episode: 1953, duration: 4.714s, episode steps:  40, steps per second:   8, episode reward: 57.700, mean reward:  1.442 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.520585, mean_q: 17.546221, mean_eps: 0.100000\n","      72467/2000000000: episode: 1954, duration: 4.675s, episode steps:  40, steps per second:   9, episode reward: -61.300, mean reward: -1.533 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.083858, mean_q: 17.767649, mean_eps: 0.100000\n","      72507/2000000000: episode: 1955, duration: 4.617s, episode steps:  40, steps per second:   9, episode reward: -0.700, mean reward: -0.018 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.780907, mean_q: 17.451812, mean_eps: 0.100000\n","      72547/2000000000: episode: 1956, duration: 4.737s, episode steps:  40, steps per second:   8, episode reward: 60.900, mean reward:  1.522 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 63.775209, mean_q: 18.274243, mean_eps: 0.100000\n","      72587/2000000000: episode: 1957, duration: 4.760s, episode steps:  40, steps per second:   8, episode reward: -2.100, mean reward: -0.053 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.072988, mean_q: 17.705602, mean_eps: 0.100000\n","      72627/2000000000: episode: 1958, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: -49.700, mean reward: -1.243 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.145524, mean_q: 18.069186, mean_eps: 0.100000\n","      72665/2000000000: episode: 1959, duration: 4.415s, episode steps:  38, steps per second:   9, episode reward: 132.400, mean reward:  3.484 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 68.381615, mean_q: 17.764078, mean_eps: 0.100000\n","      72705/2000000000: episode: 1960, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: -26.800, mean reward: -0.670 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.118744, mean_q: 17.641714, mean_eps: 0.100000\n","      72745/2000000000: episode: 1961, duration: 4.908s, episode steps:  40, steps per second:   8, episode reward: 72.900, mean reward:  1.822 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.409536, mean_q: 18.026974, mean_eps: 0.100000\n","      72781/2000000000: episode: 1962, duration: 4.715s, episode steps:  36, steps per second:   8, episode reward: 45.700, mean reward:  1.269 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 69.967578, mean_q: 17.549446, mean_eps: 0.100000\n","      72810/2000000000: episode: 1963, duration: 3.781s, episode steps:  29, steps per second:   8, episode reward: -55.000, mean reward: -1.897 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 68.546378, mean_q: 18.035554, mean_eps: 0.100000\n","      72846/2000000000: episode: 1964, duration: 4.337s, episode steps:  36, steps per second:   8, episode reward: -55.500, mean reward: -1.542 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 65.640290, mean_q: 17.941917, mean_eps: 0.100000\n","      72886/2000000000: episode: 1965, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: 39.500, mean reward:  0.988 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.817873, mean_q: 18.051066, mean_eps: 0.100000\n","      72926/2000000000: episode: 1966, duration: 5.431s, episode steps:  40, steps per second:   7, episode reward: -42.500, mean reward: -1.062 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.158641, mean_q: 17.630509, mean_eps: 0.100000\n","      72961/2000000000: episode: 1967, duration: 4.687s, episode steps:  35, steps per second:   7, episode reward: -65.900, mean reward: -1.883 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 70.779178, mean_q: 18.059515, mean_eps: 0.100000\n","      73001/2000000000: episode: 1968, duration: 5.239s, episode steps:  40, steps per second:   8, episode reward: 58.900, mean reward:  1.473 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 59.650670, mean_q: 17.289028, mean_eps: 0.100000\n","      73041/2000000000: episode: 1969, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward: -31.500, mean reward: -0.788 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 66.231021, mean_q: 18.060913, mean_eps: 0.100000\n","      73077/2000000000: episode: 1970, duration: 4.706s, episode steps:  36, steps per second:   8, episode reward:  5.600, mean reward:  0.156 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 65.020037, mean_q: 18.138869, mean_eps: 0.100000\n","      73117/2000000000: episode: 1971, duration: 5.041s, episode steps:  40, steps per second:   8, episode reward: 31.300, mean reward:  0.783 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 67.051881, mean_q: 17.869621, mean_eps: 0.100000\n","      73157/2000000000: episode: 1972, duration: 4.801s, episode steps:  40, steps per second:   8, episode reward: 49.900, mean reward:  1.247 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.828652, mean_q: 17.765676, mean_eps: 0.100000\n","      73187/2000000000: episode: 1973, duration: 3.789s, episode steps:  30, steps per second:   8, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 65.598299, mean_q: 17.608044, mean_eps: 0.100000\n","      73227/2000000000: episode: 1974, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 62.486058, mean_q: 17.112631, mean_eps: 0.100000\n","      73267/2000000000: episode: 1975, duration: 4.984s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.001661, mean_q: 17.316577, mean_eps: 0.100000\n","      73307/2000000000: episode: 1976, duration: 4.688s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.054191, mean_q: 18.964174, mean_eps: 0.100000\n","      73347/2000000000: episode: 1977, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward: -8.000, mean reward: -0.200 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.524524, mean_q: 17.665424, mean_eps: 0.100000\n","      73387/2000000000: episode: 1978, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: -2.400, mean reward: -0.060 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.596880, mean_q: 18.054819, mean_eps: 0.100000\n","      73427/2000000000: episode: 1979, duration: 4.729s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.087813, mean_q: 17.777507, mean_eps: 0.100000\n","      73467/2000000000: episode: 1980, duration: 4.962s, episode steps:  40, steps per second:   8, episode reward: 78.600, mean reward:  1.965 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.576822, mean_q: 17.801940, mean_eps: 0.100000\n","      73507/2000000000: episode: 1981, duration: 4.839s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 64.786961, mean_q: 17.583054, mean_eps: 0.100000\n","      73539/2000000000: episode: 1982, duration: 3.645s, episode steps:  32, steps per second:   9, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 73.446828, mean_q: 18.329350, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      73579/2000000000: episode: 1983, duration: 4.587s, episode steps:  40, steps per second:   9, episode reward: -33.000, mean reward: -0.825 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.524004, mean_q: 18.541934, mean_eps: 0.100000\n","      73619/2000000000: episode: 1984, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: -19.800, mean reward: -0.495 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.605766, mean_q: 17.082509, mean_eps: 0.100000\n","      73659/2000000000: episode: 1985, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: -45.200, mean reward: -1.130 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.119813, mean_q: 18.314584, mean_eps: 0.100000\n","      73699/2000000000: episode: 1986, duration: 4.720s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 61.655421, mean_q: 17.494459, mean_eps: 0.100000\n","      73739/2000000000: episode: 1987, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: -33.300, mean reward: -0.832 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.531379, mean_q: 17.414693, mean_eps: 0.100000\n","      73779/2000000000: episode: 1988, duration: 4.569s, episode steps:  40, steps per second:   9, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.740240, mean_q: 17.681465, mean_eps: 0.100000\n","      73819/2000000000: episode: 1989, duration: 4.569s, episode steps:  40, steps per second:   9, episode reward: -112.000, mean reward: -2.800 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 66.271918, mean_q: 17.335227, mean_eps: 0.100000\n","      73859/2000000000: episode: 1990, duration: 4.686s, episode steps:  40, steps per second:   9, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.402440, mean_q: 17.787701, mean_eps: 0.100000\n","      73899/2000000000: episode: 1991, duration: 4.544s, episode steps:  40, steps per second:   9, episode reward: -88.000, mean reward: -2.200 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 75.981373, mean_q: 18.062280, mean_eps: 0.100000\n","      73939/2000000000: episode: 1992, duration: 4.757s, episode steps:  40, steps per second:   8, episode reward:  2.400, mean reward:  0.060 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.719373, mean_q: 17.542718, mean_eps: 0.100000\n","      73967/2000000000: episode: 1993, duration: 3.624s, episode steps:  28, steps per second:   8, episode reward: -34.700, mean reward: -1.239 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 70.381008, mean_q: 17.998757, mean_eps: 0.100000\n","      74005/2000000000: episode: 1994, duration: 4.734s, episode steps:  38, steps per second:   8, episode reward: 83.400, mean reward:  2.195 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 71.973513, mean_q: 18.913090, mean_eps: 0.100000\n","      74045/2000000000: episode: 1995, duration: 4.599s, episode steps:  40, steps per second:   9, episode reward: 101.100, mean reward:  2.528 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.390083, mean_q: 18.076263, mean_eps: 0.100000\n","      74085/2000000000: episode: 1996, duration: 4.868s, episode steps:  40, steps per second:   8, episode reward: -17.800, mean reward: -0.445 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.200916, mean_q: 17.602810, mean_eps: 0.100000\n","      74125/2000000000: episode: 1997, duration: 5.068s, episode steps:  40, steps per second:   8, episode reward: 20.400, mean reward:  0.510 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.692214, mean_q: 18.083232, mean_eps: 0.100000\n","      74165/2000000000: episode: 1998, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward:  0.100, mean reward:  0.002 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 63.286285, mean_q: 17.454991, mean_eps: 0.100000\n","      74205/2000000000: episode: 1999, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 174.000, mean reward:  4.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.863186, mean_q: 17.471280, mean_eps: 0.100000\n","      74245/2000000000: episode: 2000, duration: 5.045s, episode steps:  40, steps per second:   8, episode reward: -152.500, mean reward: -3.813 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 63.952296, mean_q: 17.419172, mean_eps: 0.100000\n","      74284/2000000000: episode: 2001, duration: 5.047s, episode steps:  39, steps per second:   8, episode reward: 14.600, mean reward:  0.374 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 63.298300, mean_q: 17.770613, mean_eps: 0.100000\n","      74324/2000000000: episode: 2002, duration: 5.330s, episode steps:  40, steps per second:   8, episode reward: -5.600, mean reward: -0.140 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.576540, mean_q: 17.852801, mean_eps: 0.100000\n","      74364/2000000000: episode: 2003, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: 108.000, mean reward:  2.700 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.641906, mean_q: 18.135650, mean_eps: 0.100000\n","      74404/2000000000: episode: 2004, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward: 51.300, mean reward:  1.282 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 64.383891, mean_q: 18.194395, mean_eps: 0.100000\n","      74444/2000000000: episode: 2005, duration: 5.403s, episode steps:  40, steps per second:   7, episode reward: -62.300, mean reward: -1.558 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.666529, mean_q: 17.512144, mean_eps: 0.100000\n","      74484/2000000000: episode: 2006, duration: 5.504s, episode steps:  40, steps per second:   7, episode reward: 60.900, mean reward:  1.522 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 63.928976, mean_q: 17.629890, mean_eps: 0.100000\n","      74523/2000000000: episode: 2007, duration: 5.297s, episode steps:  39, steps per second:   7, episode reward: -12.900, mean reward: -0.331 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 68.335947, mean_q: 18.062995, mean_eps: 0.100000\n","      74557/2000000000: episode: 2008, duration: 4.549s, episode steps:  34, steps per second:   7, episode reward: -98.000, mean reward: -2.882 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 71.974199, mean_q: 17.495046, mean_eps: 0.100000\n","      74587/2000000000: episode: 2009, duration: 4.009s, episode steps:  30, steps per second:   7, episode reward: 13.900, mean reward:  0.463 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 67.984899, mean_q: 18.384349, mean_eps: 0.100000\n","      74625/2000000000: episode: 2010, duration: 5.131s, episode steps:  38, steps per second:   7, episode reward: 65.400, mean reward:  1.721 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 68.866836, mean_q: 17.964438, mean_eps: 0.100000\n","      74661/2000000000: episode: 2011, duration: 4.795s, episode steps:  36, steps per second:   8, episode reward: -62.800, mean reward: -1.744 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 62.587126, mean_q: 17.483495, mean_eps: 0.100000\n","      74698/2000000000: episode: 2012, duration: 5.036s, episode steps:  37, steps per second:   7, episode reward: -32.600, mean reward: -0.881 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 63.130541, mean_q: 17.810629, mean_eps: 0.100000\n","      74738/2000000000: episode: 2013, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: 11.600, mean reward:  0.290 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.256468, mean_q: 17.965783, mean_eps: 0.100000\n","      74778/2000000000: episode: 2014, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.403873, mean_q: 18.142051, mean_eps: 0.100000\n","      74818/2000000000: episode: 2015, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: -87.500, mean reward: -2.187 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 70.001320, mean_q: 18.194882, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      74858/2000000000: episode: 2016, duration: 5.228s, episode steps:  40, steps per second:   8, episode reward: 131.000, mean reward:  3.275 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.709444, mean_q: 18.340455, mean_eps: 0.100000\n","      74898/2000000000: episode: 2017, duration: 5.376s, episode steps:  40, steps per second:   7, episode reward: -47.200, mean reward: -1.180 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.987802, mean_q: 17.655231, mean_eps: 0.100000\n","      74938/2000000000: episode: 2018, duration: 5.259s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.452821, mean_q: 17.332225, mean_eps: 0.100000\n","      74978/2000000000: episode: 2019, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: 58.100, mean reward:  1.453 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.798490, mean_q: 17.996868, mean_eps: 0.100000\n","      75018/2000000000: episode: 2020, duration: 4.941s, episode steps:  40, steps per second:   8, episode reward: 73.600, mean reward:  1.840 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 59.326736, mean_q: 17.437111, mean_eps: 0.100000\n","      75058/2000000000: episode: 2021, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward: -64.700, mean reward: -1.618 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.845580, mean_q: 17.889851, mean_eps: 0.100000\n","      75098/2000000000: episode: 2022, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward: -54.800, mean reward: -1.370 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.098936, mean_q: 17.369691, mean_eps: 0.100000\n","      75138/2000000000: episode: 2023, duration: 5.011s, episode steps:  40, steps per second:   8, episode reward: 72.000, mean reward:  1.800 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.757370, mean_q: 18.359862, mean_eps: 0.100000\n","      75174/2000000000: episode: 2024, duration: 4.870s, episode steps:  36, steps per second:   7, episode reward: 156.600, mean reward:  4.350 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 65.663472, mean_q: 17.478785, mean_eps: 0.100000\n","      75214/2000000000: episode: 2025, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward: 19.900, mean reward:  0.497 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.746571, mean_q: 17.854435, mean_eps: 0.100000\n","      75253/2000000000: episode: 2026, duration: 5.089s, episode steps:  39, steps per second:   8, episode reward: 40.700, mean reward:  1.044 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 69.987514, mean_q: 18.196832, mean_eps: 0.100000\n","      75283/2000000000: episode: 2027, duration: 3.978s, episode steps:  30, steps per second:   8, episode reward: 153.500, mean reward:  5.117 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 63.835807, mean_q: 18.146319, mean_eps: 0.100000\n","      75323/2000000000: episode: 2028, duration: 5.140s, episode steps:  40, steps per second:   8, episode reward: 41.000, mean reward:  1.025 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.226314, mean_q: 18.056571, mean_eps: 0.100000\n","      75363/2000000000: episode: 2029, duration: 5.322s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.485765, mean_q: 17.605272, mean_eps: 0.100000\n","      75395/2000000000: episode: 2030, duration: 4.392s, episode steps:  32, steps per second:   7, episode reward: -43.900, mean reward: -1.372 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 72.456777, mean_q: 18.000034, mean_eps: 0.100000\n","      75435/2000000000: episode: 2031, duration: 5.421s, episode steps:  40, steps per second:   7, episode reward: 89.200, mean reward:  2.230 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 61.325750, mean_q: 18.661458, mean_eps: 0.100000\n","      75475/2000000000: episode: 2032, duration: 5.451s, episode steps:  40, steps per second:   7, episode reward: 77.000, mean reward:  1.925 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.262013, mean_q: 17.762255, mean_eps: 0.100000\n","      75511/2000000000: episode: 2033, duration: 4.947s, episode steps:  36, steps per second:   7, episode reward: -62.500, mean reward: -1.736 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 68.808500, mean_q: 17.372529, mean_eps: 0.100000\n","      75551/2000000000: episode: 2034, duration: 5.217s, episode steps:  40, steps per second:   8, episode reward: -27.200, mean reward: -0.680 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 67.469105, mean_q: 17.792334, mean_eps: 0.100000\n","      75587/2000000000: episode: 2035, duration: 5.002s, episode steps:  36, steps per second:   7, episode reward: 14.000, mean reward:  0.389 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 68.634731, mean_q: 17.614478, mean_eps: 0.100000\n","      75626/2000000000: episode: 2036, duration: 5.160s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 63.634798, mean_q: 18.144530, mean_eps: 0.100000\n","      75666/2000000000: episode: 2037, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 64.482048, mean_q: 17.830224, mean_eps: 0.100000\n","      75701/2000000000: episode: 2038, duration: 4.618s, episode steps:  35, steps per second:   8, episode reward: 81.000, mean reward:  2.314 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 68.695454, mean_q: 18.382893, mean_eps: 0.100000\n","      75741/2000000000: episode: 2039, duration: 5.302s, episode steps:  40, steps per second:   8, episode reward: 99.100, mean reward:  2.477 [-20.000, 19.400], mean action: 1.600 [0.000, 2.000],  loss: 69.738836, mean_q: 17.633242, mean_eps: 0.100000\n","      75774/2000000000: episode: 2040, duration: 4.655s, episode steps:  33, steps per second:   7, episode reward: 12.600, mean reward:  0.382 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 68.578318, mean_q: 18.612635, mean_eps: 0.100000\n","      75800/2000000000: episode: 2041, duration: 3.744s, episode steps:  26, steps per second:   7, episode reward: -163.400, mean reward: -6.285 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 65.341451, mean_q: 17.676298, mean_eps: 0.100000\n","      75840/2000000000: episode: 2042, duration: 5.952s, episode steps:  40, steps per second:   7, episode reward: 36.300, mean reward:  0.908 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.317725, mean_q: 17.425596, mean_eps: 0.100000\n","      75880/2000000000: episode: 2043, duration: 5.648s, episode steps:  40, steps per second:   7, episode reward: 80.600, mean reward:  2.015 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 60.413618, mean_q: 17.729382, mean_eps: 0.100000\n","      75916/2000000000: episode: 2044, duration: 5.112s, episode steps:  36, steps per second:   7, episode reward: -29.900, mean reward: -0.831 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 67.002390, mean_q: 17.558856, mean_eps: 0.100000\n","      75956/2000000000: episode: 2045, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: 14.500, mean reward:  0.363 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.636537, mean_q: 18.189971, mean_eps: 0.100000\n","      75995/2000000000: episode: 2046, duration: 5.538s, episode steps:  39, steps per second:   7, episode reward: 110.300, mean reward:  2.828 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 62.387280, mean_q: 17.635171, mean_eps: 0.100000\n","      76035/2000000000: episode: 2047, duration: 5.526s, episode steps:  40, steps per second:   7, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.156745, mean_q: 18.129618, mean_eps: 0.100000\n","      76075/2000000000: episode: 2048, duration: 5.532s, episode steps:  40, steps per second:   7, episode reward:  5.800, mean reward:  0.145 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.705044, mean_q: 17.656039, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      76115/2000000000: episode: 2049, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: -12.500, mean reward: -0.313 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.929657, mean_q: 17.678673, mean_eps: 0.100000\n","      76155/2000000000: episode: 2050, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward: 44.400, mean reward:  1.110 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.140728, mean_q: 17.767454, mean_eps: 0.100000\n","      76184/2000000000: episode: 2051, duration: 3.928s, episode steps:  29, steps per second:   7, episode reward: -2.900, mean reward: -0.100 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 70.803165, mean_q: 17.497696, mean_eps: 0.100000\n","      76222/2000000000: episode: 2052, duration: 4.845s, episode steps:  38, steps per second:   8, episode reward:  3.000, mean reward:  0.079 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 62.201203, mean_q: 17.765089, mean_eps: 0.100000\n","      76262/2000000000: episode: 2053, duration: 5.476s, episode steps:  40, steps per second:   7, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.985571, mean_q: 17.848302, mean_eps: 0.100000\n","      76296/2000000000: episode: 2054, duration: 4.615s, episode steps:  34, steps per second:   7, episode reward: -44.400, mean reward: -1.306 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 60.579372, mean_q: 18.250276, mean_eps: 0.100000\n","      76336/2000000000: episode: 2055, duration: 5.339s, episode steps:  40, steps per second:   7, episode reward: 18.800, mean reward:  0.470 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.001840, mean_q: 18.483877, mean_eps: 0.100000\n","      76371/2000000000: episode: 2056, duration: 4.690s, episode steps:  35, steps per second:   7, episode reward: -97.700, mean reward: -2.791 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 64.570466, mean_q: 18.067692, mean_eps: 0.100000\n","      76411/2000000000: episode: 2057, duration: 5.394s, episode steps:  40, steps per second:   7, episode reward: 54.500, mean reward:  1.363 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.963633, mean_q: 18.341595, mean_eps: 0.100000\n","      76451/2000000000: episode: 2058, duration: 5.487s, episode steps:  40, steps per second:   7, episode reward: 72.500, mean reward:  1.812 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 66.636549, mean_q: 17.795165, mean_eps: 0.100000\n","      76491/2000000000: episode: 2059, duration: 6.123s, episode steps:  40, steps per second:   7, episode reward: 73.900, mean reward:  1.848 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.192958, mean_q: 18.381992, mean_eps: 0.100000\n","      76531/2000000000: episode: 2060, duration: 6.118s, episode steps:  40, steps per second:   7, episode reward: 35.400, mean reward:  0.885 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.350929, mean_q: 17.935792, mean_eps: 0.100000\n","      76566/2000000000: episode: 2061, duration: 5.219s, episode steps:  35, steps per second:   7, episode reward: -33.200, mean reward: -0.949 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 75.241419, mean_q: 18.802980, mean_eps: 0.100000\n","      76606/2000000000: episode: 2062, duration: 5.422s, episode steps:  40, steps per second:   7, episode reward: -52.700, mean reward: -1.317 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.106010, mean_q: 18.227111, mean_eps: 0.100000\n","      76641/2000000000: episode: 2063, duration: 5.030s, episode steps:  35, steps per second:   7, episode reward: 23.400, mean reward:  0.669 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 68.519567, mean_q: 18.040695, mean_eps: 0.100000\n","      76681/2000000000: episode: 2064, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward:  2.400, mean reward:  0.060 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.568134, mean_q: 17.767850, mean_eps: 0.100000\n","      76717/2000000000: episode: 2065, duration: 4.602s, episode steps:  36, steps per second:   8, episode reward: -28.200, mean reward: -0.783 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 67.399922, mean_q: 18.760038, mean_eps: 0.100000\n","      76757/2000000000: episode: 2066, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: -44.700, mean reward: -1.118 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 63.231525, mean_q: 18.444601, mean_eps: 0.100000\n","      76794/2000000000: episode: 2067, duration: 4.641s, episode steps:  37, steps per second:   8, episode reward: -44.200, mean reward: -1.195 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 71.968130, mean_q: 18.293810, mean_eps: 0.100000\n","      76832/2000000000: episode: 2068, duration: 4.923s, episode steps:  38, steps per second:   8, episode reward: -10.000, mean reward: -0.263 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 64.496324, mean_q: 18.015163, mean_eps: 0.100000\n","      76872/2000000000: episode: 2069, duration: 4.959s, episode steps:  40, steps per second:   8, episode reward: 106.400, mean reward:  2.660 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.610383, mean_q: 17.522163, mean_eps: 0.100000\n","      76912/2000000000: episode: 2070, duration: 5.159s, episode steps:  40, steps per second:   8, episode reward: -39.800, mean reward: -0.995 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 67.820436, mean_q: 17.973042, mean_eps: 0.100000\n","      76951/2000000000: episode: 2071, duration: 5.088s, episode steps:  39, steps per second:   8, episode reward: -34.700, mean reward: -0.890 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 64.968386, mean_q: 18.009730, mean_eps: 0.100000\n","      76991/2000000000: episode: 2072, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: -4.300, mean reward: -0.107 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 60.525920, mean_q: 17.802924, mean_eps: 0.100000\n","      77029/2000000000: episode: 2073, duration: 5.362s, episode steps:  38, steps per second:   7, episode reward: -2.500, mean reward: -0.066 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 69.176371, mean_q: 17.302387, mean_eps: 0.100000\n","      77059/2000000000: episode: 2074, duration: 4.195s, episode steps:  30, steps per second:   7, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 66.909883, mean_q: 17.828796, mean_eps: 0.100000\n","      77099/2000000000: episode: 2075, duration: 5.317s, episode steps:  40, steps per second:   8, episode reward: -39.900, mean reward: -0.998 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.001991, mean_q: 18.193964, mean_eps: 0.100000\n","      77139/2000000000: episode: 2076, duration: 5.050s, episode steps:  40, steps per second:   8, episode reward: 82.700, mean reward:  2.067 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.203345, mean_q: 17.436907, mean_eps: 0.100000\n","      77174/2000000000: episode: 2077, duration: 4.572s, episode steps:  35, steps per second:   8, episode reward: -38.100, mean reward: -1.089 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 64.032072, mean_q: 17.152152, mean_eps: 0.100000\n","      77209/2000000000: episode: 2078, duration: 4.600s, episode steps:  35, steps per second:   8, episode reward: -29.100, mean reward: -0.831 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 59.286035, mean_q: 17.717906, mean_eps: 0.100000\n","      77249/2000000000: episode: 2079, duration: 5.299s, episode steps:  40, steps per second:   8, episode reward: -50.900, mean reward: -1.272 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.206802, mean_q: 17.795757, mean_eps: 0.100000\n","      77289/2000000000: episode: 2080, duration: 5.350s, episode steps:  40, steps per second:   7, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 58.117376, mean_q: 18.271342, mean_eps: 0.100000\n","      77329/2000000000: episode: 2081, duration: 5.541s, episode steps:  40, steps per second:   7, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.684940, mean_q: 17.777847, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      77369/2000000000: episode: 2082, duration: 5.230s, episode steps:  40, steps per second:   8, episode reward: -88.000, mean reward: -2.200 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 70.284126, mean_q: 18.258770, mean_eps: 0.100000\n","      77409/2000000000: episode: 2083, duration: 5.361s, episode steps:  40, steps per second:   7, episode reward: -30.000, mean reward: -0.750 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 63.467189, mean_q: 17.762738, mean_eps: 0.100000\n","      77444/2000000000: episode: 2084, duration: 4.904s, episode steps:  35, steps per second:   7, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 62.088251, mean_q: 17.993492, mean_eps: 0.100000\n","      77484/2000000000: episode: 2085, duration: 5.326s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 64.603467, mean_q: 18.607753, mean_eps: 0.100000\n","      77524/2000000000: episode: 2086, duration: 5.842s, episode steps:  40, steps per second:   7, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.458028, mean_q: 18.084680, mean_eps: 0.100000\n","      77562/2000000000: episode: 2087, duration: 5.143s, episode steps:  38, steps per second:   7, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 61.753280, mean_q: 18.255809, mean_eps: 0.100000\n","      77594/2000000000: episode: 2088, duration: 4.130s, episode steps:  32, steps per second:   8, episode reward: -324.000, mean reward: -10.125 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 68.855019, mean_q: 18.633904, mean_eps: 0.100000\n","      77627/2000000000: episode: 2089, duration: 4.845s, episode steps:  33, steps per second:   7, episode reward: -30.200, mean reward: -0.915 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 70.246318, mean_q: 18.047292, mean_eps: 0.100000\n","      77667/2000000000: episode: 2090, duration: 5.171s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 59.202397, mean_q: 18.177331, mean_eps: 0.100000\n","      77707/2000000000: episode: 2091, duration: 5.701s, episode steps:  40, steps per second:   7, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.376265, mean_q: 18.440121, mean_eps: 0.100000\n","      77747/2000000000: episode: 2092, duration: 5.696s, episode steps:  40, steps per second:   7, episode reward: -6.900, mean reward: -0.172 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.394947, mean_q: 17.077697, mean_eps: 0.100000\n","      77787/2000000000: episode: 2093, duration: 6.032s, episode steps:  40, steps per second:   7, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.103915, mean_q: 17.638848, mean_eps: 0.100000\n","      77827/2000000000: episode: 2094, duration: 6.313s, episode steps:  40, steps per second:   6, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.871983, mean_q: 17.924348, mean_eps: 0.100000\n","      77867/2000000000: episode: 2095, duration: 5.665s, episode steps:  40, steps per second:   7, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 62.193131, mean_q: 17.631927, mean_eps: 0.100000\n","      77901/2000000000: episode: 2096, duration: 4.772s, episode steps:  34, steps per second:   7, episode reward: -35.800, mean reward: -1.053 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 68.789277, mean_q: 17.858740, mean_eps: 0.100000\n","      77937/2000000000: episode: 2097, duration: 4.992s, episode steps:  36, steps per second:   7, episode reward: -73.100, mean reward: -2.031 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.046383, mean_q: 18.089775, mean_eps: 0.100000\n","      77977/2000000000: episode: 2098, duration: 5.742s, episode steps:  40, steps per second:   7, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.753582, mean_q: 18.233227, mean_eps: 0.100000\n","      78017/2000000000: episode: 2099, duration: 6.183s, episode steps:  40, steps per second:   6, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.779918, mean_q: 17.793792, mean_eps: 0.100000\n","      78057/2000000000: episode: 2100, duration: 6.401s, episode steps:  40, steps per second:   6, episode reward: 44.500, mean reward:  1.112 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.520877, mean_q: 17.676755, mean_eps: 0.100000\n","      78095/2000000000: episode: 2101, duration: 5.406s, episode steps:  38, steps per second:   7, episode reward: -5.000, mean reward: -0.132 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 62.832958, mean_q: 17.758779, mean_eps: 0.100000\n","      78128/2000000000: episode: 2102, duration: 4.589s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 66.096222, mean_q: 18.963561, mean_eps: 0.100000\n","      78168/2000000000: episode: 2103, duration: 5.492s, episode steps:  40, steps per second:   7, episode reward: 39.400, mean reward:  0.985 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 61.863975, mean_q: 16.927472, mean_eps: 0.100000\n","      78208/2000000000: episode: 2104, duration: 5.238s, episode steps:  40, steps per second:   8, episode reward: -39.200, mean reward: -0.980 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 62.692057, mean_q: 17.988447, mean_eps: 0.100000\n","      78242/2000000000: episode: 2105, duration: 4.441s, episode steps:  34, steps per second:   8, episode reward:  9.900, mean reward:  0.291 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 61.699367, mean_q: 18.323659, mean_eps: 0.100000\n","      78282/2000000000: episode: 2106, duration: 5.166s, episode steps:  40, steps per second:   8, episode reward: -83.800, mean reward: -2.095 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.013927, mean_q: 18.271318, mean_eps: 0.100000\n","      78322/2000000000: episode: 2107, duration: 5.162s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.614201, mean_q: 17.857402, mean_eps: 0.100000\n","      78358/2000000000: episode: 2108, duration: 4.601s, episode steps:  36, steps per second:   8, episode reward: 69.100, mean reward:  1.919 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 65.219221, mean_q: 17.925313, mean_eps: 0.100000\n","      78398/2000000000: episode: 2109, duration: 5.084s, episode steps:  40, steps per second:   8, episode reward: 43.500, mean reward:  1.088 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.180111, mean_q: 17.596677, mean_eps: 0.100000\n","      78438/2000000000: episode: 2110, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: -29.500, mean reward: -0.737 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.206351, mean_q: 17.692259, mean_eps: 0.100000\n","      78473/2000000000: episode: 2111, duration: 4.443s, episode steps:  35, steps per second:   8, episode reward: -23.900, mean reward: -0.683 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 68.919381, mean_q: 17.809637, mean_eps: 0.100000\n","      78506/2000000000: episode: 2112, duration: 4.215s, episode steps:  33, steps per second:   8, episode reward: -71.900, mean reward: -2.179 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 66.856409, mean_q: 17.940212, mean_eps: 0.100000\n","      78546/2000000000: episode: 2113, duration: 5.282s, episode steps:  40, steps per second:   8, episode reward: 131.900, mean reward:  3.298 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.052433, mean_q: 17.588108, mean_eps: 0.100000\n","      78586/2000000000: episode: 2114, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: -35.500, mean reward: -0.888 [-20.000, 18.000], mean action: 1.675 [0.000, 2.000],  loss: 68.419267, mean_q: 18.057635, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      78626/2000000000: episode: 2115, duration: 5.040s, episode steps:  40, steps per second:   8, episode reward: 21.500, mean reward:  0.537 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 65.777557, mean_q: 17.869517, mean_eps: 0.100000\n","      78665/2000000000: episode: 2116, duration: 5.036s, episode steps:  39, steps per second:   8, episode reward: 27.600, mean reward:  0.708 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 66.381952, mean_q: 18.028608, mean_eps: 0.100000\n","      78705/2000000000: episode: 2117, duration: 5.286s, episode steps:  40, steps per second:   8, episode reward: -75.800, mean reward: -1.895 [-20.000, 18.200], mean action: 1.450 [0.000, 2.000],  loss: 64.061213, mean_q: 17.919558, mean_eps: 0.100000\n","      78745/2000000000: episode: 2118, duration: 5.278s, episode steps:  40, steps per second:   8, episode reward: 182.400, mean reward:  4.560 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.485486, mean_q: 18.920143, mean_eps: 0.100000\n","      78781/2000000000: episode: 2119, duration: 4.688s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 64.491692, mean_q: 18.370116, mean_eps: 0.100000\n","      78821/2000000000: episode: 2120, duration: 5.445s, episode steps:  40, steps per second:   7, episode reward: -8.800, mean reward: -0.220 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 61.434263, mean_q: 17.822948, mean_eps: 0.100000\n","      78857/2000000000: episode: 2121, duration: 4.686s, episode steps:  36, steps per second:   8, episode reward: 148.000, mean reward:  4.111 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 63.563295, mean_q: 17.788560, mean_eps: 0.100000\n","      78897/2000000000: episode: 2122, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 83.700, mean reward:  2.093 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.556767, mean_q: 17.190893, mean_eps: 0.100000\n","      78921/2000000000: episode: 2123, duration: 3.398s, episode steps:  24, steps per second:   7, episode reward: -67.100, mean reward: -2.796 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 72.154592, mean_q: 18.957780, mean_eps: 0.100000\n","      78961/2000000000: episode: 2124, duration: 5.322s, episode steps:  40, steps per second:   8, episode reward: 14.100, mean reward:  0.352 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.583058, mean_q: 17.733716, mean_eps: 0.100000\n","      79001/2000000000: episode: 2125, duration: 5.379s, episode steps:  40, steps per second:   7, episode reward: 35.900, mean reward:  0.898 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.437785, mean_q: 18.238963, mean_eps: 0.100000\n","      79041/2000000000: episode: 2126, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 62.102065, mean_q: 17.862553, mean_eps: 0.100000\n","      79081/2000000000: episode: 2127, duration: 5.411s, episode steps:  40, steps per second:   7, episode reward: -61.200, mean reward: -1.530 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.026347, mean_q: 17.975253, mean_eps: 0.100000\n","      79117/2000000000: episode: 2128, duration: 4.567s, episode steps:  36, steps per second:   8, episode reward:  9.300, mean reward:  0.258 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 61.604752, mean_q: 17.787955, mean_eps: 0.100000\n","      79155/2000000000: episode: 2129, duration: 5.036s, episode steps:  38, steps per second:   8, episode reward: -7.900, mean reward: -0.208 [-20.000, 18.000], mean action: 1.053 [0.000, 2.000],  loss: 65.164589, mean_q: 17.580269, mean_eps: 0.100000\n","      79195/2000000000: episode: 2130, duration: 4.941s, episode steps:  40, steps per second:   8, episode reward: -15.700, mean reward: -0.393 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.479886, mean_q: 17.418721, mean_eps: 0.100000\n","      79234/2000000000: episode: 2131, duration: 4.939s, episode steps:  39, steps per second:   8, episode reward: -46.000, mean reward: -1.179 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 66.276410, mean_q: 17.806631, mean_eps: 0.100000\n","      79274/2000000000: episode: 2132, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.777284, mean_q: 17.753591, mean_eps: 0.100000\n","      79314/2000000000: episode: 2133, duration: 5.323s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 58.981758, mean_q: 17.891691, mean_eps: 0.100000\n","      79346/2000000000: episode: 2134, duration: 4.067s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 60.448596, mean_q: 18.522906, mean_eps: 0.100000\n","      79386/2000000000: episode: 2135, duration: 5.110s, episode steps:  40, steps per second:   8, episode reward: 89.800, mean reward:  2.245 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 64.478979, mean_q: 17.424279, mean_eps: 0.100000\n","      79426/2000000000: episode: 2136, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: 92.000, mean reward:  2.300 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.024540, mean_q: 18.216835, mean_eps: 0.100000\n","      79464/2000000000: episode: 2137, duration: 5.031s, episode steps:  38, steps per second:   8, episode reward: -34.800, mean reward: -0.916 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 65.909793, mean_q: 18.099571, mean_eps: 0.100000\n","      79497/2000000000: episode: 2138, duration: 4.401s, episode steps:  33, steps per second:   7, episode reward: -172.000, mean reward: -5.212 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 70.987482, mean_q: 17.964153, mean_eps: 0.100000\n","      79537/2000000000: episode: 2139, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.638463, mean_q: 17.765113, mean_eps: 0.100000\n","      79577/2000000000: episode: 2140, duration: 5.546s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.012954, mean_q: 18.057516, mean_eps: 0.100000\n","      79617/2000000000: episode: 2141, duration: 5.124s, episode steps:  40, steps per second:   8, episode reward: 52.900, mean reward:  1.323 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 59.884082, mean_q: 18.421625, mean_eps: 0.100000\n","      79657/2000000000: episode: 2142, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: -75.600, mean reward: -1.890 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.737511, mean_q: 18.158222, mean_eps: 0.100000\n","      79689/2000000000: episode: 2143, duration: 4.268s, episode steps:  32, steps per second:   7, episode reward: 15.600, mean reward:  0.488 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 65.906808, mean_q: 18.448380, mean_eps: 0.100000\n","      79717/2000000000: episode: 2144, duration: 4.263s, episode steps:  28, steps per second:   7, episode reward: 88.700, mean reward:  3.168 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 61.965505, mean_q: 18.130242, mean_eps: 0.100000\n","      79753/2000000000: episode: 2145, duration: 4.622s, episode steps:  36, steps per second:   8, episode reward: -69.600, mean reward: -1.933 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 65.337401, mean_q: 17.322072, mean_eps: 0.100000\n","      79793/2000000000: episode: 2146, duration: 5.401s, episode steps:  40, steps per second:   7, episode reward: -130.000, mean reward: -3.250 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.367364, mean_q: 17.344228, mean_eps: 0.100000\n","      79833/2000000000: episode: 2147, duration: 5.633s, episode steps:  40, steps per second:   7, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.147763, mean_q: 18.693444, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      79873/2000000000: episode: 2148, duration: 5.553s, episode steps:  40, steps per second:   7, episode reward: 79.600, mean reward:  1.990 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.119720, mean_q: 18.124773, mean_eps: 0.100000\n","      79913/2000000000: episode: 2149, duration: 5.430s, episode steps:  40, steps per second:   7, episode reward: -60.400, mean reward: -1.510 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.939988, mean_q: 17.821379, mean_eps: 0.100000\n","      79948/2000000000: episode: 2150, duration: 4.657s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 68.592463, mean_q: 18.011541, mean_eps: 0.100000\n","      79987/2000000000: episode: 2151, duration: 5.602s, episode steps:  39, steps per second:   7, episode reward: 24.000, mean reward:  0.615 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 62.645196, mean_q: 17.803909, mean_eps: 0.100000\n","      80027/2000000000: episode: 2152, duration: 4.955s, episode steps:  40, steps per second:   8, episode reward: 82.000, mean reward:  2.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.981814, mean_q: 19.376024, mean_eps: 0.100000\n","      80060/2000000000: episode: 2153, duration: 4.142s, episode steps:  33, steps per second:   8, episode reward: -111.500, mean reward: -3.379 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 78.494897, mean_q: 19.863324, mean_eps: 0.100000\n","      80096/2000000000: episode: 2154, duration: 4.260s, episode steps:  36, steps per second:   8, episode reward: 31.400, mean reward:  0.872 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 84.489594, mean_q: 19.650269, mean_eps: 0.100000\n","      80136/2000000000: episode: 2155, duration: 4.686s, episode steps:  40, steps per second:   9, episode reward: -33.100, mean reward: -0.828 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.738627, mean_q: 19.796324, mean_eps: 0.100000\n","      80176/2000000000: episode: 2156, duration: 4.798s, episode steps:  40, steps per second:   8, episode reward: 19.100, mean reward:  0.478 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.960577, mean_q: 19.089765, mean_eps: 0.100000\n","      80216/2000000000: episode: 2157, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: -68.900, mean reward: -1.722 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.363121, mean_q: 20.354608, mean_eps: 0.100000\n","      80256/2000000000: episode: 2158, duration: 4.867s, episode steps:  40, steps per second:   8, episode reward: -3.200, mean reward: -0.080 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.174134, mean_q: 20.091190, mean_eps: 0.100000\n","      80293/2000000000: episode: 2159, duration: 4.396s, episode steps:  37, steps per second:   8, episode reward: -55.500, mean reward: -1.500 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 71.379778, mean_q: 19.726013, mean_eps: 0.100000\n","      80333/2000000000: episode: 2160, duration: 4.674s, episode steps:  40, steps per second:   9, episode reward: 147.400, mean reward:  3.685 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.093763, mean_q: 20.088112, mean_eps: 0.100000\n","      80373/2000000000: episode: 2161, duration: 4.733s, episode steps:  40, steps per second:   8, episode reward: 110.300, mean reward:  2.758 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.659053, mean_q: 20.107554, mean_eps: 0.100000\n","      80408/2000000000: episode: 2162, duration: 4.208s, episode steps:  35, steps per second:   8, episode reward: 130.600, mean reward:  3.731 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 74.669767, mean_q: 19.614394, mean_eps: 0.100000\n","      80444/2000000000: episode: 2163, duration: 4.391s, episode steps:  36, steps per second:   8, episode reward: 36.400, mean reward:  1.011 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 74.821487, mean_q: 20.335735, mean_eps: 0.100000\n","      80484/2000000000: episode: 2164, duration: 4.782s, episode steps:  40, steps per second:   8, episode reward: 92.900, mean reward:  2.322 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.013805, mean_q: 19.695976, mean_eps: 0.100000\n","      80524/2000000000: episode: 2165, duration: 4.687s, episode steps:  40, steps per second:   9, episode reward: 116.800, mean reward:  2.920 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.383634, mean_q: 19.835306, mean_eps: 0.100000\n","      80564/2000000000: episode: 2166, duration: 4.811s, episode steps:  40, steps per second:   8, episode reward: 89.300, mean reward:  2.232 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.659020, mean_q: 20.108492, mean_eps: 0.100000\n","      80604/2000000000: episode: 2167, duration: 4.819s, episode steps:  40, steps per second:   8, episode reward: -28.000, mean reward: -0.700 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.775375, mean_q: 20.098000, mean_eps: 0.100000\n","      80642/2000000000: episode: 2168, duration: 4.727s, episode steps:  38, steps per second:   8, episode reward: -46.400, mean reward: -1.221 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 68.585502, mean_q: 20.558991, mean_eps: 0.100000\n","      80682/2000000000: episode: 2169, duration: 4.785s, episode steps:  40, steps per second:   8, episode reward: -73.200, mean reward: -1.830 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 69.793592, mean_q: 19.861917, mean_eps: 0.100000\n","      80722/2000000000: episode: 2170, duration: 4.858s, episode steps:  40, steps per second:   8, episode reward: -29.100, mean reward: -0.728 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.151812, mean_q: 20.201488, mean_eps: 0.100000\n","      80762/2000000000: episode: 2171, duration: 4.791s, episode steps:  40, steps per second:   8, episode reward: -44.500, mean reward: -1.113 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.386109, mean_q: 19.403691, mean_eps: 0.100000\n","      80800/2000000000: episode: 2172, duration: 4.598s, episode steps:  38, steps per second:   8, episode reward: -53.900, mean reward: -1.418 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 74.777141, mean_q: 19.928448, mean_eps: 0.100000\n","      80840/2000000000: episode: 2173, duration: 5.264s, episode steps:  40, steps per second:   8, episode reward: -17.800, mean reward: -0.445 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.558370, mean_q: 19.993730, mean_eps: 0.100000\n","      80875/2000000000: episode: 2174, duration: 4.283s, episode steps:  35, steps per second:   8, episode reward: 105.400, mean reward:  3.011 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 68.563705, mean_q: 19.425250, mean_eps: 0.100000\n","      80915/2000000000: episode: 2175, duration: 4.696s, episode steps:  40, steps per second:   9, episode reward: -37.700, mean reward: -0.942 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.271990, mean_q: 19.593775, mean_eps: 0.100000\n","      80955/2000000000: episode: 2176, duration: 4.617s, episode steps:  40, steps per second:   9, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 64.805014, mean_q: 20.108865, mean_eps: 0.100000\n","      80995/2000000000: episode: 2177, duration: 4.690s, episode steps:  40, steps per second:   9, episode reward: 137.600, mean reward:  3.440 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.036396, mean_q: 20.081591, mean_eps: 0.100000\n","      81035/2000000000: episode: 2178, duration: 4.887s, episode steps:  40, steps per second:   8, episode reward: -104.500, mean reward: -2.613 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.685345, mean_q: 18.983295, mean_eps: 0.100000\n","      81075/2000000000: episode: 2179, duration: 4.909s, episode steps:  40, steps per second:   8, episode reward: 228.000, mean reward:  5.700 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.802558, mean_q: 19.691266, mean_eps: 0.100000\n","      81102/2000000000: episode: 2180, duration: 3.325s, episode steps:  27, steps per second:   8, episode reward: -96.000, mean reward: -3.556 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 70.241036, mean_q: 19.973821, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      81139/2000000000: episode: 2181, duration: 4.675s, episode steps:  37, steps per second:   8, episode reward: 158.400, mean reward:  4.281 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 66.960310, mean_q: 19.301940, mean_eps: 0.100000\n","      81179/2000000000: episode: 2182, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: 134.800, mean reward:  3.370 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 65.939578, mean_q: 19.786003, mean_eps: 0.100000\n","      81219/2000000000: episode: 2183, duration: 4.845s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.365166, mean_q: 20.055663, mean_eps: 0.100000\n","      81259/2000000000: episode: 2184, duration: 4.912s, episode steps:  40, steps per second:   8, episode reward: -45.600, mean reward: -1.140 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.985259, mean_q: 20.877513, mean_eps: 0.100000\n","      81299/2000000000: episode: 2185, duration: 4.790s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.113657, mean_q: 19.685110, mean_eps: 0.100000\n","      81334/2000000000: episode: 2186, duration: 4.379s, episode steps:  35, steps per second:   8, episode reward: -7.400, mean reward: -0.211 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 68.926939, mean_q: 20.552102, mean_eps: 0.100000\n","      81374/2000000000: episode: 2187, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 69.286291, mean_q: 19.553988, mean_eps: 0.100000\n","      81414/2000000000: episode: 2188, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.519104, mean_q: 19.919748, mean_eps: 0.100000\n","      81454/2000000000: episode: 2189, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.371168, mean_q: 19.623884, mean_eps: 0.100000\n","      81494/2000000000: episode: 2190, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: 26.000, mean reward:  0.650 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 71.336901, mean_q: 19.758338, mean_eps: 0.100000\n","      81534/2000000000: episode: 2191, duration: 4.618s, episode steps:  40, steps per second:   9, episode reward: -38.900, mean reward: -0.973 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.858196, mean_q: 19.891609, mean_eps: 0.100000\n","      81574/2000000000: episode: 2192, duration: 4.379s, episode steps:  40, steps per second:   9, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.392646, mean_q: 19.925218, mean_eps: 0.100000\n","      81614/2000000000: episode: 2193, duration: 4.696s, episode steps:  40, steps per second:   9, episode reward: 51.800, mean reward:  1.295 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.011411, mean_q: 19.478043, mean_eps: 0.100000\n","      81654/2000000000: episode: 2194, duration: 4.750s, episode steps:  40, steps per second:   8, episode reward:  6.000, mean reward:  0.150 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.383185, mean_q: 19.528219, mean_eps: 0.100000\n","      81692/2000000000: episode: 2195, duration: 4.893s, episode steps:  38, steps per second:   8, episode reward: -95.600, mean reward: -2.516 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 72.912783, mean_q: 20.086753, mean_eps: 0.100000\n","      81732/2000000000: episode: 2196, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward: -108.700, mean reward: -2.718 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.967179, mean_q: 19.711985, mean_eps: 0.100000\n","      81772/2000000000: episode: 2197, duration: 4.997s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.177541, mean_q: 19.771698, mean_eps: 0.100000\n","      81807/2000000000: episode: 2198, duration: 4.606s, episode steps:  35, steps per second:   8, episode reward: -6.700, mean reward: -0.191 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 62.709524, mean_q: 20.160606, mean_eps: 0.100000\n","      81838/2000000000: episode: 2199, duration: 3.925s, episode steps:  31, steps per second:   8, episode reward: -97.900, mean reward: -3.158 [-20.000, 18.000], mean action: 0.839 [0.000, 2.000],  loss: 67.798480, mean_q: 19.821006, mean_eps: 0.100000\n","      81875/2000000000: episode: 2200, duration: 4.514s, episode steps:  37, steps per second:   8, episode reward:  3.300, mean reward:  0.089 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 71.829976, mean_q: 19.982033, mean_eps: 0.100000\n","      81915/2000000000: episode: 2201, duration: 4.867s, episode steps:  40, steps per second:   8, episode reward: -49.800, mean reward: -1.245 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.494285, mean_q: 19.858547, mean_eps: 0.100000\n","      81955/2000000000: episode: 2202, duration: 4.641s, episode steps:  40, steps per second:   9, episode reward: 66.600, mean reward:  1.665 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.996835, mean_q: 19.899048, mean_eps: 0.100000\n","      81995/2000000000: episode: 2203, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.697833, mean_q: 19.841344, mean_eps: 0.100000\n","      82035/2000000000: episode: 2204, duration: 4.731s, episode steps:  40, steps per second:   8, episode reward: 73.800, mean reward:  1.845 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.071259, mean_q: 20.358791, mean_eps: 0.100000\n","      82075/2000000000: episode: 2205, duration: 4.780s, episode steps:  40, steps per second:   8, episode reward: -43.400, mean reward: -1.085 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 77.485544, mean_q: 19.812756, mean_eps: 0.100000\n","      82115/2000000000: episode: 2206, duration: 4.920s, episode steps:  40, steps per second:   8, episode reward: -53.800, mean reward: -1.345 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.206222, mean_q: 20.211230, mean_eps: 0.100000\n","      82155/2000000000: episode: 2207, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.698510, mean_q: 19.248249, mean_eps: 0.100000\n","      82195/2000000000: episode: 2208, duration: 4.726s, episode steps:  40, steps per second:   8, episode reward: -38.600, mean reward: -0.965 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 68.642833, mean_q: 20.345040, mean_eps: 0.100000\n","      82235/2000000000: episode: 2209, duration: 4.864s, episode steps:  40, steps per second:   8, episode reward: -59.900, mean reward: -1.498 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.182831, mean_q: 20.077022, mean_eps: 0.100000\n","      82273/2000000000: episode: 2210, duration: 4.752s, episode steps:  38, steps per second:   8, episode reward: -248.000, mean reward: -6.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 69.880705, mean_q: 19.563035, mean_eps: 0.100000\n","      82313/2000000000: episode: 2211, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 137.000, mean reward:  3.425 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.287606, mean_q: 19.706170, mean_eps: 0.100000\n","      82353/2000000000: episode: 2212, duration: 4.735s, episode steps:  40, steps per second:   8, episode reward: -18.500, mean reward: -0.462 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.151070, mean_q: 19.815270, mean_eps: 0.100000\n","      82393/2000000000: episode: 2213, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward:  8.800, mean reward:  0.220 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.947705, mean_q: 20.514764, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      82433/2000000000: episode: 2214, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 125.700, mean reward:  3.142 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.561308, mean_q: 19.677687, mean_eps: 0.100000\n","      82473/2000000000: episode: 2215, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: -20.100, mean reward: -0.502 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.003186, mean_q: 19.758726, mean_eps: 0.100000\n","      82506/2000000000: episode: 2216, duration: 4.284s, episode steps:  33, steps per second:   8, episode reward: 105.200, mean reward:  3.188 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 77.677673, mean_q: 20.640654, mean_eps: 0.100000\n","      82546/2000000000: episode: 2217, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 57.600, mean reward:  1.440 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 74.400515, mean_q: 19.351206, mean_eps: 0.100000\n","      82585/2000000000: episode: 2218, duration: 5.087s, episode steps:  39, steps per second:   8, episode reward: -10.500, mean reward: -0.269 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 71.354718, mean_q: 19.996389, mean_eps: 0.100000\n","      82621/2000000000: episode: 2219, duration: 4.890s, episode steps:  36, steps per second:   7, episode reward: 78.800, mean reward:  2.189 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 67.333796, mean_q: 19.774656, mean_eps: 0.100000\n","      82657/2000000000: episode: 2220, duration: 4.472s, episode steps:  36, steps per second:   8, episode reward: 71.600, mean reward:  1.989 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 78.490901, mean_q: 19.930549, mean_eps: 0.100000\n","      82697/2000000000: episode: 2221, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.463578, mean_q: 19.639515, mean_eps: 0.100000\n","      82728/2000000000: episode: 2222, duration: 3.639s, episode steps:  31, steps per second:   9, episode reward: -38.400, mean reward: -1.239 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 68.803876, mean_q: 20.206783, mean_eps: 0.100000\n","      82768/2000000000: episode: 2223, duration: 4.740s, episode steps:  40, steps per second:   8, episode reward: -17.700, mean reward: -0.443 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.523480, mean_q: 20.003591, mean_eps: 0.100000\n","      82808/2000000000: episode: 2224, duration: 4.526s, episode steps:  40, steps per second:   9, episode reward: 70.800, mean reward:  1.770 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.244566, mean_q: 19.773941, mean_eps: 0.100000\n","      82848/2000000000: episode: 2225, duration: 4.639s, episode steps:  40, steps per second:   9, episode reward: 113.100, mean reward:  2.827 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.430908, mean_q: 20.501654, mean_eps: 0.100000\n","      82888/2000000000: episode: 2226, duration: 4.523s, episode steps:  40, steps per second:   9, episode reward: 107.400, mean reward:  2.685 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.360316, mean_q: 20.554872, mean_eps: 0.100000\n","      82928/2000000000: episode: 2227, duration: 4.441s, episode steps:  40, steps per second:   9, episode reward: -107.100, mean reward: -2.677 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.121703, mean_q: 19.946066, mean_eps: 0.100000\n","      82960/2000000000: episode: 2228, duration: 3.619s, episode steps:  32, steps per second:   9, episode reward: -43.400, mean reward: -1.356 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.114052, mean_q: 19.982640, mean_eps: 0.100000\n","      82998/2000000000: episode: 2229, duration: 4.347s, episode steps:  38, steps per second:   9, episode reward: -76.200, mean reward: -2.005 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 66.261224, mean_q: 20.530273, mean_eps: 0.100000\n","      83038/2000000000: episode: 2230, duration: 4.911s, episode steps:  40, steps per second:   8, episode reward: 171.200, mean reward:  4.280 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.638188, mean_q: 20.086621, mean_eps: 0.100000\n","      83069/2000000000: episode: 2231, duration: 3.712s, episode steps:  31, steps per second:   8, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 72.469631, mean_q: 19.705485, mean_eps: 0.100000\n","      83109/2000000000: episode: 2232, duration: 5.027s, episode steps:  40, steps per second:   8, episode reward: 156.100, mean reward:  3.903 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.322953, mean_q: 19.982530, mean_eps: 0.100000\n","      83149/2000000000: episode: 2233, duration: 4.797s, episode steps:  40, steps per second:   8, episode reward: -18.300, mean reward: -0.458 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.695938, mean_q: 20.225016, mean_eps: 0.100000\n","      83189/2000000000: episode: 2234, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: -39.500, mean reward: -0.988 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.838056, mean_q: 19.975362, mean_eps: 0.100000\n","      83222/2000000000: episode: 2235, duration: 4.034s, episode steps:  33, steps per second:   8, episode reward: -4.000, mean reward: -0.121 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.426401, mean_q: 19.813611, mean_eps: 0.100000\n","      83261/2000000000: episode: 2236, duration: 4.627s, episode steps:  39, steps per second:   8, episode reward: 162.900, mean reward:  4.177 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 73.883904, mean_q: 20.516789, mean_eps: 0.100000\n","      83301/2000000000: episode: 2237, duration: 4.701s, episode steps:  40, steps per second:   9, episode reward: -4.700, mean reward: -0.117 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.392470, mean_q: 19.821418, mean_eps: 0.100000\n","      83341/2000000000: episode: 2238, duration: 4.920s, episode steps:  40, steps per second:   8, episode reward: -0.800, mean reward: -0.020 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.723598, mean_q: 19.289058, mean_eps: 0.100000\n","      83381/2000000000: episode: 2239, duration: 4.769s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.077151, mean_q: 19.782047, mean_eps: 0.100000\n","      83421/2000000000: episode: 2240, duration: 5.167s, episode steps:  40, steps per second:   8, episode reward: -7.000, mean reward: -0.175 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.575714, mean_q: 19.829729, mean_eps: 0.100000\n","      83459/2000000000: episode: 2241, duration: 4.780s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 68.802385, mean_q: 20.143052, mean_eps: 0.100000\n","      83497/2000000000: episode: 2242, duration: 4.598s, episode steps:  38, steps per second:   8, episode reward: -149.800, mean reward: -3.942 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 71.187368, mean_q: 20.547834, mean_eps: 0.100000\n","      83537/2000000000: episode: 2243, duration: 4.736s, episode steps:  40, steps per second:   8, episode reward: 23.900, mean reward:  0.597 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.511634, mean_q: 19.657293, mean_eps: 0.100000\n","      83569/2000000000: episode: 2244, duration: 3.878s, episode steps:  32, steps per second:   8, episode reward: -100.900, mean reward: -3.153 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 72.063252, mean_q: 20.434436, mean_eps: 0.100000\n","      83609/2000000000: episode: 2245, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: 190.500, mean reward:  4.762 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.665295, mean_q: 20.212610, mean_eps: 0.100000\n","      83649/2000000000: episode: 2246, duration: 4.975s, episode steps:  40, steps per second:   8, episode reward: 52.600, mean reward:  1.315 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.928563, mean_q: 19.849817, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      83689/2000000000: episode: 2247, duration: 4.781s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 65.142770, mean_q: 20.061054, mean_eps: 0.100000\n","      83729/2000000000: episode: 2248, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.822932, mean_q: 19.642149, mean_eps: 0.100000\n","      83768/2000000000: episode: 2249, duration: 4.787s, episode steps:  39, steps per second:   8, episode reward:  5.900, mean reward:  0.151 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 67.869265, mean_q: 19.866169, mean_eps: 0.100000\n","      83807/2000000000: episode: 2250, duration: 4.664s, episode steps:  39, steps per second:   8, episode reward: -129.000, mean reward: -3.308 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 71.794126, mean_q: 20.837719, mean_eps: 0.100000\n","      83847/2000000000: episode: 2251, duration: 4.658s, episode steps:  40, steps per second:   9, episode reward: -29.200, mean reward: -0.730 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.038494, mean_q: 19.185083, mean_eps: 0.100000\n","      83887/2000000000: episode: 2252, duration: 4.837s, episode steps:  40, steps per second:   8, episode reward: -1.300, mean reward: -0.032 [-20.000, 18.400], mean action: 1.250 [0.000, 2.000],  loss: 64.727858, mean_q: 19.695493, mean_eps: 0.100000\n","      83927/2000000000: episode: 2253, duration: 4.887s, episode steps:  40, steps per second:   8, episode reward: 22.100, mean reward:  0.552 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.243414, mean_q: 19.528648, mean_eps: 0.100000\n","      83961/2000000000: episode: 2254, duration: 4.287s, episode steps:  34, steps per second:   8, episode reward: -88.300, mean reward: -2.597 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 65.981413, mean_q: 19.959065, mean_eps: 0.100000\n","      84000/2000000000: episode: 2255, duration: 4.505s, episode steps:  39, steps per second:   9, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 63.223662, mean_q: 19.681300, mean_eps: 0.100000\n","      84040/2000000000: episode: 2256, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward: 86.100, mean reward:  2.152 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.096278, mean_q: 20.159338, mean_eps: 0.100000\n","      84077/2000000000: episode: 2257, duration: 4.579s, episode steps:  37, steps per second:   8, episode reward: 61.700, mean reward:  1.668 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.917847, mean_q: 19.548532, mean_eps: 0.100000\n","      84115/2000000000: episode: 2258, duration: 4.582s, episode steps:  38, steps per second:   8, episode reward: 99.000, mean reward:  2.605 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 66.278151, mean_q: 20.624663, mean_eps: 0.100000\n","      84155/2000000000: episode: 2259, duration: 4.698s, episode steps:  40, steps per second:   9, episode reward: -51.400, mean reward: -1.285 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.677355, mean_q: 19.726695, mean_eps: 0.100000\n","      84195/2000000000: episode: 2260, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: 55.900, mean reward:  1.398 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.731508, mean_q: 20.178333, mean_eps: 0.100000\n","      84230/2000000000: episode: 2261, duration: 4.287s, episode steps:  35, steps per second:   8, episode reward: 41.500, mean reward:  1.186 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 67.909562, mean_q: 20.172448, mean_eps: 0.100000\n","      84269/2000000000: episode: 2262, duration: 4.848s, episode steps:  39, steps per second:   8, episode reward: 60.300, mean reward:  1.546 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 68.386302, mean_q: 20.189882, mean_eps: 0.100000\n","      84309/2000000000: episode: 2263, duration: 5.027s, episode steps:  40, steps per second:   8, episode reward: 61.500, mean reward:  1.538 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.061523, mean_q: 20.282437, mean_eps: 0.100000\n","      84345/2000000000: episode: 2264, duration: 4.257s, episode steps:  36, steps per second:   8, episode reward:  7.000, mean reward:  0.194 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 73.285427, mean_q: 20.089520, mean_eps: 0.100000\n","      84377/2000000000: episode: 2265, duration: 3.922s, episode steps:  32, steps per second:   8, episode reward: -36.100, mean reward: -1.128 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 67.823556, mean_q: 20.206242, mean_eps: 0.100000\n","      84417/2000000000: episode: 2266, duration: 5.058s, episode steps:  40, steps per second:   8, episode reward: -17.500, mean reward: -0.437 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.091397, mean_q: 19.690679, mean_eps: 0.100000\n","      84457/2000000000: episode: 2267, duration: 4.780s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.187109, mean_q: 19.508808, mean_eps: 0.100000\n","      84493/2000000000: episode: 2268, duration: 4.162s, episode steps:  36, steps per second:   9, episode reward: 91.800, mean reward:  2.550 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 69.012759, mean_q: 20.150667, mean_eps: 0.100000\n","      84527/2000000000: episode: 2269, duration: 4.131s, episode steps:  34, steps per second:   8, episode reward: 39.200, mean reward:  1.153 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 68.182666, mean_q: 19.838749, mean_eps: 0.100000\n","      84567/2000000000: episode: 2270, duration: 4.539s, episode steps:  40, steps per second:   9, episode reward: 138.900, mean reward:  3.473 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.153777, mean_q: 19.615813, mean_eps: 0.100000\n","      84607/2000000000: episode: 2271, duration: 4.791s, episode steps:  40, steps per second:   8, episode reward: -17.900, mean reward: -0.448 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.372979, mean_q: 20.153790, mean_eps: 0.100000\n","      84646/2000000000: episode: 2272, duration: 4.698s, episode steps:  39, steps per second:   8, episode reward: -85.100, mean reward: -2.182 [-20.000, 18.000], mean action: 1.077 [0.000, 2.000],  loss: 75.272059, mean_q: 19.922251, mean_eps: 0.100000\n","      84676/2000000000: episode: 2273, duration: 3.659s, episode steps:  30, steps per second:   8, episode reward: 58.400, mean reward:  1.947 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.915314, mean_q: 20.705292, mean_eps: 0.100000\n","      84713/2000000000: episode: 2274, duration: 4.230s, episode steps:  37, steps per second:   9, episode reward: 44.000, mean reward:  1.189 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 71.628054, mean_q: 19.525552, mean_eps: 0.100000\n","      84753/2000000000: episode: 2275, duration: 5.042s, episode steps:  40, steps per second:   8, episode reward: 78.800, mean reward:  1.970 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.942980, mean_q: 19.768249, mean_eps: 0.100000\n","      84793/2000000000: episode: 2276, duration: 4.611s, episode steps:  40, steps per second:   9, episode reward: 46.300, mean reward:  1.157 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.854256, mean_q: 19.420655, mean_eps: 0.100000\n","      84833/2000000000: episode: 2277, duration: 4.721s, episode steps:  40, steps per second:   8, episode reward: -114.300, mean reward: -2.858 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.679950, mean_q: 19.282315, mean_eps: 0.100000\n","      84873/2000000000: episode: 2278, duration: 4.692s, episode steps:  40, steps per second:   9, episode reward: -1.400, mean reward: -0.035 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.300492, mean_q: 20.286001, mean_eps: 0.100000\n","      84913/2000000000: episode: 2279, duration: 4.645s, episode steps:  40, steps per second:   9, episode reward:  0.900, mean reward:  0.023 [-20.000, 18.900], mean action: 1.200 [0.000, 2.000],  loss: 73.100148, mean_q: 20.507160, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      84951/2000000000: episode: 2280, duration: 4.543s, episode steps:  38, steps per second:   8, episode reward: -66.800, mean reward: -1.758 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 72.315989, mean_q: 19.738653, mean_eps: 0.100000\n","      84988/2000000000: episode: 2281, duration: 4.403s, episode steps:  37, steps per second:   8, episode reward: 170.000, mean reward:  4.595 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 66.161677, mean_q: 20.155283, mean_eps: 0.100000\n","      85028/2000000000: episode: 2282, duration: 4.927s, episode steps:  40, steps per second:   8, episode reward: 144.100, mean reward:  3.602 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.840319, mean_q: 19.725702, mean_eps: 0.100000\n","      85058/2000000000: episode: 2283, duration: 3.762s, episode steps:  30, steps per second:   8, episode reward: 81.200, mean reward:  2.707 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 70.856751, mean_q: 20.048233, mean_eps: 0.100000\n","      85098/2000000000: episode: 2284, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: -19.300, mean reward: -0.483 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.049802, mean_q: 19.592724, mean_eps: 0.100000\n","      85138/2000000000: episode: 2285, duration: 4.703s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.780382, mean_q: 19.856211, mean_eps: 0.100000\n","      85167/2000000000: episode: 2286, duration: 3.314s, episode steps:  29, steps per second:   9, episode reward: 167.700, mean reward:  5.783 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 70.052549, mean_q: 20.191278, mean_eps: 0.100000\n","      85207/2000000000: episode: 2287, duration: 4.586s, episode steps:  40, steps per second:   9, episode reward: -24.000, mean reward: -0.600 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 64.725921, mean_q: 20.173755, mean_eps: 0.100000\n","      85247/2000000000: episode: 2288, duration: 4.791s, episode steps:  40, steps per second:   8, episode reward: 99.800, mean reward:  2.495 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.024311, mean_q: 19.202878, mean_eps: 0.100000\n","      85287/2000000000: episode: 2289, duration: 4.606s, episode steps:  40, steps per second:   9, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 71.449049, mean_q: 20.312535, mean_eps: 0.100000\n","      85321/2000000000: episode: 2290, duration: 4.026s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 64.812985, mean_q: 20.190125, mean_eps: 0.100000\n","      85361/2000000000: episode: 2291, duration: 4.485s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.733211, mean_q: 19.829668, mean_eps: 0.100000\n","      85395/2000000000: episode: 2292, duration: 4.197s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 65.143951, mean_q: 20.457975, mean_eps: 0.100000\n","      85424/2000000000: episode: 2293, duration: 3.667s, episode steps:  29, steps per second:   8, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.671310, mean_q: 20.168922, mean_eps: 0.100000\n","      85464/2000000000: episode: 2294, duration: 4.774s, episode steps:  40, steps per second:   8, episode reward: 172.000, mean reward:  4.300 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 62.273027, mean_q: 19.188121, mean_eps: 0.100000\n","      85504/2000000000: episode: 2295, duration: 4.840s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 63.608334, mean_q: 19.864003, mean_eps: 0.100000\n","      85540/2000000000: episode: 2296, duration: 4.468s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 70.826968, mean_q: 20.334783, mean_eps: 0.100000\n","      85569/2000000000: episode: 2297, duration: 3.581s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 75.631152, mean_q: 20.557846, mean_eps: 0.100000\n","      85603/2000000000: episode: 2298, duration: 4.027s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 73.056658, mean_q: 20.129718, mean_eps: 0.100000\n","      85636/2000000000: episode: 2299, duration: 4.010s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 64.888520, mean_q: 20.178478, mean_eps: 0.100000\n","      85676/2000000000: episode: 2300, duration: 4.757s, episode steps:  40, steps per second:   8, episode reward: -66.400, mean reward: -1.660 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.763863, mean_q: 19.074712, mean_eps: 0.100000\n","      85714/2000000000: episode: 2301, duration: 4.467s, episode steps:  38, steps per second:   9, episode reward: -120.000, mean reward: -3.158 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 72.169651, mean_q: 19.767384, mean_eps: 0.100000\n","      85745/2000000000: episode: 2302, duration: 3.579s, episode steps:  31, steps per second:   9, episode reward: 20.200, mean reward:  0.652 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 68.495847, mean_q: 19.460334, mean_eps: 0.100000\n","      85777/2000000000: episode: 2303, duration: 3.967s, episode steps:  32, steps per second:   8, episode reward: 42.900, mean reward:  1.341 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 67.017783, mean_q: 20.146469, mean_eps: 0.100000\n","      85813/2000000000: episode: 2304, duration: 4.242s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 71.787572, mean_q: 20.274881, mean_eps: 0.100000\n","      85850/2000000000: episode: 2305, duration: 4.308s, episode steps:  37, steps per second:   9, episode reward: -172.000, mean reward: -4.649 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 65.295099, mean_q: 20.828147, mean_eps: 0.100000\n","      85877/2000000000: episode: 2306, duration: 3.206s, episode steps:  27, steps per second:   8, episode reward: -21.700, mean reward: -0.804 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 71.762548, mean_q: 20.157064, mean_eps: 0.100000\n","      85917/2000000000: episode: 2307, duration: 4.697s, episode steps:  40, steps per second:   9, episode reward: -10.100, mean reward: -0.252 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 69.025410, mean_q: 19.614168, mean_eps: 0.100000\n","      85957/2000000000: episode: 2308, duration: 4.787s, episode steps:  40, steps per second:   8, episode reward: 36.900, mean reward:  0.922 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.735404, mean_q: 19.964283, mean_eps: 0.100000\n","      85997/2000000000: episode: 2309, duration: 4.676s, episode steps:  40, steps per second:   9, episode reward:  5.400, mean reward:  0.135 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 70.792776, mean_q: 19.332064, mean_eps: 0.100000\n","      86037/2000000000: episode: 2310, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 70.633270, mean_q: 19.627109, mean_eps: 0.100000\n","      86076/2000000000: episode: 2311, duration: 4.859s, episode steps:  39, steps per second:   8, episode reward: -38.400, mean reward: -0.985 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 70.119939, mean_q: 19.816641, mean_eps: 0.100000\n","      86116/2000000000: episode: 2312, duration: 4.737s, episode steps:  40, steps per second:   8, episode reward: -35.900, mean reward: -0.898 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 69.816381, mean_q: 19.645125, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      86156/2000000000: episode: 2313, duration: 4.587s, episode steps:  40, steps per second:   9, episode reward: 91.700, mean reward:  2.292 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.353713, mean_q: 19.695095, mean_eps: 0.100000\n","      86196/2000000000: episode: 2314, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: -66.500, mean reward: -1.663 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.467607, mean_q: 20.288468, mean_eps: 0.100000\n","      86236/2000000000: episode: 2315, duration: 4.766s, episode steps:  40, steps per second:   8, episode reward: -30.700, mean reward: -0.768 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.833404, mean_q: 19.627546, mean_eps: 0.100000\n","      86276/2000000000: episode: 2316, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: 60.300, mean reward:  1.507 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.390092, mean_q: 19.656048, mean_eps: 0.100000\n","      86311/2000000000: episode: 2317, duration: 4.263s, episode steps:  35, steps per second:   8, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 66.745505, mean_q: 19.839146, mean_eps: 0.100000\n","      86351/2000000000: episode: 2318, duration: 5.128s, episode steps:  40, steps per second:   8, episode reward: -37.400, mean reward: -0.935 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.602096, mean_q: 19.674528, mean_eps: 0.100000\n","      86391/2000000000: episode: 2319, duration: 5.397s, episode steps:  40, steps per second:   7, episode reward: 75.500, mean reward:  1.888 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.762002, mean_q: 19.297850, mean_eps: 0.100000\n","      86429/2000000000: episode: 2320, duration: 4.910s, episode steps:  38, steps per second:   8, episode reward: -8.500, mean reward: -0.224 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 70.322097, mean_q: 20.026741, mean_eps: 0.100000\n","      86469/2000000000: episode: 2321, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward:  5.300, mean reward:  0.132 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.810196, mean_q: 19.765017, mean_eps: 0.100000\n","      86505/2000000000: episode: 2322, duration: 4.561s, episode steps:  36, steps per second:   8, episode reward: 120.600, mean reward:  3.350 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 68.345521, mean_q: 20.069229, mean_eps: 0.100000\n","      86545/2000000000: episode: 2323, duration: 5.231s, episode steps:  40, steps per second:   8, episode reward: 86.900, mean reward:  2.173 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.446373, mean_q: 19.378261, mean_eps: 0.100000\n","      86584/2000000000: episode: 2324, duration: 4.909s, episode steps:  39, steps per second:   8, episode reward: 170.000, mean reward:  4.359 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 67.555010, mean_q: 19.840180, mean_eps: 0.100000\n","      86624/2000000000: episode: 2325, duration: 5.294s, episode steps:  40, steps per second:   8, episode reward: -54.700, mean reward: -1.368 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.749831, mean_q: 20.434989, mean_eps: 0.100000\n","      86664/2000000000: episode: 2326, duration: 4.977s, episode steps:  40, steps per second:   8, episode reward: -1.300, mean reward: -0.032 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.724781, mean_q: 19.507303, mean_eps: 0.100000\n","      86699/2000000000: episode: 2327, duration: 4.447s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 70.231701, mean_q: 20.434634, mean_eps: 0.100000\n","      86739/2000000000: episode: 2328, duration: 5.097s, episode steps:  40, steps per second:   8, episode reward: -31.900, mean reward: -0.797 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.512115, mean_q: 19.734551, mean_eps: 0.100000\n","      86768/2000000000: episode: 2329, duration: 3.730s, episode steps:  29, steps per second:   8, episode reward: 109.300, mean reward:  3.769 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 74.956062, mean_q: 20.081360, mean_eps: 0.100000\n","      86808/2000000000: episode: 2330, duration: 5.492s, episode steps:  40, steps per second:   7, episode reward: 17.800, mean reward:  0.445 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.106846, mean_q: 20.674357, mean_eps: 0.100000\n","      86844/2000000000: episode: 2331, duration: 4.781s, episode steps:  36, steps per second:   8, episode reward: -16.200, mean reward: -0.450 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 67.643524, mean_q: 19.771789, mean_eps: 0.100000\n","      86884/2000000000: episode: 2332, duration: 4.717s, episode steps:  40, steps per second:   8, episode reward: -44.500, mean reward: -1.112 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.096789, mean_q: 20.262666, mean_eps: 0.100000\n","      86913/2000000000: episode: 2333, duration: 3.842s, episode steps:  29, steps per second:   8, episode reward: 14.400, mean reward:  0.497 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 67.489674, mean_q: 20.285670, mean_eps: 0.100000\n","      86949/2000000000: episode: 2334, duration: 4.438s, episode steps:  36, steps per second:   8, episode reward: -72.200, mean reward: -2.006 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 68.294367, mean_q: 20.486294, mean_eps: 0.100000\n","      86985/2000000000: episode: 2335, duration: 4.682s, episode steps:  36, steps per second:   8, episode reward: 20.500, mean reward:  0.569 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 62.994065, mean_q: 20.088695, mean_eps: 0.100000\n","      87023/2000000000: episode: 2336, duration: 5.065s, episode steps:  38, steps per second:   8, episode reward:  0.700, mean reward:  0.018 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 66.632291, mean_q: 19.842973, mean_eps: 0.100000\n","      87063/2000000000: episode: 2337, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: -44.000, mean reward: -1.100 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.535060, mean_q: 19.956963, mean_eps: 0.100000\n","      87103/2000000000: episode: 2338, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: 21.200, mean reward:  0.530 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.431869, mean_q: 19.711193, mean_eps: 0.100000\n","      87143/2000000000: episode: 2339, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: 29.200, mean reward:  0.730 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.447423, mean_q: 20.033500, mean_eps: 0.100000\n","      87179/2000000000: episode: 2340, duration: 4.613s, episode steps:  36, steps per second:   8, episode reward: -1.500, mean reward: -0.042 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 64.875069, mean_q: 19.979237, mean_eps: 0.100000\n","      87219/2000000000: episode: 2341, duration: 5.936s, episode steps:  40, steps per second:   7, episode reward: 37.300, mean reward:  0.932 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 69.423383, mean_q: 19.912006, mean_eps: 0.100000\n","      87255/2000000000: episode: 2342, duration: 5.072s, episode steps:  36, steps per second:   7, episode reward: 49.400, mean reward:  1.372 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 71.866222, mean_q: 21.318942, mean_eps: 0.100000\n","      87295/2000000000: episode: 2343, duration: 5.495s, episode steps:  40, steps per second:   7, episode reward: -21.300, mean reward: -0.533 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.797423, mean_q: 19.713310, mean_eps: 0.100000\n","      87335/2000000000: episode: 2344, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: -11.400, mean reward: -0.285 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.523370, mean_q: 19.950799, mean_eps: 0.100000\n","      87370/2000000000: episode: 2345, duration: 4.602s, episode steps:  35, steps per second:   8, episode reward: 63.500, mean reward:  1.814 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 72.136878, mean_q: 20.514283, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      87410/2000000000: episode: 2346, duration: 4.993s, episode steps:  40, steps per second:   8, episode reward: 123.600, mean reward:  3.090 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 62.213222, mean_q: 19.820141, mean_eps: 0.100000\n","      87450/2000000000: episode: 2347, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: -5.800, mean reward: -0.145 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.284604, mean_q: 19.835942, mean_eps: 0.100000\n","      87490/2000000000: episode: 2348, duration: 4.912s, episode steps:  40, steps per second:   8, episode reward: -43.600, mean reward: -1.090 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.036270, mean_q: 20.048288, mean_eps: 0.100000\n","      87525/2000000000: episode: 2349, duration: 4.397s, episode steps:  35, steps per second:   8, episode reward: 56.400, mean reward:  1.611 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.439514, mean_q: 20.545656, mean_eps: 0.100000\n","      87565/2000000000: episode: 2350, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: -4.300, mean reward: -0.107 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.738744, mean_q: 20.388592, mean_eps: 0.100000\n","      87603/2000000000: episode: 2351, duration: 5.215s, episode steps:  38, steps per second:   7, episode reward: 69.400, mean reward:  1.826 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 67.468714, mean_q: 20.098020, mean_eps: 0.100000\n","      87640/2000000000: episode: 2352, duration: 4.461s, episode steps:  37, steps per second:   8, episode reward: 125.900, mean reward:  3.403 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.330493, mean_q: 20.714738, mean_eps: 0.100000\n","      87680/2000000000: episode: 2353, duration: 5.118s, episode steps:  40, steps per second:   8, episode reward: -52.100, mean reward: -1.302 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 68.005892, mean_q: 20.028279, mean_eps: 0.100000\n","      87720/2000000000: episode: 2354, duration: 5.191s, episode steps:  40, steps per second:   8, episode reward: -57.600, mean reward: -1.440 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.868254, mean_q: 19.955574, mean_eps: 0.100000\n","      87751/2000000000: episode: 2355, duration: 3.993s, episode steps:  31, steps per second:   8, episode reward: 61.100, mean reward:  1.971 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 66.949249, mean_q: 20.546067, mean_eps: 0.100000\n","      87786/2000000000: episode: 2356, duration: 4.368s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 67.295775, mean_q: 20.803486, mean_eps: 0.100000\n","      87826/2000000000: episode: 2357, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.353890, mean_q: 20.151636, mean_eps: 0.100000\n","      87865/2000000000: episode: 2358, duration: 4.970s, episode steps:  39, steps per second:   8, episode reward: -134.000, mean reward: -3.436 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 60.463858, mean_q: 20.157056, mean_eps: 0.100000\n","      87905/2000000000: episode: 2359, duration: 5.158s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 59.465627, mean_q: 19.868307, mean_eps: 0.100000\n","      87942/2000000000: episode: 2360, duration: 4.810s, episode steps:  37, steps per second:   8, episode reward: 82.700, mean reward:  2.235 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 70.067192, mean_q: 20.974498, mean_eps: 0.100000\n","      87982/2000000000: episode: 2361, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.025587, mean_q: 20.021047, mean_eps: 0.100000\n","      88012/2000000000: episode: 2362, duration: 4.143s, episode steps:  30, steps per second:   7, episode reward: -36.500, mean reward: -1.217 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 69.096196, mean_q: 19.818905, mean_eps: 0.100000\n","      88051/2000000000: episode: 2363, duration: 5.423s, episode steps:  39, steps per second:   7, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 64.166890, mean_q: 19.803437, mean_eps: 0.100000\n","      88091/2000000000: episode: 2364, duration: 5.234s, episode steps:  40, steps per second:   8, episode reward: -37.200, mean reward: -0.930 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.028497, mean_q: 20.540667, mean_eps: 0.100000\n","      88131/2000000000: episode: 2365, duration: 5.487s, episode steps:  40, steps per second:   7, episode reward: 76.300, mean reward:  1.907 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.608638, mean_q: 19.568025, mean_eps: 0.100000\n","      88171/2000000000: episode: 2366, duration: 5.718s, episode steps:  40, steps per second:   7, episode reward:  0.900, mean reward:  0.022 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.397398, mean_q: 20.765707, mean_eps: 0.100000\n","      88211/2000000000: episode: 2367, duration: 5.032s, episode steps:  40, steps per second:   8, episode reward: 75.400, mean reward:  1.885 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.336888, mean_q: 19.913516, mean_eps: 0.100000\n","      88249/2000000000: episode: 2368, duration: 5.266s, episode steps:  38, steps per second:   7, episode reward: 84.100, mean reward:  2.213 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 72.843036, mean_q: 20.360958, mean_eps: 0.100000\n","      88289/2000000000: episode: 2369, duration: 5.487s, episode steps:  40, steps per second:   7, episode reward: -46.000, mean reward: -1.150 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.300298, mean_q: 19.948453, mean_eps: 0.100000\n","      88322/2000000000: episode: 2370, duration: 4.552s, episode steps:  33, steps per second:   7, episode reward: 133.800, mean reward:  4.055 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 64.193567, mean_q: 20.222557, mean_eps: 0.100000\n","      88362/2000000000: episode: 2371, duration: 6.031s, episode steps:  40, steps per second:   7, episode reward: 55.800, mean reward:  1.395 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.504617, mean_q: 20.383924, mean_eps: 0.100000\n","      88402/2000000000: episode: 2372, duration: 5.524s, episode steps:  40, steps per second:   7, episode reward: 77.700, mean reward:  1.942 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.550310, mean_q: 19.532470, mean_eps: 0.100000\n","      88442/2000000000: episode: 2373, duration: 5.544s, episode steps:  40, steps per second:   7, episode reward: -64.400, mean reward: -1.610 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 65.390333, mean_q: 19.595213, mean_eps: 0.100000\n","      88478/2000000000: episode: 2374, duration: 4.773s, episode steps:  36, steps per second:   8, episode reward: -90.700, mean reward: -2.519 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 69.039270, mean_q: 20.647090, mean_eps: 0.100000\n","      88507/2000000000: episode: 2375, duration: 3.846s, episode steps:  29, steps per second:   8, episode reward: -188.600, mean reward: -6.503 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 66.437066, mean_q: 20.155581, mean_eps: 0.100000\n","      88544/2000000000: episode: 2376, duration: 4.842s, episode steps:  37, steps per second:   8, episode reward: 65.500, mean reward:  1.770 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 64.055835, mean_q: 19.206711, mean_eps: 0.100000\n","      88577/2000000000: episode: 2377, duration: 4.486s, episode steps:  33, steps per second:   7, episode reward: -48.300, mean reward: -1.464 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 63.866472, mean_q: 19.383329, mean_eps: 0.100000\n","      88617/2000000000: episode: 2378, duration: 5.134s, episode steps:  40, steps per second:   8, episode reward: 12.500, mean reward:  0.313 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.693334, mean_q: 18.972809, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      88657/2000000000: episode: 2379, duration: 5.183s, episode steps:  40, steps per second:   8, episode reward: 146.600, mean reward:  3.665 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.473946, mean_q: 20.190870, mean_eps: 0.100000\n","      88697/2000000000: episode: 2380, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: -39.700, mean reward: -0.992 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.692185, mean_q: 19.878410, mean_eps: 0.100000\n","      88737/2000000000: episode: 2381, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: 27.800, mean reward:  0.695 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.965331, mean_q: 20.143345, mean_eps: 0.100000\n","      88777/2000000000: episode: 2382, duration: 5.218s, episode steps:  40, steps per second:   8, episode reward: 32.900, mean reward:  0.822 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.104991, mean_q: 19.717412, mean_eps: 0.100000\n","      88817/2000000000: episode: 2383, duration: 5.326s, episode steps:  40, steps per second:   8, episode reward: 33.000, mean reward:  0.825 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.681226, mean_q: 20.108492, mean_eps: 0.100000\n","      88857/2000000000: episode: 2384, duration: 5.412s, episode steps:  40, steps per second:   7, episode reward: 113.700, mean reward:  2.842 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.496836, mean_q: 19.125345, mean_eps: 0.100000\n","      88894/2000000000: episode: 2385, duration: 4.793s, episode steps:  37, steps per second:   8, episode reward: -6.300, mean reward: -0.170 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 69.637623, mean_q: 20.358459, mean_eps: 0.100000\n","      88934/2000000000: episode: 2386, duration: 5.202s, episode steps:  40, steps per second:   8, episode reward: 40.600, mean reward:  1.015 [-20.000, 18.600], mean action: 1.300 [0.000, 2.000],  loss: 66.650505, mean_q: 20.199438, mean_eps: 0.100000\n","      88974/2000000000: episode: 2387, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: -51.100, mean reward: -1.277 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 69.938840, mean_q: 19.608605, mean_eps: 0.100000\n","      89014/2000000000: episode: 2388, duration: 5.340s, episode steps:  40, steps per second:   7, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 68.032408, mean_q: 20.252542, mean_eps: 0.100000\n","      89054/2000000000: episode: 2389, duration: 5.383s, episode steps:  40, steps per second:   7, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.314693, mean_q: 19.560214, mean_eps: 0.100000\n","      89094/2000000000: episode: 2390, duration: 5.505s, episode steps:  40, steps per second:   7, episode reward: -44.400, mean reward: -1.110 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 66.323456, mean_q: 19.222974, mean_eps: 0.100000\n","      89134/2000000000: episode: 2391, duration: 5.105s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.489250, mean_q: 20.352952, mean_eps: 0.100000\n","      89172/2000000000: episode: 2392, duration: 5.075s, episode steps:  38, steps per second:   7, episode reward: 51.300, mean reward:  1.350 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 65.213066, mean_q: 20.415847, mean_eps: 0.100000\n","      89212/2000000000: episode: 2393, duration: 5.397s, episode steps:  40, steps per second:   7, episode reward: -130.000, mean reward: -3.250 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.260422, mean_q: 20.249000, mean_eps: 0.100000\n","      89252/2000000000: episode: 2394, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.052583, mean_q: 20.260395, mean_eps: 0.100000\n","      89288/2000000000: episode: 2395, duration: 4.484s, episode steps:  36, steps per second:   8, episode reward: -134.000, mean reward: -3.722 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 64.206954, mean_q: 19.680974, mean_eps: 0.100000\n","      89328/2000000000: episode: 2396, duration: 4.919s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.160305, mean_q: 20.698857, mean_eps: 0.100000\n","      89368/2000000000: episode: 2397, duration: 5.397s, episode steps:  40, steps per second:   7, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.645995, mean_q: 19.696583, mean_eps: 0.100000\n","      89407/2000000000: episode: 2398, duration: 4.933s, episode steps:  39, steps per second:   8, episode reward: -68.800, mean reward: -1.764 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 62.927249, mean_q: 20.180469, mean_eps: 0.100000\n","      89447/2000000000: episode: 2399, duration: 5.136s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.022335, mean_q: 20.163991, mean_eps: 0.100000\n","      89479/2000000000: episode: 2400, duration: 4.103s, episode steps:  32, steps per second:   8, episode reward: 149.600, mean reward:  4.675 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 67.884479, mean_q: 20.135992, mean_eps: 0.100000\n","      89519/2000000000: episode: 2401, duration: 5.172s, episode steps:  40, steps per second:   8, episode reward: -134.000, mean reward: -3.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.265554, mean_q: 19.941224, mean_eps: 0.100000\n","      89559/2000000000: episode: 2402, duration: 4.992s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 66.852891, mean_q: 19.863049, mean_eps: 0.100000\n","      89599/2000000000: episode: 2403, duration: 5.908s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.906184, mean_q: 20.130263, mean_eps: 0.100000\n","      89639/2000000000: episode: 2404, duration: 5.610s, episode steps:  40, steps per second:   7, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 65.987889, mean_q: 19.476228, mean_eps: 0.100000\n","      89679/2000000000: episode: 2405, duration: 5.613s, episode steps:  40, steps per second:   7, episode reward: 22.100, mean reward:  0.553 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.598649, mean_q: 19.623523, mean_eps: 0.100000\n","      89719/2000000000: episode: 2406, duration: 5.824s, episode steps:  40, steps per second:   7, episode reward:  6.700, mean reward:  0.167 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.504941, mean_q: 19.746373, mean_eps: 0.100000\n","      89759/2000000000: episode: 2407, duration: 4.953s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.215148, mean_q: 19.318117, mean_eps: 0.100000\n","      89798/2000000000: episode: 2408, duration: 5.443s, episode steps:  39, steps per second:   7, episode reward: 112.000, mean reward:  2.872 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 69.913435, mean_q: 20.467072, mean_eps: 0.100000\n","      89834/2000000000: episode: 2409, duration: 4.778s, episode steps:  36, steps per second:   8, episode reward: 27.300, mean reward:  0.758 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 71.260963, mean_q: 20.019705, mean_eps: 0.100000\n","      89874/2000000000: episode: 2410, duration: 5.339s, episode steps:  40, steps per second:   7, episode reward:  0.900, mean reward:  0.023 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.874617, mean_q: 19.986471, mean_eps: 0.100000\n","      89914/2000000000: episode: 2411, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.956935, mean_q: 19.856204, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      89953/2000000000: episode: 2412, duration: 5.197s, episode steps:  39, steps per second:   8, episode reward: 17.900, mean reward:  0.459 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 66.302060, mean_q: 18.930803, mean_eps: 0.100000\n","      89993/2000000000: episode: 2413, duration: 5.183s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.713834, mean_q: 20.147346, mean_eps: 0.100000\n","      90033/2000000000: episode: 2414, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward: -30.700, mean reward: -0.768 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.786987, mean_q: 21.650705, mean_eps: 0.100000\n","      90073/2000000000: episode: 2415, duration: 4.995s, episode steps:  40, steps per second:   8, episode reward: -67.000, mean reward: -1.675 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 76.332691, mean_q: 23.585578, mean_eps: 0.100000\n","      90113/2000000000: episode: 2416, duration: 4.700s, episode steps:  40, steps per second:   9, episode reward: 43.500, mean reward:  1.087 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 76.988067, mean_q: 22.668880, mean_eps: 0.100000\n","      90150/2000000000: episode: 2417, duration: 4.500s, episode steps:  37, steps per second:   8, episode reward: -119.200, mean reward: -3.222 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 72.761879, mean_q: 22.031718, mean_eps: 0.100000\n","      90190/2000000000: episode: 2418, duration: 4.738s, episode steps:  40, steps per second:   8, episode reward: 137.100, mean reward:  3.428 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 76.511401, mean_q: 22.319546, mean_eps: 0.100000\n","      90227/2000000000: episode: 2419, duration: 4.366s, episode steps:  37, steps per second:   8, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 74.433629, mean_q: 22.686052, mean_eps: 0.100000\n","      90267/2000000000: episode: 2420, duration: 4.937s, episode steps:  40, steps per second:   8, episode reward: 114.500, mean reward:  2.862 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.819764, mean_q: 22.391339, mean_eps: 0.100000\n","      90307/2000000000: episode: 2421, duration: 4.800s, episode steps:  40, steps per second:   8, episode reward: 13.800, mean reward:  0.345 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.139437, mean_q: 23.253478, mean_eps: 0.100000\n","      90339/2000000000: episode: 2422, duration: 3.885s, episode steps:  32, steps per second:   8, episode reward: 21.100, mean reward:  0.659 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 70.679726, mean_q: 22.962670, mean_eps: 0.100000\n","      90379/2000000000: episode: 2423, duration: 4.762s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.763778, mean_q: 21.882857, mean_eps: 0.100000\n","      90419/2000000000: episode: 2424, duration: 4.821s, episode steps:  40, steps per second:   8, episode reward: -2.900, mean reward: -0.073 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.994265, mean_q: 23.035981, mean_eps: 0.100000\n","      90459/2000000000: episode: 2425, duration: 4.807s, episode steps:  40, steps per second:   8, episode reward:  6.700, mean reward:  0.167 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.580173, mean_q: 22.495471, mean_eps: 0.100000\n","      90499/2000000000: episode: 2426, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.360378, mean_q: 22.400816, mean_eps: 0.100000\n","      90530/2000000000: episode: 2427, duration: 3.788s, episode steps:  31, steps per second:   8, episode reward: -36.800, mean reward: -1.187 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 68.483590, mean_q: 23.249439, mean_eps: 0.100000\n","      90564/2000000000: episode: 2428, duration: 4.238s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 70.231260, mean_q: 22.551161, mean_eps: 0.100000\n","      90592/2000000000: episode: 2429, duration: 3.415s, episode steps:  28, steps per second:   8, episode reward: -101.600, mean reward: -3.629 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 80.388583, mean_q: 23.128686, mean_eps: 0.100000\n","      90632/2000000000: episode: 2430, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward:  3.100, mean reward:  0.077 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.839918, mean_q: 22.731921, mean_eps: 0.100000\n","      90671/2000000000: episode: 2431, duration: 4.702s, episode steps:  39, steps per second:   8, episode reward: 134.300, mean reward:  3.444 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 70.512007, mean_q: 23.300613, mean_eps: 0.100000\n","      90702/2000000000: episode: 2432, duration: 3.911s, episode steps:  31, steps per second:   8, episode reward: 42.700, mean reward:  1.377 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 74.294983, mean_q: 23.050643, mean_eps: 0.100000\n","      90742/2000000000: episode: 2433, duration: 4.901s, episode steps:  40, steps per second:   8, episode reward: 112.900, mean reward:  2.822 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.712094, mean_q: 22.734279, mean_eps: 0.100000\n","      90782/2000000000: episode: 2434, duration: 4.784s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.020883, mean_q: 21.943014, mean_eps: 0.100000\n","      90814/2000000000: episode: 2435, duration: 3.819s, episode steps:  32, steps per second:   8, episode reward: -91.400, mean reward: -2.856 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 77.778595, mean_q: 22.711755, mean_eps: 0.100000\n","      90849/2000000000: episode: 2436, duration: 4.267s, episode steps:  35, steps per second:   8, episode reward: 37.100, mean reward:  1.060 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.634727, mean_q: 22.883987, mean_eps: 0.100000\n","      90889/2000000000: episode: 2437, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: -77.900, mean reward: -1.947 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.423124, mean_q: 22.718325, mean_eps: 0.100000\n","      90929/2000000000: episode: 2438, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: -48.500, mean reward: -1.213 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.466594, mean_q: 22.749138, mean_eps: 0.100000\n","      90962/2000000000: episode: 2439, duration: 4.182s, episode steps:  33, steps per second:   8, episode reward: 99.600, mean reward:  3.018 [-20.000, 18.300], mean action: 1.091 [0.000, 2.000],  loss: 75.588362, mean_q: 22.838174, mean_eps: 0.100000\n","      90997/2000000000: episode: 2440, duration: 4.391s, episode steps:  35, steps per second:   8, episode reward: -9.000, mean reward: -0.257 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 63.807171, mean_q: 22.787379, mean_eps: 0.100000\n","      91037/2000000000: episode: 2441, duration: 4.786s, episode steps:  40, steps per second:   8, episode reward: 13.700, mean reward:  0.342 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.878524, mean_q: 22.280236, mean_eps: 0.100000\n","      91071/2000000000: episode: 2442, duration: 4.321s, episode steps:  34, steps per second:   8, episode reward: -62.700, mean reward: -1.844 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 74.563731, mean_q: 22.292335, mean_eps: 0.100000\n","      91111/2000000000: episode: 2443, duration: 4.802s, episode steps:  40, steps per second:   8, episode reward: -78.700, mean reward: -1.967 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.908487, mean_q: 22.434746, mean_eps: 0.100000\n","      91149/2000000000: episode: 2444, duration: 4.538s, episode steps:  38, steps per second:   8, episode reward: 112.500, mean reward:  2.961 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 75.767822, mean_q: 22.836342, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      91189/2000000000: episode: 2445, duration: 4.766s, episode steps:  40, steps per second:   8, episode reward: -55.200, mean reward: -1.380 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.168790, mean_q: 23.221020, mean_eps: 0.100000\n","      91229/2000000000: episode: 2446, duration: 4.719s, episode steps:  40, steps per second:   8, episode reward: 74.000, mean reward:  1.850 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.867342, mean_q: 22.255175, mean_eps: 0.100000\n","      91269/2000000000: episode: 2447, duration: 5.043s, episode steps:  40, steps per second:   8, episode reward: -55.500, mean reward: -1.388 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 71.812285, mean_q: 22.188131, mean_eps: 0.100000\n","      91309/2000000000: episode: 2448, duration: 5.123s, episode steps:  40, steps per second:   8, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.509933, mean_q: 21.925139, mean_eps: 0.100000\n","      91349/2000000000: episode: 2449, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: -102.600, mean reward: -2.565 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.620830, mean_q: 22.799177, mean_eps: 0.100000\n","      91389/2000000000: episode: 2450, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.745681, mean_q: 22.195166, mean_eps: 0.100000\n","      91429/2000000000: episode: 2451, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 74.409288, mean_q: 23.101131, mean_eps: 0.100000\n","      91463/2000000000: episode: 2452, duration: 4.405s, episode steps:  34, steps per second:   8, episode reward: 115.500, mean reward:  3.397 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 73.581880, mean_q: 22.368854, mean_eps: 0.100000\n","      91500/2000000000: episode: 2453, duration: 4.680s, episode steps:  37, steps per second:   8, episode reward: -74.100, mean reward: -2.003 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 76.628801, mean_q: 23.246306, mean_eps: 0.100000\n","      91540/2000000000: episode: 2454, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: 17.600, mean reward:  0.440 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.195357, mean_q: 22.506440, mean_eps: 0.100000\n","      91580/2000000000: episode: 2455, duration: 4.919s, episode steps:  40, steps per second:   8, episode reward: 188.100, mean reward:  4.702 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.519401, mean_q: 21.990410, mean_eps: 0.100000\n","      91620/2000000000: episode: 2456, duration: 4.994s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.610945, mean_q: 22.750716, mean_eps: 0.100000\n","      91655/2000000000: episode: 2457, duration: 4.263s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 68.835422, mean_q: 23.289023, mean_eps: 0.100000\n","      91695/2000000000: episode: 2458, duration: 4.994s, episode steps:  40, steps per second:   8, episode reward: 16.100, mean reward:  0.403 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.867073, mean_q: 22.788323, mean_eps: 0.100000\n","      91735/2000000000: episode: 2459, duration: 5.295s, episode steps:  40, steps per second:   8, episode reward: -27.300, mean reward: -0.682 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.813128, mean_q: 22.774942, mean_eps: 0.100000\n","      91775/2000000000: episode: 2460, duration: 5.309s, episode steps:  40, steps per second:   8, episode reward: 54.300, mean reward:  1.358 [-20.000, 18.000], mean action: 1.075 [0.000, 2.000],  loss: 72.113302, mean_q: 22.688562, mean_eps: 0.100000\n","      91805/2000000000: episode: 2461, duration: 4.016s, episode steps:  30, steps per second:   7, episode reward: -47.500, mean reward: -1.583 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.567345, mean_q: 22.890239, mean_eps: 0.100000\n","      91836/2000000000: episode: 2462, duration: 3.952s, episode steps:  31, steps per second:   8, episode reward: -135.700, mean reward: -4.377 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 66.197456, mean_q: 23.067770, mean_eps: 0.100000\n","      91871/2000000000: episode: 2463, duration: 4.496s, episode steps:  35, steps per second:   8, episode reward: 81.700, mean reward:  2.334 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 72.316442, mean_q: 22.982804, mean_eps: 0.100000\n","      91911/2000000000: episode: 2464, duration: 4.911s, episode steps:  40, steps per second:   8, episode reward: -23.200, mean reward: -0.580 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.881443, mean_q: 23.070527, mean_eps: 0.100000\n","      91948/2000000000: episode: 2465, duration: 4.646s, episode steps:  37, steps per second:   8, episode reward: 90.500, mean reward:  2.446 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.853122, mean_q: 22.564018, mean_eps: 0.100000\n","      91988/2000000000: episode: 2466, duration: 5.055s, episode steps:  40, steps per second:   8, episode reward:  7.000, mean reward:  0.175 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.537727, mean_q: 22.769526, mean_eps: 0.100000\n","      92027/2000000000: episode: 2467, duration: 5.075s, episode steps:  39, steps per second:   8, episode reward: -48.100, mean reward: -1.233 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 67.990033, mean_q: 22.446790, mean_eps: 0.100000\n","      92067/2000000000: episode: 2468, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 99.400, mean reward:  2.485 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.379599, mean_q: 22.484143, mean_eps: 0.100000\n","      92100/2000000000: episode: 2469, duration: 4.241s, episode steps:  33, steps per second:   8, episode reward: -78.600, mean reward: -2.382 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 73.409563, mean_q: 22.813373, mean_eps: 0.100000\n","      92140/2000000000: episode: 2470, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 61.600, mean reward:  1.540 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.038963, mean_q: 23.012809, mean_eps: 0.100000\n","      92175/2000000000: episode: 2471, duration: 4.281s, episode steps:  35, steps per second:   8, episode reward: -47.100, mean reward: -1.346 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 69.500887, mean_q: 22.428973, mean_eps: 0.100000\n","      92215/2000000000: episode: 2472, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: -36.800, mean reward: -0.920 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.762910, mean_q: 22.888167, mean_eps: 0.100000\n","      92255/2000000000: episode: 2473, duration: 4.670s, episode steps:  40, steps per second:   9, episode reward:  9.100, mean reward:  0.227 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.990348, mean_q: 22.583951, mean_eps: 0.100000\n","      92295/2000000000: episode: 2474, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: -32.100, mean reward: -0.802 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.147174, mean_q: 22.519478, mean_eps: 0.100000\n","      92335/2000000000: episode: 2475, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: -79.900, mean reward: -1.998 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.479827, mean_q: 21.848594, mean_eps: 0.100000\n","      92375/2000000000: episode: 2476, duration: 5.056s, episode steps:  40, steps per second:   8, episode reward: 34.800, mean reward:  0.870 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.319054, mean_q: 22.631296, mean_eps: 0.100000\n","      92415/2000000000: episode: 2477, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: -20.700, mean reward: -0.517 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.819198, mean_q: 22.002077, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      92454/2000000000: episode: 2478, duration: 4.844s, episode steps:  39, steps per second:   8, episode reward: -49.200, mean reward: -1.262 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 68.414154, mean_q: 22.243622, mean_eps: 0.100000\n","      92494/2000000000: episode: 2479, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: 27.000, mean reward:  0.675 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.567778, mean_q: 22.357432, mean_eps: 0.100000\n","      92526/2000000000: episode: 2480, duration: 3.973s, episode steps:  32, steps per second:   8, episode reward: 143.000, mean reward:  4.469 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 71.262159, mean_q: 22.064225, mean_eps: 0.100000\n","      92566/2000000000: episode: 2481, duration: 4.778s, episode steps:  40, steps per second:   8, episode reward: 145.400, mean reward:  3.635 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.963901, mean_q: 22.597293, mean_eps: 0.100000\n","      92606/2000000000: episode: 2482, duration: 5.046s, episode steps:  40, steps per second:   8, episode reward: 38.400, mean reward:  0.960 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.079050, mean_q: 22.753849, mean_eps: 0.100000\n","      92643/2000000000: episode: 2483, duration: 4.757s, episode steps:  37, steps per second:   8, episode reward: -3.500, mean reward: -0.095 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 74.090599, mean_q: 22.635633, mean_eps: 0.100000\n","      92680/2000000000: episode: 2484, duration: 4.677s, episode steps:  37, steps per second:   8, episode reward: 29.400, mean reward:  0.795 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 75.894599, mean_q: 23.340496, mean_eps: 0.100000\n","      92718/2000000000: episode: 2485, duration: 5.100s, episode steps:  38, steps per second:   7, episode reward: 54.600, mean reward:  1.437 [-20.000, 18.600], mean action: 1.079 [0.000, 2.000],  loss: 62.657749, mean_q: 22.618356, mean_eps: 0.100000\n","      92758/2000000000: episode: 2486, duration: 5.322s, episode steps:  40, steps per second:   8, episode reward: -26.600, mean reward: -0.665 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.868548, mean_q: 22.980719, mean_eps: 0.100000\n","      92795/2000000000: episode: 2487, duration: 4.897s, episode steps:  37, steps per second:   8, episode reward:  7.000, mean reward:  0.189 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 73.606459, mean_q: 22.895464, mean_eps: 0.100000\n","      92824/2000000000: episode: 2488, duration: 3.924s, episode steps:  29, steps per second:   7, episode reward: 10.800, mean reward:  0.372 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 72.145790, mean_q: 22.479416, mean_eps: 0.100000\n","      92864/2000000000: episode: 2489, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.337898, mean_q: 23.046141, mean_eps: 0.100000\n","      92900/2000000000: episode: 2490, duration: 4.832s, episode steps:  36, steps per second:   7, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 74.895871, mean_q: 23.003193, mean_eps: 0.100000\n","      92940/2000000000: episode: 2491, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: 10.100, mean reward:  0.253 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 61.507471, mean_q: 22.457258, mean_eps: 0.100000\n","      92980/2000000000: episode: 2492, duration: 5.086s, episode steps:  40, steps per second:   8, episode reward: 51.300, mean reward:  1.283 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.634355, mean_q: 22.576123, mean_eps: 0.100000\n","      93020/2000000000: episode: 2493, duration: 5.532s, episode steps:  40, steps per second:   7, episode reward: -19.700, mean reward: -0.492 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.446830, mean_q: 22.818623, mean_eps: 0.100000\n","      93060/2000000000: episode: 2494, duration: 5.305s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 65.311571, mean_q: 22.970140, mean_eps: 0.100000\n","      93098/2000000000: episode: 2495, duration: 5.028s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 70.367126, mean_q: 22.427890, mean_eps: 0.100000\n","      93138/2000000000: episode: 2496, duration: 5.434s, episode steps:  40, steps per second:   7, episode reward: -139.700, mean reward: -3.492 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.085155, mean_q: 23.144858, mean_eps: 0.100000\n","      93178/2000000000: episode: 2497, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: -89.900, mean reward: -2.247 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.069257, mean_q: 22.594970, mean_eps: 0.100000\n","      93218/2000000000: episode: 2498, duration: 5.128s, episode steps:  40, steps per second:   8, episode reward: 125.400, mean reward:  3.135 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 76.289384, mean_q: 22.071207, mean_eps: 0.100000\n","      93258/2000000000: episode: 2499, duration: 5.719s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.255527, mean_q: 22.516385, mean_eps: 0.100000\n","      93290/2000000000: episode: 2500, duration: 4.493s, episode steps:  32, steps per second:   7, episode reward: -105.400, mean reward: -3.294 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 71.753786, mean_q: 22.677508, mean_eps: 0.100000\n","      93330/2000000000: episode: 2501, duration: 5.738s, episode steps:  40, steps per second:   7, episode reward: -48.100, mean reward: -1.202 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 66.758669, mean_q: 22.855744, mean_eps: 0.100000\n","      93370/2000000000: episode: 2502, duration: 5.478s, episode steps:  40, steps per second:   7, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 63.631361, mean_q: 22.950788, mean_eps: 0.100000\n","      93410/2000000000: episode: 2503, duration: 5.540s, episode steps:  40, steps per second:   7, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.683585, mean_q: 22.592717, mean_eps: 0.100000\n","      93450/2000000000: episode: 2504, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.656644, mean_q: 22.756487, mean_eps: 0.100000\n","      93490/2000000000: episode: 2505, duration: 5.742s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.609906, mean_q: 23.187393, mean_eps: 0.100000\n","      93530/2000000000: episode: 2506, duration: 5.726s, episode steps:  40, steps per second:   7, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.834335, mean_q: 22.566694, mean_eps: 0.100000\n","      93570/2000000000: episode: 2507, duration: 5.728s, episode steps:  40, steps per second:   7, episode reward: -105.900, mean reward: -2.647 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 68.303241, mean_q: 22.086125, mean_eps: 0.100000\n","      93609/2000000000: episode: 2508, duration: 5.105s, episode steps:  39, steps per second:   8, episode reward: 72.400, mean reward:  1.856 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 70.656512, mean_q: 22.249394, mean_eps: 0.100000\n","      93649/2000000000: episode: 2509, duration: 5.351s, episode steps:  40, steps per second:   7, episode reward: -5.800, mean reward: -0.145 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.145592, mean_q: 23.627275, mean_eps: 0.100000\n","      93685/2000000000: episode: 2510, duration: 5.103s, episode steps:  36, steps per second:   7, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 70.514979, mean_q: 22.449145, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      93725/2000000000: episode: 2511, duration: 5.654s, episode steps:  40, steps per second:   7, episode reward: -110.000, mean reward: -2.750 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.708072, mean_q: 22.515943, mean_eps: 0.100000\n","      93752/2000000000: episode: 2512, duration: 3.795s, episode steps:  27, steps per second:   7, episode reward: 37.800, mean reward:  1.400 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 68.133265, mean_q: 22.543340, mean_eps: 0.100000\n","      93792/2000000000: episode: 2513, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.450783, mean_q: 22.701336, mean_eps: 0.100000\n","      93832/2000000000: episode: 2514, duration: 5.478s, episode steps:  40, steps per second:   7, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.091180, mean_q: 21.979258, mean_eps: 0.100000\n","      93872/2000000000: episode: 2515, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: -56.100, mean reward: -1.402 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 65.028435, mean_q: 22.230917, mean_eps: 0.100000\n","      93912/2000000000: episode: 2516, duration: 5.713s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.118752, mean_q: 22.883944, mean_eps: 0.100000\n","      93950/2000000000: episode: 2517, duration: 4.910s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 72.158863, mean_q: 22.128609, mean_eps: 0.100000\n","      93990/2000000000: episode: 2518, duration: 5.460s, episode steps:  40, steps per second:   7, episode reward: 45.200, mean reward:  1.130 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.528129, mean_q: 22.463610, mean_eps: 0.100000\n","      94027/2000000000: episode: 2519, duration: 5.102s, episode steps:  37, steps per second:   7, episode reward:  3.500, mean reward:  0.095 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 69.955632, mean_q: 22.616323, mean_eps: 0.100000\n","      94062/2000000000: episode: 2520, duration: 4.823s, episode steps:  35, steps per second:   7, episode reward: -32.700, mean reward: -0.934 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.047688, mean_q: 22.935185, mean_eps: 0.100000\n","      94102/2000000000: episode: 2521, duration: 5.344s, episode steps:  40, steps per second:   7, episode reward: 178.300, mean reward:  4.458 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.995987, mean_q: 22.298053, mean_eps: 0.100000\n","      94142/2000000000: episode: 2522, duration: 5.308s, episode steps:  40, steps per second:   8, episode reward: -105.200, mean reward: -2.630 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.264164, mean_q: 22.225481, mean_eps: 0.100000\n","      94182/2000000000: episode: 2523, duration: 5.593s, episode steps:  40, steps per second:   7, episode reward: 138.000, mean reward:  3.450 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.821125, mean_q: 22.660870, mean_eps: 0.100000\n","      94207/2000000000: episode: 2524, duration: 3.440s, episode steps:  25, steps per second:   7, episode reward: -107.000, mean reward: -4.280 [-20.000, 18.000], mean action: 0.600 [0.000, 2.000],  loss: 67.317886, mean_q: 22.674909, mean_eps: 0.100000\n","      94243/2000000000: episode: 2525, duration: 4.893s, episode steps:  36, steps per second:   7, episode reward: -40.700, mean reward: -1.131 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 61.848997, mean_q: 22.710920, mean_eps: 0.100000\n","      94283/2000000000: episode: 2526, duration: 5.587s, episode steps:  40, steps per second:   7, episode reward: 30.500, mean reward:  0.763 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.631626, mean_q: 22.376183, mean_eps: 0.100000\n","      94321/2000000000: episode: 2527, duration: 5.142s, episode steps:  38, steps per second:   7, episode reward: -28.400, mean reward: -0.747 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 73.471977, mean_q: 23.037648, mean_eps: 0.100000\n","      94361/2000000000: episode: 2528, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: -31.000, mean reward: -0.775 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.739468, mean_q: 22.144783, mean_eps: 0.100000\n","      94401/2000000000: episode: 2529, duration: 5.414s, episode steps:  40, steps per second:   7, episode reward: -45.800, mean reward: -1.145 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.130386, mean_q: 22.504160, mean_eps: 0.100000\n","      94441/2000000000: episode: 2530, duration: 5.420s, episode steps:  40, steps per second:   7, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.857717, mean_q: 22.298650, mean_eps: 0.100000\n","      94481/2000000000: episode: 2531, duration: 5.459s, episode steps:  40, steps per second:   7, episode reward: 36.200, mean reward:  0.905 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.993491, mean_q: 22.146550, mean_eps: 0.100000\n","      94511/2000000000: episode: 2532, duration: 4.173s, episode steps:  30, steps per second:   7, episode reward: 47.600, mean reward:  1.587 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 64.928102, mean_q: 23.370353, mean_eps: 0.100000\n","      94551/2000000000: episode: 2533, duration: 5.541s, episode steps:  40, steps per second:   7, episode reward: -30.200, mean reward: -0.755 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.032653, mean_q: 22.453662, mean_eps: 0.100000\n","      94591/2000000000: episode: 2534, duration: 5.379s, episode steps:  40, steps per second:   7, episode reward: 109.600, mean reward:  2.740 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.665768, mean_q: 22.298003, mean_eps: 0.100000\n","      94628/2000000000: episode: 2535, duration: 5.164s, episode steps:  37, steps per second:   7, episode reward: 223.800, mean reward:  6.049 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 68.328447, mean_q: 22.588699, mean_eps: 0.100000\n","      94665/2000000000: episode: 2536, duration: 4.982s, episode steps:  37, steps per second:   7, episode reward: -7.400, mean reward: -0.200 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 59.734485, mean_q: 23.130257, mean_eps: 0.100000\n","      94705/2000000000: episode: 2537, duration: 5.364s, episode steps:  40, steps per second:   7, episode reward: 116.300, mean reward:  2.907 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.696783, mean_q: 21.972365, mean_eps: 0.100000\n","      94739/2000000000: episode: 2538, duration: 4.692s, episode steps:  34, steps per second:   7, episode reward: -18.600, mean reward: -0.547 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 71.125810, mean_q: 23.218411, mean_eps: 0.100000\n","      94779/2000000000: episode: 2539, duration: 5.525s, episode steps:  40, steps per second:   7, episode reward: 11.000, mean reward:  0.275 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.234016, mean_q: 22.729672, mean_eps: 0.100000\n","      94819/2000000000: episode: 2540, duration: 4.870s, episode steps:  40, steps per second:   8, episode reward: -25.800, mean reward: -0.645 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.726203, mean_q: 22.560441, mean_eps: 0.100000\n","      94859/2000000000: episode: 2541, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: 70.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.762570, mean_q: 23.000022, mean_eps: 0.100000\n","      94897/2000000000: episode: 2542, duration: 5.170s, episode steps:  38, steps per second:   7, episode reward: 23.500, mean reward:  0.618 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 69.098512, mean_q: 22.824910, mean_eps: 0.100000\n","      94933/2000000000: episode: 2543, duration: 4.910s, episode steps:  36, steps per second:   7, episode reward: -65.300, mean reward: -1.814 [-20.000, 18.000], mean action: 0.972 [0.000, 2.000],  loss: 68.566684, mean_q: 23.178555, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      94970/2000000000: episode: 2544, duration: 4.865s, episode steps:  37, steps per second:   8, episode reward:  0.400, mean reward:  0.011 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 74.240270, mean_q: 22.138748, mean_eps: 0.100000\n","      95010/2000000000: episode: 2545, duration: 5.260s, episode steps:  40, steps per second:   8, episode reward: -24.000, mean reward: -0.600 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.248349, mean_q: 22.221061, mean_eps: 0.100000\n","      95050/2000000000: episode: 2546, duration: 5.731s, episode steps:  40, steps per second:   7, episode reward: 74.600, mean reward:  1.865 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.484668, mean_q: 22.115570, mean_eps: 0.100000\n","      95085/2000000000: episode: 2547, duration: 4.580s, episode steps:  35, steps per second:   8, episode reward: 101.500, mean reward:  2.900 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 61.845508, mean_q: 23.324927, mean_eps: 0.100000\n","      95119/2000000000: episode: 2548, duration: 4.608s, episode steps:  34, steps per second:   7, episode reward: 63.700, mean reward:  1.874 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 69.645015, mean_q: 22.214106, mean_eps: 0.100000\n","      95159/2000000000: episode: 2549, duration: 4.916s, episode steps:  40, steps per second:   8, episode reward: -32.500, mean reward: -0.812 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.133354, mean_q: 22.174308, mean_eps: 0.100000\n","      95199/2000000000: episode: 2550, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 178.600, mean reward:  4.465 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.708568, mean_q: 22.942929, mean_eps: 0.100000\n","      95234/2000000000: episode: 2551, duration: 4.644s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 69.616118, mean_q: 22.820594, mean_eps: 0.100000\n","      95274/2000000000: episode: 2552, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: 163.200, mean reward:  4.080 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.196211, mean_q: 22.682851, mean_eps: 0.100000\n","      95310/2000000000: episode: 2553, duration: 4.458s, episode steps:  36, steps per second:   8, episode reward: -134.000, mean reward: -3.722 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 69.449789, mean_q: 22.826997, mean_eps: 0.100000\n","      95347/2000000000: episode: 2554, duration: 4.441s, episode steps:  37, steps per second:   8, episode reward:  3.700, mean reward:  0.100 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 69.120031, mean_q: 22.495000, mean_eps: 0.100000\n","      95381/2000000000: episode: 2555, duration: 4.075s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 70.823867, mean_q: 22.935783, mean_eps: 0.100000\n","      95417/2000000000: episode: 2556, duration: 4.272s, episode steps:  36, steps per second:   8, episode reward: 143.800, mean reward:  3.994 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 73.525397, mean_q: 22.496015, mean_eps: 0.100000\n","      95457/2000000000: episode: 2557, duration: 4.655s, episode steps:  40, steps per second:   9, episode reward: -60.900, mean reward: -1.523 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 69.515894, mean_q: 22.969023, mean_eps: 0.100000\n","      95494/2000000000: episode: 2558, duration: 4.559s, episode steps:  37, steps per second:   8, episode reward: -41.400, mean reward: -1.119 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 73.418855, mean_q: 23.196594, mean_eps: 0.100000\n","      95534/2000000000: episode: 2559, duration: 4.614s, episode steps:  40, steps per second:   9, episode reward: 42.600, mean reward:  1.065 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.324015, mean_q: 21.988944, mean_eps: 0.100000\n","      95574/2000000000: episode: 2560, duration: 4.614s, episode steps:  40, steps per second:   9, episode reward: 230.000, mean reward:  5.750 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.069541, mean_q: 22.688708, mean_eps: 0.100000\n","      95614/2000000000: episode: 2561, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward: -26.300, mean reward: -0.657 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.626096, mean_q: 22.182451, mean_eps: 0.100000\n","      95652/2000000000: episode: 2562, duration: 4.791s, episode steps:  38, steps per second:   8, episode reward: -21.600, mean reward: -0.568 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 67.790650, mean_q: 22.331557, mean_eps: 0.100000\n","      95692/2000000000: episode: 2563, duration: 4.738s, episode steps:  40, steps per second:   8, episode reward: 37.800, mean reward:  0.945 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 63.345761, mean_q: 23.062453, mean_eps: 0.100000\n","      95730/2000000000: episode: 2564, duration: 4.402s, episode steps:  38, steps per second:   9, episode reward: -0.400, mean reward: -0.011 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 65.995489, mean_q: 23.051547, mean_eps: 0.100000\n","      95768/2000000000: episode: 2565, duration: 4.665s, episode steps:  38, steps per second:   8, episode reward: -11.200, mean reward: -0.295 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 63.602344, mean_q: 23.208947, mean_eps: 0.100000\n","      95808/2000000000: episode: 2566, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: 144.900, mean reward:  3.622 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.662297, mean_q: 22.244725, mean_eps: 0.100000\n","      95848/2000000000: episode: 2567, duration: 5.058s, episode steps:  40, steps per second:   8, episode reward: -5.400, mean reward: -0.135 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.128711, mean_q: 22.744904, mean_eps: 0.100000\n","      95888/2000000000: episode: 2568, duration: 4.627s, episode steps:  40, steps per second:   9, episode reward: -102.900, mean reward: -2.573 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 65.286495, mean_q: 22.237479, mean_eps: 0.100000\n","      95923/2000000000: episode: 2569, duration: 4.002s, episode steps:  35, steps per second:   9, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 66.739456, mean_q: 23.425609, mean_eps: 0.100000\n","      95963/2000000000: episode: 2570, duration: 4.613s, episode steps:  40, steps per second:   9, episode reward: 53.800, mean reward:  1.345 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.071090, mean_q: 22.641748, mean_eps: 0.100000\n","      96003/2000000000: episode: 2571, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: -80.300, mean reward: -2.008 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.757291, mean_q: 21.954239, mean_eps: 0.100000\n","      96043/2000000000: episode: 2572, duration: 4.946s, episode steps:  40, steps per second:   8, episode reward: 58.100, mean reward:  1.453 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.199647, mean_q: 22.750792, mean_eps: 0.100000\n","      96083/2000000000: episode: 2573, duration: 5.031s, episode steps:  40, steps per second:   8, episode reward: 15.100, mean reward:  0.378 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.137569, mean_q: 22.591280, mean_eps: 0.100000\n","      96122/2000000000: episode: 2574, duration: 4.752s, episode steps:  39, steps per second:   8, episode reward: 26.700, mean reward:  0.685 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 70.275126, mean_q: 23.400547, mean_eps: 0.100000\n","      96162/2000000000: episode: 2575, duration: 4.870s, episode steps:  40, steps per second:   8, episode reward: 44.900, mean reward:  1.122 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.708658, mean_q: 22.553702, mean_eps: 0.100000\n","      96202/2000000000: episode: 2576, duration: 5.156s, episode steps:  40, steps per second:   8, episode reward: 13.500, mean reward:  0.338 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.987548, mean_q: 22.745836, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      96238/2000000000: episode: 2577, duration: 5.118s, episode steps:  36, steps per second:   7, episode reward: 17.100, mean reward:  0.475 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 71.350632, mean_q: 23.659398, mean_eps: 0.100000\n","      96278/2000000000: episode: 2578, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: -33.100, mean reward: -0.828 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.172711, mean_q: 22.736033, mean_eps: 0.100000\n","      96306/2000000000: episode: 2579, duration: 3.328s, episode steps:  28, steps per second:   8, episode reward: 98.700, mean reward:  3.525 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 65.064019, mean_q: 23.421959, mean_eps: 0.100000\n","      96346/2000000000: episode: 2580, duration: 4.990s, episode steps:  40, steps per second:   8, episode reward: 187.900, mean reward:  4.698 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.667021, mean_q: 22.575413, mean_eps: 0.100000\n","      96386/2000000000: episode: 2581, duration: 5.090s, episode steps:  40, steps per second:   8, episode reward: 58.800, mean reward:  1.470 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.158105, mean_q: 22.830790, mean_eps: 0.100000\n","      96426/2000000000: episode: 2582, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward:  5.400, mean reward:  0.135 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 76.947755, mean_q: 22.368808, mean_eps: 0.100000\n","      96462/2000000000: episode: 2583, duration: 4.535s, episode steps:  36, steps per second:   8, episode reward: 12.300, mean reward:  0.342 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 69.890961, mean_q: 22.805980, mean_eps: 0.100000\n","      96502/2000000000: episode: 2584, duration: 5.102s, episode steps:  40, steps per second:   8, episode reward: 24.600, mean reward:  0.615 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 74.971644, mean_q: 22.858647, mean_eps: 0.100000\n","      96542/2000000000: episode: 2585, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: 92.800, mean reward:  2.320 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.177925, mean_q: 21.666329, mean_eps: 0.100000\n","      96580/2000000000: episode: 2586, duration: 4.553s, episode steps:  38, steps per second:   8, episode reward: -45.400, mean reward: -1.195 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 71.023554, mean_q: 22.571920, mean_eps: 0.100000\n","      96620/2000000000: episode: 2587, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward: -5.900, mean reward: -0.148 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.209581, mean_q: 23.112203, mean_eps: 0.100000\n","      96654/2000000000: episode: 2588, duration: 4.685s, episode steps:  34, steps per second:   7, episode reward: 38.700, mean reward:  1.138 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 73.581487, mean_q: 22.776703, mean_eps: 0.100000\n","      96694/2000000000: episode: 2589, duration: 5.578s, episode steps:  40, steps per second:   7, episode reward: -53.600, mean reward: -1.340 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.306778, mean_q: 22.862989, mean_eps: 0.100000\n","      96734/2000000000: episode: 2590, duration: 5.126s, episode steps:  40, steps per second:   8, episode reward: -17.000, mean reward: -0.425 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.066788, mean_q: 23.694446, mean_eps: 0.100000\n","      96774/2000000000: episode: 2591, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.843279, mean_q: 22.395852, mean_eps: 0.100000\n","      96808/2000000000: episode: 2592, duration: 4.418s, episode steps:  34, steps per second:   8, episode reward: -64.600, mean reward: -1.900 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 63.771037, mean_q: 23.337584, mean_eps: 0.100000\n","      96848/2000000000: episode: 2593, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: -45.600, mean reward: -1.140 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.965776, mean_q: 22.803201, mean_eps: 0.100000\n","      96888/2000000000: episode: 2594, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: 16.700, mean reward:  0.418 [-20.000, 18.800], mean action: 1.375 [0.000, 2.000],  loss: 73.204116, mean_q: 22.500620, mean_eps: 0.100000\n","      96928/2000000000: episode: 2595, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: -74.100, mean reward: -1.853 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.638234, mean_q: 22.677995, mean_eps: 0.100000\n","      96961/2000000000: episode: 2596, duration: 4.126s, episode steps:  33, steps per second:   8, episode reward: 208.000, mean reward:  6.303 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.113238, mean_q: 22.492308, mean_eps: 0.100000\n","      97001/2000000000: episode: 2597, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: -166.000, mean reward: -4.150 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.757449, mean_q: 22.188409, mean_eps: 0.100000\n","      97041/2000000000: episode: 2598, duration: 4.802s, episode steps:  40, steps per second:   8, episode reward: 63.900, mean reward:  1.597 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.018455, mean_q: 22.901632, mean_eps: 0.100000\n","      97081/2000000000: episode: 2599, duration: 4.608s, episode steps:  40, steps per second:   9, episode reward: 118.900, mean reward:  2.973 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 68.926385, mean_q: 22.772865, mean_eps: 0.100000\n","      97121/2000000000: episode: 2600, duration: 4.886s, episode steps:  40, steps per second:   8, episode reward: 53.200, mean reward:  1.330 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.221914, mean_q: 22.933149, mean_eps: 0.100000\n","      97161/2000000000: episode: 2601, duration: 4.874s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.458141, mean_q: 22.980240, mean_eps: 0.100000\n","      97201/2000000000: episode: 2602, duration: 4.725s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.304607, mean_q: 22.806508, mean_eps: 0.100000\n","      97241/2000000000: episode: 2603, duration: 4.554s, episode steps:  40, steps per second:   9, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.293088, mean_q: 22.830774, mean_eps: 0.100000\n","      97281/2000000000: episode: 2604, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.840856, mean_q: 23.040218, mean_eps: 0.100000\n","      97321/2000000000: episode: 2605, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward: 156.000, mean reward:  3.900 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.393679, mean_q: 22.128265, mean_eps: 0.100000\n","      97361/2000000000: episode: 2606, duration: 4.861s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 68.654007, mean_q: 22.013321, mean_eps: 0.100000\n","      97401/2000000000: episode: 2607, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: -35.700, mean reward: -0.892 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 67.998247, mean_q: 21.674828, mean_eps: 0.100000\n","      97441/2000000000: episode: 2608, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: 126.200, mean reward:  3.155 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 66.073640, mean_q: 22.500469, mean_eps: 0.100000\n","      97481/2000000000: episode: 2609, duration: 4.942s, episode steps:  40, steps per second:   8, episode reward:  6.600, mean reward:  0.165 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.821937, mean_q: 22.075485, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      97516/2000000000: episode: 2610, duration: 4.408s, episode steps:  35, steps per second:   8, episode reward: -26.700, mean reward: -0.763 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 69.376590, mean_q: 22.633767, mean_eps: 0.100000\n","      97556/2000000000: episode: 2611, duration: 5.044s, episode steps:  40, steps per second:   8, episode reward: 35.400, mean reward:  0.885 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 66.119429, mean_q: 22.889682, mean_eps: 0.100000\n","      97594/2000000000: episode: 2612, duration: 4.820s, episode steps:  38, steps per second:   8, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 71.641968, mean_q: 22.161489, mean_eps: 0.100000\n","      97634/2000000000: episode: 2613, duration: 5.152s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.800176, mean_q: 22.413851, mean_eps: 0.100000\n","      97674/2000000000: episode: 2614, duration: 4.601s, episode steps:  40, steps per second:   9, episode reward: -11.200, mean reward: -0.280 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.204931, mean_q: 22.599706, mean_eps: 0.100000\n","      97714/2000000000: episode: 2615, duration: 4.600s, episode steps:  40, steps per second:   9, episode reward: -83.300, mean reward: -2.082 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 81.541490, mean_q: 22.515321, mean_eps: 0.100000\n","      97754/2000000000: episode: 2616, duration: 4.776s, episode steps:  40, steps per second:   8, episode reward: 133.800, mean reward:  3.345 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.549455, mean_q: 21.760124, mean_eps: 0.100000\n","      97794/2000000000: episode: 2617, duration: 4.609s, episode steps:  40, steps per second:   9, episode reward: -70.200, mean reward: -1.755 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.436924, mean_q: 22.757101, mean_eps: 0.100000\n","      97833/2000000000: episode: 2618, duration: 4.584s, episode steps:  39, steps per second:   9, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 69.777166, mean_q: 22.675909, mean_eps: 0.100000\n","      97871/2000000000: episode: 2619, duration: 4.670s, episode steps:  38, steps per second:   8, episode reward: 18.400, mean reward:  0.484 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 73.080208, mean_q: 22.223622, mean_eps: 0.100000\n","      97911/2000000000: episode: 2620, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: -32.100, mean reward: -0.802 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.747366, mean_q: 22.372193, mean_eps: 0.100000\n","      97950/2000000000: episode: 2621, duration: 4.734s, episode steps:  39, steps per second:   8, episode reward: 112.400, mean reward:  2.882 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 72.796372, mean_q: 22.855140, mean_eps: 0.100000\n","      97990/2000000000: episode: 2622, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: -43.000, mean reward: -1.075 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 62.980544, mean_q: 22.613073, mean_eps: 0.100000\n","      98019/2000000000: episode: 2623, duration: 3.448s, episode steps:  29, steps per second:   8, episode reward: 158.000, mean reward:  5.448 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 63.461408, mean_q: 22.545185, mean_eps: 0.100000\n","      98059/2000000000: episode: 2624, duration: 4.557s, episode steps:  40, steps per second:   9, episode reward: -62.000, mean reward: -1.550 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 65.491056, mean_q: 22.519453, mean_eps: 0.100000\n","      98094/2000000000: episode: 2625, duration: 4.074s, episode steps:  35, steps per second:   9, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.777399, mean_q: 22.807491, mean_eps: 0.100000\n","      98131/2000000000: episode: 2626, duration: 4.491s, episode steps:  37, steps per second:   8, episode reward: -6.700, mean reward: -0.181 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 73.629438, mean_q: 23.064181, mean_eps: 0.100000\n","      98171/2000000000: episode: 2627, duration: 4.713s, episode steps:  40, steps per second:   8, episode reward: -62.300, mean reward: -1.557 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.328218, mean_q: 22.289499, mean_eps: 0.100000\n","      98211/2000000000: episode: 2628, duration: 4.609s, episode steps:  40, steps per second:   9, episode reward: -77.800, mean reward: -1.945 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.500766, mean_q: 23.093031, mean_eps: 0.100000\n","      98246/2000000000: episode: 2629, duration: 4.193s, episode steps:  35, steps per second:   8, episode reward: 33.100, mean reward:  0.946 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 68.578533, mean_q: 22.790940, mean_eps: 0.100000\n","      98285/2000000000: episode: 2630, duration: 5.343s, episode steps:  39, steps per second:   7, episode reward: 70.300, mean reward:  1.803 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 68.983418, mean_q: 23.385580, mean_eps: 0.100000\n","      98325/2000000000: episode: 2631, duration: 5.182s, episode steps:  40, steps per second:   8, episode reward: 151.300, mean reward:  3.782 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.023106, mean_q: 23.013410, mean_eps: 0.100000\n","      98365/2000000000: episode: 2632, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward: 47.200, mean reward:  1.180 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.913826, mean_q: 22.598309, mean_eps: 0.100000\n","      98400/2000000000: episode: 2633, duration: 4.283s, episode steps:  35, steps per second:   8, episode reward: -172.000, mean reward: -4.914 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 69.581303, mean_q: 23.258203, mean_eps: 0.100000\n","      98435/2000000000: episode: 2634, duration: 4.081s, episode steps:  35, steps per second:   9, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 65.052746, mean_q: 22.311186, mean_eps: 0.100000\n","      98475/2000000000: episode: 2635, duration: 4.591s, episode steps:  40, steps per second:   9, episode reward: 155.700, mean reward:  3.893 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.009498, mean_q: 23.135474, mean_eps: 0.100000\n","      98515/2000000000: episode: 2636, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: -26.400, mean reward: -0.660 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.345539, mean_q: 22.927193, mean_eps: 0.100000\n","      98555/2000000000: episode: 2637, duration: 4.909s, episode steps:  40, steps per second:   8, episode reward: -24.700, mean reward: -0.617 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.869080, mean_q: 22.708926, mean_eps: 0.100000\n","      98589/2000000000: episode: 2638, duration: 4.074s, episode steps:  34, steps per second:   8, episode reward: 182.200, mean reward:  5.359 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 67.903296, mean_q: 22.719775, mean_eps: 0.100000\n","      98629/2000000000: episode: 2639, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: 155.900, mean reward:  3.898 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.530571, mean_q: 22.182557, mean_eps: 0.100000\n","      98665/2000000000: episode: 2640, duration: 4.342s, episode steps:  36, steps per second:   8, episode reward: 46.500, mean reward:  1.292 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 66.497870, mean_q: 23.002232, mean_eps: 0.100000\n","      98703/2000000000: episode: 2641, duration: 4.672s, episode steps:  38, steps per second:   8, episode reward: 65.600, mean reward:  1.726 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 73.137908, mean_q: 22.564942, mean_eps: 0.100000\n","      98743/2000000000: episode: 2642, duration: 4.800s, episode steps:  40, steps per second:   8, episode reward:  3.600, mean reward:  0.090 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.217377, mean_q: 22.986347, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["      98781/2000000000: episode: 2643, duration: 4.505s, episode steps:  38, steps per second:   8, episode reward: -31.400, mean reward: -0.826 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 66.876909, mean_q: 22.767411, mean_eps: 0.100000\n","      98821/2000000000: episode: 2644, duration: 5.103s, episode steps:  40, steps per second:   8, episode reward: 137.700, mean reward:  3.442 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.668297, mean_q: 22.280464, mean_eps: 0.100000\n","      98850/2000000000: episode: 2645, duration: 3.490s, episode steps:  29, steps per second:   8, episode reward: 23.300, mean reward:  0.803 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 71.555321, mean_q: 22.593528, mean_eps: 0.100000\n","      98884/2000000000: episode: 2646, duration: 3.861s, episode steps:  34, steps per second:   9, episode reward: -101.500, mean reward: -2.985 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 71.040452, mean_q: 22.892620, mean_eps: 0.100000\n","      98924/2000000000: episode: 2647, duration: 4.698s, episode steps:  40, steps per second:   9, episode reward: -53.500, mean reward: -1.337 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.510537, mean_q: 22.239659, mean_eps: 0.100000\n","      98959/2000000000: episode: 2648, duration: 4.102s, episode steps:  35, steps per second:   9, episode reward: 28.900, mean reward:  0.826 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 66.288811, mean_q: 22.274773, mean_eps: 0.100000\n","      98991/2000000000: episode: 2649, duration: 3.863s, episode steps:  32, steps per second:   8, episode reward: 81.200, mean reward:  2.537 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 66.717267, mean_q: 22.185760, mean_eps: 0.100000\n","      99031/2000000000: episode: 2650, duration: 4.979s, episode steps:  40, steps per second:   8, episode reward: 75.900, mean reward:  1.897 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 65.681405, mean_q: 22.420864, mean_eps: 0.100000\n","      99063/2000000000: episode: 2651, duration: 3.960s, episode steps:  32, steps per second:   8, episode reward: 75.700, mean reward:  2.366 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 63.842227, mean_q: 22.488809, mean_eps: 0.100000\n","      99103/2000000000: episode: 2652, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward: -23.500, mean reward: -0.587 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.194302, mean_q: 22.908986, mean_eps: 0.100000\n","      99143/2000000000: episode: 2653, duration: 4.830s, episode steps:  40, steps per second:   8, episode reward: 10.200, mean reward:  0.255 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.181424, mean_q: 22.584270, mean_eps: 0.100000\n","      99183/2000000000: episode: 2654, duration: 4.792s, episode steps:  40, steps per second:   8, episode reward: -154.500, mean reward: -3.863 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.698554, mean_q: 22.532505, mean_eps: 0.100000\n","      99221/2000000000: episode: 2655, duration: 4.514s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 68.666167, mean_q: 23.404440, mean_eps: 0.100000\n","      99261/2000000000: episode: 2656, duration: 4.801s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.121339, mean_q: 22.974422, mean_eps: 0.100000\n","      99301/2000000000: episode: 2657, duration: 4.928s, episode steps:  40, steps per second:   8, episode reward: 140.500, mean reward:  3.513 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.415954, mean_q: 22.323934, mean_eps: 0.100000\n","      99341/2000000000: episode: 2658, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.659619, mean_q: 22.640078, mean_eps: 0.100000\n","      99379/2000000000: episode: 2659, duration: 4.805s, episode steps:  38, steps per second:   8, episode reward: 49.800, mean reward:  1.311 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 71.693863, mean_q: 22.804061, mean_eps: 0.100000\n","      99415/2000000000: episode: 2660, duration: 4.741s, episode steps:  36, steps per second:   8, episode reward: 15.400, mean reward:  0.428 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 63.695876, mean_q: 22.567599, mean_eps: 0.100000\n","      99450/2000000000: episode: 2661, duration: 4.686s, episode steps:  35, steps per second:   7, episode reward: -36.600, mean reward: -1.046 [-20.000, 18.400], mean action: 1.114 [0.000, 2.000],  loss: 69.720206, mean_q: 23.430820, mean_eps: 0.100000\n","      99484/2000000000: episode: 2662, duration: 4.273s, episode steps:  34, steps per second:   8, episode reward: -35.800, mean reward: -1.053 [-20.000, 18.000], mean action: 0.941 [0.000, 2.000],  loss: 70.733811, mean_q: 22.713990, mean_eps: 0.100000\n","      99524/2000000000: episode: 2663, duration: 5.076s, episode steps:  40, steps per second:   8, episode reward:  7.800, mean reward:  0.195 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.212937, mean_q: 21.945435, mean_eps: 0.100000\n","      99551/2000000000: episode: 2664, duration: 3.455s, episode steps:  27, steps per second:   8, episode reward: 47.800, mean reward:  1.770 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 75.886466, mean_q: 23.313932, mean_eps: 0.100000\n","      99590/2000000000: episode: 2665, duration: 5.140s, episode steps:  39, steps per second:   8, episode reward: -2.000, mean reward: -0.051 [-20.000, 18.000], mean action: 1.410 [0.000, 2.000],  loss: 76.647066, mean_q: 22.895404, mean_eps: 0.100000\n","      99625/2000000000: episode: 2666, duration: 4.838s, episode steps:  35, steps per second:   7, episode reward: 142.700, mean reward:  4.077 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 67.260434, mean_q: 22.609079, mean_eps: 0.100000\n","      99665/2000000000: episode: 2667, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: 129.400, mean reward:  3.235 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.223111, mean_q: 22.740893, mean_eps: 0.100000\n","      99705/2000000000: episode: 2668, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.604477, mean_q: 23.298370, mean_eps: 0.100000\n","      99745/2000000000: episode: 2669, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward: 21.400, mean reward:  0.535 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.099829, mean_q: 22.079471, mean_eps: 0.100000\n","      99785/2000000000: episode: 2670, duration: 5.143s, episode steps:  40, steps per second:   8, episode reward: -248.000, mean reward: -6.200 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.488132, mean_q: 21.985364, mean_eps: 0.100000\n","      99824/2000000000: episode: 2671, duration: 5.192s, episode steps:  39, steps per second:   8, episode reward: 12.900, mean reward:  0.331 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 64.783724, mean_q: 22.310421, mean_eps: 0.100000\n","      99864/2000000000: episode: 2672, duration: 5.802s, episode steps:  40, steps per second:   7, episode reward: -2.300, mean reward: -0.057 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.076585, mean_q: 21.912819, mean_eps: 0.100000\n","      99904/2000000000: episode: 2673, duration: 5.319s, episode steps:  40, steps per second:   8, episode reward: -55.300, mean reward: -1.382 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.717269, mean_q: 22.672241, mean_eps: 0.100000\n","      99944/2000000000: episode: 2674, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: -24.900, mean reward: -0.623 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.136012, mean_q: 22.429246, mean_eps: 0.100000\n","      99984/2000000000: episode: 2675, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: -9.300, mean reward: -0.233 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.184913, mean_q: 22.632136, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     100018/2000000000: episode: 2676, duration: 4.261s, episode steps:  34, steps per second:   8, episode reward: 48.600, mean reward:  1.429 [-20.000, 18.600], mean action: 1.029 [0.000, 2.000],  loss: 77.769838, mean_q: 22.626342, mean_eps: 0.100000\n","     100058/2000000000: episode: 2677, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: 85.400, mean reward:  2.135 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.585680, mean_q: 23.985103, mean_eps: 0.100000\n","     100097/2000000000: episode: 2678, duration: 4.716s, episode steps:  39, steps per second:   8, episode reward: 29.400, mean reward:  0.754 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 74.440414, mean_q: 23.427716, mean_eps: 0.100000\n","     100135/2000000000: episode: 2679, duration: 5.030s, episode steps:  38, steps per second:   8, episode reward:  7.800, mean reward:  0.205 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 74.238757, mean_q: 23.194441, mean_eps: 0.100000\n","     100175/2000000000: episode: 2680, duration: 4.889s, episode steps:  40, steps per second:   8, episode reward: 49.500, mean reward:  1.238 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 76.895933, mean_q: 24.012579, mean_eps: 0.100000\n","     100215/2000000000: episode: 2681, duration: 4.912s, episode steps:  40, steps per second:   8, episode reward: 15.600, mean reward:  0.390 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.065589, mean_q: 23.424439, mean_eps: 0.100000\n","     100255/2000000000: episode: 2682, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: 34.700, mean reward:  0.868 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.169152, mean_q: 23.642752, mean_eps: 0.100000\n","     100291/2000000000: episode: 2683, duration: 4.278s, episode steps:  36, steps per second:   8, episode reward: -40.200, mean reward: -1.117 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 69.942701, mean_q: 23.421734, mean_eps: 0.100000\n","     100331/2000000000: episode: 2684, duration: 4.771s, episode steps:  40, steps per second:   8, episode reward: 37.000, mean reward:  0.925 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.783810, mean_q: 23.096480, mean_eps: 0.100000\n","     100371/2000000000: episode: 2685, duration: 4.813s, episode steps:  40, steps per second:   8, episode reward: 67.900, mean reward:  1.697 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.288564, mean_q: 23.677888, mean_eps: 0.100000\n","     100411/2000000000: episode: 2686, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward: -70.200, mean reward: -1.755 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.640910, mean_q: 23.732554, mean_eps: 0.100000\n","     100451/2000000000: episode: 2687, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward:  3.700, mean reward:  0.093 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.143967, mean_q: 23.014823, mean_eps: 0.100000\n","     100491/2000000000: episode: 2688, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: -82.700, mean reward: -2.068 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.124822, mean_q: 23.451064, mean_eps: 0.100000\n","     100526/2000000000: episode: 2689, duration: 4.603s, episode steps:  35, steps per second:   8, episode reward: 18.100, mean reward:  0.517 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.083884, mean_q: 24.485662, mean_eps: 0.100000\n","     100566/2000000000: episode: 2690, duration: 5.177s, episode steps:  40, steps per second:   8, episode reward: 47.400, mean reward:  1.185 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 72.384124, mean_q: 23.852479, mean_eps: 0.100000\n","     100606/2000000000: episode: 2691, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: -24.500, mean reward: -0.612 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.628903, mean_q: 23.554609, mean_eps: 0.100000\n","     100646/2000000000: episode: 2692, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: 36.900, mean reward:  0.923 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.858494, mean_q: 24.104078, mean_eps: 0.100000\n","     100685/2000000000: episode: 2693, duration: 4.956s, episode steps:  39, steps per second:   8, episode reward: 48.000, mean reward:  1.231 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 80.640001, mean_q: 23.837542, mean_eps: 0.100000\n","     100725/2000000000: episode: 2694, duration: 5.564s, episode steps:  40, steps per second:   7, episode reward: 88.300, mean reward:  2.207 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.464676, mean_q: 23.242088, mean_eps: 0.100000\n","     100755/2000000000: episode: 2695, duration: 3.753s, episode steps:  30, steps per second:   8, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 70.381343, mean_q: 24.131096, mean_eps: 0.100000\n","     100795/2000000000: episode: 2696, duration: 5.010s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 68.501616, mean_q: 23.284032, mean_eps: 0.100000\n","     100835/2000000000: episode: 2697, duration: 5.018s, episode steps:  40, steps per second:   8, episode reward: 151.400, mean reward:  3.785 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.613760, mean_q: 23.503462, mean_eps: 0.100000\n","     100867/2000000000: episode: 2698, duration: 4.260s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 69.883310, mean_q: 23.906122, mean_eps: 0.100000\n","     100907/2000000000: episode: 2699, duration: 4.970s, episode steps:  40, steps per second:   8, episode reward: 96.500, mean reward:  2.412 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.123366, mean_q: 24.028161, mean_eps: 0.100000\n","     100947/2000000000: episode: 2700, duration: 5.460s, episode steps:  40, steps per second:   7, episode reward:  6.000, mean reward:  0.150 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 67.023322, mean_q: 23.464437, mean_eps: 0.100000\n","     100987/2000000000: episode: 2701, duration: 5.255s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.916508, mean_q: 23.142486, mean_eps: 0.100000\n","     101018/2000000000: episode: 2702, duration: 4.099s, episode steps:  31, steps per second:   8, episode reward: -70.100, mean reward: -2.261 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 77.730345, mean_q: 23.407004, mean_eps: 0.100000\n","     101058/2000000000: episode: 2703, duration: 5.257s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 64.832820, mean_q: 23.200492, mean_eps: 0.100000\n","     101095/2000000000: episode: 2704, duration: 4.589s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 67.864533, mean_q: 23.310816, mean_eps: 0.100000\n","     101135/2000000000: episode: 2705, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.293004, mean_q: 23.543111, mean_eps: 0.100000\n","     101175/2000000000: episode: 2706, duration: 5.102s, episode steps:  40, steps per second:   8, episode reward: 97.800, mean reward:  2.445 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.786056, mean_q: 23.415229, mean_eps: 0.100000\n","     101204/2000000000: episode: 2707, duration: 3.565s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 71.969276, mean_q: 23.596837, mean_eps: 0.100000\n","     101244/2000000000: episode: 2708, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: -87.400, mean reward: -2.185 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.428708, mean_q: 23.570987, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     101284/2000000000: episode: 2709, duration: 4.814s, episode steps:  40, steps per second:   8, episode reward: -7.800, mean reward: -0.195 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.012555, mean_q: 23.859396, mean_eps: 0.100000\n","     101324/2000000000: episode: 2710, duration: 5.044s, episode steps:  40, steps per second:   8, episode reward: -108.000, mean reward: -2.700 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.451758, mean_q: 23.023700, mean_eps: 0.100000\n","     101359/2000000000: episode: 2711, duration: 4.293s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 68.510346, mean_q: 23.528203, mean_eps: 0.100000\n","     101397/2000000000: episode: 2712, duration: 4.654s, episode steps:  38, steps per second:   8, episode reward: -134.000, mean reward: -3.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 71.539903, mean_q: 23.464967, mean_eps: 0.100000\n","     101437/2000000000: episode: 2713, duration: 5.070s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.207816, mean_q: 23.818443, mean_eps: 0.100000\n","     101469/2000000000: episode: 2714, duration: 3.994s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 66.802241, mean_q: 23.271856, mean_eps: 0.100000\n","     101499/2000000000: episode: 2715, duration: 4.118s, episode steps:  30, steps per second:   7, episode reward: -96.000, mean reward: -3.200 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 67.655266, mean_q: 24.355670, mean_eps: 0.100000\n","     101536/2000000000: episode: 2716, duration: 5.377s, episode steps:  37, steps per second:   7, episode reward: -20.200, mean reward: -0.546 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 73.705573, mean_q: 24.300088, mean_eps: 0.100000\n","     101576/2000000000: episode: 2717, duration: 5.088s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.370159, mean_q: 23.537689, mean_eps: 0.100000\n","     101616/2000000000: episode: 2718, duration: 5.309s, episode steps:  40, steps per second:   8, episode reward: 106.300, mean reward:  2.657 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.841764, mean_q: 23.757130, mean_eps: 0.100000\n","     101656/2000000000: episode: 2719, duration: 5.204s, episode steps:  40, steps per second:   8, episode reward: 139.000, mean reward:  3.475 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.843679, mean_q: 24.089288, mean_eps: 0.100000\n","     101696/2000000000: episode: 2720, duration: 5.121s, episode steps:  40, steps per second:   8, episode reward: -213.000, mean reward: -5.325 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.409805, mean_q: 23.404766, mean_eps: 0.100000\n","     101736/2000000000: episode: 2721, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: 44.400, mean reward:  1.110 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.818534, mean_q: 22.770414, mean_eps: 0.100000\n","     101776/2000000000: episode: 2722, duration: 5.348s, episode steps:  40, steps per second:   7, episode reward: -110.000, mean reward: -2.750 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.750503, mean_q: 23.980017, mean_eps: 0.100000\n","     101816/2000000000: episode: 2723, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.538305, mean_q: 23.369344, mean_eps: 0.100000\n","     101856/2000000000: episode: 2724, duration: 4.919s, episode steps:  40, steps per second:   8, episode reward: 137.100, mean reward:  3.428 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.842494, mean_q: 23.943426, mean_eps: 0.100000\n","     101896/2000000000: episode: 2725, duration: 5.146s, episode steps:  40, steps per second:   8, episode reward: -28.400, mean reward: -0.710 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.611668, mean_q: 24.387775, mean_eps: 0.100000\n","     101936/2000000000: episode: 2726, duration: 5.113s, episode steps:  40, steps per second:   8, episode reward: -100.000, mean reward: -2.500 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.332077, mean_q: 23.474534, mean_eps: 0.100000\n","     101976/2000000000: episode: 2727, duration: 5.127s, episode steps:  40, steps per second:   8, episode reward:  1.200, mean reward:  0.030 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.283340, mean_q: 23.539454, mean_eps: 0.100000\n","     102008/2000000000: episode: 2728, duration: 4.087s, episode steps:  32, steps per second:   8, episode reward: 32.500, mean reward:  1.016 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 61.671433, mean_q: 24.971548, mean_eps: 0.100000\n","     102041/2000000000: episode: 2729, duration: 4.254s, episode steps:  33, steps per second:   8, episode reward: 103.200, mean reward:  3.127 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 72.265587, mean_q: 23.597601, mean_eps: 0.100000\n","     102078/2000000000: episode: 2730, duration: 4.670s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 68.652189, mean_q: 23.523948, mean_eps: 0.100000\n","     102118/2000000000: episode: 2731, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: -10.700, mean reward: -0.268 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.682103, mean_q: 23.885556, mean_eps: 0.100000\n","     102145/2000000000: episode: 2732, duration: 3.605s, episode steps:  27, steps per second:   7, episode reward: -11.100, mean reward: -0.411 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 71.176015, mean_q: 23.974111, mean_eps: 0.100000\n","     102185/2000000000: episode: 2733, duration: 5.432s, episode steps:  40, steps per second:   7, episode reward: 92.800, mean reward:  2.320 [-20.000, 18.700], mean action: 1.400 [0.000, 2.000],  loss: 75.877620, mean_q: 23.543624, mean_eps: 0.100000\n","     102225/2000000000: episode: 2734, duration: 5.644s, episode steps:  40, steps per second:   7, episode reward: -51.500, mean reward: -1.287 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 70.301630, mean_q: 23.709996, mean_eps: 0.100000\n","     102265/2000000000: episode: 2735, duration: 5.381s, episode steps:  40, steps per second:   7, episode reward:  0.300, mean reward:  0.007 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.731728, mean_q: 23.761630, mean_eps: 0.100000\n","     102299/2000000000: episode: 2736, duration: 4.582s, episode steps:  34, steps per second:   7, episode reward: 26.100, mean reward:  0.768 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 72.084399, mean_q: 24.770918, mean_eps: 0.100000\n","     102339/2000000000: episode: 2737, duration: 5.524s, episode steps:  40, steps per second:   7, episode reward: 17.300, mean reward:  0.432 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.197082, mean_q: 24.085036, mean_eps: 0.100000\n","     102375/2000000000: episode: 2738, duration: 4.921s, episode steps:  36, steps per second:   7, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 66.442559, mean_q: 24.169428, mean_eps: 0.100000\n","     102408/2000000000: episode: 2739, duration: 4.570s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 68.827856, mean_q: 23.662298, mean_eps: 0.100000\n","     102448/2000000000: episode: 2740, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: 12.200, mean reward:  0.305 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.589153, mean_q: 23.342717, mean_eps: 0.100000\n","     102475/2000000000: episode: 2741, duration: 3.463s, episode steps:  27, steps per second:   8, episode reward: -32.600, mean reward: -1.207 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 66.124755, mean_q: 23.525325, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     102515/2000000000: episode: 2742, duration: 5.081s, episode steps:  40, steps per second:   8, episode reward: 128.800, mean reward:  3.220 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.345184, mean_q: 23.419982, mean_eps: 0.100000\n","     102555/2000000000: episode: 2743, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward: -13.600, mean reward: -0.340 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.928414, mean_q: 23.653324, mean_eps: 0.100000\n","     102591/2000000000: episode: 2744, duration: 4.658s, episode steps:  36, steps per second:   8, episode reward: 92.300, mean reward:  2.564 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 70.590813, mean_q: 24.056661, mean_eps: 0.100000\n","     102631/2000000000: episode: 2745, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: -84.100, mean reward: -2.103 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.139637, mean_q: 22.945899, mean_eps: 0.100000\n","     102671/2000000000: episode: 2746, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: -45.400, mean reward: -1.135 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.051058, mean_q: 23.456791, mean_eps: 0.100000\n","     102711/2000000000: episode: 2747, duration: 5.441s, episode steps:  40, steps per second:   7, episode reward: 47.200, mean reward:  1.180 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.363713, mean_q: 23.753044, mean_eps: 0.100000\n","     102744/2000000000: episode: 2748, duration: 4.465s, episode steps:  33, steps per second:   7, episode reward: -3.800, mean reward: -0.115 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 69.870263, mean_q: 23.648701, mean_eps: 0.100000\n","     102784/2000000000: episode: 2749, duration: 5.556s, episode steps:  40, steps per second:   7, episode reward: 26.800, mean reward:  0.670 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.665945, mean_q: 23.000807, mean_eps: 0.100000\n","     102824/2000000000: episode: 2750, duration: 5.667s, episode steps:  40, steps per second:   7, episode reward: 38.600, mean reward:  0.965 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.960087, mean_q: 23.571784, mean_eps: 0.100000\n","     102862/2000000000: episode: 2751, duration: 5.205s, episode steps:  38, steps per second:   7, episode reward: -58.300, mean reward: -1.534 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 70.020124, mean_q: 23.750537, mean_eps: 0.100000\n","     102902/2000000000: episode: 2752, duration: 5.588s, episode steps:  40, steps per second:   7, episode reward: 40.900, mean reward:  1.022 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 66.137107, mean_q: 23.650326, mean_eps: 0.100000\n","     102942/2000000000: episode: 2753, duration: 5.210s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.485010, mean_q: 23.951297, mean_eps: 0.100000\n","     102978/2000000000: episode: 2754, duration: 4.796s, episode steps:  36, steps per second:   8, episode reward: 110.100, mean reward:  3.058 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 70.808732, mean_q: 22.934392, mean_eps: 0.100000\n","     103016/2000000000: episode: 2755, duration: 5.355s, episode steps:  38, steps per second:   7, episode reward: -121.500, mean reward: -3.197 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 69.560159, mean_q: 23.540179, mean_eps: 0.100000\n","     103056/2000000000: episode: 2756, duration: 5.495s, episode steps:  40, steps per second:   7, episode reward: 55.700, mean reward:  1.393 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.728273, mean_q: 23.746013, mean_eps: 0.100000\n","     103090/2000000000: episode: 2757, duration: 4.115s, episode steps:  34, steps per second:   8, episode reward: 82.300, mean reward:  2.421 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 70.316952, mean_q: 23.302157, mean_eps: 0.100000\n","     103129/2000000000: episode: 2758, duration: 5.089s, episode steps:  39, steps per second:   8, episode reward: 33.100, mean reward:  0.849 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 69.585567, mean_q: 22.922326, mean_eps: 0.100000\n","     103169/2000000000: episode: 2759, duration: 5.265s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.441131, mean_q: 24.307162, mean_eps: 0.100000\n","     103198/2000000000: episode: 2760, duration: 3.902s, episode steps:  29, steps per second:   7, episode reward: 50.700, mean reward:  1.748 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 63.101750, mean_q: 23.556814, mean_eps: 0.100000\n","     103236/2000000000: episode: 2761, duration: 5.243s, episode steps:  38, steps per second:   7, episode reward: -76.200, mean reward: -2.005 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 67.134796, mean_q: 23.506852, mean_eps: 0.100000\n","     103271/2000000000: episode: 2762, duration: 4.551s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 69.219048, mean_q: 23.196209, mean_eps: 0.100000\n","     103311/2000000000: episode: 2763, duration: 5.538s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.472971, mean_q: 22.893315, mean_eps: 0.100000\n","     103347/2000000000: episode: 2764, duration: 5.363s, episode steps:  36, steps per second:   7, episode reward: -45.000, mean reward: -1.250 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 73.300400, mean_q: 24.194190, mean_eps: 0.100000\n","     103379/2000000000: episode: 2765, duration: 4.535s, episode steps:  32, steps per second:   7, episode reward: 136.600, mean reward:  4.269 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 69.740071, mean_q: 24.650477, mean_eps: 0.100000\n","     103418/2000000000: episode: 2766, duration: 5.191s, episode steps:  39, steps per second:   8, episode reward: 84.000, mean reward:  2.154 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 68.760867, mean_q: 24.066555, mean_eps: 0.100000\n","     103456/2000000000: episode: 2767, duration: 5.055s, episode steps:  38, steps per second:   8, episode reward: -5.600, mean reward: -0.147 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 75.341221, mean_q: 24.122962, mean_eps: 0.100000\n","     103496/2000000000: episode: 2768, duration: 5.374s, episode steps:  40, steps per second:   7, episode reward: 92.200, mean reward:  2.305 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.507029, mean_q: 23.553024, mean_eps: 0.100000\n","     103536/2000000000: episode: 2769, duration: 5.316s, episode steps:  40, steps per second:   8, episode reward: 14.600, mean reward:  0.365 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.402828, mean_q: 23.944756, mean_eps: 0.100000\n","     103576/2000000000: episode: 2770, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: -25.100, mean reward: -0.628 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.407595, mean_q: 23.610334, mean_eps: 0.100000\n","     103616/2000000000: episode: 2771, duration: 5.433s, episode steps:  40, steps per second:   7, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.643199, mean_q: 23.515449, mean_eps: 0.100000\n","     103656/2000000000: episode: 2772, duration: 5.288s, episode steps:  40, steps per second:   8, episode reward: 177.200, mean reward:  4.430 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.898551, mean_q: 23.162271, mean_eps: 0.100000\n","     103690/2000000000: episode: 2773, duration: 4.991s, episode steps:  34, steps per second:   7, episode reward: 11.400, mean reward:  0.335 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 63.760229, mean_q: 23.764020, mean_eps: 0.100000\n","     103729/2000000000: episode: 2774, duration: 6.037s, episode steps:  39, steps per second:   6, episode reward: 21.600, mean reward:  0.554 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 69.183129, mean_q: 23.408828, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     103767/2000000000: episode: 2775, duration: 5.580s, episode steps:  38, steps per second:   7, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.079 [0.000, 2.000],  loss: 68.218743, mean_q: 23.749970, mean_eps: 0.100000\n","     103807/2000000000: episode: 2776, duration: 5.475s, episode steps:  40, steps per second:   7, episode reward: 24.100, mean reward:  0.603 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.168619, mean_q: 24.382325, mean_eps: 0.100000\n","     103847/2000000000: episode: 2777, duration: 5.407s, episode steps:  40, steps per second:   7, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.969942, mean_q: 23.681238, mean_eps: 0.100000\n","     103887/2000000000: episode: 2778, duration: 5.391s, episode steps:  40, steps per second:   7, episode reward: -89.500, mean reward: -2.238 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.746505, mean_q: 23.296192, mean_eps: 0.100000\n","     103922/2000000000: episode: 2779, duration: 4.719s, episode steps:  35, steps per second:   7, episode reward: -39.800, mean reward: -1.137 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 74.832681, mean_q: 23.417091, mean_eps: 0.100000\n","     103962/2000000000: episode: 2780, duration: 5.041s, episode steps:  40, steps per second:   8, episode reward: -142.500, mean reward: -3.563 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.091367, mean_q: 24.018050, mean_eps: 0.100000\n","     103998/2000000000: episode: 2781, duration: 5.021s, episode steps:  36, steps per second:   7, episode reward: -43.500, mean reward: -1.208 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 67.775910, mean_q: 23.870238, mean_eps: 0.100000\n","     104038/2000000000: episode: 2782, duration: 5.326s, episode steps:  40, steps per second:   8, episode reward: 105.000, mean reward:  2.625 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.191126, mean_q: 23.541224, mean_eps: 0.100000\n","     104078/2000000000: episode: 2783, duration: 5.534s, episode steps:  40, steps per second:   7, episode reward: 53.000, mean reward:  1.325 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.652905, mean_q: 23.971318, mean_eps: 0.100000\n","     104112/2000000000: episode: 2784, duration: 4.586s, episode steps:  34, steps per second:   7, episode reward: 27.000, mean reward:  0.794 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 71.119553, mean_q: 24.698895, mean_eps: 0.100000\n","     104152/2000000000: episode: 2785, duration: 5.448s, episode steps:  40, steps per second:   7, episode reward: 64.800, mean reward:  1.620 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.405633, mean_q: 23.249973, mean_eps: 0.100000\n","     104192/2000000000: episode: 2786, duration: 5.493s, episode steps:  40, steps per second:   7, episode reward: 70.900, mean reward:  1.772 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.659535, mean_q: 23.591254, mean_eps: 0.100000\n","     104229/2000000000: episode: 2787, duration: 5.188s, episode steps:  37, steps per second:   7, episode reward: -30.400, mean reward: -0.822 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.037650, mean_q: 23.807380, mean_eps: 0.100000\n","     104267/2000000000: episode: 2788, duration: 5.102s, episode steps:  38, steps per second:   7, episode reward: -76.500, mean reward: -2.013 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 66.709192, mean_q: 23.898836, mean_eps: 0.100000\n","     104307/2000000000: episode: 2789, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: 42.300, mean reward:  1.057 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.430101, mean_q: 23.331862, mean_eps: 0.100000\n","     104342/2000000000: episode: 2790, duration: 4.690s, episode steps:  35, steps per second:   7, episode reward: -49.100, mean reward: -1.403 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 70.015608, mean_q: 24.098500, mean_eps: 0.100000\n","     104372/2000000000: episode: 2791, duration: 4.165s, episode steps:  30, steps per second:   7, episode reward: 205.900, mean reward:  6.863 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 67.237207, mean_q: 24.067225, mean_eps: 0.100000\n","     104411/2000000000: episode: 2792, duration: 5.339s, episode steps:  39, steps per second:   7, episode reward: 77.500, mean reward:  1.987 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 71.391960, mean_q: 24.074578, mean_eps: 0.100000\n","     104441/2000000000: episode: 2793, duration: 4.228s, episode steps:  30, steps per second:   7, episode reward: 96.400, mean reward:  3.213 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 68.641670, mean_q: 23.757734, mean_eps: 0.100000\n","     104480/2000000000: episode: 2794, duration: 5.229s, episode steps:  39, steps per second:   7, episode reward: 127.900, mean reward:  3.279 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 71.939528, mean_q: 23.334090, mean_eps: 0.100000\n","     104520/2000000000: episode: 2795, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: 93.800, mean reward:  2.345 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 70.375681, mean_q: 22.962080, mean_eps: 0.100000\n","     104560/2000000000: episode: 2796, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: 84.300, mean reward:  2.107 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 74.846838, mean_q: 24.156731, mean_eps: 0.100000\n","     104600/2000000000: episode: 2797, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: 62.200, mean reward:  1.555 [-15.300, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 69.427617, mean_q: 24.002441, mean_eps: 0.100000\n","     104640/2000000000: episode: 2798, duration: 5.523s, episode steps:  40, steps per second:   7, episode reward: 154.100, mean reward:  3.852 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.663850, mean_q: 23.836717, mean_eps: 0.100000\n","     104680/2000000000: episode: 2799, duration: 5.715s, episode steps:  40, steps per second:   7, episode reward: -99.500, mean reward: -2.487 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.025554, mean_q: 23.784764, mean_eps: 0.100000\n","     104720/2000000000: episode: 2800, duration: 5.402s, episode steps:  40, steps per second:   7, episode reward: 27.400, mean reward:  0.685 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.873478, mean_q: 23.631669, mean_eps: 0.100000\n","     104760/2000000000: episode: 2801, duration: 5.647s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 65.911817, mean_q: 23.301705, mean_eps: 0.100000\n","     104800/2000000000: episode: 2802, duration: 5.457s, episode steps:  40, steps per second:   7, episode reward:  5.000, mean reward:  0.125 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.103401, mean_q: 23.709254, mean_eps: 0.100000\n","     104831/2000000000: episode: 2803, duration: 4.208s, episode steps:  31, steps per second:   7, episode reward: 127.100, mean reward:  4.100 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 67.417815, mean_q: 23.872689, mean_eps: 0.100000\n","     104871/2000000000: episode: 2804, duration: 5.229s, episode steps:  40, steps per second:   8, episode reward: 34.100, mean reward:  0.852 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.116409, mean_q: 24.433608, mean_eps: 0.100000\n","     104908/2000000000: episode: 2805, duration: 4.862s, episode steps:  37, steps per second:   8, episode reward: 160.800, mean reward:  4.346 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 69.754155, mean_q: 23.952246, mean_eps: 0.100000\n","     104948/2000000000: episode: 2806, duration: 5.462s, episode steps:  40, steps per second:   7, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.973873, mean_q: 23.038091, mean_eps: 0.100000\n","     104985/2000000000: episode: 2807, duration: 5.037s, episode steps:  37, steps per second:   7, episode reward: -20.300, mean reward: -0.549 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 73.833341, mean_q: 23.710508, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     105017/2000000000: episode: 2808, duration: 4.231s, episode steps:  32, steps per second:   8, episode reward: -180.700, mean reward: -5.647 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 64.592328, mean_q: 24.423564, mean_eps: 0.100000\n","     105057/2000000000: episode: 2809, duration: 5.315s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.458143, mean_q: 24.231485, mean_eps: 0.100000\n","     105090/2000000000: episode: 2810, duration: 4.346s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 74.530125, mean_q: 23.328340, mean_eps: 0.100000\n","     105130/2000000000: episode: 2811, duration: 4.948s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.187189, mean_q: 23.888679, mean_eps: 0.100000\n","     105170/2000000000: episode: 2812, duration: 4.768s, episode steps:  40, steps per second:   8, episode reward: -228.000, mean reward: -5.700 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.456734, mean_q: 23.185967, mean_eps: 0.100000\n","     105203/2000000000: episode: 2813, duration: 3.929s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 67.743917, mean_q: 23.578171, mean_eps: 0.100000\n","     105243/2000000000: episode: 2814, duration: 4.851s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.622433, mean_q: 23.660915, mean_eps: 0.100000\n","     105279/2000000000: episode: 2815, duration: 4.304s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 70.853442, mean_q: 23.507491, mean_eps: 0.100000\n","     105319/2000000000: episode: 2816, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.373488, mean_q: 23.869622, mean_eps: 0.100000\n","     105354/2000000000: episode: 2817, duration: 4.536s, episode steps:  35, steps per second:   8, episode reward: -32.400, mean reward: -0.926 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 67.823801, mean_q: 23.586259, mean_eps: 0.100000\n","     105394/2000000000: episode: 2818, duration: 5.172s, episode steps:  40, steps per second:   8, episode reward: 73.500, mean reward:  1.837 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.345074, mean_q: 23.252247, mean_eps: 0.100000\n","     105434/2000000000: episode: 2819, duration: 4.820s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.024012, mean_q: 23.222989, mean_eps: 0.100000\n","     105473/2000000000: episode: 2820, duration: 4.686s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 69.802936, mean_q: 23.926762, mean_eps: 0.100000\n","     105513/2000000000: episode: 2821, duration: 4.766s, episode steps:  40, steps per second:   8, episode reward: 23.100, mean reward:  0.578 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.140874, mean_q: 24.261950, mean_eps: 0.100000\n","     105552/2000000000: episode: 2822, duration: 4.533s, episode steps:  39, steps per second:   9, episode reward: 46.900, mean reward:  1.203 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 67.837515, mean_q: 22.672900, mean_eps: 0.100000\n","     105582/2000000000: episode: 2823, duration: 3.479s, episode steps:  30, steps per second:   9, episode reward: 12.300, mean reward:  0.410 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 71.047358, mean_q: 23.829670, mean_eps: 0.100000\n","     105609/2000000000: episode: 2824, duration: 3.207s, episode steps:  27, steps per second:   8, episode reward: -102.400, mean reward: -3.793 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 70.535856, mean_q: 22.670222, mean_eps: 0.100000\n","     105649/2000000000: episode: 2825, duration: 4.521s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.195772, mean_q: 23.587743, mean_eps: 0.100000\n","     105689/2000000000: episode: 2826, duration: 4.745s, episode steps:  40, steps per second:   8, episode reward: 129.900, mean reward:  3.248 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.341506, mean_q: 24.023140, mean_eps: 0.100000\n","     105729/2000000000: episode: 2827, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: -58.100, mean reward: -1.452 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.531703, mean_q: 23.647892, mean_eps: 0.100000\n","     105769/2000000000: episode: 2828, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.160229, mean_q: 23.310880, mean_eps: 0.100000\n","     105809/2000000000: episode: 2829, duration: 4.785s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.675841, mean_q: 23.696146, mean_eps: 0.100000\n","     105849/2000000000: episode: 2830, duration: 4.664s, episode steps:  40, steps per second:   9, episode reward: -34.800, mean reward: -0.870 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.624097, mean_q: 23.894147, mean_eps: 0.100000\n","     105889/2000000000: episode: 2831, duration: 4.730s, episode steps:  40, steps per second:   8, episode reward:  3.900, mean reward:  0.097 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.967252, mean_q: 23.857648, mean_eps: 0.100000\n","     105920/2000000000: episode: 2832, duration: 3.828s, episode steps:  31, steps per second:   8, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.150811, mean_q: 23.937404, mean_eps: 0.100000\n","     105954/2000000000: episode: 2833, duration: 4.225s, episode steps:  34, steps per second:   8, episode reward: -20.600, mean reward: -0.606 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 72.758284, mean_q: 23.679606, mean_eps: 0.100000\n","     105985/2000000000: episode: 2834, duration: 3.832s, episode steps:  31, steps per second:   8, episode reward: -9.300, mean reward: -0.300 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 69.546445, mean_q: 22.849667, mean_eps: 0.100000\n","     106019/2000000000: episode: 2835, duration: 4.145s, episode steps:  34, steps per second:   8, episode reward: -121.000, mean reward: -3.559 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 71.350510, mean_q: 23.896611, mean_eps: 0.100000\n","     106056/2000000000: episode: 2836, duration: 4.566s, episode steps:  37, steps per second:   8, episode reward: 96.300, mean reward:  2.603 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 77.332536, mean_q: 23.081195, mean_eps: 0.100000\n","     106092/2000000000: episode: 2837, duration: 4.482s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 67.309948, mean_q: 23.863885, mean_eps: 0.100000\n","     106132/2000000000: episode: 2838, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 47.700, mean reward:  1.192 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.591298, mean_q: 23.964175, mean_eps: 0.100000\n","     106163/2000000000: episode: 2839, duration: 3.803s, episode steps:  31, steps per second:   8, episode reward: -1.700, mean reward: -0.055 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 68.199863, mean_q: 23.989993, mean_eps: 0.100000\n","     106203/2000000000: episode: 2840, duration: 5.112s, episode steps:  40, steps per second:   8, episode reward: 14.500, mean reward:  0.362 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.327517, mean_q: 23.329125, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     106243/2000000000: episode: 2841, duration: 5.188s, episode steps:  40, steps per second:   8, episode reward: -108.300, mean reward: -2.707 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.014634, mean_q: 23.696684, mean_eps: 0.100000\n","     106277/2000000000: episode: 2842, duration: 4.529s, episode steps:  34, steps per second:   8, episode reward: 107.900, mean reward:  3.174 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.376576, mean_q: 23.609964, mean_eps: 0.100000\n","     106317/2000000000: episode: 2843, duration: 5.160s, episode steps:  40, steps per second:   8, episode reward: -34.800, mean reward: -0.870 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.752435, mean_q: 24.179854, mean_eps: 0.100000\n","     106357/2000000000: episode: 2844, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: 132.900, mean reward:  3.322 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.209653, mean_q: 23.467911, mean_eps: 0.100000\n","     106397/2000000000: episode: 2845, duration: 4.992s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 65.885995, mean_q: 23.257094, mean_eps: 0.100000\n","     106437/2000000000: episode: 2846, duration: 4.992s, episode steps:  40, steps per second:   8, episode reward: 210.000, mean reward:  5.250 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 64.877370, mean_q: 24.151020, mean_eps: 0.100000\n","     106477/2000000000: episode: 2847, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward: -54.800, mean reward: -1.370 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 68.947666, mean_q: 23.325299, mean_eps: 0.100000\n","     106517/2000000000: episode: 2848, duration: 4.909s, episode steps:  40, steps per second:   8, episode reward: 155.700, mean reward:  3.893 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 71.208932, mean_q: 23.328725, mean_eps: 0.100000\n","     106556/2000000000: episode: 2849, duration: 4.868s, episode steps:  39, steps per second:   8, episode reward: -167.400, mean reward: -4.292 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 64.793538, mean_q: 23.600939, mean_eps: 0.100000\n","     106596/2000000000: episode: 2850, duration: 5.078s, episode steps:  40, steps per second:   8, episode reward: 49.100, mean reward:  1.228 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 73.098633, mean_q: 23.495140, mean_eps: 0.100000\n","     106636/2000000000: episode: 2851, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward: -72.400, mean reward: -1.810 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.538109, mean_q: 22.746015, mean_eps: 0.100000\n","     106676/2000000000: episode: 2852, duration: 5.094s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.597592, mean_q: 23.960231, mean_eps: 0.100000\n","     106712/2000000000: episode: 2853, duration: 4.651s, episode steps:  36, steps per second:   8, episode reward: -4.200, mean reward: -0.117 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.410877, mean_q: 23.640538, mean_eps: 0.100000\n","     106745/2000000000: episode: 2854, duration: 4.227s, episode steps:  33, steps per second:   8, episode reward: -65.000, mean reward: -1.970 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 76.552502, mean_q: 23.457024, mean_eps: 0.100000\n","     106775/2000000000: episode: 2855, duration: 3.763s, episode steps:  30, steps per second:   8, episode reward:  9.700, mean reward:  0.323 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 66.690705, mean_q: 24.230334, mean_eps: 0.100000\n","     106808/2000000000: episode: 2856, duration: 4.064s, episode steps:  33, steps per second:   8, episode reward: 146.800, mean reward:  4.448 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 70.041875, mean_q: 23.247222, mean_eps: 0.100000\n","     106839/2000000000: episode: 2857, duration: 3.902s, episode steps:  31, steps per second:   8, episode reward: 71.200, mean reward:  2.297 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 71.294394, mean_q: 23.423359, mean_eps: 0.100000\n","     106873/2000000000: episode: 2858, duration: 4.202s, episode steps:  34, steps per second:   8, episode reward: 25.300, mean reward:  0.744 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.126884, mean_q: 23.645057, mean_eps: 0.100000\n","     106905/2000000000: episode: 2859, duration: 3.993s, episode steps:  32, steps per second:   8, episode reward:  6.400, mean reward:  0.200 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.120040, mean_q: 23.343999, mean_eps: 0.100000\n","     106945/2000000000: episode: 2860, duration: 4.899s, episode steps:  40, steps per second:   8, episode reward: 155.300, mean reward:  3.882 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.551285, mean_q: 23.587685, mean_eps: 0.100000\n","     106982/2000000000: episode: 2861, duration: 4.661s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 72.605140, mean_q: 23.255676, mean_eps: 0.100000\n","     107022/2000000000: episode: 2862, duration: 5.198s, episode steps:  40, steps per second:   8, episode reward: 35.600, mean reward:  0.890 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.279379, mean_q: 23.405864, mean_eps: 0.100000\n","     107062/2000000000: episode: 2863, duration: 5.234s, episode steps:  40, steps per second:   8, episode reward: 30.100, mean reward:  0.753 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.843479, mean_q: 24.439979, mean_eps: 0.100000\n","     107102/2000000000: episode: 2864, duration: 4.787s, episode steps:  40, steps per second:   8, episode reward: 15.100, mean reward:  0.377 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.154138, mean_q: 23.896646, mean_eps: 0.100000\n","     107142/2000000000: episode: 2865, duration: 4.933s, episode steps:  40, steps per second:   8, episode reward: 12.900, mean reward:  0.323 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 74.481897, mean_q: 23.770989, mean_eps: 0.100000\n","     107179/2000000000: episode: 2866, duration: 4.598s, episode steps:  37, steps per second:   8, episode reward: -132.900, mean reward: -3.592 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 69.829267, mean_q: 23.300309, mean_eps: 0.100000\n","     107219/2000000000: episode: 2867, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: 101.200, mean reward:  2.530 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.235500, mean_q: 24.189694, mean_eps: 0.100000\n","     107252/2000000000: episode: 2868, duration: 4.025s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 70.778138, mean_q: 24.688570, mean_eps: 0.100000\n","     107292/2000000000: episode: 2869, duration: 5.082s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.848496, mean_q: 23.600036, mean_eps: 0.100000\n","     107320/2000000000: episode: 2870, duration: 3.519s, episode steps:  28, steps per second:   8, episode reward: -112.900, mean reward: -4.032 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 59.496435, mean_q: 23.342372, mean_eps: 0.100000\n","     107360/2000000000: episode: 2871, duration: 5.115s, episode steps:  40, steps per second:   8, episode reward: 73.000, mean reward:  1.825 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 62.812967, mean_q: 23.738265, mean_eps: 0.100000\n","     107400/2000000000: episode: 2872, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: -88.900, mean reward: -2.223 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.587863, mean_q: 23.339968, mean_eps: 0.100000\n","     107434/2000000000: episode: 2873, duration: 4.419s, episode steps:  34, steps per second:   8, episode reward:  7.200, mean reward:  0.212 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 68.422540, mean_q: 24.213491, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     107467/2000000000: episode: 2874, duration: 4.486s, episode steps:  33, steps per second:   7, episode reward: 80.400, mean reward:  2.436 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 65.654195, mean_q: 23.317932, mean_eps: 0.100000\n","     107507/2000000000: episode: 2875, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: -27.000, mean reward: -0.675 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.885847, mean_q: 24.077031, mean_eps: 0.100000\n","     107547/2000000000: episode: 2876, duration: 5.405s, episode steps:  40, steps per second:   7, episode reward: -43.100, mean reward: -1.077 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 63.575490, mean_q: 23.234270, mean_eps: 0.100000\n","     107583/2000000000: episode: 2877, duration: 4.639s, episode steps:  36, steps per second:   8, episode reward: 19.200, mean reward:  0.533 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 73.089234, mean_q: 22.994063, mean_eps: 0.100000\n","     107619/2000000000: episode: 2878, duration: 4.594s, episode steps:  36, steps per second:   8, episode reward: 17.500, mean reward:  0.486 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 64.917486, mean_q: 23.249892, mean_eps: 0.100000\n","     107655/2000000000: episode: 2879, duration: 4.561s, episode steps:  36, steps per second:   8, episode reward: -53.600, mean reward: -1.489 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 65.824600, mean_q: 23.506651, mean_eps: 0.100000\n","     107695/2000000000: episode: 2880, duration: 5.364s, episode steps:  40, steps per second:   7, episode reward: -101.300, mean reward: -2.533 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 62.399711, mean_q: 24.339077, mean_eps: 0.100000\n","     107735/2000000000: episode: 2881, duration: 5.278s, episode steps:  40, steps per second:   8, episode reward: 21.800, mean reward:  0.545 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.030937, mean_q: 23.438180, mean_eps: 0.100000\n","     107775/2000000000: episode: 2882, duration: 5.369s, episode steps:  40, steps per second:   7, episode reward: 50.100, mean reward:  1.253 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.677454, mean_q: 23.145241, mean_eps: 0.100000\n","     107812/2000000000: episode: 2883, duration: 5.156s, episode steps:  37, steps per second:   7, episode reward: 92.400, mean reward:  2.497 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 67.358177, mean_q: 23.835340, mean_eps: 0.100000\n","     107852/2000000000: episode: 2884, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: 55.100, mean reward:  1.377 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.700286, mean_q: 23.846178, mean_eps: 0.100000\n","     107892/2000000000: episode: 2885, duration: 5.380s, episode steps:  40, steps per second:   7, episode reward: -58.700, mean reward: -1.468 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.719926, mean_q: 23.899064, mean_eps: 0.100000\n","     107926/2000000000: episode: 2886, duration: 5.039s, episode steps:  34, steps per second:   7, episode reward: 85.400, mean reward:  2.512 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 69.991755, mean_q: 23.621030, mean_eps: 0.100000\n","     107966/2000000000: episode: 2887, duration: 5.431s, episode steps:  40, steps per second:   7, episode reward: 122.500, mean reward:  3.062 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.118328, mean_q: 23.829053, mean_eps: 0.100000\n","     108005/2000000000: episode: 2888, duration: 5.444s, episode steps:  39, steps per second:   7, episode reward: 90.600, mean reward:  2.323 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 68.993464, mean_q: 23.463356, mean_eps: 0.100000\n","     108045/2000000000: episode: 2889, duration: 5.481s, episode steps:  40, steps per second:   7, episode reward: -59.700, mean reward: -1.493 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.136688, mean_q: 23.481877, mean_eps: 0.100000\n","     108085/2000000000: episode: 2890, duration: 5.098s, episode steps:  40, steps per second:   8, episode reward: 46.400, mean reward:  1.160 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.220165, mean_q: 23.394764, mean_eps: 0.100000\n","     108125/2000000000: episode: 2891, duration: 5.450s, episode steps:  40, steps per second:   7, episode reward: -80.300, mean reward: -2.007 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.498819, mean_q: 24.477782, mean_eps: 0.100000\n","     108164/2000000000: episode: 2892, duration: 5.008s, episode steps:  39, steps per second:   8, episode reward: 77.400, mean reward:  1.985 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 71.159179, mean_q: 23.385887, mean_eps: 0.100000\n","     108193/2000000000: episode: 2893, duration: 3.595s, episode steps:  29, steps per second:   8, episode reward: -20.200, mean reward: -0.697 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 75.261286, mean_q: 23.655837, mean_eps: 0.100000\n","     108228/2000000000: episode: 2894, duration: 4.537s, episode steps:  35, steps per second:   8, episode reward: -40.700, mean reward: -1.163 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 71.519131, mean_q: 23.570787, mean_eps: 0.100000\n","     108268/2000000000: episode: 2895, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: 58.200, mean reward:  1.455 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.625580, mean_q: 23.675421, mean_eps: 0.100000\n","     108308/2000000000: episode: 2896, duration: 5.180s, episode steps:  40, steps per second:   8, episode reward: 25.400, mean reward:  0.635 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.976287, mean_q: 22.965728, mean_eps: 0.100000\n","     108342/2000000000: episode: 2897, duration: 4.349s, episode steps:  34, steps per second:   8, episode reward: 17.900, mean reward:  0.526 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 69.497728, mean_q: 23.524719, mean_eps: 0.100000\n","     108382/2000000000: episode: 2898, duration: 5.344s, episode steps:  40, steps per second:   7, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.340755, mean_q: 23.803414, mean_eps: 0.100000\n","     108409/2000000000: episode: 2899, duration: 3.418s, episode steps:  27, steps per second:   8, episode reward: 33.300, mean reward:  1.233 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 73.819035, mean_q: 24.280881, mean_eps: 0.100000\n","     108449/2000000000: episode: 2900, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 29.000, mean reward:  0.725 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.624238, mean_q: 23.535626, mean_eps: 0.100000\n","     108483/2000000000: episode: 2901, duration: 4.631s, episode steps:  34, steps per second:   7, episode reward: 68.800, mean reward:  2.024 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.333843, mean_q: 23.407921, mean_eps: 0.100000\n","     108523/2000000000: episode: 2902, duration: 5.318s, episode steps:  40, steps per second:   8, episode reward: -25.600, mean reward: -0.640 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 66.433243, mean_q: 23.693198, mean_eps: 0.100000\n","     108563/2000000000: episode: 2903, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 150.800, mean reward:  3.770 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.860781, mean_q: 24.112902, mean_eps: 0.100000\n","     108593/2000000000: episode: 2904, duration: 4.027s, episode steps:  30, steps per second:   7, episode reward: 73.200, mean reward:  2.440 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 67.103987, mean_q: 23.701397, mean_eps: 0.100000\n","     108631/2000000000: episode: 2905, duration: 5.208s, episode steps:  38, steps per second:   7, episode reward: 191.400, mean reward:  5.037 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 68.123393, mean_q: 23.645720, mean_eps: 0.100000\n","     108671/2000000000: episode: 2906, duration: 5.086s, episode steps:  40, steps per second:   8, episode reward: 94.800, mean reward:  2.370 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.618461, mean_q: 23.612090, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     108706/2000000000: episode: 2907, duration: 4.406s, episode steps:  35, steps per second:   8, episode reward: -107.200, mean reward: -3.063 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 67.877678, mean_q: 24.256571, mean_eps: 0.100000\n","     108746/2000000000: episode: 2908, duration: 4.981s, episode steps:  40, steps per second:   8, episode reward: -33.400, mean reward: -0.835 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.954336, mean_q: 22.927582, mean_eps: 0.100000\n","     108778/2000000000: episode: 2909, duration: 4.315s, episode steps:  32, steps per second:   7, episode reward:  0.300, mean reward:  0.009 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 68.398492, mean_q: 24.134898, mean_eps: 0.100000\n","     108818/2000000000: episode: 2910, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: -45.500, mean reward: -1.137 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.596920, mean_q: 23.671734, mean_eps: 0.100000\n","     108856/2000000000: episode: 2911, duration: 5.071s, episode steps:  38, steps per second:   7, episode reward: 49.900, mean reward:  1.313 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 67.394263, mean_q: 23.668109, mean_eps: 0.100000\n","     108894/2000000000: episode: 2912, duration: 4.973s, episode steps:  38, steps per second:   8, episode reward: -18.300, mean reward: -0.482 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 74.703995, mean_q: 23.490131, mean_eps: 0.100000\n","     108932/2000000000: episode: 2913, duration: 4.915s, episode steps:  38, steps per second:   8, episode reward: -149.600, mean reward: -3.937 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 69.963303, mean_q: 23.170317, mean_eps: 0.100000\n","     108970/2000000000: episode: 2914, duration: 4.847s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 67.917112, mean_q: 23.886511, mean_eps: 0.100000\n","     109010/2000000000: episode: 2915, duration: 5.306s, episode steps:  40, steps per second:   8, episode reward: -86.700, mean reward: -2.168 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 64.137078, mean_q: 23.716560, mean_eps: 0.100000\n","     109046/2000000000: episode: 2916, duration: 4.877s, episode steps:  36, steps per second:   7, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 64.421369, mean_q: 23.526726, mean_eps: 0.100000\n","     109080/2000000000: episode: 2917, duration: 4.650s, episode steps:  34, steps per second:   7, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 70.775813, mean_q: 23.560874, mean_eps: 0.100000\n","     109120/2000000000: episode: 2918, duration: 5.366s, episode steps:  40, steps per second:   7, episode reward: -77.100, mean reward: -1.928 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.322096, mean_q: 23.621028, mean_eps: 0.100000\n","     109157/2000000000: episode: 2919, duration: 4.984s, episode steps:  37, steps per second:   7, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 73.957709, mean_q: 23.270182, mean_eps: 0.100000\n","     109196/2000000000: episode: 2920, duration: 4.843s, episode steps:  39, steps per second:   8, episode reward: 58.600, mean reward:  1.503 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 68.154637, mean_q: 23.598458, mean_eps: 0.100000\n","     109234/2000000000: episode: 2921, duration: 4.884s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.026 [0.000, 2.000],  loss: 70.125796, mean_q: 24.387971, mean_eps: 0.100000\n","     109274/2000000000: episode: 2922, duration: 5.295s, episode steps:  40, steps per second:   8, episode reward: -32.600, mean reward: -0.815 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.166333, mean_q: 23.693979, mean_eps: 0.100000\n","     109314/2000000000: episode: 2923, duration: 5.056s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 78.242466, mean_q: 23.258512, mean_eps: 0.100000\n","     109354/2000000000: episode: 2924, duration: 5.089s, episode steps:  40, steps per second:   8, episode reward: -170.000, mean reward: -4.250 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.442933, mean_q: 23.010772, mean_eps: 0.100000\n","     109394/2000000000: episode: 2925, duration: 5.319s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.469843, mean_q: 23.398046, mean_eps: 0.100000\n","     109426/2000000000: episode: 2926, duration: 4.175s, episode steps:  32, steps per second:   8, episode reward: -3.900, mean reward: -0.122 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 72.260443, mean_q: 24.160099, mean_eps: 0.100000\n","     109466/2000000000: episode: 2927, duration: 5.244s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.552082, mean_q: 23.553186, mean_eps: 0.100000\n","     109506/2000000000: episode: 2928, duration: 5.323s, episode steps:  40, steps per second:   8, episode reward: -74.100, mean reward: -1.852 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.746311, mean_q: 22.962408, mean_eps: 0.100000\n","     109539/2000000000: episode: 2929, duration: 4.346s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 68.892084, mean_q: 23.811832, mean_eps: 0.100000\n","     109579/2000000000: episode: 2930, duration: 5.430s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.020724, mean_q: 23.352523, mean_eps: 0.100000\n","     109619/2000000000: episode: 2931, duration: 5.301s, episode steps:  40, steps per second:   8, episode reward:  8.000, mean reward:  0.200 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 64.795845, mean_q: 23.586882, mean_eps: 0.100000\n","     109659/2000000000: episode: 2932, duration: 5.463s, episode steps:  40, steps per second:   7, episode reward: 146.300, mean reward:  3.657 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.346212, mean_q: 24.990931, mean_eps: 0.100000\n","     109689/2000000000: episode: 2933, duration: 4.070s, episode steps:  30, steps per second:   7, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 69.764707, mean_q: 23.020187, mean_eps: 0.100000\n","     109729/2000000000: episode: 2934, duration: 5.357s, episode steps:  40, steps per second:   7, episode reward: -11.300, mean reward: -0.283 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 66.183106, mean_q: 23.727117, mean_eps: 0.100000\n","     109754/2000000000: episode: 2935, duration: 3.452s, episode steps:  25, steps per second:   7, episode reward: -74.100, mean reward: -2.964 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 72.752397, mean_q: 24.519529, mean_eps: 0.100000\n","     109789/2000000000: episode: 2936, duration: 4.613s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 74.015099, mean_q: 23.248824, mean_eps: 0.100000\n","     109825/2000000000: episode: 2937, duration: 4.651s, episode steps:  36, steps per second:   8, episode reward: -20.400, mean reward: -0.567 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.739553, mean_q: 23.544130, mean_eps: 0.100000\n","     109865/2000000000: episode: 2938, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: -29.000, mean reward: -0.725 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.477773, mean_q: 24.358505, mean_eps: 0.100000\n","     109905/2000000000: episode: 2939, duration: 5.460s, episode steps:  40, steps per second:   7, episode reward: -51.500, mean reward: -1.287 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.242082, mean_q: 23.514143, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     109944/2000000000: episode: 2940, duration: 5.205s, episode steps:  39, steps per second:   7, episode reward: 74.900, mean reward:  1.921 [-20.000, 18.000], mean action: 1.077 [0.000, 2.000],  loss: 68.763995, mean_q: 24.213250, mean_eps: 0.100000\n","     109976/2000000000: episode: 2941, duration: 4.203s, episode steps:  32, steps per second:   8, episode reward: 111.000, mean reward:  3.469 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 73.260415, mean_q: 23.033741, mean_eps: 0.100000\n","     110016/2000000000: episode: 2942, duration: 4.984s, episode steps:  40, steps per second:   8, episode reward: -10.300, mean reward: -0.257 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.123242, mean_q: 24.153230, mean_eps: 0.100000\n","     110048/2000000000: episode: 2943, duration: 3.705s, episode steps:  32, steps per second:   9, episode reward:  6.600, mean reward:  0.206 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 79.764161, mean_q: 26.462710, mean_eps: 0.100000\n","     110085/2000000000: episode: 2944, duration: 4.251s, episode steps:  37, steps per second:   9, episode reward: 139.800, mean reward:  3.778 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 73.819726, mean_q: 26.921128, mean_eps: 0.100000\n","     110120/2000000000: episode: 2945, duration: 4.141s, episode steps:  35, steps per second:   8, episode reward: -134.000, mean reward: -3.829 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.761323, mean_q: 26.347139, mean_eps: 0.100000\n","     110152/2000000000: episode: 2946, duration: 3.922s, episode steps:  32, steps per second:   8, episode reward: -51.600, mean reward: -1.613 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 75.464103, mean_q: 27.020231, mean_eps: 0.100000\n","     110177/2000000000: episode: 2947, duration: 2.878s, episode steps:  25, steps per second:   9, episode reward: 63.300, mean reward:  2.532 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 78.979128, mean_q: 25.319550, mean_eps: 0.100000\n","     110217/2000000000: episode: 2948, duration: 4.748s, episode steps:  40, steps per second:   8, episode reward: 21.100, mean reward:  0.528 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 68.443034, mean_q: 25.900847, mean_eps: 0.100000\n","     110257/2000000000: episode: 2949, duration: 4.593s, episode steps:  40, steps per second:   9, episode reward: 57.400, mean reward:  1.435 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.858446, mean_q: 26.197275, mean_eps: 0.100000\n","     110297/2000000000: episode: 2950, duration: 4.721s, episode steps:  40, steps per second:   8, episode reward: 15.300, mean reward:  0.382 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.908486, mean_q: 26.808477, mean_eps: 0.100000\n","     110337/2000000000: episode: 2951, duration: 4.593s, episode steps:  40, steps per second:   9, episode reward: -27.400, mean reward: -0.685 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.954634, mean_q: 26.007551, mean_eps: 0.100000\n","     110377/2000000000: episode: 2952, duration: 4.546s, episode steps:  40, steps per second:   9, episode reward: 157.900, mean reward:  3.947 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.832364, mean_q: 26.322421, mean_eps: 0.100000\n","     110413/2000000000: episode: 2953, duration: 4.240s, episode steps:  36, steps per second:   8, episode reward: -1.200, mean reward: -0.033 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.525824, mean_q: 26.326364, mean_eps: 0.100000\n","     110453/2000000000: episode: 2954, duration: 5.025s, episode steps:  40, steps per second:   8, episode reward: 60.500, mean reward:  1.513 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.615698, mean_q: 26.269961, mean_eps: 0.100000\n","     110493/2000000000: episode: 2955, duration: 5.516s, episode steps:  40, steps per second:   7, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 78.434156, mean_q: 27.118718, mean_eps: 0.100000\n","     110533/2000000000: episode: 2956, duration: 5.202s, episode steps:  40, steps per second:   8, episode reward: 172.000, mean reward:  4.300 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.920332, mean_q: 26.428518, mean_eps: 0.100000\n","     110570/2000000000: episode: 2957, duration: 5.072s, episode steps:  37, steps per second:   7, episode reward: 25.000, mean reward:  0.676 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 73.738427, mean_q: 26.239185, mean_eps: 0.100000\n","     110610/2000000000: episode: 2958, duration: 5.671s, episode steps:  40, steps per second:   7, episode reward: -37.300, mean reward: -0.932 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.097682, mean_q: 25.751486, mean_eps: 0.100000\n","     110650/2000000000: episode: 2959, duration: 5.599s, episode steps:  40, steps per second:   7, episode reward:  8.100, mean reward:  0.203 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 74.333088, mean_q: 26.918544, mean_eps: 0.100000\n","     110689/2000000000: episode: 2960, duration: 5.885s, episode steps:  39, steps per second:   7, episode reward: 13.100, mean reward:  0.336 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 63.984534, mean_q: 26.323456, mean_eps: 0.100000\n","     110729/2000000000: episode: 2961, duration: 5.712s, episode steps:  40, steps per second:   7, episode reward: 49.500, mean reward:  1.238 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.620561, mean_q: 26.225993, mean_eps: 0.100000\n","     110760/2000000000: episode: 2962, duration: 4.177s, episode steps:  31, steps per second:   7, episode reward: 47.000, mean reward:  1.516 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 77.913842, mean_q: 26.346667, mean_eps: 0.100000\n","     110790/2000000000: episode: 2963, duration: 3.903s, episode steps:  30, steps per second:   8, episode reward: 103.900, mean reward:  3.463 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 69.973642, mean_q: 27.015039, mean_eps: 0.100000\n","     110828/2000000000: episode: 2964, duration: 5.010s, episode steps:  38, steps per second:   8, episode reward: 74.900, mean reward:  1.971 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 72.044116, mean_q: 26.364551, mean_eps: 0.100000\n","     110868/2000000000: episode: 2965, duration: 5.418s, episode steps:  40, steps per second:   7, episode reward: 169.600, mean reward:  4.240 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 69.839031, mean_q: 26.510278, mean_eps: 0.100000\n","     110908/2000000000: episode: 2966, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: -29.600, mean reward: -0.740 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.055020, mean_q: 25.514660, mean_eps: 0.100000\n","     110943/2000000000: episode: 2967, duration: 4.683s, episode steps:  35, steps per second:   7, episode reward: 160.100, mean reward:  4.574 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 80.848952, mean_q: 25.842926, mean_eps: 0.100000\n","     110983/2000000000: episode: 2968, duration: 5.061s, episode steps:  40, steps per second:   8, episode reward: -17.100, mean reward: -0.428 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.826858, mean_q: 26.815799, mean_eps: 0.100000\n","     111006/2000000000: episode: 2969, duration: 3.140s, episode steps:  23, steps per second:   7, episode reward: -108.500, mean reward: -4.717 [-20.000, 18.000], mean action: 0.565 [0.000, 2.000],  loss: 68.681622, mean_q: 26.730230, mean_eps: 0.100000\n","     111046/2000000000: episode: 2970, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: 139.100, mean reward:  3.478 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.762853, mean_q: 26.129466, mean_eps: 0.100000\n","     111076/2000000000: episode: 2971, duration: 3.933s, episode steps:  30, steps per second:   8, episode reward: 101.400, mean reward:  3.380 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 70.715622, mean_q: 26.610684, mean_eps: 0.100000\n","     111116/2000000000: episode: 2972, duration: 5.097s, episode steps:  40, steps per second:   8, episode reward: 93.800, mean reward:  2.345 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.422442, mean_q: 26.315686, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     111156/2000000000: episode: 2973, duration: 5.030s, episode steps:  40, steps per second:   8, episode reward: 23.600, mean reward:  0.590 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 72.706326, mean_q: 25.900459, mean_eps: 0.100000\n","     111191/2000000000: episode: 2974, duration: 4.732s, episode steps:  35, steps per second:   7, episode reward:  5.400, mean reward:  0.154 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 75.560596, mean_q: 26.169954, mean_eps: 0.100000\n","     111223/2000000000: episode: 2975, duration: 4.229s, episode steps:  32, steps per second:   8, episode reward: -54.500, mean reward: -1.703 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 77.260882, mean_q: 26.192690, mean_eps: 0.100000\n","     111263/2000000000: episode: 2976, duration: 5.334s, episode steps:  40, steps per second:   7, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.275927, mean_q: 26.039562, mean_eps: 0.100000\n","     111303/2000000000: episode: 2977, duration: 5.275s, episode steps:  40, steps per second:   8, episode reward: 34.900, mean reward:  0.872 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.409465, mean_q: 26.434810, mean_eps: 0.100000\n","     111343/2000000000: episode: 2978, duration: 5.371s, episode steps:  40, steps per second:   7, episode reward: 184.500, mean reward:  4.612 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.844060, mean_q: 26.445542, mean_eps: 0.100000\n","     111376/2000000000: episode: 2979, duration: 4.418s, episode steps:  33, steps per second:   7, episode reward: -17.400, mean reward: -0.527 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 70.243766, mean_q: 26.274733, mean_eps: 0.100000\n","     111413/2000000000: episode: 2980, duration: 4.626s, episode steps:  37, steps per second:   8, episode reward: 99.200, mean reward:  2.681 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 67.871392, mean_q: 26.688196, mean_eps: 0.100000\n","     111449/2000000000: episode: 2981, duration: 4.675s, episode steps:  36, steps per second:   8, episode reward: 163.200, mean reward:  4.533 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 71.675340, mean_q: 26.139832, mean_eps: 0.100000\n","     111489/2000000000: episode: 2982, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: -86.000, mean reward: -2.150 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 74.225104, mean_q: 25.509390, mean_eps: 0.100000\n","     111529/2000000000: episode: 2983, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.394795, mean_q: 26.393175, mean_eps: 0.100000\n","     111569/2000000000: episode: 2984, duration: 5.196s, episode steps:  40, steps per second:   8, episode reward: 23.700, mean reward:  0.593 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.327017, mean_q: 26.205774, mean_eps: 0.100000\n","     111606/2000000000: episode: 2985, duration: 4.844s, episode steps:  37, steps per second:   8, episode reward:  4.100, mean reward:  0.111 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 76.743376, mean_q: 26.285429, mean_eps: 0.100000\n","     111639/2000000000: episode: 2986, duration: 4.214s, episode steps:  33, steps per second:   8, episode reward: 163.700, mean reward:  4.961 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.063119, mean_q: 26.623048, mean_eps: 0.100000\n","     111679/2000000000: episode: 2987, duration: 5.031s, episode steps:  40, steps per second:   8, episode reward: -126.300, mean reward: -3.157 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.574681, mean_q: 25.720779, mean_eps: 0.100000\n","     111719/2000000000: episode: 2988, duration: 4.778s, episode steps:  40, steps per second:   8, episode reward: 174.000, mean reward:  4.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.223551, mean_q: 25.886608, mean_eps: 0.100000\n","     111759/2000000000: episode: 2989, duration: 5.360s, episode steps:  40, steps per second:   7, episode reward: 67.000, mean reward:  1.675 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.121255, mean_q: 26.256058, mean_eps: 0.100000\n","     111787/2000000000: episode: 2990, duration: 3.700s, episode steps:  28, steps per second:   8, episode reward: 23.200, mean reward:  0.829 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.033202, mean_q: 25.975001, mean_eps: 0.100000\n","     111827/2000000000: episode: 2991, duration: 5.050s, episode steps:  40, steps per second:   8, episode reward: 43.700, mean reward:  1.093 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.366010, mean_q: 26.774071, mean_eps: 0.100000\n","     111859/2000000000: episode: 2992, duration: 3.842s, episode steps:  32, steps per second:   8, episode reward: -16.700, mean reward: -0.522 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.126082, mean_q: 26.541656, mean_eps: 0.100000\n","     111893/2000000000: episode: 2993, duration: 4.229s, episode steps:  34, steps per second:   8, episode reward: -54.500, mean reward: -1.603 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 71.893011, mean_q: 26.630250, mean_eps: 0.100000\n","     111933/2000000000: episode: 2994, duration: 5.193s, episode steps:  40, steps per second:   8, episode reward: 31.300, mean reward:  0.783 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.682376, mean_q: 25.851417, mean_eps: 0.100000\n","     111973/2000000000: episode: 2995, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward: 116.300, mean reward:  2.907 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.653839, mean_q: 26.109623, mean_eps: 0.100000\n","     112013/2000000000: episode: 2996, duration: 5.315s, episode steps:  40, steps per second:   8, episode reward: -39.700, mean reward: -0.993 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 65.102214, mean_q: 25.651518, mean_eps: 0.100000\n","     112053/2000000000: episode: 2997, duration: 5.267s, episode steps:  40, steps per second:   8, episode reward: 80.600, mean reward:  2.015 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 67.945732, mean_q: 26.866362, mean_eps: 0.100000\n","     112092/2000000000: episode: 2998, duration: 5.153s, episode steps:  39, steps per second:   8, episode reward: 63.500, mean reward:  1.628 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 70.629560, mean_q: 26.550052, mean_eps: 0.100000\n","     112132/2000000000: episode: 2999, duration: 5.191s, episode steps:  40, steps per second:   8, episode reward: 65.100, mean reward:  1.627 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.820083, mean_q: 26.368688, mean_eps: 0.100000\n","     112172/2000000000: episode: 3000, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: 25.000, mean reward:  0.625 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.877285, mean_q: 26.882127, mean_eps: 0.100000\n","     112203/2000000000: episode: 3001, duration: 4.331s, episode steps:  31, steps per second:   7, episode reward: 145.900, mean reward:  4.706 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 71.934364, mean_q: 26.363816, mean_eps: 0.100000\n","     112236/2000000000: episode: 3002, duration: 4.688s, episode steps:  33, steps per second:   7, episode reward: -6.900, mean reward: -0.209 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 72.035683, mean_q: 26.133148, mean_eps: 0.100000\n","     112268/2000000000: episode: 3003, duration: 4.442s, episode steps:  32, steps per second:   7, episode reward: 10.400, mean reward:  0.325 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 70.000502, mean_q: 26.249167, mean_eps: 0.100000\n","     112308/2000000000: episode: 3004, duration: 5.844s, episode steps:  40, steps per second:   7, episode reward: 184.600, mean reward:  4.615 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.865379, mean_q: 25.861729, mean_eps: 0.100000\n","     112346/2000000000: episode: 3005, duration: 5.155s, episode steps:  38, steps per second:   7, episode reward: -43.800, mean reward: -1.153 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 67.417686, mean_q: 25.870913, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     112386/2000000000: episode: 3006, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: -4.400, mean reward: -0.110 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.268847, mean_q: 26.092556, mean_eps: 0.100000\n","     112426/2000000000: episode: 3007, duration: 5.461s, episode steps:  40, steps per second:   7, episode reward: 48.300, mean reward:  1.208 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.550809, mean_q: 27.252650, mean_eps: 0.100000\n","     112462/2000000000: episode: 3008, duration: 4.729s, episode steps:  36, steps per second:   8, episode reward: 93.400, mean reward:  2.594 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 73.984056, mean_q: 26.552178, mean_eps: 0.100000\n","     112502/2000000000: episode: 3009, duration: 5.304s, episode steps:  40, steps per second:   8, episode reward: 62.400, mean reward:  1.560 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.206177, mean_q: 26.063684, mean_eps: 0.100000\n","     112542/2000000000: episode: 3010, duration: 5.444s, episode steps:  40, steps per second:   7, episode reward: -29.400, mean reward: -0.735 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.971494, mean_q: 26.544326, mean_eps: 0.100000\n","     112582/2000000000: episode: 3011, duration: 5.602s, episode steps:  40, steps per second:   7, episode reward: 96.600, mean reward:  2.415 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.017604, mean_q: 26.599386, mean_eps: 0.100000\n","     112620/2000000000: episode: 3012, duration: 5.053s, episode steps:  38, steps per second:   8, episode reward: -9.100, mean reward: -0.239 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 73.834695, mean_q: 26.575094, mean_eps: 0.100000\n","     112660/2000000000: episode: 3013, duration: 5.564s, episode steps:  40, steps per second:   7, episode reward: -59.800, mean reward: -1.495 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.638757, mean_q: 26.106202, mean_eps: 0.100000\n","     112700/2000000000: episode: 3014, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: 83.900, mean reward:  2.097 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.288916, mean_q: 26.737003, mean_eps: 0.100000\n","     112740/2000000000: episode: 3015, duration: 5.268s, episode steps:  40, steps per second:   8, episode reward: 65.700, mean reward:  1.642 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.767459, mean_q: 26.982962, mean_eps: 0.100000\n","     112773/2000000000: episode: 3016, duration: 4.321s, episode steps:  33, steps per second:   8, episode reward: 13.900, mean reward:  0.421 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.677837, mean_q: 26.589954, mean_eps: 0.100000\n","     112807/2000000000: episode: 3017, duration: 4.490s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 73.226693, mean_q: 25.564929, mean_eps: 0.100000\n","     112838/2000000000: episode: 3018, duration: 4.093s, episode steps:  31, steps per second:   8, episode reward: 116.900, mean reward:  3.771 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 70.583387, mean_q: 27.152434, mean_eps: 0.100000\n","     112872/2000000000: episode: 3019, duration: 4.658s, episode steps:  34, steps per second:   7, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 64.845218, mean_q: 26.340892, mean_eps: 0.100000\n","     112910/2000000000: episode: 3020, duration: 5.099s, episode steps:  38, steps per second:   7, episode reward: -0.100, mean reward: -0.003 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 67.668097, mean_q: 26.575128, mean_eps: 0.100000\n","     112950/2000000000: episode: 3021, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 156.000, mean reward:  3.900 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.223522, mean_q: 26.249166, mean_eps: 0.100000\n","     112981/2000000000: episode: 3022, duration: 4.093s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 67.555619, mean_q: 27.046313, mean_eps: 0.100000\n","     113020/2000000000: episode: 3023, duration: 5.570s, episode steps:  39, steps per second:   7, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 73.476989, mean_q: 26.130052, mean_eps: 0.100000\n","     113049/2000000000: episode: 3024, duration: 3.803s, episode steps:  29, steps per second:   8, episode reward: 78.100, mean reward:  2.693 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 77.021881, mean_q: 26.378243, mean_eps: 0.100000\n","     113089/2000000000: episode: 3025, duration: 5.413s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.081159, mean_q: 26.273275, mean_eps: 0.100000\n","     113129/2000000000: episode: 3026, duration: 5.856s, episode steps:  40, steps per second:   7, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.354486, mean_q: 26.511482, mean_eps: 0.100000\n","     113169/2000000000: episode: 3027, duration: 5.647s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.984049, mean_q: 26.372353, mean_eps: 0.100000\n","     113206/2000000000: episode: 3028, duration: 5.097s, episode steps:  37, steps per second:   7, episode reward: -172.000, mean reward: -4.649 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 74.014468, mean_q: 26.335478, mean_eps: 0.100000\n","     113242/2000000000: episode: 3029, duration: 5.102s, episode steps:  36, steps per second:   7, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 74.088046, mean_q: 26.040891, mean_eps: 0.100000\n","     113282/2000000000: episode: 3030, duration: 5.304s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.609904, mean_q: 26.847084, mean_eps: 0.100000\n","     113322/2000000000: episode: 3031, duration: 5.412s, episode steps:  40, steps per second:   7, episode reward: 174.000, mean reward:  4.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.227467, mean_q: 25.721573, mean_eps: 0.100000\n","     113362/2000000000: episode: 3032, duration: 5.306s, episode steps:  40, steps per second:   8, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.644675, mean_q: 26.874820, mean_eps: 0.100000\n","     113401/2000000000: episode: 3033, duration: 5.078s, episode steps:  39, steps per second:   8, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 76.958847, mean_q: 26.173702, mean_eps: 0.100000\n","     113438/2000000000: episode: 3034, duration: 4.757s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 67.838857, mean_q: 26.446451, mean_eps: 0.100000\n","     113475/2000000000: episode: 3035, duration: 4.822s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 71.751024, mean_q: 26.568278, mean_eps: 0.100000\n","     113515/2000000000: episode: 3036, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 151.600, mean reward:  3.790 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.664941, mean_q: 26.892660, mean_eps: 0.100000\n","     113553/2000000000: episode: 3037, duration: 5.108s, episode steps:  38, steps per second:   7, episode reward: 162.700, mean reward:  4.282 [-20.000, 18.000], mean action: 1.368 [0.000, 2.000],  loss: 73.363239, mean_q: 26.595733, mean_eps: 0.100000\n","     113593/2000000000: episode: 3038, duration: 5.501s, episode steps:  40, steps per second:   7, episode reward: 57.200, mean reward:  1.430 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.686411, mean_q: 26.744594, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     113633/2000000000: episode: 3039, duration: 5.299s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.253457, mean_q: 25.687823, mean_eps: 0.100000\n","     113673/2000000000: episode: 3040, duration: 5.517s, episode steps:  40, steps per second:   7, episode reward: 53.300, mean reward:  1.333 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.415651, mean_q: 26.612890, mean_eps: 0.100000\n","     113711/2000000000: episode: 3041, duration: 5.038s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 70.891694, mean_q: 26.197141, mean_eps: 0.100000\n","     113751/2000000000: episode: 3042, duration: 5.166s, episode steps:  40, steps per second:   8, episode reward: 40.600, mean reward:  1.015 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.157678, mean_q: 26.449202, mean_eps: 0.100000\n","     113785/2000000000: episode: 3043, duration: 4.356s, episode steps:  34, steps per second:   8, episode reward: -94.500, mean reward: -2.779 [-20.000, 19.500], mean action: 1.118 [0.000, 2.000],  loss: 71.148850, mean_q: 26.209119, mean_eps: 0.100000\n","     113825/2000000000: episode: 3044, duration: 5.356s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.196527, mean_q: 26.180930, mean_eps: 0.100000\n","     113864/2000000000: episode: 3045, duration: 5.229s, episode steps:  39, steps per second:   7, episode reward: -96.000, mean reward: -2.462 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 69.682954, mean_q: 25.956062, mean_eps: 0.100000\n","     113902/2000000000: episode: 3046, duration: 5.137s, episode steps:  38, steps per second:   7, episode reward: 72.600, mean reward:  1.911 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 73.150322, mean_q: 26.130325, mean_eps: 0.100000\n","     113942/2000000000: episode: 3047, duration: 5.329s, episode steps:  40, steps per second:   8, episode reward: 61.200, mean reward:  1.530 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.523182, mean_q: 26.683169, mean_eps: 0.100000\n","     113977/2000000000: episode: 3048, duration: 4.670s, episode steps:  35, steps per second:   7, episode reward: -185.200, mean reward: -5.291 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 77.453608, mean_q: 26.424533, mean_eps: 0.100000\n","     114004/2000000000: episode: 3049, duration: 3.641s, episode steps:  27, steps per second:   7, episode reward: 75.200, mean reward:  2.785 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 78.731720, mean_q: 26.791910, mean_eps: 0.100000\n","     114044/2000000000: episode: 3050, duration: 5.295s, episode steps:  40, steps per second:   8, episode reward: 79.400, mean reward:  1.985 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 72.292653, mean_q: 26.562337, mean_eps: 0.100000\n","     114079/2000000000: episode: 3051, duration: 4.481s, episode steps:  35, steps per second:   8, episode reward: -115.900, mean reward: -3.311 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 78.359971, mean_q: 26.124777, mean_eps: 0.100000\n","     114119/2000000000: episode: 3052, duration: 4.977s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.514320, mean_q: 26.481685, mean_eps: 0.100000\n","     114153/2000000000: episode: 3053, duration: 4.177s, episode steps:  34, steps per second:   8, episode reward: -18.900, mean reward: -0.556 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 64.571568, mean_q: 26.611955, mean_eps: 0.100000\n","     114192/2000000000: episode: 3054, duration: 4.736s, episode steps:  39, steps per second:   8, episode reward: 65.500, mean reward:  1.679 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 67.127177, mean_q: 26.467367, mean_eps: 0.100000\n","     114218/2000000000: episode: 3055, duration: 3.327s, episode steps:  26, steps per second:   8, episode reward: -81.600, mean reward: -3.138 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 71.726638, mean_q: 26.151288, mean_eps: 0.100000\n","     114258/2000000000: episode: 3056, duration: 5.044s, episode steps:  40, steps per second:   8, episode reward: 22.700, mean reward:  0.568 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.202247, mean_q: 25.813981, mean_eps: 0.100000\n","     114294/2000000000: episode: 3057, duration: 4.491s, episode steps:  36, steps per second:   8, episode reward: 17.800, mean reward:  0.494 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 67.031917, mean_q: 26.147982, mean_eps: 0.100000\n","     114334/2000000000: episode: 3058, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: 107.400, mean reward:  2.685 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 64.667926, mean_q: 26.935495, mean_eps: 0.100000\n","     114367/2000000000: episode: 3059, duration: 3.921s, episode steps:  33, steps per second:   8, episode reward: -4.700, mean reward: -0.142 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 71.979657, mean_q: 27.563880, mean_eps: 0.100000\n","     114407/2000000000: episode: 3060, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.328983, mean_q: 26.579042, mean_eps: 0.100000\n","     114447/2000000000: episode: 3061, duration: 4.765s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.404293, mean_q: 26.156136, mean_eps: 0.100000\n","     114483/2000000000: episode: 3062, duration: 4.376s, episode steps:  36, steps per second:   8, episode reward: -35.300, mean reward: -0.981 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 63.967622, mean_q: 26.578559, mean_eps: 0.100000\n","     114518/2000000000: episode: 3063, duration: 4.277s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 66.284631, mean_q: 26.408826, mean_eps: 0.100000\n","     114551/2000000000: episode: 3064, duration: 3.835s, episode steps:  33, steps per second:   9, episode reward: -79.100, mean reward: -2.397 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.501497, mean_q: 26.825381, mean_eps: 0.100000\n","     114585/2000000000: episode: 3065, duration: 3.970s, episode steps:  34, steps per second:   9, episode reward: 131.100, mean reward:  3.856 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 77.803099, mean_q: 26.053242, mean_eps: 0.100000\n","     114619/2000000000: episode: 3066, duration: 4.083s, episode steps:  34, steps per second:   8, episode reward: 27.500, mean reward:  0.809 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 66.349170, mean_q: 26.131001, mean_eps: 0.100000\n","     114659/2000000000: episode: 3067, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: -102.400, mean reward: -2.560 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.638406, mean_q: 26.437755, mean_eps: 0.100000\n","     114699/2000000000: episode: 3068, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 10.100, mean reward:  0.253 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.043204, mean_q: 25.885132, mean_eps: 0.100000\n","     114739/2000000000: episode: 3069, duration: 4.816s, episode steps:  40, steps per second:   8, episode reward:  9.900, mean reward:  0.248 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.970710, mean_q: 25.890382, mean_eps: 0.100000\n","     114779/2000000000: episode: 3070, duration: 4.815s, episode steps:  40, steps per second:   8, episode reward:  6.900, mean reward:  0.172 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.679343, mean_q: 26.116345, mean_eps: 0.100000\n","     114819/2000000000: episode: 3071, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: 97.800, mean reward:  2.445 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.943134, mean_q: 26.814614, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     114859/2000000000: episode: 3072, duration: 4.792s, episode steps:  40, steps per second:   8, episode reward: 102.600, mean reward:  2.565 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.618235, mean_q: 26.512156, mean_eps: 0.100000\n","     114899/2000000000: episode: 3073, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: 21.800, mean reward:  0.545 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.559419, mean_q: 26.529364, mean_eps: 0.100000\n","     114931/2000000000: episode: 3074, duration: 3.886s, episode steps:  32, steps per second:   8, episode reward: 86.100, mean reward:  2.691 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 70.318843, mean_q: 26.838718, mean_eps: 0.100000\n","     114967/2000000000: episode: 3075, duration: 4.576s, episode steps:  36, steps per second:   8, episode reward: 123.100, mean reward:  3.419 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 74.059522, mean_q: 28.104826, mean_eps: 0.100000\n","     115007/2000000000: episode: 3076, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 58.200, mean reward:  1.455 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.372244, mean_q: 26.899155, mean_eps: 0.100000\n","     115047/2000000000: episode: 3077, duration: 5.044s, episode steps:  40, steps per second:   8, episode reward: 30.800, mean reward:  0.770 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.634926, mean_q: 26.292124, mean_eps: 0.100000\n","     115077/2000000000: episode: 3078, duration: 3.755s, episode steps:  30, steps per second:   8, episode reward: -18.900, mean reward: -0.630 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 64.555595, mean_q: 26.838964, mean_eps: 0.100000\n","     115117/2000000000: episode: 3079, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: 79.600, mean reward:  1.990 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.985681, mean_q: 26.253726, mean_eps: 0.100000\n","     115157/2000000000: episode: 3080, duration: 4.794s, episode steps:  40, steps per second:   8, episode reward: 62.900, mean reward:  1.573 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.583382, mean_q: 27.092470, mean_eps: 0.100000\n","     115197/2000000000: episode: 3081, duration: 4.678s, episode steps:  40, steps per second:   9, episode reward: 18.200, mean reward:  0.455 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.747231, mean_q: 26.618805, mean_eps: 0.100000\n","     115233/2000000000: episode: 3082, duration: 4.434s, episode steps:  36, steps per second:   8, episode reward: -42.800, mean reward: -1.189 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 67.669005, mean_q: 26.508906, mean_eps: 0.100000\n","     115271/2000000000: episode: 3083, duration: 4.733s, episode steps:  38, steps per second:   8, episode reward: 246.000, mean reward:  6.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 76.312098, mean_q: 26.147733, mean_eps: 0.100000\n","     115311/2000000000: episode: 3084, duration: 4.942s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.314596, mean_q: 26.389297, mean_eps: 0.100000\n","     115347/2000000000: episode: 3085, duration: 5.062s, episode steps:  36, steps per second:   7, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 73.412033, mean_q: 26.705081, mean_eps: 0.100000\n","     115387/2000000000: episode: 3086, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 194.900, mean reward:  4.873 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.246255, mean_q: 26.137198, mean_eps: 0.100000\n","     115427/2000000000: episode: 3087, duration: 4.794s, episode steps:  40, steps per second:   8, episode reward: -38.400, mean reward: -0.960 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.305777, mean_q: 26.225562, mean_eps: 0.100000\n","     115465/2000000000: episode: 3088, duration: 4.854s, episode steps:  38, steps per second:   8, episode reward: 105.800, mean reward:  2.784 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 66.797654, mean_q: 26.603290, mean_eps: 0.100000\n","     115505/2000000000: episode: 3089, duration: 4.878s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.182806, mean_q: 26.762157, mean_eps: 0.100000\n","     115545/2000000000: episode: 3090, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.768031, mean_q: 26.580177, mean_eps: 0.100000\n","     115585/2000000000: episode: 3091, duration: 4.824s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 80.346919, mean_q: 26.299668, mean_eps: 0.100000\n","     115625/2000000000: episode: 3092, duration: 4.694s, episode steps:  40, steps per second:   9, episode reward: 180.100, mean reward:  4.503 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.603691, mean_q: 26.219463, mean_eps: 0.100000\n","     115660/2000000000: episode: 3093, duration: 3.949s, episode steps:  35, steps per second:   9, episode reward: 87.200, mean reward:  2.491 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 76.866609, mean_q: 26.494055, mean_eps: 0.100000\n","     115688/2000000000: episode: 3094, duration: 3.167s, episode steps:  28, steps per second:   9, episode reward: 64.600, mean reward:  2.307 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 69.303890, mean_q: 26.706920, mean_eps: 0.100000\n","     115719/2000000000: episode: 3095, duration: 3.606s, episode steps:  31, steps per second:   9, episode reward: -23.400, mean reward: -0.755 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.906654, mean_q: 26.346319, mean_eps: 0.100000\n","     115747/2000000000: episode: 3096, duration: 3.313s, episode steps:  28, steps per second:   8, episode reward: 52.400, mean reward:  1.871 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 64.848461, mean_q: 26.168318, mean_eps: 0.100000\n","     115782/2000000000: episode: 3097, duration: 4.193s, episode steps:  35, steps per second:   8, episode reward: 140.900, mean reward:  4.026 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 69.670276, mean_q: 26.316454, mean_eps: 0.100000\n","     115815/2000000000: episode: 3098, duration: 3.792s, episode steps:  33, steps per second:   9, episode reward: -27.400, mean reward: -0.830 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 74.499073, mean_q: 27.361687, mean_eps: 0.100000\n","     115850/2000000000: episode: 3099, duration: 4.126s, episode steps:  35, steps per second:   8, episode reward: -53.000, mean reward: -1.514 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 67.215293, mean_q: 26.854158, mean_eps: 0.100000\n","     115890/2000000000: episode: 3100, duration: 4.532s, episode steps:  40, steps per second:   9, episode reward: -83.900, mean reward: -2.098 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.642465, mean_q: 26.447881, mean_eps: 0.100000\n","     115925/2000000000: episode: 3101, duration: 4.096s, episode steps:  35, steps per second:   9, episode reward: -93.700, mean reward: -2.677 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 71.316176, mean_q: 26.710751, mean_eps: 0.100000\n","     115962/2000000000: episode: 3102, duration: 4.316s, episode steps:  37, steps per second:   9, episode reward: 190.500, mean reward:  5.149 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 71.722331, mean_q: 25.907509, mean_eps: 0.100000\n","     116002/2000000000: episode: 3103, duration: 4.718s, episode steps:  40, steps per second:   8, episode reward: -54.300, mean reward: -1.357 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.057202, mean_q: 27.203102, mean_eps: 0.100000\n","     116034/2000000000: episode: 3104, duration: 3.908s, episode steps:  32, steps per second:   8, episode reward: 81.300, mean reward:  2.541 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 65.551451, mean_q: 26.179838, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     116074/2000000000: episode: 3105, duration: 4.707s, episode steps:  40, steps per second:   8, episode reward: -35.000, mean reward: -0.875 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.320516, mean_q: 26.762905, mean_eps: 0.100000\n","     116114/2000000000: episode: 3106, duration: 4.664s, episode steps:  40, steps per second:   9, episode reward: 26.200, mean reward:  0.655 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 68.169505, mean_q: 25.972494, mean_eps: 0.100000\n","     116154/2000000000: episode: 3107, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward: -14.200, mean reward: -0.355 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.123444, mean_q: 26.458634, mean_eps: 0.100000\n","     116194/2000000000: episode: 3108, duration: 4.865s, episode steps:  40, steps per second:   8, episode reward:  3.000, mean reward:  0.075 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.331835, mean_q: 26.477814, mean_eps: 0.100000\n","     116234/2000000000: episode: 3109, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward:  9.900, mean reward:  0.248 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.910969, mean_q: 26.762066, mean_eps: 0.100000\n","     116270/2000000000: episode: 3110, duration: 4.459s, episode steps:  36, steps per second:   8, episode reward: 26.400, mean reward:  0.733 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 71.379115, mean_q: 26.253128, mean_eps: 0.100000\n","     116310/2000000000: episode: 3111, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward:  3.300, mean reward:  0.082 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.724936, mean_q: 26.610783, mean_eps: 0.100000\n","     116340/2000000000: episode: 3112, duration: 3.705s, episode steps:  30, steps per second:   8, episode reward: -151.400, mean reward: -5.047 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 72.993171, mean_q: 25.972441, mean_eps: 0.100000\n","     116380/2000000000: episode: 3113, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -45.000, mean reward: -1.125 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.416768, mean_q: 26.096727, mean_eps: 0.100000\n","     116420/2000000000: episode: 3114, duration: 4.751s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.481373, mean_q: 26.278734, mean_eps: 0.100000\n","     116459/2000000000: episode: 3115, duration: 4.637s, episode steps:  39, steps per second:   8, episode reward: 41.700, mean reward:  1.069 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 71.154829, mean_q: 27.034710, mean_eps: 0.100000\n","     116499/2000000000: episode: 3116, duration: 4.684s, episode steps:  40, steps per second:   9, episode reward:  3.600, mean reward:  0.090 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.060289, mean_q: 26.099347, mean_eps: 0.100000\n","     116536/2000000000: episode: 3117, duration: 4.437s, episode steps:  37, steps per second:   8, episode reward: 108.400, mean reward:  2.930 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.342234, mean_q: 26.447891, mean_eps: 0.100000\n","     116573/2000000000: episode: 3118, duration: 4.399s, episode steps:  37, steps per second:   8, episode reward: 30.300, mean reward:  0.819 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 70.167144, mean_q: 25.956031, mean_eps: 0.100000\n","     116613/2000000000: episode: 3119, duration: 4.763s, episode steps:  40, steps per second:   8, episode reward: 52.500, mean reward:  1.313 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.812781, mean_q: 26.335581, mean_eps: 0.100000\n","     116650/2000000000: episode: 3120, duration: 4.447s, episode steps:  37, steps per second:   8, episode reward: 34.000, mean reward:  0.919 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 66.621542, mean_q: 26.914302, mean_eps: 0.100000\n","     116685/2000000000: episode: 3121, duration: 4.022s, episode steps:  35, steps per second:   9, episode reward: 68.000, mean reward:  1.943 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 72.689380, mean_q: 26.184131, mean_eps: 0.100000\n","     116719/2000000000: episode: 3122, duration: 4.172s, episode steps:  34, steps per second:   8, episode reward: 54.200, mean reward:  1.594 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 72.868198, mean_q: 27.172363, mean_eps: 0.100000\n","     116755/2000000000: episode: 3123, duration: 4.928s, episode steps:  36, steps per second:   7, episode reward:  1.000, mean reward:  0.028 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 72.149156, mean_q: 27.095048, mean_eps: 0.100000\n","     116794/2000000000: episode: 3124, duration: 4.910s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 71.364296, mean_q: 26.128053, mean_eps: 0.100000\n","     116834/2000000000: episode: 3125, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: 72.500, mean reward:  1.812 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.732721, mean_q: 26.802980, mean_eps: 0.100000\n","     116874/2000000000: episode: 3126, duration: 4.783s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 68.884416, mean_q: 26.415494, mean_eps: 0.100000\n","     116913/2000000000: episode: 3127, duration: 4.575s, episode steps:  39, steps per second:   9, episode reward: 35.000, mean reward:  0.897 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 77.059919, mean_q: 26.417447, mean_eps: 0.100000\n","     116953/2000000000: episode: 3128, duration: 4.690s, episode steps:  40, steps per second:   9, episode reward: 203.800, mean reward:  5.095 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.887654, mean_q: 26.571479, mean_eps: 0.100000\n","     116993/2000000000: episode: 3129, duration: 4.788s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.579544, mean_q: 26.691200, mean_eps: 0.100000\n","     117027/2000000000: episode: 3130, duration: 4.015s, episode steps:  34, steps per second:   8, episode reward: 25.900, mean reward:  0.762 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.633756, mean_q: 26.873536, mean_eps: 0.100000\n","     117067/2000000000: episode: 3131, duration: 4.877s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.613766, mean_q: 25.937741, mean_eps: 0.100000\n","     117102/2000000000: episode: 3132, duration: 4.285s, episode steps:  35, steps per second:   8, episode reward: 66.400, mean reward:  1.897 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 75.525554, mean_q: 26.551622, mean_eps: 0.100000\n","     117142/2000000000: episode: 3133, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.937557, mean_q: 26.240244, mean_eps: 0.100000\n","     117182/2000000000: episode: 3134, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 62.884284, mean_q: 25.915896, mean_eps: 0.100000\n","     117213/2000000000: episode: 3135, duration: 3.990s, episode steps:  31, steps per second:   8, episode reward: -157.900, mean reward: -5.094 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.537587, mean_q: 26.344998, mean_eps: 0.100000\n","     117252/2000000000: episode: 3136, duration: 5.283s, episode steps:  39, steps per second:   7, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 76.397777, mean_q: 26.693102, mean_eps: 0.100000\n","     117292/2000000000: episode: 3137, duration: 5.089s, episode steps:  40, steps per second:   8, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.311282, mean_q: 26.647964, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     117332/2000000000: episode: 3138, duration: 4.864s, episode steps:  40, steps per second:   8, episode reward: -90.000, mean reward: -2.250 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 72.135003, mean_q: 26.106378, mean_eps: 0.100000\n","     117372/2000000000: episode: 3139, duration: 5.306s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.006086, mean_q: 25.884485, mean_eps: 0.100000\n","     117407/2000000000: episode: 3140, duration: 4.511s, episode steps:  35, steps per second:   8, episode reward: 72.100, mean reward:  2.060 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.721154, mean_q: 25.941742, mean_eps: 0.100000\n","     117447/2000000000: episode: 3141, duration: 5.248s, episode steps:  40, steps per second:   8, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.369782, mean_q: 26.286781, mean_eps: 0.100000\n","     117487/2000000000: episode: 3142, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.258254, mean_q: 26.563861, mean_eps: 0.100000\n","     117518/2000000000: episode: 3143, duration: 3.710s, episode steps:  31, steps per second:   8, episode reward: 65.100, mean reward:  2.100 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 68.638306, mean_q: 25.837452, mean_eps: 0.100000\n","     117546/2000000000: episode: 3144, duration: 3.658s, episode steps:  28, steps per second:   8, episode reward: -82.400, mean reward: -2.943 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 73.965174, mean_q: 26.095702, mean_eps: 0.100000\n","     117578/2000000000: episode: 3145, duration: 4.042s, episode steps:  32, steps per second:   8, episode reward: 151.000, mean reward:  4.719 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 68.317324, mean_q: 26.951595, mean_eps: 0.100000\n","     117618/2000000000: episode: 3146, duration: 5.144s, episode steps:  40, steps per second:   8, episode reward: 30.900, mean reward:  0.773 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.624057, mean_q: 27.281127, mean_eps: 0.100000\n","     117654/2000000000: episode: 3147, duration: 4.558s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 71.594379, mean_q: 26.195789, mean_eps: 0.100000\n","     117694/2000000000: episode: 3148, duration: 5.110s, episode steps:  40, steps per second:   8, episode reward:  3.900, mean reward:  0.098 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 70.408779, mean_q: 26.302568, mean_eps: 0.100000\n","     117719/2000000000: episode: 3149, duration: 3.138s, episode steps:  25, steps per second:   8, episode reward: 31.800, mean reward:  1.272 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 62.766817, mean_q: 26.694414, mean_eps: 0.100000\n","     117759/2000000000: episode: 3150, duration: 5.034s, episode steps:  40, steps per second:   8, episode reward: -92.600, mean reward: -2.315 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.351278, mean_q: 26.003958, mean_eps: 0.100000\n","     117799/2000000000: episode: 3151, duration: 5.647s, episode steps:  40, steps per second:   7, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.466812, mean_q: 26.344166, mean_eps: 0.100000\n","     117839/2000000000: episode: 3152, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.315407, mean_q: 26.488199, mean_eps: 0.100000\n","     117879/2000000000: episode: 3153, duration: 5.019s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.245326, mean_q: 27.242405, mean_eps: 0.100000\n","     117917/2000000000: episode: 3154, duration: 4.675s, episode steps:  38, steps per second:   8, episode reward: 29.000, mean reward:  0.763 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 66.934329, mean_q: 26.112358, mean_eps: 0.100000\n","     117949/2000000000: episode: 3155, duration: 3.983s, episode steps:  32, steps per second:   8, episode reward: 46.200, mean reward:  1.444 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 70.628743, mean_q: 27.425068, mean_eps: 0.100000\n","     117989/2000000000: episode: 3156, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward: -2.800, mean reward: -0.070 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.513929, mean_q: 26.713508, mean_eps: 0.100000\n","     118027/2000000000: episode: 3157, duration: 4.581s, episode steps:  38, steps per second:   8, episode reward: 111.100, mean reward:  2.924 [-20.000, 18.000], mean action: 1.368 [0.000, 2.000],  loss: 69.229955, mean_q: 26.301506, mean_eps: 0.100000\n","     118057/2000000000: episode: 3158, duration: 3.861s, episode steps:  30, steps per second:   8, episode reward: -56.600, mean reward: -1.887 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 67.904465, mean_q: 26.624538, mean_eps: 0.100000\n","     118097/2000000000: episode: 3159, duration: 5.013s, episode steps:  40, steps per second:   8, episode reward: -45.400, mean reward: -1.135 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.235249, mean_q: 25.864264, mean_eps: 0.100000\n","     118137/2000000000: episode: 3160, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: 43.800, mean reward:  1.095 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.372919, mean_q: 26.248694, mean_eps: 0.100000\n","     118175/2000000000: episode: 3161, duration: 4.654s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 64.207390, mean_q: 26.397852, mean_eps: 0.100000\n","     118212/2000000000: episode: 3162, duration: 4.814s, episode steps:  37, steps per second:   8, episode reward: -48.500, mean reward: -1.311 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 76.687663, mean_q: 27.110509, mean_eps: 0.100000\n","     118252/2000000000: episode: 3163, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: -63.900, mean reward: -1.598 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 77.427322, mean_q: 26.439051, mean_eps: 0.100000\n","     118292/2000000000: episode: 3164, duration: 5.367s, episode steps:  40, steps per second:   7, episode reward: 102.100, mean reward:  2.552 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 72.396720, mean_q: 26.749840, mean_eps: 0.100000\n","     118330/2000000000: episode: 3165, duration: 4.958s, episode steps:  38, steps per second:   8, episode reward: 68.900, mean reward:  1.813 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 66.969570, mean_q: 26.633882, mean_eps: 0.100000\n","     118370/2000000000: episode: 3166, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: 32.700, mean reward:  0.817 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.963494, mean_q: 26.600140, mean_eps: 0.100000\n","     118408/2000000000: episode: 3167, duration: 4.507s, episode steps:  38, steps per second:   8, episode reward: 67.300, mean reward:  1.771 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 71.545100, mean_q: 26.371692, mean_eps: 0.100000\n","     118441/2000000000: episode: 3168, duration: 3.946s, episode steps:  33, steps per second:   8, episode reward: -0.800, mean reward: -0.024 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 69.318326, mean_q: 26.158746, mean_eps: 0.100000\n","     118481/2000000000: episode: 3169, duration: 4.606s, episode steps:  40, steps per second:   9, episode reward: 45.800, mean reward:  1.145 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.833346, mean_q: 26.418403, mean_eps: 0.100000\n","     118520/2000000000: episode: 3170, duration: 4.711s, episode steps:  39, steps per second:   8, episode reward: -109.600, mean reward: -2.810 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 67.390328, mean_q: 26.371921, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     118554/2000000000: episode: 3171, duration: 3.928s, episode steps:  34, steps per second:   9, episode reward: 98.500, mean reward:  2.897 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 71.668503, mean_q: 26.463981, mean_eps: 0.100000\n","     118594/2000000000: episode: 3172, duration: 4.653s, episode steps:  40, steps per second:   9, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.447395, mean_q: 26.280658, mean_eps: 0.100000\n","     118634/2000000000: episode: 3173, duration: 4.720s, episode steps:  40, steps per second:   8, episode reward: 37.900, mean reward:  0.947 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.163451, mean_q: 25.685602, mean_eps: 0.100000\n","     118663/2000000000: episode: 3174, duration: 3.601s, episode steps:  29, steps per second:   8, episode reward: -47.100, mean reward: -1.624 [-20.000, 18.200], mean action: 1.000 [0.000, 2.000],  loss: 71.911152, mean_q: 26.976249, mean_eps: 0.100000\n","     118696/2000000000: episode: 3175, duration: 4.026s, episode steps:  33, steps per second:   8, episode reward: -84.000, mean reward: -2.545 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 78.131489, mean_q: 26.024255, mean_eps: 0.100000\n","     118723/2000000000: episode: 3176, duration: 3.307s, episode steps:  27, steps per second:   8, episode reward: -1.600, mean reward: -0.059 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 62.600017, mean_q: 26.367059, mean_eps: 0.100000\n","     118763/2000000000: episode: 3177, duration: 4.787s, episode steps:  40, steps per second:   8, episode reward: -75.900, mean reward: -1.897 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.116264, mean_q: 26.261877, mean_eps: 0.100000\n","     118798/2000000000: episode: 3178, duration: 4.195s, episode steps:  35, steps per second:   8, episode reward: 218.100, mean reward:  6.231 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 74.182665, mean_q: 26.676761, mean_eps: 0.100000\n","     118835/2000000000: episode: 3179, duration: 4.273s, episode steps:  37, steps per second:   9, episode reward: 161.900, mean reward:  4.376 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 73.173571, mean_q: 27.028687, mean_eps: 0.100000\n","     118875/2000000000: episode: 3180, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: 21.400, mean reward:  0.535 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.567558, mean_q: 26.395949, mean_eps: 0.100000\n","     118915/2000000000: episode: 3181, duration: 4.908s, episode steps:  40, steps per second:   8, episode reward: 117.500, mean reward:  2.937 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.730647, mean_q: 26.647434, mean_eps: 0.100000\n","     118948/2000000000: episode: 3182, duration: 4.071s, episode steps:  33, steps per second:   8, episode reward: -88.000, mean reward: -2.667 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 69.978248, mean_q: 27.111757, mean_eps: 0.100000\n","     118981/2000000000: episode: 3183, duration: 3.973s, episode steps:  33, steps per second:   8, episode reward: 122.300, mean reward:  3.706 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 67.796934, mean_q: 26.448450, mean_eps: 0.100000\n","     119021/2000000000: episode: 3184, duration: 4.722s, episode steps:  40, steps per second:   8, episode reward: 36.300, mean reward:  0.908 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.039151, mean_q: 26.504893, mean_eps: 0.100000\n","     119055/2000000000: episode: 3185, duration: 4.233s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 68.414758, mean_q: 26.217838, mean_eps: 0.100000\n","     119095/2000000000: episode: 3186, duration: 4.821s, episode steps:  40, steps per second:   8, episode reward: -96.100, mean reward: -2.402 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 70.703299, mean_q: 26.534956, mean_eps: 0.100000\n","     119135/2000000000: episode: 3187, duration: 5.160s, episode steps:  40, steps per second:   8, episode reward:  2.800, mean reward:  0.070 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.656761, mean_q: 27.258799, mean_eps: 0.100000\n","     119175/2000000000: episode: 3188, duration: 5.140s, episode steps:  40, steps per second:   8, episode reward: 97.600, mean reward:  2.440 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.579797, mean_q: 25.855712, mean_eps: 0.100000\n","     119215/2000000000: episode: 3189, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward:  3.100, mean reward:  0.078 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.728234, mean_q: 26.031952, mean_eps: 0.100000\n","     119254/2000000000: episode: 3190, duration: 4.800s, episode steps:  39, steps per second:   8, episode reward: 83.000, mean reward:  2.128 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 68.018988, mean_q: 27.155354, mean_eps: 0.100000\n","     119294/2000000000: episode: 3191, duration: 4.848s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.027447, mean_q: 26.465039, mean_eps: 0.100000\n","     119328/2000000000: episode: 3192, duration: 4.352s, episode steps:  34, steps per second:   8, episode reward: -36.600, mean reward: -1.076 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 75.427625, mean_q: 26.053273, mean_eps: 0.100000\n","     119368/2000000000: episode: 3193, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: 23.300, mean reward:  0.583 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.362003, mean_q: 26.433375, mean_eps: 0.100000\n","     119408/2000000000: episode: 3194, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: 21.100, mean reward:  0.528 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 66.613314, mean_q: 26.361172, mean_eps: 0.100000\n","     119448/2000000000: episode: 3195, duration: 4.990s, episode steps:  40, steps per second:   8, episode reward: 124.800, mean reward:  3.120 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.550897, mean_q: 26.147150, mean_eps: 0.100000\n","     119481/2000000000: episode: 3196, duration: 4.064s, episode steps:  33, steps per second:   8, episode reward: 75.600, mean reward:  2.291 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.052582, mean_q: 27.009516, mean_eps: 0.100000\n","     119517/2000000000: episode: 3197, duration: 4.632s, episode steps:  36, steps per second:   8, episode reward: 211.200, mean reward:  5.867 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 65.931370, mean_q: 26.586102, mean_eps: 0.100000\n","     119557/2000000000: episode: 3198, duration: 5.035s, episode steps:  40, steps per second:   8, episode reward: 38.300, mean reward:  0.958 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.557658, mean_q: 26.531419, mean_eps: 0.100000\n","     119595/2000000000: episode: 3199, duration: 4.637s, episode steps:  38, steps per second:   8, episode reward: 64.600, mean reward:  1.700 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 75.965587, mean_q: 26.374267, mean_eps: 0.100000\n","     119629/2000000000: episode: 3200, duration: 4.323s, episode steps:  34, steps per second:   8, episode reward: 89.300, mean reward:  2.626 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 69.778924, mean_q: 25.637642, mean_eps: 0.100000\n","     119666/2000000000: episode: 3201, duration: 4.672s, episode steps:  37, steps per second:   8, episode reward: -69.400, mean reward: -1.876 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 69.540222, mean_q: 26.504042, mean_eps: 0.100000\n","     119693/2000000000: episode: 3202, duration: 3.427s, episode steps:  27, steps per second:   8, episode reward: -28.200, mean reward: -1.044 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.289140, mean_q: 27.246115, mean_eps: 0.100000\n","     119732/2000000000: episode: 3203, duration: 4.760s, episode steps:  39, steps per second:   8, episode reward: 138.100, mean reward:  3.541 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 75.904705, mean_q: 25.879244, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     119772/2000000000: episode: 3204, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 34.400, mean reward:  0.860 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.373271, mean_q: 26.534464, mean_eps: 0.100000\n","     119809/2000000000: episode: 3205, duration: 4.562s, episode steps:  37, steps per second:   8, episode reward: -118.200, mean reward: -3.195 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 71.521241, mean_q: 26.928259, mean_eps: 0.100000\n","     119840/2000000000: episode: 3206, duration: 3.927s, episode steps:  31, steps per second:   8, episode reward: -139.600, mean reward: -4.503 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 69.166061, mean_q: 26.471101, mean_eps: 0.100000\n","     119871/2000000000: episode: 3207, duration: 3.879s, episode steps:  31, steps per second:   8, episode reward: 120.000, mean reward:  3.871 [-20.000, 18.600], mean action: 1.032 [0.000, 2.000],  loss: 73.486669, mean_q: 26.544357, mean_eps: 0.100000\n","     119911/2000000000: episode: 3208, duration: 5.206s, episode steps:  40, steps per second:   8, episode reward: 113.100, mean reward:  2.827 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.970168, mean_q: 26.291791, mean_eps: 0.100000\n","     119942/2000000000: episode: 3209, duration: 3.999s, episode steps:  31, steps per second:   8, episode reward: 93.500, mean reward:  3.016 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 76.100800, mean_q: 26.729965, mean_eps: 0.100000\n","     119979/2000000000: episode: 3210, duration: 4.566s, episode steps:  37, steps per second:   8, episode reward: 86.000, mean reward:  2.324 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 68.368257, mean_q: 26.513599, mean_eps: 0.100000\n","     120019/2000000000: episode: 3211, duration: 4.861s, episode steps:  40, steps per second:   8, episode reward: 70.600, mean reward:  1.765 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.372632, mean_q: 26.323916, mean_eps: 0.100000\n","     120056/2000000000: episode: 3212, duration: 4.425s, episode steps:  37, steps per second:   8, episode reward: -36.100, mean reward: -0.976 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.230120, mean_q: 27.929602, mean_eps: 0.100000\n","     120095/2000000000: episode: 3213, duration: 4.604s, episode steps:  39, steps per second:   8, episode reward: -119.200, mean reward: -3.056 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 76.584612, mean_q: 28.118699, mean_eps: 0.100000\n","     120135/2000000000: episode: 3214, duration: 4.664s, episode steps:  40, steps per second:   9, episode reward: 29.200, mean reward:  0.730 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.531150, mean_q: 27.696412, mean_eps: 0.100000\n","     120162/2000000000: episode: 3215, duration: 3.296s, episode steps:  27, steps per second:   8, episode reward: 99.700, mean reward:  3.693 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 69.472954, mean_q: 27.160660, mean_eps: 0.100000\n","     120202/2000000000: episode: 3216, duration: 4.741s, episode steps:  40, steps per second:   8, episode reward: 108.400, mean reward:  2.710 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 71.214702, mean_q: 28.067712, mean_eps: 0.100000\n","     120236/2000000000: episode: 3217, duration: 4.231s, episode steps:  34, steps per second:   8, episode reward: 115.600, mean reward:  3.400 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 74.568400, mean_q: 27.922310, mean_eps: 0.100000\n","     120276/2000000000: episode: 3218, duration: 5.050s, episode steps:  40, steps per second:   8, episode reward: 160.500, mean reward:  4.012 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.546331, mean_q: 27.496019, mean_eps: 0.100000\n","     120316/2000000000: episode: 3219, duration: 4.701s, episode steps:  40, steps per second:   9, episode reward: -14.400, mean reward: -0.360 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.492484, mean_q: 27.290455, mean_eps: 0.100000\n","     120350/2000000000: episode: 3220, duration: 4.609s, episode steps:  34, steps per second:   7, episode reward:  5.100, mean reward:  0.150 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 72.336334, mean_q: 27.891834, mean_eps: 0.100000\n","     120388/2000000000: episode: 3221, duration: 4.861s, episode steps:  38, steps per second:   8, episode reward: 111.000, mean reward:  2.921 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 72.335327, mean_q: 27.436148, mean_eps: 0.100000\n","     120428/2000000000: episode: 3222, duration: 4.717s, episode steps:  40, steps per second:   8, episode reward: -27.700, mean reward: -0.692 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.982932, mean_q: 27.773694, mean_eps: 0.100000\n","     120458/2000000000: episode: 3223, duration: 3.554s, episode steps:  30, steps per second:   8, episode reward: 82.100, mean reward:  2.737 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.349480, mean_q: 27.870833, mean_eps: 0.100000\n","     120498/2000000000: episode: 3224, duration: 4.658s, episode steps:  40, steps per second:   9, episode reward: 80.200, mean reward:  2.005 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.592576, mean_q: 28.139257, mean_eps: 0.100000\n","     120538/2000000000: episode: 3225, duration: 4.553s, episode steps:  40, steps per second:   9, episode reward: -53.100, mean reward: -1.328 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.859523, mean_q: 27.240241, mean_eps: 0.100000\n","     120577/2000000000: episode: 3226, duration: 4.687s, episode steps:  39, steps per second:   8, episode reward: 144.000, mean reward:  3.692 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 78.043612, mean_q: 27.371970, mean_eps: 0.100000\n","     120607/2000000000: episode: 3227, duration: 3.803s, episode steps:  30, steps per second:   8, episode reward: -92.500, mean reward: -3.083 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 73.533212, mean_q: 27.848460, mean_eps: 0.100000\n","     120636/2000000000: episode: 3228, duration: 3.706s, episode steps:  29, steps per second:   8, episode reward: 16.300, mean reward:  0.562 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 72.295041, mean_q: 27.952018, mean_eps: 0.100000\n","     120676/2000000000: episode: 3229, duration: 4.875s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.517881, mean_q: 27.809912, mean_eps: 0.100000\n","     120702/2000000000: episode: 3230, duration: 3.093s, episode steps:  26, steps per second:   8, episode reward: 58.200, mean reward:  2.238 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 86.510906, mean_q: 27.102488, mean_eps: 0.100000\n","     120742/2000000000: episode: 3231, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: 51.300, mean reward:  1.282 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 69.733918, mean_q: 28.223251, mean_eps: 0.100000\n","     120782/2000000000: episode: 3232, duration: 4.832s, episode steps:  40, steps per second:   8, episode reward: -26.800, mean reward: -0.670 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.067463, mean_q: 27.877716, mean_eps: 0.100000\n","     120822/2000000000: episode: 3233, duration: 4.959s, episode steps:  40, steps per second:   8, episode reward: 95.500, mean reward:  2.388 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.244290, mean_q: 27.751907, mean_eps: 0.100000\n","     120857/2000000000: episode: 3234, duration: 4.385s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.072165, mean_q: 27.351520, mean_eps: 0.100000\n","     120893/2000000000: episode: 3235, duration: 4.508s, episode steps:  36, steps per second:   8, episode reward: 38.500, mean reward:  1.069 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 74.281254, mean_q: 27.150296, mean_eps: 0.100000\n","     120931/2000000000: episode: 3236, duration: 4.705s, episode steps:  38, steps per second:   8, episode reward: -114.400, mean reward: -3.011 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 71.265801, mean_q: 27.758632, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     120962/2000000000: episode: 3237, duration: 3.962s, episode steps:  31, steps per second:   8, episode reward: 57.500, mean reward:  1.855 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 73.227143, mean_q: 27.913293, mean_eps: 0.100000\n","     120996/2000000000: episode: 3238, duration: 4.174s, episode steps:  34, steps per second:   8, episode reward: -272.900, mean reward: -8.026 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.488870, mean_q: 27.623656, mean_eps: 0.100000\n","     121026/2000000000: episode: 3239, duration: 3.751s, episode steps:  30, steps per second:   8, episode reward: 17.200, mean reward:  0.573 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.656998, mean_q: 28.484815, mean_eps: 0.100000\n","     121058/2000000000: episode: 3240, duration: 3.814s, episode steps:  32, steps per second:   8, episode reward: 50.100, mean reward:  1.566 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 74.120117, mean_q: 28.011566, mean_eps: 0.100000\n","     121098/2000000000: episode: 3241, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.077381, mean_q: 27.421313, mean_eps: 0.100000\n","     121138/2000000000: episode: 3242, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: -19.900, mean reward: -0.498 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.846280, mean_q: 27.062542, mean_eps: 0.100000\n","     121178/2000000000: episode: 3243, duration: 5.324s, episode steps:  40, steps per second:   8, episode reward: 105.300, mean reward:  2.632 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.070315, mean_q: 27.701834, mean_eps: 0.100000\n","     121208/2000000000: episode: 3244, duration: 3.775s, episode steps:  30, steps per second:   8, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 79.772341, mean_q: 28.250903, mean_eps: 0.100000\n","     121248/2000000000: episode: 3245, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.626614, mean_q: 28.016130, mean_eps: 0.100000\n","     121288/2000000000: episode: 3246, duration: 4.883s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.209872, mean_q: 27.256353, mean_eps: 0.100000\n","     121328/2000000000: episode: 3247, duration: 4.865s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.621893, mean_q: 28.022736, mean_eps: 0.100000\n","     121368/2000000000: episode: 3248, duration: 4.736s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.386897, mean_q: 27.773221, mean_eps: 0.100000\n","     121403/2000000000: episode: 3249, duration: 4.114s, episode steps:  35, steps per second:   9, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.194926, mean_q: 27.636116, mean_eps: 0.100000\n","     121443/2000000000: episode: 3250, duration: 5.004s, episode steps:  40, steps per second:   8, episode reward: 26.000, mean reward:  0.650 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 79.282169, mean_q: 27.955958, mean_eps: 0.100000\n","     121475/2000000000: episode: 3251, duration: 4.144s, episode steps:  32, steps per second:   8, episode reward: 50.200, mean reward:  1.569 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 70.641783, mean_q: 28.183685, mean_eps: 0.100000\n","     121515/2000000000: episode: 3252, duration: 5.027s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 72.904444, mean_q: 27.416065, mean_eps: 0.100000\n","     121550/2000000000: episode: 3253, duration: 4.232s, episode steps:  35, steps per second:   8, episode reward: -40.900, mean reward: -1.169 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 72.794184, mean_q: 27.041218, mean_eps: 0.100000\n","     121590/2000000000: episode: 3254, duration: 4.891s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.766852, mean_q: 27.900674, mean_eps: 0.100000\n","     121625/2000000000: episode: 3255, duration: 4.080s, episode steps:  35, steps per second:   9, episode reward: 28.600, mean reward:  0.817 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 74.443032, mean_q: 27.795909, mean_eps: 0.100000\n","     121665/2000000000: episode: 3256, duration: 4.726s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.837519, mean_q: 27.198206, mean_eps: 0.100000\n","     121704/2000000000: episode: 3257, duration: 5.013s, episode steps:  39, steps per second:   8, episode reward: 125.700, mean reward:  3.223 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 77.989693, mean_q: 27.789561, mean_eps: 0.100000\n","     121734/2000000000: episode: 3258, duration: 3.737s, episode steps:  30, steps per second:   8, episode reward: -96.000, mean reward: -3.200 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.650388, mean_q: 27.805363, mean_eps: 0.100000\n","     121771/2000000000: episode: 3259, duration: 4.743s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 71.684994, mean_q: 27.591093, mean_eps: 0.100000\n","     121805/2000000000: episode: 3260, duration: 4.152s, episode steps:  34, steps per second:   8, episode reward: 37.800, mean reward:  1.112 [-20.000, 18.000], mean action: 0.941 [0.000, 2.000],  loss: 71.738147, mean_q: 27.287562, mean_eps: 0.100000\n","     121845/2000000000: episode: 3261, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.119258, mean_q: 28.126373, mean_eps: 0.100000\n","     121885/2000000000: episode: 3262, duration: 5.048s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.218304, mean_q: 28.057152, mean_eps: 0.100000\n","     121919/2000000000: episode: 3263, duration: 4.065s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.986161, mean_q: 28.202479, mean_eps: 0.100000\n","     121959/2000000000: episode: 3264, duration: 4.939s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.146187, mean_q: 26.745799, mean_eps: 0.100000\n","     121999/2000000000: episode: 3265, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: 174.000, mean reward:  4.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.125287, mean_q: 27.680359, mean_eps: 0.100000\n","     122035/2000000000: episode: 3266, duration: 4.304s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.257801, mean_q: 27.831342, mean_eps: 0.100000\n","     122075/2000000000: episode: 3267, duration: 5.032s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.036742, mean_q: 27.762441, mean_eps: 0.100000\n","     122115/2000000000: episode: 3268, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 61.000, mean reward:  1.525 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.792700, mean_q: 28.161517, mean_eps: 0.100000\n","     122145/2000000000: episode: 3269, duration: 3.724s, episode steps:  30, steps per second:   8, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 68.759867, mean_q: 27.475498, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     122181/2000000000: episode: 3270, duration: 4.659s, episode steps:  36, steps per second:   8, episode reward: -85.300, mean reward: -2.369 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 70.776335, mean_q: 27.553587, mean_eps: 0.100000\n","     122221/2000000000: episode: 3271, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward: 17.400, mean reward:  0.435 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.427818, mean_q: 27.369463, mean_eps: 0.100000\n","     122261/2000000000: episode: 3272, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.643763, mean_q: 27.972544, mean_eps: 0.100000\n","     122301/2000000000: episode: 3273, duration: 4.750s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.883337, mean_q: 27.474962, mean_eps: 0.100000\n","     122341/2000000000: episode: 3274, duration: 4.800s, episode steps:  40, steps per second:   8, episode reward: -65.300, mean reward: -1.632 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 64.972416, mean_q: 27.570946, mean_eps: 0.100000\n","     122374/2000000000: episode: 3275, duration: 4.023s, episode steps:  33, steps per second:   8, episode reward: -1.100, mean reward: -0.033 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 67.814662, mean_q: 27.385537, mean_eps: 0.100000\n","     122411/2000000000: episode: 3276, duration: 4.560s, episode steps:  37, steps per second:   8, episode reward: 36.700, mean reward:  0.992 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 75.497175, mean_q: 28.475330, mean_eps: 0.100000\n","     122451/2000000000: episode: 3277, duration: 4.715s, episode steps:  40, steps per second:   8, episode reward: 37.500, mean reward:  0.937 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.901201, mean_q: 27.503828, mean_eps: 0.100000\n","     122491/2000000000: episode: 3278, duration: 4.897s, episode steps:  40, steps per second:   8, episode reward: 141.100, mean reward:  3.527 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.489527, mean_q: 27.652586, mean_eps: 0.100000\n","     122521/2000000000: episode: 3279, duration: 3.846s, episode steps:  30, steps per second:   8, episode reward: -26.200, mean reward: -0.873 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 74.743422, mean_q: 28.073555, mean_eps: 0.100000\n","     122561/2000000000: episode: 3280, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: -36.300, mean reward: -0.907 [-20.000, 19.700], mean action: 1.250 [0.000, 2.000],  loss: 72.553275, mean_q: 28.099229, mean_eps: 0.100000\n","     122601/2000000000: episode: 3281, duration: 5.482s, episode steps:  40, steps per second:   7, episode reward: 20.900, mean reward:  0.522 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.313641, mean_q: 27.566722, mean_eps: 0.100000\n","     122641/2000000000: episode: 3282, duration: 5.146s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.205249, mean_q: 27.821662, mean_eps: 0.100000\n","     122675/2000000000: episode: 3283, duration: 4.653s, episode steps:  34, steps per second:   7, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 72.554304, mean_q: 28.104676, mean_eps: 0.100000\n","     122715/2000000000: episode: 3284, duration: 4.794s, episode steps:  40, steps per second:   8, episode reward: -90.900, mean reward: -2.272 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.660434, mean_q: 27.503255, mean_eps: 0.100000\n","     122755/2000000000: episode: 3285, duration: 4.670s, episode steps:  40, steps per second:   9, episode reward: 142.700, mean reward:  3.567 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.137149, mean_q: 28.367745, mean_eps: 0.100000\n","     122791/2000000000: episode: 3286, duration: 4.146s, episode steps:  36, steps per second:   9, episode reward: -40.900, mean reward: -1.136 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 69.557968, mean_q: 27.518544, mean_eps: 0.100000\n","     122831/2000000000: episode: 3287, duration: 4.645s, episode steps:  40, steps per second:   9, episode reward: 94.400, mean reward:  2.360 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.603852, mean_q: 27.760710, mean_eps: 0.100000\n","     122866/2000000000: episode: 3288, duration: 4.212s, episode steps:  35, steps per second:   8, episode reward: -23.500, mean reward: -0.671 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 70.173920, mean_q: 27.564579, mean_eps: 0.100000\n","     122897/2000000000: episode: 3289, duration: 3.828s, episode steps:  31, steps per second:   8, episode reward: 114.400, mean reward:  3.690 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 68.530861, mean_q: 28.165137, mean_eps: 0.100000\n","     122937/2000000000: episode: 3290, duration: 5.046s, episode steps:  40, steps per second:   8, episode reward: -177.800, mean reward: -4.445 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.501116, mean_q: 27.593957, mean_eps: 0.100000\n","     122967/2000000000: episode: 3291, duration: 3.723s, episode steps:  30, steps per second:   8, episode reward: 75.500, mean reward:  2.517 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.887976, mean_q: 28.289855, mean_eps: 0.100000\n","     123007/2000000000: episode: 3292, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: -24.100, mean reward: -0.603 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.525414, mean_q: 28.294424, mean_eps: 0.100000\n","     123047/2000000000: episode: 3293, duration: 4.734s, episode steps:  40, steps per second:   8, episode reward: 20.700, mean reward:  0.518 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.589522, mean_q: 27.576232, mean_eps: 0.100000\n","     123087/2000000000: episode: 3294, duration: 4.489s, episode steps:  40, steps per second:   9, episode reward: 17.200, mean reward:  0.430 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.685966, mean_q: 27.514555, mean_eps: 0.100000\n","     123120/2000000000: episode: 3295, duration: 3.803s, episode steps:  33, steps per second:   9, episode reward:  0.600, mean reward:  0.018 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 67.735531, mean_q: 28.270797, mean_eps: 0.100000\n","     123158/2000000000: episode: 3296, duration: 4.400s, episode steps:  38, steps per second:   9, episode reward: -85.800, mean reward: -2.258 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 68.099076, mean_q: 27.538207, mean_eps: 0.100000\n","     123194/2000000000: episode: 3297, duration: 4.174s, episode steps:  36, steps per second:   9, episode reward: 125.800, mean reward:  3.494 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 68.671806, mean_q: 27.809452, mean_eps: 0.100000\n","     123228/2000000000: episode: 3298, duration: 4.069s, episode steps:  34, steps per second:   8, episode reward: -65.200, mean reward: -1.918 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.382049, mean_q: 28.475921, mean_eps: 0.100000\n","     123261/2000000000: episode: 3299, duration: 4.082s, episode steps:  33, steps per second:   8, episode reward:  7.800, mean reward:  0.236 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.008001, mean_q: 27.924901, mean_eps: 0.100000\n","     123296/2000000000: episode: 3300, duration: 4.329s, episode steps:  35, steps per second:   8, episode reward: 22.000, mean reward:  0.629 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 67.429579, mean_q: 27.997046, mean_eps: 0.100000\n","     123324/2000000000: episode: 3301, duration: 3.348s, episode steps:  28, steps per second:   8, episode reward: 25.600, mean reward:  0.914 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 73.733438, mean_q: 27.146528, mean_eps: 0.100000\n","     123359/2000000000: episode: 3302, duration: 4.054s, episode steps:  35, steps per second:   9, episode reward: 15.700, mean reward:  0.449 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 80.408535, mean_q: 27.568885, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     123395/2000000000: episode: 3303, duration: 4.131s, episode steps:  36, steps per second:   9, episode reward: 58.900, mean reward:  1.636 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 69.705175, mean_q: 27.572222, mean_eps: 0.100000\n","     123430/2000000000: episode: 3304, duration: 4.308s, episode steps:  35, steps per second:   8, episode reward: 231.800, mean reward:  6.623 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.627814, mean_q: 27.448226, mean_eps: 0.100000\n","     123470/2000000000: episode: 3305, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward: 94.600, mean reward:  2.365 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.201126, mean_q: 27.749630, mean_eps: 0.100000\n","     123502/2000000000: episode: 3306, duration: 3.979s, episode steps:  32, steps per second:   8, episode reward:  6.400, mean reward:  0.200 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 70.043713, mean_q: 27.895793, mean_eps: 0.100000\n","     123542/2000000000: episode: 3307, duration: 5.457s, episode steps:  40, steps per second:   7, episode reward: 79.800, mean reward:  1.995 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.009734, mean_q: 27.366931, mean_eps: 0.100000\n","     123582/2000000000: episode: 3308, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: 66.200, mean reward:  1.655 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.179970, mean_q: 27.709412, mean_eps: 0.100000\n","     123622/2000000000: episode: 3309, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 14.100, mean reward:  0.352 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.861963, mean_q: 26.627357, mean_eps: 0.100000\n","     123662/2000000000: episode: 3310, duration: 4.953s, episode steps:  40, steps per second:   8, episode reward: -79.900, mean reward: -1.998 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.333774, mean_q: 28.002006, mean_eps: 0.100000\n","     123692/2000000000: episode: 3311, duration: 3.603s, episode steps:  30, steps per second:   8, episode reward: 104.800, mean reward:  3.493 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 68.698512, mean_q: 28.345656, mean_eps: 0.100000\n","     123732/2000000000: episode: 3312, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: 10.900, mean reward:  0.272 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.391279, mean_q: 27.184127, mean_eps: 0.100000\n","     123772/2000000000: episode: 3313, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.669063, mean_q: 27.589537, mean_eps: 0.100000\n","     123812/2000000000: episode: 3314, duration: 4.814s, episode steps:  40, steps per second:   8, episode reward: -43.500, mean reward: -1.087 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 73.336024, mean_q: 27.799449, mean_eps: 0.100000\n","     123846/2000000000: episode: 3315, duration: 4.118s, episode steps:  34, steps per second:   8, episode reward: 22.800, mean reward:  0.671 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 69.027511, mean_q: 28.074798, mean_eps: 0.100000\n","     123886/2000000000: episode: 3316, duration: 5.402s, episode steps:  40, steps per second:   7, episode reward: -32.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 75.582538, mean_q: 27.670529, mean_eps: 0.100000\n","     123926/2000000000: episode: 3317, duration: 5.472s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.131952, mean_q: 27.295162, mean_eps: 0.100000\n","     123966/2000000000: episode: 3318, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: -76.900, mean reward: -1.923 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.671879, mean_q: 27.896476, mean_eps: 0.100000\n","     124005/2000000000: episode: 3319, duration: 4.870s, episode steps:  39, steps per second:   8, episode reward: 37.900, mean reward:  0.972 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 68.918948, mean_q: 28.133829, mean_eps: 0.100000\n","     124045/2000000000: episode: 3320, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: -64.000, mean reward: -1.600 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.129850, mean_q: 27.304145, mean_eps: 0.100000\n","     124082/2000000000: episode: 3321, duration: 4.602s, episode steps:  37, steps per second:   8, episode reward: -5.500, mean reward: -0.149 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 72.744271, mean_q: 28.933389, mean_eps: 0.100000\n","     124117/2000000000: episode: 3322, duration: 4.326s, episode steps:  35, steps per second:   8, episode reward: 156.200, mean reward:  4.463 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 71.148622, mean_q: 27.511054, mean_eps: 0.100000\n","     124157/2000000000: episode: 3323, duration: 5.042s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.509211, mean_q: 27.044790, mean_eps: 0.100000\n","     124197/2000000000: episode: 3324, duration: 4.840s, episode steps:  40, steps per second:   8, episode reward: 28.900, mean reward:  0.723 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.650424, mean_q: 28.202888, mean_eps: 0.100000\n","     124237/2000000000: episode: 3325, duration: 4.756s, episode steps:  40, steps per second:   8, episode reward: 85.200, mean reward:  2.130 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.533805, mean_q: 27.969194, mean_eps: 0.100000\n","     124261/2000000000: episode: 3326, duration: 3.012s, episode steps:  24, steps per second:   8, episode reward: -117.900, mean reward: -4.913 [-20.000, 18.000], mean action: 0.583 [0.000, 2.000],  loss: 69.485117, mean_q: 27.595313, mean_eps: 0.100000\n","     124301/2000000000: episode: 3327, duration: 4.710s, episode steps:  40, steps per second:   8, episode reward: 96.400, mean reward:  2.410 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.733956, mean_q: 28.324497, mean_eps: 0.100000\n","     124341/2000000000: episode: 3328, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: 117.700, mean reward:  2.943 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.208196, mean_q: 27.329200, mean_eps: 0.100000\n","     124379/2000000000: episode: 3329, duration: 4.898s, episode steps:  38, steps per second:   8, episode reward: 67.600, mean reward:  1.779 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 68.973672, mean_q: 27.599090, mean_eps: 0.100000\n","     124407/2000000000: episode: 3330, duration: 3.465s, episode steps:  28, steps per second:   8, episode reward: 40.700, mean reward:  1.454 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 70.986273, mean_q: 27.490236, mean_eps: 0.100000\n","     124447/2000000000: episode: 3331, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: 82.000, mean reward:  2.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.789904, mean_q: 27.825524, mean_eps: 0.100000\n","     124477/2000000000: episode: 3332, duration: 3.978s, episode steps:  30, steps per second:   8, episode reward: 50.800, mean reward:  1.693 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 70.155165, mean_q: 28.033148, mean_eps: 0.100000\n","     124517/2000000000: episode: 3333, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: 183.400, mean reward:  4.585 [-11.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 67.611859, mean_q: 28.424703, mean_eps: 0.100000\n","     124547/2000000000: episode: 3334, duration: 3.629s, episode steps:  30, steps per second:   8, episode reward: -17.700, mean reward: -0.590 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.879487, mean_q: 27.748867, mean_eps: 0.100000\n","     124584/2000000000: episode: 3335, duration: 4.417s, episode steps:  37, steps per second:   8, episode reward: -91.000, mean reward: -2.459 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 78.008537, mean_q: 27.060898, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     124620/2000000000: episode: 3336, duration: 4.351s, episode steps:  36, steps per second:   8, episode reward:  9.500, mean reward:  0.264 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 74.155564, mean_q: 27.782214, mean_eps: 0.100000\n","     124660/2000000000: episode: 3337, duration: 4.947s, episode steps:  40, steps per second:   8, episode reward: 70.300, mean reward:  1.757 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 76.091377, mean_q: 27.488739, mean_eps: 0.100000\n","     124700/2000000000: episode: 3338, duration: 4.895s, episode steps:  40, steps per second:   8, episode reward: -48.000, mean reward: -1.200 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 67.771537, mean_q: 28.193745, mean_eps: 0.100000\n","     124735/2000000000: episode: 3339, duration: 4.328s, episode steps:  35, steps per second:   8, episode reward: 96.200, mean reward:  2.749 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.323410, mean_q: 27.503185, mean_eps: 0.100000\n","     124771/2000000000: episode: 3340, duration: 4.202s, episode steps:  36, steps per second:   9, episode reward: 80.500, mean reward:  2.236 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 64.830737, mean_q: 27.779503, mean_eps: 0.100000\n","     124809/2000000000: episode: 3341, duration: 4.583s, episode steps:  38, steps per second:   8, episode reward: 35.900, mean reward:  0.945 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 75.249163, mean_q: 27.164148, mean_eps: 0.100000\n","     124849/2000000000: episode: 3342, duration: 4.822s, episode steps:  40, steps per second:   8, episode reward: 28.400, mean reward:  0.710 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 66.088279, mean_q: 27.859742, mean_eps: 0.100000\n","     124889/2000000000: episode: 3343, duration: 4.781s, episode steps:  40, steps per second:   8, episode reward: 39.100, mean reward:  0.977 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.766517, mean_q: 27.638001, mean_eps: 0.100000\n","     124929/2000000000: episode: 3344, duration: 4.761s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 71.357287, mean_q: 28.531987, mean_eps: 0.100000\n","     124968/2000000000: episode: 3345, duration: 4.860s, episode steps:  39, steps per second:   8, episode reward: 68.500, mean reward:  1.756 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 69.219379, mean_q: 26.956565, mean_eps: 0.100000\n","     125008/2000000000: episode: 3346, duration: 4.838s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.726743, mean_q: 27.836743, mean_eps: 0.100000\n","     125042/2000000000: episode: 3347, duration: 4.479s, episode steps:  34, steps per second:   8, episode reward: 12.700, mean reward:  0.374 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 72.822970, mean_q: 27.287814, mean_eps: 0.100000\n","     125082/2000000000: episode: 3348, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.998093, mean_q: 28.203067, mean_eps: 0.100000\n","     125122/2000000000: episode: 3349, duration: 5.163s, episode steps:  40, steps per second:   8, episode reward: 144.100, mean reward:  3.602 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.400000, mean_q: 27.343494, mean_eps: 0.100000\n","     125162/2000000000: episode: 3350, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.792276, mean_q: 26.590126, mean_eps: 0.100000\n","     125198/2000000000: episode: 3351, duration: 4.752s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 69.391941, mean_q: 27.348179, mean_eps: 0.100000\n","     125231/2000000000: episode: 3352, duration: 4.350s, episode steps:  33, steps per second:   8, episode reward: 54.200, mean reward:  1.642 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 73.042471, mean_q: 28.169161, mean_eps: 0.100000\n","     125271/2000000000: episode: 3353, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 67.586750, mean_q: 27.155499, mean_eps: 0.100000\n","     125304/2000000000: episode: 3354, duration: 4.141s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 74.954216, mean_q: 28.774429, mean_eps: 0.100000\n","     125330/2000000000: episode: 3355, duration: 3.227s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 79.549404, mean_q: 28.365844, mean_eps: 0.100000\n","     125362/2000000000: episode: 3356, duration: 3.882s, episode steps:  32, steps per second:   8, episode reward: -134.000, mean reward: -4.188 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 74.333313, mean_q: 28.108572, mean_eps: 0.100000\n","     125398/2000000000: episode: 3357, duration: 4.254s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 73.270778, mean_q: 28.270894, mean_eps: 0.100000\n","     125438/2000000000: episode: 3358, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.124035, mean_q: 27.736218, mean_eps: 0.100000\n","     125474/2000000000: episode: 3359, duration: 4.363s, episode steps:  36, steps per second:   8, episode reward: -134.000, mean reward: -3.722 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 68.737343, mean_q: 28.021553, mean_eps: 0.100000\n","     125510/2000000000: episode: 3360, duration: 4.354s, episode steps:  36, steps per second:   8, episode reward: -102.100, mean reward: -2.836 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 71.845061, mean_q: 27.813359, mean_eps: 0.100000\n","     125548/2000000000: episode: 3361, duration: 4.491s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 75.971837, mean_q: 27.639137, mean_eps: 0.100000\n","     125588/2000000000: episode: 3362, duration: 4.815s, episode steps:  40, steps per second:   8, episode reward: 37.500, mean reward:  0.937 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 66.765269, mean_q: 27.562060, mean_eps: 0.100000\n","     125623/2000000000: episode: 3363, duration: 4.253s, episode steps:  35, steps per second:   8, episode reward: -121.200, mean reward: -3.463 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 67.720977, mean_q: 27.665969, mean_eps: 0.100000\n","     125658/2000000000: episode: 3364, duration: 4.329s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 74.629574, mean_q: 27.253822, mean_eps: 0.100000\n","     125698/2000000000: episode: 3365, duration: 4.916s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.999550, mean_q: 27.585285, mean_eps: 0.100000\n","     125735/2000000000: episode: 3366, duration: 4.426s, episode steps:  37, steps per second:   8, episode reward: 38.400, mean reward:  1.038 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 72.399070, mean_q: 28.344920, mean_eps: 0.100000\n","     125773/2000000000: episode: 3367, duration: 4.830s, episode steps:  38, steps per second:   8, episode reward: 159.600, mean reward:  4.200 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 69.236654, mean_q: 28.352378, mean_eps: 0.100000\n","     125813/2000000000: episode: 3368, duration: 4.820s, episode steps:  40, steps per second:   8, episode reward: 72.200, mean reward:  1.805 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.966043, mean_q: 27.753716, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     125847/2000000000: episode: 3369, duration: 4.115s, episode steps:  34, steps per second:   8, episode reward: -41.200, mean reward: -1.212 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 66.582348, mean_q: 27.459287, mean_eps: 0.100000\n","     125887/2000000000: episode: 3370, duration: 4.952s, episode steps:  40, steps per second:   8, episode reward: 170.300, mean reward:  4.258 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.934445, mean_q: 27.344381, mean_eps: 0.100000\n","     125919/2000000000: episode: 3371, duration: 3.921s, episode steps:  32, steps per second:   8, episode reward: 31.500, mean reward:  0.984 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 73.514953, mean_q: 27.902109, mean_eps: 0.100000\n","     125959/2000000000: episode: 3372, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: -109.800, mean reward: -2.745 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.491488, mean_q: 27.169758, mean_eps: 0.100000\n","     125999/2000000000: episode: 3373, duration: 5.163s, episode steps:  40, steps per second:   8, episode reward: -7.200, mean reward: -0.180 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.637948, mean_q: 27.500400, mean_eps: 0.100000\n","     126035/2000000000: episode: 3374, duration: 4.201s, episode steps:  36, steps per second:   9, episode reward: -96.000, mean reward: -2.667 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 76.219817, mean_q: 27.583796, mean_eps: 0.100000\n","     126072/2000000000: episode: 3375, duration: 4.649s, episode steps:  37, steps per second:   8, episode reward: -40.800, mean reward: -1.103 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.384921, mean_q: 27.747374, mean_eps: 0.100000\n","     126109/2000000000: episode: 3376, duration: 4.732s, episode steps:  37, steps per second:   8, episode reward: -3.100, mean reward: -0.084 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 73.245665, mean_q: 27.200700, mean_eps: 0.100000\n","     126149/2000000000: episode: 3377, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: 118.700, mean reward:  2.968 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 73.742951, mean_q: 27.503185, mean_eps: 0.100000\n","     126182/2000000000: episode: 3378, duration: 4.199s, episode steps:  33, steps per second:   8, episode reward: -18.000, mean reward: -0.545 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 78.263234, mean_q: 27.125644, mean_eps: 0.100000\n","     126214/2000000000: episode: 3379, duration: 3.952s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 72.348854, mean_q: 27.391834, mean_eps: 0.100000\n","     126254/2000000000: episode: 3380, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: 39.600, mean reward:  0.990 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 77.590546, mean_q: 27.089823, mean_eps: 0.100000\n","     126294/2000000000: episode: 3381, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: 14.200, mean reward:  0.355 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.422123, mean_q: 27.203236, mean_eps: 0.100000\n","     126326/2000000000: episode: 3382, duration: 3.909s, episode steps:  32, steps per second:   8, episode reward: -39.600, mean reward: -1.237 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.302707, mean_q: 27.571453, mean_eps: 0.100000\n","     126366/2000000000: episode: 3383, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: -13.800, mean reward: -0.345 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.041513, mean_q: 27.959916, mean_eps: 0.100000\n","     126406/2000000000: episode: 3384, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: 99.100, mean reward:  2.477 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 67.903176, mean_q: 27.956896, mean_eps: 0.100000\n","     126444/2000000000: episode: 3385, duration: 4.701s, episode steps:  38, steps per second:   8, episode reward: 143.400, mean reward:  3.774 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 75.594621, mean_q: 28.344316, mean_eps: 0.100000\n","     126474/2000000000: episode: 3386, duration: 3.659s, episode steps:  30, steps per second:   8, episode reward: 169.000, mean reward:  5.633 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 66.874186, mean_q: 28.042398, mean_eps: 0.100000\n","     126511/2000000000: episode: 3387, duration: 4.482s, episode steps:  37, steps per second:   8, episode reward: 59.700, mean reward:  1.614 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 75.166814, mean_q: 27.800399, mean_eps: 0.100000\n","     126547/2000000000: episode: 3388, duration: 4.351s, episode steps:  36, steps per second:   8, episode reward: 25.000, mean reward:  0.694 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 66.669875, mean_q: 27.793166, mean_eps: 0.100000\n","     126582/2000000000: episode: 3389, duration: 4.252s, episode steps:  35, steps per second:   8, episode reward: -50.300, mean reward: -1.437 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 73.584188, mean_q: 27.808324, mean_eps: 0.100000\n","     126622/2000000000: episode: 3390, duration: 4.756s, episode steps:  40, steps per second:   8, episode reward: -81.000, mean reward: -2.025 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.466064, mean_q: 27.687169, mean_eps: 0.100000\n","     126662/2000000000: episode: 3391, duration: 4.874s, episode steps:  40, steps per second:   8, episode reward: -22.800, mean reward: -0.570 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.481766, mean_q: 28.007343, mean_eps: 0.100000\n","     126694/2000000000: episode: 3392, duration: 3.900s, episode steps:  32, steps per second:   8, episode reward: -10.400, mean reward: -0.325 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 76.516863, mean_q: 27.836180, mean_eps: 0.100000\n","     126734/2000000000: episode: 3393, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: 35.800, mean reward:  0.895 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.866882, mean_q: 27.918221, mean_eps: 0.100000\n","     126774/2000000000: episode: 3394, duration: 5.183s, episode steps:  40, steps per second:   8, episode reward: 60.500, mean reward:  1.512 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.538447, mean_q: 27.276650, mean_eps: 0.100000\n","     126806/2000000000: episode: 3395, duration: 4.029s, episode steps:  32, steps per second:   8, episode reward: -57.900, mean reward: -1.809 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 77.281311, mean_q: 28.048109, mean_eps: 0.100000\n","     126843/2000000000: episode: 3396, duration: 4.431s, episode steps:  37, steps per second:   8, episode reward: -54.700, mean reward: -1.478 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 67.309034, mean_q: 28.634998, mean_eps: 0.100000\n","     126883/2000000000: episode: 3397, duration: 4.826s, episode steps:  40, steps per second:   8, episode reward: -76.300, mean reward: -1.907 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.407666, mean_q: 27.197903, mean_eps: 0.100000\n","     126923/2000000000: episode: 3398, duration: 4.849s, episode steps:  40, steps per second:   8, episode reward: -58.900, mean reward: -1.473 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 67.052197, mean_q: 27.771254, mean_eps: 0.100000\n","     126960/2000000000: episode: 3399, duration: 4.587s, episode steps:  37, steps per second:   8, episode reward: 44.400, mean reward:  1.200 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 66.683229, mean_q: 27.158459, mean_eps: 0.100000\n","     127000/2000000000: episode: 3400, duration: 4.779s, episode steps:  40, steps per second:   8, episode reward: 73.100, mean reward:  1.828 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.654953, mean_q: 28.344002, mean_eps: 0.100000\n","     127040/2000000000: episode: 3401, duration: 4.874s, episode steps:  40, steps per second:   8, episode reward: 92.100, mean reward:  2.303 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.665487, mean_q: 27.872831, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     127080/2000000000: episode: 3402, duration: 4.960s, episode steps:  40, steps per second:   8, episode reward: -4.800, mean reward: -0.120 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.008918, mean_q: 28.367790, mean_eps: 0.100000\n","     127120/2000000000: episode: 3403, duration: 4.889s, episode steps:  40, steps per second:   8, episode reward: -102.500, mean reward: -2.562 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.403324, mean_q: 28.204200, mean_eps: 0.100000\n","     127160/2000000000: episode: 3404, duration: 4.919s, episode steps:  40, steps per second:   8, episode reward: 86.000, mean reward:  2.150 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.711300, mean_q: 27.797410, mean_eps: 0.100000\n","     127200/2000000000: episode: 3405, duration: 4.855s, episode steps:  40, steps per second:   8, episode reward: 26.400, mean reward:  0.660 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.250318, mean_q: 27.674162, mean_eps: 0.100000\n","     127238/2000000000: episode: 3406, duration: 4.661s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 71.501852, mean_q: 27.948416, mean_eps: 0.100000\n","     127278/2000000000: episode: 3407, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.742507, mean_q: 27.739728, mean_eps: 0.100000\n","     127318/2000000000: episode: 3408, duration: 4.872s, episode steps:  40, steps per second:   8, episode reward: 70.800, mean reward:  1.770 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.650643, mean_q: 27.708139, mean_eps: 0.100000\n","     127358/2000000000: episode: 3409, duration: 4.948s, episode steps:  40, steps per second:   8, episode reward: 90.600, mean reward:  2.265 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.861547, mean_q: 26.893963, mean_eps: 0.100000\n","     127398/2000000000: episode: 3410, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: 40.400, mean reward:  1.010 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.116658, mean_q: 27.697892, mean_eps: 0.100000\n","     127438/2000000000: episode: 3411, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: -129.600, mean reward: -3.240 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.240317, mean_q: 28.076164, mean_eps: 0.100000\n","     127478/2000000000: episode: 3412, duration: 5.223s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.039507, mean_q: 28.169776, mean_eps: 0.100000\n","     127510/2000000000: episode: 3413, duration: 4.162s, episode steps:  32, steps per second:   8, episode reward:  3.900, mean reward:  0.122 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 64.368453, mean_q: 28.139441, mean_eps: 0.100000\n","     127545/2000000000: episode: 3414, duration: 4.343s, episode steps:  35, steps per second:   8, episode reward: 68.200, mean reward:  1.949 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 76.624188, mean_q: 27.828097, mean_eps: 0.100000\n","     127585/2000000000: episode: 3415, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: -36.700, mean reward: -0.917 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.502212, mean_q: 27.422266, mean_eps: 0.100000\n","     127608/2000000000: episode: 3416, duration: 2.873s, episode steps:  23, steps per second:   8, episode reward: 56.000, mean reward:  2.435 [-20.000, 18.000], mean action: 0.826 [0.000, 2.000],  loss: 67.192104, mean_q: 28.753625, mean_eps: 0.100000\n","     127645/2000000000: episode: 3417, duration: 4.607s, episode steps:  37, steps per second:   8, episode reward: -6.800, mean reward: -0.184 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.306865, mean_q: 28.569936, mean_eps: 0.100000\n","     127684/2000000000: episode: 3418, duration: 4.907s, episode steps:  39, steps per second:   8, episode reward: -51.900, mean reward: -1.331 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 70.658119, mean_q: 26.943783, mean_eps: 0.100000\n","     127724/2000000000: episode: 3419, duration: 4.925s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.289986, mean_q: 27.214159, mean_eps: 0.100000\n","     127764/2000000000: episode: 3420, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.217662, mean_q: 28.470814, mean_eps: 0.100000\n","     127804/2000000000: episode: 3421, duration: 4.755s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.876954, mean_q: 28.241089, mean_eps: 0.100000\n","     127844/2000000000: episode: 3422, duration: 4.981s, episode steps:  40, steps per second:   8, episode reward: 12.500, mean reward:  0.312 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.825353, mean_q: 27.621879, mean_eps: 0.100000\n","     127884/2000000000: episode: 3423, duration: 4.949s, episode steps:  40, steps per second:   8, episode reward: 76.900, mean reward:  1.923 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.482484, mean_q: 27.986099, mean_eps: 0.100000\n","     127915/2000000000: episode: 3424, duration: 3.689s, episode steps:  31, steps per second:   8, episode reward: 106.600, mean reward:  3.439 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.857623, mean_q: 27.384936, mean_eps: 0.100000\n","     127945/2000000000: episode: 3425, duration: 3.808s, episode steps:  30, steps per second:   8, episode reward: -15.600, mean reward: -0.520 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 66.686696, mean_q: 27.808781, mean_eps: 0.100000\n","     127985/2000000000: episode: 3426, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: 20.400, mean reward:  0.510 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 70.270204, mean_q: 27.745442, mean_eps: 0.100000\n","     128024/2000000000: episode: 3427, duration: 4.984s, episode steps:  39, steps per second:   8, episode reward: -99.100, mean reward: -2.541 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 78.624825, mean_q: 28.033288, mean_eps: 0.100000\n","     128064/2000000000: episode: 3428, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: -20.800, mean reward: -0.520 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.745297, mean_q: 27.534368, mean_eps: 0.100000\n","     128104/2000000000: episode: 3429, duration: 5.096s, episode steps:  40, steps per second:   8, episode reward: -9.400, mean reward: -0.235 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 74.478301, mean_q: 28.051134, mean_eps: 0.100000\n","     128144/2000000000: episode: 3430, duration: 4.748s, episode steps:  40, steps per second:   8, episode reward: 70.800, mean reward:  1.770 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 65.927457, mean_q: 27.801344, mean_eps: 0.100000\n","     128172/2000000000: episode: 3431, duration: 3.596s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 70.702291, mean_q: 28.251946, mean_eps: 0.100000\n","     128207/2000000000: episode: 3432, duration: 4.444s, episode steps:  35, steps per second:   8, episode reward: 27.100, mean reward:  0.774 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.422105, mean_q: 27.926826, mean_eps: 0.100000\n","     128240/2000000000: episode: 3433, duration: 4.120s, episode steps:  33, steps per second:   8, episode reward: -27.600, mean reward: -0.836 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 71.289054, mean_q: 27.972849, mean_eps: 0.100000\n","     128272/2000000000: episode: 3434, duration: 4.238s, episode steps:  32, steps per second:   8, episode reward: -107.200, mean reward: -3.350 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 66.519755, mean_q: 28.384009, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     128303/2000000000: episode: 3435, duration: 4.108s, episode steps:  31, steps per second:   8, episode reward: 45.200, mean reward:  1.458 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 74.072228, mean_q: 26.991991, mean_eps: 0.100000\n","     128343/2000000000: episode: 3436, duration: 4.833s, episode steps:  40, steps per second:   8, episode reward: -81.900, mean reward: -2.047 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.356826, mean_q: 27.276602, mean_eps: 0.100000\n","     128380/2000000000: episode: 3437, duration: 4.529s, episode steps:  37, steps per second:   8, episode reward: 86.400, mean reward:  2.335 [-20.000, 19.000], mean action: 1.027 [0.000, 2.000],  loss: 71.731894, mean_q: 27.453758, mean_eps: 0.100000\n","     128420/2000000000: episode: 3438, duration: 4.894s, episode steps:  40, steps per second:   8, episode reward: 90.900, mean reward:  2.273 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.705123, mean_q: 28.154226, mean_eps: 0.100000\n","     128460/2000000000: episode: 3439, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward: 57.600, mean reward:  1.440 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 70.368620, mean_q: 27.224539, mean_eps: 0.100000\n","     128488/2000000000: episode: 3440, duration: 3.597s, episode steps:  28, steps per second:   8, episode reward: -28.900, mean reward: -1.032 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 65.566276, mean_q: 28.630258, mean_eps: 0.100000\n","     128518/2000000000: episode: 3441, duration: 3.733s, episode steps:  30, steps per second:   8, episode reward: -111.000, mean reward: -3.700 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 69.888153, mean_q: 28.119064, mean_eps: 0.100000\n","     128547/2000000000: episode: 3442, duration: 3.566s, episode steps:  29, steps per second:   8, episode reward: -0.500, mean reward: -0.017 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 68.047599, mean_q: 28.210019, mean_eps: 0.100000\n","     128582/2000000000: episode: 3443, duration: 4.295s, episode steps:  35, steps per second:   8, episode reward: -15.000, mean reward: -0.429 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.130598, mean_q: 27.717961, mean_eps: 0.100000\n","     128622/2000000000: episode: 3444, duration: 4.720s, episode steps:  40, steps per second:   8, episode reward: 115.200, mean reward:  2.880 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.511446, mean_q: 27.441809, mean_eps: 0.100000\n","     128650/2000000000: episode: 3445, duration: 3.452s, episode steps:  28, steps per second:   8, episode reward: -34.900, mean reward: -1.246 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 66.908133, mean_q: 27.999426, mean_eps: 0.100000\n","     128690/2000000000: episode: 3446, duration: 4.928s, episode steps:  40, steps per second:   8, episode reward:  8.900, mean reward:  0.222 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.675438, mean_q: 27.174385, mean_eps: 0.100000\n","     128725/2000000000: episode: 3447, duration: 4.371s, episode steps:  35, steps per second:   8, episode reward: -2.500, mean reward: -0.071 [-20.000, 18.000], mean action: 1.343 [0.000, 2.000],  loss: 67.293886, mean_q: 27.630098, mean_eps: 0.100000\n","     128757/2000000000: episode: 3448, duration: 4.106s, episode steps:  32, steps per second:   8, episode reward: 86.900, mean reward:  2.716 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 66.528135, mean_q: 28.613265, mean_eps: 0.100000\n","     128797/2000000000: episode: 3449, duration: 4.771s, episode steps:  40, steps per second:   8, episode reward: -21.500, mean reward: -0.538 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 63.209229, mean_q: 27.917746, mean_eps: 0.100000\n","     128832/2000000000: episode: 3450, duration: 4.358s, episode steps:  35, steps per second:   8, episode reward: 50.500, mean reward:  1.443 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.996667, mean_q: 27.872512, mean_eps: 0.100000\n","     128864/2000000000: episode: 3451, duration: 4.069s, episode steps:  32, steps per second:   8, episode reward:  7.300, mean reward:  0.228 [-20.000, 18.000], mean action: 1.281 [0.000, 2.000],  loss: 78.049660, mean_q: 27.882932, mean_eps: 0.100000\n","     128904/2000000000: episode: 3452, duration: 4.926s, episode steps:  40, steps per second:   8, episode reward: 31.100, mean reward:  0.778 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.566936, mean_q: 28.509558, mean_eps: 0.100000\n","     128942/2000000000: episode: 3453, duration: 4.721s, episode steps:  38, steps per second:   8, episode reward: 81.900, mean reward:  2.155 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 73.585220, mean_q: 28.668082, mean_eps: 0.100000\n","     128981/2000000000: episode: 3454, duration: 4.836s, episode steps:  39, steps per second:   8, episode reward: -63.300, mean reward: -1.623 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 75.540581, mean_q: 27.198971, mean_eps: 0.100000\n","     129015/2000000000: episode: 3455, duration: 4.081s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 67.578602, mean_q: 27.702973, mean_eps: 0.100000\n","     129045/2000000000: episode: 3456, duration: 3.683s, episode steps:  30, steps per second:   8, episode reward: -84.500, mean reward: -2.817 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 70.126430, mean_q: 28.455951, mean_eps: 0.100000\n","     129085/2000000000: episode: 3457, duration: 4.942s, episode steps:  40, steps per second:   8, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.952052, mean_q: 27.722194, mean_eps: 0.100000\n","     129117/2000000000: episode: 3458, duration: 4.059s, episode steps:  32, steps per second:   8, episode reward: 93.100, mean reward:  2.909 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 68.910895, mean_q: 27.423757, mean_eps: 0.100000\n","     129148/2000000000: episode: 3459, duration: 3.768s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 72.323037, mean_q: 27.953819, mean_eps: 0.100000\n","     129188/2000000000: episode: 3460, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: 50.100, mean reward:  1.252 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.692512, mean_q: 27.754015, mean_eps: 0.100000\n","     129211/2000000000: episode: 3461, duration: 2.820s, episode steps:  23, steps per second:   8, episode reward: 18.000, mean reward:  0.783 [-20.000, 18.000], mean action: 0.783 [0.000, 2.000],  loss: 74.936635, mean_q: 26.935188, mean_eps: 0.100000\n","     129248/2000000000: episode: 3462, duration: 4.412s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.806418, mean_q: 28.059868, mean_eps: 0.100000\n","     129288/2000000000: episode: 3463, duration: 4.871s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 67.305297, mean_q: 27.774559, mean_eps: 0.100000\n","     129320/2000000000: episode: 3464, duration: 3.933s, episode steps:  32, steps per second:   8, episode reward: -39.500, mean reward: -1.234 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 73.082090, mean_q: 28.007615, mean_eps: 0.100000\n","     129350/2000000000: episode: 3465, duration: 3.911s, episode steps:  30, steps per second:   8, episode reward: 11.900, mean reward:  0.397 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.493374, mean_q: 27.932093, mean_eps: 0.100000\n","     129386/2000000000: episode: 3466, duration: 4.712s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.542272, mean_q: 27.478307, mean_eps: 0.100000\n","     129426/2000000000: episode: 3467, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.810989, mean_q: 27.705591, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     129464/2000000000: episode: 3468, duration: 4.826s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 71.494103, mean_q: 27.882779, mean_eps: 0.100000\n","     129502/2000000000: episode: 3469, duration: 4.787s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.846915, mean_q: 27.014011, mean_eps: 0.100000\n","     129542/2000000000: episode: 3470, duration: 5.076s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.200284, mean_q: 27.548946, mean_eps: 0.100000\n","     129582/2000000000: episode: 3471, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: -170.000, mean reward: -4.250 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.362358, mean_q: 27.978712, mean_eps: 0.100000\n","     129619/2000000000: episode: 3472, duration: 4.500s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 70.643303, mean_q: 27.550741, mean_eps: 0.100000\n","     129657/2000000000: episode: 3473, duration: 4.609s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 72.311069, mean_q: 27.334123, mean_eps: 0.100000\n","     129697/2000000000: episode: 3474, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: -6.700, mean reward: -0.167 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.733041, mean_q: 27.716164, mean_eps: 0.100000\n","     129728/2000000000: episode: 3475, duration: 3.833s, episode steps:  31, steps per second:   8, episode reward: 72.700, mean reward:  2.345 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 64.379043, mean_q: 28.100882, mean_eps: 0.100000\n","     129768/2000000000: episode: 3476, duration: 4.881s, episode steps:  40, steps per second:   8, episode reward: 96.400, mean reward:  2.410 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.126443, mean_q: 27.926085, mean_eps: 0.100000\n","     129803/2000000000: episode: 3477, duration: 4.265s, episode steps:  35, steps per second:   8, episode reward: 103.200, mean reward:  2.949 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 70.149554, mean_q: 27.827214, mean_eps: 0.100000\n","     129843/2000000000: episode: 3478, duration: 4.731s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 64.333951, mean_q: 28.671425, mean_eps: 0.100000\n","     129871/2000000000: episode: 3479, duration: 3.556s, episode steps:  28, steps per second:   8, episode reward: -59.300, mean reward: -2.118 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 73.949514, mean_q: 27.262522, mean_eps: 0.100000\n","     129909/2000000000: episode: 3480, duration: 4.919s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 70.447405, mean_q: 27.692562, mean_eps: 0.100000\n","     129939/2000000000: episode: 3481, duration: 3.907s, episode steps:  30, steps per second:   8, episode reward: -31.400, mean reward: -1.047 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.187027, mean_q: 28.967108, mean_eps: 0.100000\n","     129971/2000000000: episode: 3482, duration: 4.101s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.909364, mean_q: 27.555953, mean_eps: 0.100000\n","     130011/2000000000: episode: 3483, duration: 5.140s, episode steps:  40, steps per second:   8, episode reward: 94.100, mean reward:  2.352 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.724781, mean_q: 27.290447, mean_eps: 0.100000\n","     130051/2000000000: episode: 3484, duration: 5.023s, episode steps:  40, steps per second:   8, episode reward: 62.600, mean reward:  1.565 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 80.629648, mean_q: 28.869682, mean_eps: 0.100000\n","     130084/2000000000: episode: 3485, duration: 4.025s, episode steps:  33, steps per second:   8, episode reward: -96.000, mean reward: -2.909 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.850448, mean_q: 28.983928, mean_eps: 0.100000\n","     130114/2000000000: episode: 3486, duration: 3.581s, episode steps:  30, steps per second:   8, episode reward: 111.100, mean reward:  3.703 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 75.148160, mean_q: 28.624851, mean_eps: 0.100000\n","     130147/2000000000: episode: 3487, duration: 4.085s, episode steps:  33, steps per second:   8, episode reward: -112.800, mean reward: -3.418 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 76.720657, mean_q: 28.772563, mean_eps: 0.100000\n","     130185/2000000000: episode: 3488, duration: 4.549s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 80.593136, mean_q: 29.719389, mean_eps: 0.100000\n","     130225/2000000000: episode: 3489, duration: 4.847s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.599366, mean_q: 29.325388, mean_eps: 0.100000\n","     130262/2000000000: episode: 3490, duration: 4.466s, episode steps:  37, steps per second:   8, episode reward: -30.500, mean reward: -0.824 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.470207, mean_q: 28.580415, mean_eps: 0.100000\n","     130302/2000000000: episode: 3491, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: 93.500, mean reward:  2.337 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 76.704704, mean_q: 29.442271, mean_eps: 0.100000\n","     130336/2000000000: episode: 3492, duration: 4.218s, episode steps:  34, steps per second:   8, episode reward: 73.400, mean reward:  2.159 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 72.904858, mean_q: 28.791768, mean_eps: 0.100000\n","     130376/2000000000: episode: 3493, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 112.300, mean reward:  2.808 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.987352, mean_q: 28.633127, mean_eps: 0.100000\n","     130408/2000000000: episode: 3494, duration: 3.917s, episode steps:  32, steps per second:   8, episode reward: 106.200, mean reward:  3.319 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 74.454226, mean_q: 28.794435, mean_eps: 0.100000\n","     130447/2000000000: episode: 3495, duration: 5.131s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 73.820971, mean_q: 29.271621, mean_eps: 0.100000\n","     130487/2000000000: episode: 3496, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: -74.600, mean reward: -1.865 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.416209, mean_q: 28.710046, mean_eps: 0.100000\n","     130518/2000000000: episode: 3497, duration: 3.975s, episode steps:  31, steps per second:   8, episode reward: 62.400, mean reward:  2.013 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.335025, mean_q: 28.969058, mean_eps: 0.100000\n","     130558/2000000000: episode: 3498, duration: 5.401s, episode steps:  40, steps per second:   7, episode reward: 46.300, mean reward:  1.157 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.044600, mean_q: 28.346018, mean_eps: 0.100000\n","     130598/2000000000: episode: 3499, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: 53.900, mean reward:  1.347 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 70.970780, mean_q: 28.814894, mean_eps: 0.100000\n","     130638/2000000000: episode: 3500, duration: 4.978s, episode steps:  40, steps per second:   8, episode reward: 23.800, mean reward:  0.595 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.778811, mean_q: 29.027961, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     130678/2000000000: episode: 3501, duration: 5.163s, episode steps:  40, steps per second:   8, episode reward: -90.400, mean reward: -2.260 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.137144, mean_q: 28.733182, mean_eps: 0.100000\n","     130718/2000000000: episode: 3502, duration: 4.829s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.231147, mean_q: 28.946511, mean_eps: 0.100000\n","     130758/2000000000: episode: 3503, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.324206, mean_q: 29.196026, mean_eps: 0.100000\n","     130788/2000000000: episode: 3504, duration: 3.865s, episode steps:  30, steps per second:   8, episode reward:  1.100, mean reward:  0.037 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 74.995880, mean_q: 29.694237, mean_eps: 0.100000\n","     130828/2000000000: episode: 3505, duration: 5.339s, episode steps:  40, steps per second:   7, episode reward: -24.400, mean reward: -0.610 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.790882, mean_q: 28.804315, mean_eps: 0.100000\n","     130864/2000000000: episode: 3506, duration: 4.458s, episode steps:  36, steps per second:   8, episode reward:  9.100, mean reward:  0.253 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.023503, mean_q: 28.785872, mean_eps: 0.100000\n","     130904/2000000000: episode: 3507, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.873143, mean_q: 28.814031, mean_eps: 0.100000\n","     130944/2000000000: episode: 3508, duration: 5.161s, episode steps:  40, steps per second:   8, episode reward: 165.800, mean reward:  4.145 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.738853, mean_q: 28.965375, mean_eps: 0.100000\n","     130974/2000000000: episode: 3509, duration: 3.895s, episode steps:  30, steps per second:   8, episode reward: -19.800, mean reward: -0.660 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.181481, mean_q: 28.305450, mean_eps: 0.100000\n","     131014/2000000000: episode: 3510, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward: 75.300, mean reward:  1.882 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.853669, mean_q: 29.085917, mean_eps: 0.100000\n","     131048/2000000000: episode: 3511, duration: 4.252s, episode steps:  34, steps per second:   8, episode reward: 53.500, mean reward:  1.574 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.050922, mean_q: 28.487246, mean_eps: 0.100000\n","     131088/2000000000: episode: 3512, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: 109.700, mean reward:  2.742 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 69.130756, mean_q: 28.932079, mean_eps: 0.100000\n","     131120/2000000000: episode: 3513, duration: 3.985s, episode steps:  32, steps per second:   8, episode reward: 99.400, mean reward:  3.106 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 75.150162, mean_q: 29.211965, mean_eps: 0.100000\n","     131157/2000000000: episode: 3514, duration: 4.641s, episode steps:  37, steps per second:   8, episode reward: 99.200, mean reward:  2.681 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 69.453399, mean_q: 29.447424, mean_eps: 0.100000\n","     131197/2000000000: episode: 3515, duration: 4.648s, episode steps:  40, steps per second:   9, episode reward: 129.000, mean reward:  3.225 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 72.352921, mean_q: 28.824076, mean_eps: 0.100000\n","     131237/2000000000: episode: 3516, duration: 4.899s, episode steps:  40, steps per second:   8, episode reward: 18.100, mean reward:  0.452 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.899189, mean_q: 29.622826, mean_eps: 0.100000\n","     131266/2000000000: episode: 3517, duration: 3.527s, episode steps:  29, steps per second:   8, episode reward: 48.800, mean reward:  1.683 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 74.541311, mean_q: 28.674121, mean_eps: 0.100000\n","     131298/2000000000: episode: 3518, duration: 4.065s, episode steps:  32, steps per second:   8, episode reward: 90.000, mean reward:  2.813 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 75.536278, mean_q: 29.607244, mean_eps: 0.100000\n","     131338/2000000000: episode: 3519, duration: 4.798s, episode steps:  40, steps per second:   8, episode reward: -25.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.218453, mean_q: 29.086937, mean_eps: 0.100000\n","     131378/2000000000: episode: 3520, duration: 4.956s, episode steps:  40, steps per second:   8, episode reward: 58.600, mean reward:  1.465 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.683874, mean_q: 29.356714, mean_eps: 0.100000\n","     131415/2000000000: episode: 3521, duration: 4.433s, episode steps:  37, steps per second:   8, episode reward: -0.000, mean reward: -0.000 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 75.066557, mean_q: 28.623795, mean_eps: 0.100000\n","     131446/2000000000: episode: 3522, duration: 3.766s, episode steps:  31, steps per second:   8, episode reward: 13.700, mean reward:  0.442 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.400953, mean_q: 29.194699, mean_eps: 0.100000\n","     131485/2000000000: episode: 3523, duration: 4.639s, episode steps:  39, steps per second:   8, episode reward: 171.600, mean reward:  4.400 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 73.498290, mean_q: 28.655005, mean_eps: 0.100000\n","     131525/2000000000: episode: 3524, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.295929, mean_q: 28.874709, mean_eps: 0.100000\n","     131565/2000000000: episode: 3525, duration: 4.782s, episode steps:  40, steps per second:   8, episode reward: 120.000, mean reward:  3.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.831595, mean_q: 29.113139, mean_eps: 0.100000\n","     131605/2000000000: episode: 3526, duration: 4.833s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.094710, mean_q: 29.158765, mean_eps: 0.100000\n","     131642/2000000000: episode: 3527, duration: 4.542s, episode steps:  37, steps per second:   8, episode reward: -248.000, mean reward: -6.703 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 77.889203, mean_q: 28.557625, mean_eps: 0.100000\n","     131682/2000000000: episode: 3528, duration: 4.891s, episode steps:  40, steps per second:   8, episode reward: 27.300, mean reward:  0.682 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.047799, mean_q: 28.648797, mean_eps: 0.100000\n","     131713/2000000000: episode: 3529, duration: 3.686s, episode steps:  31, steps per second:   8, episode reward: 113.000, mean reward:  3.645 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.950538, mean_q: 29.099353, mean_eps: 0.100000\n","     131753/2000000000: episode: 3530, duration: 4.956s, episode steps:  40, steps per second:   8, episode reward: -42.000, mean reward: -1.050 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 72.931209, mean_q: 29.006856, mean_eps: 0.100000\n","     131793/2000000000: episode: 3531, duration: 5.247s, episode steps:  40, steps per second:   8, episode reward: -38.500, mean reward: -0.963 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.985302, mean_q: 28.109477, mean_eps: 0.100000\n","     131826/2000000000: episode: 3532, duration: 4.202s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 74.059766, mean_q: 28.942988, mean_eps: 0.100000\n","     131863/2000000000: episode: 3533, duration: 4.695s, episode steps:  37, steps per second:   8, episode reward: 125.300, mean reward:  3.386 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 75.146756, mean_q: 28.989293, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     131903/2000000000: episode: 3534, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.893862, mean_q: 28.581927, mean_eps: 0.100000\n","     131943/2000000000: episode: 3535, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: -126.000, mean reward: -3.150 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 77.851136, mean_q: 29.037603, mean_eps: 0.100000\n","     131972/2000000000: episode: 3536, duration: 3.670s, episode steps:  29, steps per second:   8, episode reward: -59.800, mean reward: -2.062 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 70.420169, mean_q: 29.723609, mean_eps: 0.100000\n","     132012/2000000000: episode: 3537, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -59.700, mean reward: -1.493 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 65.868079, mean_q: 29.132827, mean_eps: 0.100000\n","     132052/2000000000: episode: 3538, duration: 5.153s, episode steps:  40, steps per second:   8, episode reward: 28.400, mean reward:  0.710 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.919722, mean_q: 29.690852, mean_eps: 0.100000\n","     132092/2000000000: episode: 3539, duration: 5.313s, episode steps:  40, steps per second:   8, episode reward: 22.300, mean reward:  0.557 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.855874, mean_q: 28.683842, mean_eps: 0.100000\n","     132121/2000000000: episode: 3540, duration: 3.701s, episode steps:  29, steps per second:   8, episode reward: 95.600, mean reward:  3.297 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 69.961532, mean_q: 29.930732, mean_eps: 0.100000\n","     132161/2000000000: episode: 3541, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 79.510593, mean_q: 28.777040, mean_eps: 0.100000\n","     132201/2000000000: episode: 3542, duration: 5.084s, episode steps:  40, steps per second:   8, episode reward: 73.100, mean reward:  1.827 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.915869, mean_q: 28.772123, mean_eps: 0.100000\n","     132234/2000000000: episode: 3543, duration: 4.145s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 78.473508, mean_q: 29.517027, mean_eps: 0.100000\n","     132269/2000000000: episode: 3544, duration: 4.419s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 76.326563, mean_q: 29.693025, mean_eps: 0.100000\n","     132299/2000000000: episode: 3545, duration: 4.303s, episode steps:  30, steps per second:   7, episode reward: 181.000, mean reward:  6.033 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 64.725308, mean_q: 29.403732, mean_eps: 0.100000\n","     132332/2000000000: episode: 3546, duration: 4.624s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 75.519985, mean_q: 29.190878, mean_eps: 0.100000\n","     132372/2000000000: episode: 3547, duration: 5.468s, episode steps:  40, steps per second:   7, episode reward: 158.000, mean reward:  3.950 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 77.877531, mean_q: 28.527770, mean_eps: 0.100000\n","     132412/2000000000: episode: 3548, duration: 5.485s, episode steps:  40, steps per second:   7, episode reward: -40.500, mean reward: -1.013 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.705675, mean_q: 28.201391, mean_eps: 0.100000\n","     132444/2000000000: episode: 3549, duration: 4.195s, episode steps:  32, steps per second:   8, episode reward: 94.700, mean reward:  2.959 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.467073, mean_q: 29.096484, mean_eps: 0.100000\n","     132484/2000000000: episode: 3550, duration: 5.112s, episode steps:  40, steps per second:   8, episode reward: -50.200, mean reward: -1.255 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.787870, mean_q: 29.655041, mean_eps: 0.100000\n","     132524/2000000000: episode: 3551, duration: 5.048s, episode steps:  40, steps per second:   8, episode reward: 47.400, mean reward:  1.185 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.303221, mean_q: 29.149598, mean_eps: 0.100000\n","     132564/2000000000: episode: 3552, duration: 5.595s, episode steps:  40, steps per second:   7, episode reward: 81.500, mean reward:  2.038 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.517160, mean_q: 28.389616, mean_eps: 0.100000\n","     132599/2000000000: episode: 3553, duration: 4.418s, episode steps:  35, steps per second:   8, episode reward: 27.900, mean reward:  0.797 [-20.000, 19.600], mean action: 1.086 [0.000, 2.000],  loss: 66.290984, mean_q: 28.280685, mean_eps: 0.100000\n","     132639/2000000000: episode: 3554, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: 96.400, mean reward:  2.410 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 70.470034, mean_q: 29.310126, mean_eps: 0.100000\n","     132664/2000000000: episode: 3555, duration: 3.328s, episode steps:  25, steps per second:   8, episode reward: 49.900, mean reward:  1.996 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 76.419451, mean_q: 28.969300, mean_eps: 0.100000\n","     132693/2000000000: episode: 3556, duration: 3.706s, episode steps:  29, steps per second:   8, episode reward: 58.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.278307, mean_q: 28.502287, mean_eps: 0.100000\n","     132732/2000000000: episode: 3557, duration: 4.895s, episode steps:  39, steps per second:   8, episode reward: 76.400, mean reward:  1.959 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 70.662874, mean_q: 28.943089, mean_eps: 0.100000\n","     132772/2000000000: episode: 3558, duration: 5.115s, episode steps:  40, steps per second:   8, episode reward: -41.600, mean reward: -1.040 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.447073, mean_q: 28.989378, mean_eps: 0.100000\n","     132812/2000000000: episode: 3559, duration: 5.296s, episode steps:  40, steps per second:   8, episode reward: 28.100, mean reward:  0.703 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 73.887070, mean_q: 28.805785, mean_eps: 0.100000\n","     132852/2000000000: episode: 3560, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: -11.300, mean reward: -0.283 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 67.935661, mean_q: 28.434748, mean_eps: 0.100000\n","     132892/2000000000: episode: 3561, duration: 5.236s, episode steps:  40, steps per second:   8, episode reward: 89.600, mean reward:  2.240 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.294392, mean_q: 29.694373, mean_eps: 0.100000\n","     132932/2000000000: episode: 3562, duration: 5.244s, episode steps:  40, steps per second:   8, episode reward: 39.300, mean reward:  0.982 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 74.665841, mean_q: 28.933234, mean_eps: 0.100000\n","     132970/2000000000: episode: 3563, duration: 4.680s, episode steps:  38, steps per second:   8, episode reward: 113.000, mean reward:  2.974 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 69.677882, mean_q: 29.351445, mean_eps: 0.100000\n","     133006/2000000000: episode: 3564, duration: 4.357s, episode steps:  36, steps per second:   8, episode reward: 124.500, mean reward:  3.458 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 73.583532, mean_q: 29.191396, mean_eps: 0.100000\n","     133046/2000000000: episode: 3565, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: 173.100, mean reward:  4.328 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.273698, mean_q: 29.151257, mean_eps: 0.100000\n","     133082/2000000000: episode: 3566, duration: 4.499s, episode steps:  36, steps per second:   8, episode reward: -59.900, mean reward: -1.664 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 76.044078, mean_q: 27.984976, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     133122/2000000000: episode: 3567, duration: 4.895s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.331558, mean_q: 29.124863, mean_eps: 0.100000\n","     133162/2000000000: episode: 3568, duration: 4.817s, episode steps:  40, steps per second:   8, episode reward:  9.200, mean reward:  0.230 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.046534, mean_q: 28.631126, mean_eps: 0.100000\n","     133187/2000000000: episode: 3569, duration: 3.080s, episode steps:  25, steps per second:   8, episode reward: 59.500, mean reward:  2.380 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 65.039161, mean_q: 28.378937, mean_eps: 0.100000\n","     133227/2000000000: episode: 3570, duration: 4.855s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.513534, mean_q: 29.034207, mean_eps: 0.100000\n","     133256/2000000000: episode: 3571, duration: 3.634s, episode steps:  29, steps per second:   8, episode reward: -99.200, mean reward: -3.421 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 71.976727, mean_q: 29.090937, mean_eps: 0.100000\n","     133291/2000000000: episode: 3572, duration: 4.296s, episode steps:  35, steps per second:   8, episode reward:  1.900, mean reward:  0.054 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 70.514436, mean_q: 29.320516, mean_eps: 0.100000\n","     133319/2000000000: episode: 3573, duration: 3.902s, episode steps:  28, steps per second:   7, episode reward: -58.000, mean reward: -2.071 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 73.557851, mean_q: 29.037968, mean_eps: 0.100000\n","     133355/2000000000: episode: 3574, duration: 4.617s, episode steps:  36, steps per second:   8, episode reward: -107.800, mean reward: -2.994 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 73.690670, mean_q: 28.621157, mean_eps: 0.100000\n","     133395/2000000000: episode: 3575, duration: 4.908s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 66.067504, mean_q: 29.593055, mean_eps: 0.100000\n","     133435/2000000000: episode: 3576, duration: 4.722s, episode steps:  40, steps per second:   8, episode reward: 100.000, mean reward:  2.500 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 78.848044, mean_q: 28.433751, mean_eps: 0.100000\n","     133475/2000000000: episode: 3577, duration: 4.685s, episode steps:  40, steps per second:   9, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.118104, mean_q: 28.819363, mean_eps: 0.100000\n","     133515/2000000000: episode: 3578, duration: 5.032s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 76.369755, mean_q: 28.689768, mean_eps: 0.100000\n","     133555/2000000000: episode: 3579, duration: 4.917s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.158101, mean_q: 29.069983, mean_eps: 0.100000\n","     133595/2000000000: episode: 3580, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.440794, mean_q: 28.951742, mean_eps: 0.100000\n","     133635/2000000000: episode: 3581, duration: 5.601s, episode steps:  40, steps per second:   7, episode reward: 139.900, mean reward:  3.498 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.147792, mean_q: 28.934470, mean_eps: 0.100000\n","     133673/2000000000: episode: 3582, duration: 5.137s, episode steps:  38, steps per second:   7, episode reward: -7.200, mean reward: -0.189 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 75.211557, mean_q: 28.831161, mean_eps: 0.100000\n","     133713/2000000000: episode: 3583, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward:  8.300, mean reward:  0.208 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 69.987262, mean_q: 29.324011, mean_eps: 0.100000\n","     133753/2000000000: episode: 3584, duration: 5.564s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.263649, mean_q: 29.269532, mean_eps: 0.100000\n","     133793/2000000000: episode: 3585, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: -37.100, mean reward: -0.928 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.334783, mean_q: 29.122548, mean_eps: 0.100000\n","     133833/2000000000: episode: 3586, duration: 4.726s, episode steps:  40, steps per second:   8, episode reward: 40.200, mean reward:  1.005 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.453998, mean_q: 29.280305, mean_eps: 0.100000\n","     133869/2000000000: episode: 3587, duration: 4.516s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 77.852213, mean_q: 29.089050, mean_eps: 0.100000\n","     133903/2000000000: episode: 3588, duration: 4.484s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 70.605343, mean_q: 29.516113, mean_eps: 0.100000\n","     133929/2000000000: episode: 3589, duration: 3.322s, episode steps:  26, steps per second:   8, episode reward: 73.000, mean reward:  2.808 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 67.579193, mean_q: 29.457438, mean_eps: 0.100000\n","     133969/2000000000: episode: 3590, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.413517, mean_q: 28.659104, mean_eps: 0.100000\n","     134009/2000000000: episode: 3591, duration: 5.320s, episode steps:  40, steps per second:   8, episode reward: 40.300, mean reward:  1.007 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.761382, mean_q: 29.789067, mean_eps: 0.100000\n","     134049/2000000000: episode: 3592, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.059910, mean_q: 29.404088, mean_eps: 0.100000\n","     134087/2000000000: episode: 3593, duration: 4.940s, episode steps:  38, steps per second:   8, episode reward: 10.700, mean reward:  0.282 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 74.228489, mean_q: 28.301295, mean_eps: 0.100000\n","     134126/2000000000: episode: 3594, duration: 5.155s, episode steps:  39, steps per second:   8, episode reward: -151.000, mean reward: -3.872 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 74.681535, mean_q: 28.890534, mean_eps: 0.100000\n","     134166/2000000000: episode: 3595, duration: 5.394s, episode steps:  40, steps per second:   7, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.084899, mean_q: 29.406292, mean_eps: 0.100000\n","     134205/2000000000: episode: 3596, duration: 5.088s, episode steps:  39, steps per second:   8, episode reward: -64.500, mean reward: -1.654 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 73.346393, mean_q: 29.295034, mean_eps: 0.100000\n","     134245/2000000000: episode: 3597, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: 17.400, mean reward:  0.435 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 68.589207, mean_q: 28.993929, mean_eps: 0.100000\n","     134274/2000000000: episode: 3598, duration: 3.908s, episode steps:  29, steps per second:   7, episode reward: 33.200, mean reward:  1.145 [-20.000, 18.300], mean action: 0.828 [0.000, 2.000],  loss: 75.415103, mean_q: 29.705398, mean_eps: 0.100000\n","     134313/2000000000: episode: 3599, duration: 5.063s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 70.192721, mean_q: 29.308670, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     134348/2000000000: episode: 3600, duration: 4.517s, episode steps:  35, steps per second:   8, episode reward: -166.100, mean reward: -4.746 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 73.491754, mean_q: 29.299064, mean_eps: 0.100000\n","     134388/2000000000: episode: 3601, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: 66.400, mean reward:  1.660 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.280340, mean_q: 29.055217, mean_eps: 0.100000\n","     134428/2000000000: episode: 3602, duration: 5.176s, episode steps:  40, steps per second:   8, episode reward: 55.400, mean reward:  1.385 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.077512, mean_q: 28.429134, mean_eps: 0.100000\n","     134463/2000000000: episode: 3603, duration: 4.425s, episode steps:  35, steps per second:   8, episode reward: 115.300, mean reward:  3.294 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 74.877833, mean_q: 28.813804, mean_eps: 0.100000\n","     134503/2000000000: episode: 3604, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: 205.800, mean reward:  5.145 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.454848, mean_q: 29.463023, mean_eps: 0.100000\n","     134543/2000000000: episode: 3605, duration: 5.110s, episode steps:  40, steps per second:   8, episode reward: 76.700, mean reward:  1.918 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 75.887133, mean_q: 28.731201, mean_eps: 0.100000\n","     134583/2000000000: episode: 3606, duration: 4.978s, episode steps:  40, steps per second:   8, episode reward: -44.000, mean reward: -1.100 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.036552, mean_q: 29.163853, mean_eps: 0.100000\n","     134613/2000000000: episode: 3607, duration: 3.827s, episode steps:  30, steps per second:   8, episode reward: -123.000, mean reward: -4.100 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 66.348002, mean_q: 29.276042, mean_eps: 0.100000\n","     134648/2000000000: episode: 3608, duration: 4.132s, episode steps:  35, steps per second:   8, episode reward: 169.000, mean reward:  4.829 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 75.055920, mean_q: 29.039783, mean_eps: 0.100000\n","     134683/2000000000: episode: 3609, duration: 4.639s, episode steps:  35, steps per second:   8, episode reward: 71.700, mean reward:  2.049 [-20.000, 18.600], mean action: 1.143 [0.000, 2.000],  loss: 76.798721, mean_q: 29.026953, mean_eps: 0.100000\n","     134722/2000000000: episode: 3610, duration: 5.362s, episode steps:  39, steps per second:   7, episode reward: -17.600, mean reward: -0.451 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 71.323963, mean_q: 28.714653, mean_eps: 0.100000\n","     134756/2000000000: episode: 3611, duration: 4.320s, episode steps:  34, steps per second:   8, episode reward: -3.500, mean reward: -0.103 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 68.554647, mean_q: 29.871956, mean_eps: 0.100000\n","     134796/2000000000: episode: 3612, duration: 4.974s, episode steps:  40, steps per second:   8, episode reward: 18.400, mean reward:  0.460 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.053616, mean_q: 29.390333, mean_eps: 0.100000\n","     134836/2000000000: episode: 3613, duration: 4.844s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.099096, mean_q: 29.140053, mean_eps: 0.100000\n","     134876/2000000000: episode: 3614, duration: 4.826s, episode steps:  40, steps per second:   8, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 74.502830, mean_q: 29.318189, mean_eps: 0.100000\n","     134916/2000000000: episode: 3615, duration: 4.752s, episode steps:  40, steps per second:   8, episode reward: 87.800, mean reward:  2.195 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.109498, mean_q: 28.927290, mean_eps: 0.100000\n","     134954/2000000000: episode: 3616, duration: 4.539s, episode steps:  38, steps per second:   8, episode reward: -74.100, mean reward: -1.950 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 74.712524, mean_q: 28.913218, mean_eps: 0.100000\n","     134993/2000000000: episode: 3617, duration: 4.538s, episode steps:  39, steps per second:   9, episode reward: 31.200, mean reward:  0.800 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 79.483215, mean_q: 29.151980, mean_eps: 0.100000\n","     135033/2000000000: episode: 3618, duration: 4.743s, episode steps:  40, steps per second:   8, episode reward: 116.900, mean reward:  2.922 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.701563, mean_q: 27.975370, mean_eps: 0.100000\n","     135066/2000000000: episode: 3619, duration: 3.899s, episode steps:  33, steps per second:   8, episode reward: 110.800, mean reward:  3.358 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 74.135455, mean_q: 29.615807, mean_eps: 0.100000\n","     135106/2000000000: episode: 3620, duration: 4.782s, episode steps:  40, steps per second:   8, episode reward: 88.200, mean reward:  2.205 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.422763, mean_q: 28.536802, mean_eps: 0.100000\n","     135146/2000000000: episode: 3621, duration: 4.921s, episode steps:  40, steps per second:   8, episode reward: 105.100, mean reward:  2.628 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.604371, mean_q: 28.924876, mean_eps: 0.100000\n","     135186/2000000000: episode: 3622, duration: 5.291s, episode steps:  40, steps per second:   8, episode reward: 14.200, mean reward:  0.355 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.248531, mean_q: 29.206720, mean_eps: 0.100000\n","     135218/2000000000: episode: 3623, duration: 4.198s, episode steps:  32, steps per second:   8, episode reward: 70.100, mean reward:  2.191 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.429266, mean_q: 29.226728, mean_eps: 0.100000\n","     135247/2000000000: episode: 3624, duration: 3.753s, episode steps:  29, steps per second:   8, episode reward: 40.900, mean reward:  1.410 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 75.922737, mean_q: 29.207498, mean_eps: 0.100000\n","     135281/2000000000: episode: 3625, duration: 4.239s, episode steps:  34, steps per second:   8, episode reward: 87.500, mean reward:  2.574 [-20.000, 19.100], mean action: 1.206 [0.000, 2.000],  loss: 77.685630, mean_q: 28.807825, mean_eps: 0.100000\n","     135320/2000000000: episode: 3626, duration: 4.545s, episode steps:  39, steps per second:   9, episode reward: 101.500, mean reward:  2.603 [-20.000, 18.100], mean action: 1.256 [0.000, 2.000],  loss: 76.095732, mean_q: 29.081881, mean_eps: 0.100000\n","     135360/2000000000: episode: 3627, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: -12.900, mean reward: -0.323 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 74.324295, mean_q: 28.599770, mean_eps: 0.100000\n","     135397/2000000000: episode: 3628, duration: 4.635s, episode steps:  37, steps per second:   8, episode reward: 49.800, mean reward:  1.346 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 74.513914, mean_q: 29.259464, mean_eps: 0.100000\n","     135437/2000000000: episode: 3629, duration: 4.977s, episode steps:  40, steps per second:   8, episode reward: -6.100, mean reward: -0.152 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.889559, mean_q: 29.527855, mean_eps: 0.100000\n","     135471/2000000000: episode: 3630, duration: 4.393s, episode steps:  34, steps per second:   8, episode reward: 45.500, mean reward:  1.338 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 70.068170, mean_q: 28.876846, mean_eps: 0.100000\n","     135511/2000000000: episode: 3631, duration: 5.231s, episode steps:  40, steps per second:   8, episode reward: 83.900, mean reward:  2.097 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.004785, mean_q: 28.368574, mean_eps: 0.100000\n","     135544/2000000000: episode: 3632, duration: 4.491s, episode steps:  33, steps per second:   7, episode reward: -31.300, mean reward: -0.948 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 74.794346, mean_q: 29.194070, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     135584/2000000000: episode: 3633, duration: 5.401s, episode steps:  40, steps per second:   7, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.166169, mean_q: 28.768713, mean_eps: 0.100000\n","     135614/2000000000: episode: 3634, duration: 3.912s, episode steps:  30, steps per second:   8, episode reward: 51.100, mean reward:  1.703 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.689333, mean_q: 29.878578, mean_eps: 0.100000\n","     135653/2000000000: episode: 3635, duration: 5.249s, episode steps:  39, steps per second:   7, episode reward: 30.300, mean reward:  0.777 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 67.821453, mean_q: 28.573396, mean_eps: 0.100000\n","     135692/2000000000: episode: 3636, duration: 5.095s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 71.872855, mean_q: 28.931622, mean_eps: 0.100000\n","     135725/2000000000: episode: 3637, duration: 4.202s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 69.357430, mean_q: 29.221794, mean_eps: 0.100000\n","     135765/2000000000: episode: 3638, duration: 5.234s, episode steps:  40, steps per second:   8, episode reward: -77.400, mean reward: -1.935 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.369115, mean_q: 29.571690, mean_eps: 0.100000\n","     135805/2000000000: episode: 3639, duration: 5.201s, episode steps:  40, steps per second:   8, episode reward: 69.400, mean reward:  1.735 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.830988, mean_q: 29.377809, mean_eps: 0.100000\n","     135845/2000000000: episode: 3640, duration: 5.015s, episode steps:  40, steps per second:   8, episode reward: -5.100, mean reward: -0.127 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.962764, mean_q: 29.739400, mean_eps: 0.100000\n","     135885/2000000000: episode: 3641, duration: 5.118s, episode steps:  40, steps per second:   8, episode reward: -35.200, mean reward: -0.880 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.969741, mean_q: 28.834611, mean_eps: 0.100000\n","     135919/2000000000: episode: 3642, duration: 4.368s, episode steps:  34, steps per second:   8, episode reward: -3.300, mean reward: -0.097 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.394160, mean_q: 29.087331, mean_eps: 0.100000\n","     135959/2000000000: episode: 3643, duration: 5.184s, episode steps:  40, steps per second:   8, episode reward: 42.100, mean reward:  1.053 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.693311, mean_q: 28.630190, mean_eps: 0.100000\n","     135999/2000000000: episode: 3644, duration: 5.212s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.680708, mean_q: 28.733906, mean_eps: 0.100000\n","     136038/2000000000: episode: 3645, duration: 4.801s, episode steps:  39, steps per second:   8, episode reward: -43.300, mean reward: -1.110 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 75.118097, mean_q: 29.631282, mean_eps: 0.100000\n","     136068/2000000000: episode: 3646, duration: 3.698s, episode steps:  30, steps per second:   8, episode reward: -45.100, mean reward: -1.503 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 70.500565, mean_q: 29.060812, mean_eps: 0.100000\n","     136108/2000000000: episode: 3647, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: -13.200, mean reward: -0.330 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.182377, mean_q: 28.611997, mean_eps: 0.100000\n","     136139/2000000000: episode: 3648, duration: 3.939s, episode steps:  31, steps per second:   8, episode reward: -31.600, mean reward: -1.019 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 72.532539, mean_q: 29.217739, mean_eps: 0.100000\n","     136179/2000000000: episode: 3649, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: 145.500, mean reward:  3.638 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.247083, mean_q: 29.916384, mean_eps: 0.100000\n","     136219/2000000000: episode: 3650, duration: 5.097s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 72.100424, mean_q: 29.498742, mean_eps: 0.100000\n","     136256/2000000000: episode: 3651, duration: 4.769s, episode steps:  37, steps per second:   8, episode reward: -3.400, mean reward: -0.092 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 74.143643, mean_q: 29.232877, mean_eps: 0.100000\n","     136288/2000000000: episode: 3652, duration: 4.078s, episode steps:  32, steps per second:   8, episode reward: -16.700, mean reward: -0.522 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 75.675355, mean_q: 28.456657, mean_eps: 0.100000\n","     136328/2000000000: episode: 3653, duration: 4.971s, episode steps:  40, steps per second:   8, episode reward: 88.900, mean reward:  2.223 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.670941, mean_q: 29.584333, mean_eps: 0.100000\n","     136355/2000000000: episode: 3654, duration: 3.487s, episode steps:  27, steps per second:   8, episode reward: 14.700, mean reward:  0.544 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 69.746674, mean_q: 29.292331, mean_eps: 0.100000\n","     136386/2000000000: episode: 3655, duration: 3.941s, episode steps:  31, steps per second:   8, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 70.977232, mean_q: 29.269852, mean_eps: 0.100000\n","     136421/2000000000: episode: 3656, duration: 4.562s, episode steps:  35, steps per second:   8, episode reward: -3.700, mean reward: -0.106 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.545003, mean_q: 28.561690, mean_eps: 0.100000\n","     136458/2000000000: episode: 3657, duration: 4.730s, episode steps:  37, steps per second:   8, episode reward: -28.800, mean reward: -0.778 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.546551, mean_q: 29.336840, mean_eps: 0.100000\n","     136498/2000000000: episode: 3658, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: -53.700, mean reward: -1.343 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 64.480459, mean_q: 30.326804, mean_eps: 0.100000\n","     136538/2000000000: episode: 3659, duration: 5.016s, episode steps:  40, steps per second:   8, episode reward: -24.000, mean reward: -0.600 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.522132, mean_q: 28.659929, mean_eps: 0.100000\n","     136578/2000000000: episode: 3660, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: -9.300, mean reward: -0.232 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.655305, mean_q: 29.737516, mean_eps: 0.100000\n","     136618/2000000000: episode: 3661, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward:  5.000, mean reward:  0.125 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 67.297318, mean_q: 28.420536, mean_eps: 0.100000\n","     136653/2000000000: episode: 3662, duration: 4.553s, episode steps:  35, steps per second:   8, episode reward: 41.500, mean reward:  1.186 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.283931, mean_q: 29.842192, mean_eps: 0.100000\n","     136687/2000000000: episode: 3663, duration: 4.380s, episode steps:  34, steps per second:   8, episode reward: 18.900, mean reward:  0.556 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 70.863390, mean_q: 29.050679, mean_eps: 0.100000\n","     136721/2000000000: episode: 3664, duration: 4.324s, episode steps:  34, steps per second:   8, episode reward:  8.100, mean reward:  0.238 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 70.947040, mean_q: 29.470852, mean_eps: 0.100000\n","     136761/2000000000: episode: 3665, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: 157.400, mean reward:  3.935 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.833587, mean_q: 29.249798, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     136801/2000000000: episode: 3666, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: -42.900, mean reward: -1.072 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.364819, mean_q: 29.330611, mean_eps: 0.100000\n","     136841/2000000000: episode: 3667, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: -27.900, mean reward: -0.697 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.513445, mean_q: 29.557618, mean_eps: 0.100000\n","     136879/2000000000: episode: 3668, duration: 5.076s, episode steps:  38, steps per second:   7, episode reward: -118.300, mean reward: -3.113 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 73.800283, mean_q: 29.309072, mean_eps: 0.100000\n","     136919/2000000000: episode: 3669, duration: 5.401s, episode steps:  40, steps per second:   7, episode reward: -36.600, mean reward: -0.915 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.258408, mean_q: 29.154803, mean_eps: 0.100000\n","     136959/2000000000: episode: 3670, duration: 5.550s, episode steps:  40, steps per second:   7, episode reward: 62.800, mean reward:  1.570 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.189209, mean_q: 29.042890, mean_eps: 0.100000\n","     136999/2000000000: episode: 3671, duration: 5.324s, episode steps:  40, steps per second:   8, episode reward: 63.000, mean reward:  1.575 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 66.676945, mean_q: 28.419451, mean_eps: 0.100000\n","     137039/2000000000: episode: 3672, duration: 5.664s, episode steps:  40, steps per second:   7, episode reward: -66.300, mean reward: -1.657 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.259397, mean_q: 29.471746, mean_eps: 0.100000\n","     137075/2000000000: episode: 3673, duration: 4.879s, episode steps:  36, steps per second:   7, episode reward:  6.000, mean reward:  0.167 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 73.674424, mean_q: 29.227483, mean_eps: 0.100000\n","     137108/2000000000: episode: 3674, duration: 4.629s, episode steps:  33, steps per second:   7, episode reward: -27.300, mean reward: -0.827 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 80.125825, mean_q: 28.464087, mean_eps: 0.100000\n","     137148/2000000000: episode: 3675, duration: 5.490s, episode steps:  40, steps per second:   7, episode reward: -49.700, mean reward: -1.243 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.967972, mean_q: 28.671989, mean_eps: 0.100000\n","     137188/2000000000: episode: 3676, duration: 5.466s, episode steps:  40, steps per second:   7, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.667095, mean_q: 28.684093, mean_eps: 0.100000\n","     137222/2000000000: episode: 3677, duration: 4.932s, episode steps:  34, steps per second:   7, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 71.581917, mean_q: 29.156602, mean_eps: 0.100000\n","     137260/2000000000: episode: 3678, duration: 5.135s, episode steps:  38, steps per second:   7, episode reward: -104.500, mean reward: -2.750 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 66.474169, mean_q: 28.744917, mean_eps: 0.100000\n","     137296/2000000000: episode: 3679, duration: 5.006s, episode steps:  36, steps per second:   7, episode reward: 37.900, mean reward:  1.053 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.411828, mean_q: 29.245381, mean_eps: 0.100000\n","     137325/2000000000: episode: 3680, duration: 4.221s, episode steps:  29, steps per second:   7, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 76.600171, mean_q: 29.694089, mean_eps: 0.100000\n","     137361/2000000000: episode: 3681, duration: 4.950s, episode steps:  36, steps per second:   7, episode reward: 63.500, mean reward:  1.764 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 69.628137, mean_q: 28.919232, mean_eps: 0.100000\n","     137391/2000000000: episode: 3682, duration: 4.320s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 75.812028, mean_q: 29.837612, mean_eps: 0.100000\n","     137431/2000000000: episode: 3683, duration: 5.041s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.709422, mean_q: 29.076223, mean_eps: 0.100000\n","     137461/2000000000: episode: 3684, duration: 4.056s, episode steps:  30, steps per second:   7, episode reward: -172.000, mean reward: -5.733 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 72.039900, mean_q: 29.584826, mean_eps: 0.100000\n","     137489/2000000000: episode: 3685, duration: 3.758s, episode steps:  28, steps per second:   7, episode reward: 113.200, mean reward:  4.043 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 64.402502, mean_q: 29.752307, mean_eps: 0.100000\n","     137520/2000000000: episode: 3686, duration: 4.169s, episode steps:  31, steps per second:   7, episode reward: 132.900, mean reward:  4.287 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 69.694907, mean_q: 29.417827, mean_eps: 0.100000\n","     137560/2000000000: episode: 3687, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.188765, mean_q: 28.995932, mean_eps: 0.100000\n","     137600/2000000000: episode: 3688, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: 135.300, mean reward:  3.382 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.412775, mean_q: 29.119219, mean_eps: 0.100000\n","     137639/2000000000: episode: 3689, duration: 4.712s, episode steps:  39, steps per second:   8, episode reward: -44.700, mean reward: -1.146 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 68.364619, mean_q: 28.892591, mean_eps: 0.100000\n","     137679/2000000000: episode: 3690, duration: 4.983s, episode steps:  40, steps per second:   8, episode reward: 20.300, mean reward:  0.507 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.877131, mean_q: 28.888136, mean_eps: 0.100000\n","     137719/2000000000: episode: 3691, duration: 5.019s, episode steps:  40, steps per second:   8, episode reward: -62.400, mean reward: -1.560 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.238344, mean_q: 29.164859, mean_eps: 0.100000\n","     137755/2000000000: episode: 3692, duration: 4.491s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 69.877967, mean_q: 29.156406, mean_eps: 0.100000\n","     137794/2000000000: episode: 3693, duration: 5.039s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 69.921554, mean_q: 29.263644, mean_eps: 0.100000\n","     137828/2000000000: episode: 3694, duration: 4.388s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 73.854015, mean_q: 29.025517, mean_eps: 0.100000\n","     137868/2000000000: episode: 3695, duration: 4.752s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 78.775203, mean_q: 28.550534, mean_eps: 0.100000\n","     137901/2000000000: episode: 3696, duration: 4.023s, episode steps:  33, steps per second:   8, episode reward: -1.200, mean reward: -0.036 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.371821, mean_q: 29.551991, mean_eps: 0.100000\n","     137936/2000000000: episode: 3697, duration: 4.552s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 68.027099, mean_q: 28.756598, mean_eps: 0.100000\n","     137976/2000000000: episode: 3698, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: 123.000, mean reward:  3.075 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.654114, mean_q: 28.891129, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     138016/2000000000: episode: 3699, duration: 4.850s, episode steps:  40, steps per second:   8, episode reward: 55.900, mean reward:  1.398 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.426938, mean_q: 29.356499, mean_eps: 0.100000\n","     138050/2000000000: episode: 3700, duration: 4.062s, episode steps:  34, steps per second:   8, episode reward: 68.300, mean reward:  2.009 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 78.163068, mean_q: 28.860471, mean_eps: 0.100000\n","     138085/2000000000: episode: 3701, duration: 4.199s, episode steps:  35, steps per second:   8, episode reward: -7.700, mean reward: -0.220 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 76.520800, mean_q: 29.544820, mean_eps: 0.100000\n","     138122/2000000000: episode: 3702, duration: 4.505s, episode steps:  37, steps per second:   8, episode reward: -134.000, mean reward: -3.622 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 70.360154, mean_q: 28.926183, mean_eps: 0.100000\n","     138162/2000000000: episode: 3703, duration: 4.804s, episode steps:  40, steps per second:   8, episode reward: 22.600, mean reward:  0.565 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.634944, mean_q: 28.442361, mean_eps: 0.100000\n","     138195/2000000000: episode: 3704, duration: 3.947s, episode steps:  33, steps per second:   8, episode reward: -40.000, mean reward: -1.212 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 71.749776, mean_q: 28.894942, mean_eps: 0.100000\n","     138235/2000000000: episode: 3705, duration: 4.819s, episode steps:  40, steps per second:   8, episode reward: -59.400, mean reward: -1.485 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.520691, mean_q: 29.924410, mean_eps: 0.100000\n","     138275/2000000000: episode: 3706, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 71.422239, mean_q: 28.730260, mean_eps: 0.100000\n","     138308/2000000000: episode: 3707, duration: 4.056s, episode steps:  33, steps per second:   8, episode reward: 179.200, mean reward:  5.430 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.224295, mean_q: 28.364420, mean_eps: 0.100000\n","     138348/2000000000: episode: 3708, duration: 4.715s, episode steps:  40, steps per second:   8, episode reward:  8.900, mean reward:  0.222 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.417502, mean_q: 29.451065, mean_eps: 0.100000\n","     138382/2000000000: episode: 3709, duration: 4.139s, episode steps:  34, steps per second:   8, episode reward: -50.500, mean reward: -1.485 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 72.138857, mean_q: 29.637196, mean_eps: 0.100000\n","     138422/2000000000: episode: 3710, duration: 5.013s, episode steps:  40, steps per second:   8, episode reward: -19.900, mean reward: -0.498 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.424433, mean_q: 28.720533, mean_eps: 0.100000\n","     138456/2000000000: episode: 3711, duration: 4.135s, episode steps:  34, steps per second:   8, episode reward: 66.200, mean reward:  1.947 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 75.967724, mean_q: 28.708173, mean_eps: 0.100000\n","     138489/2000000000: episode: 3712, duration: 4.002s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 72.841792, mean_q: 28.757495, mean_eps: 0.100000\n","     138529/2000000000: episode: 3713, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: -12.800, mean reward: -0.320 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.926363, mean_q: 28.832963, mean_eps: 0.100000\n","     138569/2000000000: episode: 3714, duration: 4.898s, episode steps:  40, steps per second:   8, episode reward: 93.900, mean reward:  2.348 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.329321, mean_q: 29.108947, mean_eps: 0.100000\n","     138609/2000000000: episode: 3715, duration: 4.958s, episode steps:  40, steps per second:   8, episode reward: -51.400, mean reward: -1.285 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.824099, mean_q: 29.546367, mean_eps: 0.100000\n","     138642/2000000000: episode: 3716, duration: 4.029s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 70.588142, mean_q: 29.403032, mean_eps: 0.100000\n","     138679/2000000000: episode: 3717, duration: 4.666s, episode steps:  37, steps per second:   8, episode reward: -25.600, mean reward: -0.692 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 69.950942, mean_q: 28.681710, mean_eps: 0.100000\n","     138719/2000000000: episode: 3718, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 83.300, mean reward:  2.082 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.243110, mean_q: 29.522026, mean_eps: 0.100000\n","     138755/2000000000: episode: 3719, duration: 4.392s, episode steps:  36, steps per second:   8, episode reward: 149.900, mean reward:  4.164 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.236624, mean_q: 30.416326, mean_eps: 0.100000\n","     138795/2000000000: episode: 3720, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward:  9.200, mean reward:  0.230 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.804516, mean_q: 28.664247, mean_eps: 0.100000\n","     138834/2000000000: episode: 3721, duration: 4.786s, episode steps:  39, steps per second:   8, episode reward: 187.900, mean reward:  4.818 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 66.059049, mean_q: 28.736985, mean_eps: 0.100000\n","     138860/2000000000: episode: 3722, duration: 3.140s, episode steps:  26, steps per second:   8, episode reward: 199.900, mean reward:  7.688 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 68.173125, mean_q: 29.288575, mean_eps: 0.100000\n","     138900/2000000000: episode: 3723, duration: 4.896s, episode steps:  40, steps per second:   8, episode reward: -119.500, mean reward: -2.988 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.563800, mean_q: 29.483128, mean_eps: 0.100000\n","     138934/2000000000: episode: 3724, duration: 3.995s, episode steps:  34, steps per second:   9, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 74.310975, mean_q: 28.750754, mean_eps: 0.100000\n","     138963/2000000000: episode: 3725, duration: 3.561s, episode steps:  29, steps per second:   8, episode reward: -172.000, mean reward: -5.931 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 66.866591, mean_q: 29.188771, mean_eps: 0.100000\n","     139000/2000000000: episode: 3726, duration: 4.450s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 75.614468, mean_q: 28.839095, mean_eps: 0.100000\n","     139030/2000000000: episode: 3727, duration: 3.558s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 73.406212, mean_q: 28.642071, mean_eps: 0.100000\n","     139065/2000000000: episode: 3728, duration: 4.300s, episode steps:  35, steps per second:   8, episode reward: 49.800, mean reward:  1.423 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 72.790833, mean_q: 29.505209, mean_eps: 0.100000\n","     139105/2000000000: episode: 3729, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: 93.500, mean reward:  2.337 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 68.448570, mean_q: 29.521493, mean_eps: 0.100000\n","     139134/2000000000: episode: 3730, duration: 3.449s, episode steps:  29, steps per second:   8, episode reward: 47.400, mean reward:  1.634 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 67.312259, mean_q: 29.196301, mean_eps: 0.100000\n","     139174/2000000000: episode: 3731, duration: 5.007s, episode steps:  40, steps per second:   8, episode reward: 113.800, mean reward:  2.845 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.884352, mean_q: 28.741834, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     139207/2000000000: episode: 3732, duration: 3.910s, episode steps:  33, steps per second:   8, episode reward: -70.400, mean reward: -2.133 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 77.784648, mean_q: 28.938975, mean_eps: 0.100000\n","     139247/2000000000: episode: 3733, duration: 4.541s, episode steps:  40, steps per second:   9, episode reward: -103.700, mean reward: -2.593 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.516603, mean_q: 29.727688, mean_eps: 0.100000\n","     139287/2000000000: episode: 3734, duration: 4.775s, episode steps:  40, steps per second:   8, episode reward: -44.800, mean reward: -1.120 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.081325, mean_q: 29.008635, mean_eps: 0.100000\n","     139318/2000000000: episode: 3735, duration: 3.810s, episode steps:  31, steps per second:   8, episode reward: 169.200, mean reward:  5.458 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 72.531165, mean_q: 29.054972, mean_eps: 0.100000\n","     139358/2000000000: episode: 3736, duration: 4.987s, episode steps:  40, steps per second:   8, episode reward: 43.400, mean reward:  1.085 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.433270, mean_q: 29.114389, mean_eps: 0.100000\n","     139392/2000000000: episode: 3737, duration: 4.334s, episode steps:  34, steps per second:   8, episode reward: 113.100, mean reward:  3.326 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 71.682373, mean_q: 28.790817, mean_eps: 0.100000\n","     139425/2000000000: episode: 3738, duration: 4.241s, episode steps:  33, steps per second:   8, episode reward: -56.300, mean reward: -1.706 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.002718, mean_q: 28.835175, mean_eps: 0.100000\n","     139457/2000000000: episode: 3739, duration: 4.028s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 71.755593, mean_q: 28.857153, mean_eps: 0.100000\n","     139488/2000000000: episode: 3740, duration: 3.870s, episode steps:  31, steps per second:   8, episode reward: 63.700, mean reward:  2.055 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 75.206503, mean_q: 28.809408, mean_eps: 0.100000\n","     139528/2000000000: episode: 3741, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: 42.800, mean reward:  1.070 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.490390, mean_q: 28.513320, mean_eps: 0.100000\n","     139568/2000000000: episode: 3742, duration: 4.940s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.073372, mean_q: 29.354617, mean_eps: 0.100000\n","     139600/2000000000: episode: 3743, duration: 4.020s, episode steps:  32, steps per second:   8, episode reward: 53.500, mean reward:  1.672 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.635262, mean_q: 28.699977, mean_eps: 0.100000\n","     139640/2000000000: episode: 3744, duration: 4.855s, episode steps:  40, steps per second:   8, episode reward: 56.800, mean reward:  1.420 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.790940, mean_q: 28.985267, mean_eps: 0.100000\n","     139674/2000000000: episode: 3745, duration: 3.997s, episode steps:  34, steps per second:   9, episode reward: -16.700, mean reward: -0.491 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.317146, mean_q: 28.664150, mean_eps: 0.100000\n","     139712/2000000000: episode: 3746, duration: 4.543s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 66.792131, mean_q: 29.568806, mean_eps: 0.100000\n","     139752/2000000000: episode: 3747, duration: 4.903s, episode steps:  40, steps per second:   8, episode reward: 10.400, mean reward:  0.260 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.604758, mean_q: 28.292410, mean_eps: 0.100000\n","     139787/2000000000: episode: 3748, duration: 4.273s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.217603, mean_q: 29.288497, mean_eps: 0.100000\n","     139827/2000000000: episode: 3749, duration: 4.784s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 69.746963, mean_q: 29.202670, mean_eps: 0.100000\n","     139867/2000000000: episode: 3750, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.224393, mean_q: 29.146997, mean_eps: 0.100000\n","     139907/2000000000: episode: 3751, duration: 4.937s, episode steps:  40, steps per second:   8, episode reward: 25.200, mean reward:  0.630 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.320431, mean_q: 28.471553, mean_eps: 0.100000\n","     139942/2000000000: episode: 3752, duration: 4.241s, episode steps:  35, steps per second:   8, episode reward: 10.700, mean reward:  0.306 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.405323, mean_q: 29.350046, mean_eps: 0.100000\n","     139981/2000000000: episode: 3753, duration: 4.959s, episode steps:  39, steps per second:   8, episode reward: 106.400, mean reward:  2.728 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 73.244382, mean_q: 29.486223, mean_eps: 0.100000\n","     140019/2000000000: episode: 3754, duration: 4.879s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 81.067316, mean_q: 29.623978, mean_eps: 0.100000\n","     140051/2000000000: episode: 3755, duration: 4.241s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 82.008395, mean_q: 30.016037, mean_eps: 0.100000\n","     140088/2000000000: episode: 3756, duration: 4.663s, episode steps:  37, steps per second:   8, episode reward: -125.800, mean reward: -3.400 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 84.417442, mean_q: 30.805129, mean_eps: 0.100000\n","     140128/2000000000: episode: 3757, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 73.318621, mean_q: 30.374921, mean_eps: 0.100000\n","     140168/2000000000: episode: 3758, duration: 4.760s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.645435, mean_q: 30.730102, mean_eps: 0.100000\n","     140202/2000000000: episode: 3759, duration: 4.034s, episode steps:  34, steps per second:   8, episode reward: 151.100, mean reward:  4.444 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 70.422277, mean_q: 31.551096, mean_eps: 0.100000\n","     140242/2000000000: episode: 3760, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 81.036649, mean_q: 29.887847, mean_eps: 0.100000\n","     140282/2000000000: episode: 3761, duration: 4.804s, episode steps:  40, steps per second:   8, episode reward: 69.600, mean reward:  1.740 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.184838, mean_q: 30.657419, mean_eps: 0.100000\n","     140322/2000000000: episode: 3762, duration: 4.932s, episode steps:  40, steps per second:   8, episode reward: -9.900, mean reward: -0.248 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.902073, mean_q: 29.861015, mean_eps: 0.100000\n","     140358/2000000000: episode: 3763, duration: 4.374s, episode steps:  36, steps per second:   8, episode reward: -6.600, mean reward: -0.183 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 76.204269, mean_q: 30.218722, mean_eps: 0.100000\n","     140391/2000000000: episode: 3764, duration: 4.097s, episode steps:  33, steps per second:   8, episode reward: 67.400, mean reward:  2.042 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 71.900499, mean_q: 30.913546, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     140431/2000000000: episode: 3765, duration: 4.800s, episode steps:  40, steps per second:   8, episode reward: -24.000, mean reward: -0.600 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.512621, mean_q: 30.852588, mean_eps: 0.100000\n","     140470/2000000000: episode: 3766, duration: 4.758s, episode steps:  39, steps per second:   8, episode reward: -16.200, mean reward: -0.415 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 71.945535, mean_q: 30.087505, mean_eps: 0.100000\n","     140510/2000000000: episode: 3767, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.707902, mean_q: 30.949949, mean_eps: 0.100000\n","     140550/2000000000: episode: 3768, duration: 4.738s, episode steps:  40, steps per second:   8, episode reward: 154.300, mean reward:  3.858 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.725080, mean_q: 29.939198, mean_eps: 0.100000\n","     140588/2000000000: episode: 3769, duration: 4.590s, episode steps:  38, steps per second:   8, episode reward: -83.100, mean reward: -2.187 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 76.919173, mean_q: 30.171298, mean_eps: 0.100000\n","     140628/2000000000: episode: 3770, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward: 29.700, mean reward:  0.742 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.662756, mean_q: 29.911633, mean_eps: 0.100000\n","     140659/2000000000: episode: 3771, duration: 3.871s, episode steps:  31, steps per second:   8, episode reward: 46.800, mean reward:  1.510 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 75.518626, mean_q: 31.074746, mean_eps: 0.100000\n","     140699/2000000000: episode: 3772, duration: 4.713s, episode steps:  40, steps per second:   8, episode reward: 150.200, mean reward:  3.755 [-10.500, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.185481, mean_q: 30.731819, mean_eps: 0.100000\n","     140732/2000000000: episode: 3773, duration: 4.284s, episode steps:  33, steps per second:   8, episode reward: 145.200, mean reward:  4.400 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 81.007972, mean_q: 30.822080, mean_eps: 0.100000\n","     140772/2000000000: episode: 3774, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 55.500, mean reward:  1.388 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 78.712313, mean_q: 30.659736, mean_eps: 0.100000\n","     140799/2000000000: episode: 3775, duration: 3.639s, episode steps:  27, steps per second:   7, episode reward: 60.900, mean reward:  2.256 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 80.600376, mean_q: 30.217427, mean_eps: 0.100000\n","     140832/2000000000: episode: 3776, duration: 4.243s, episode steps:  33, steps per second:   8, episode reward: -20.100, mean reward: -0.609 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.100345, mean_q: 30.306132, mean_eps: 0.100000\n","     140872/2000000000: episode: 3777, duration: 5.124s, episode steps:  40, steps per second:   8, episode reward: 208.600, mean reward:  5.215 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.334749, mean_q: 30.304249, mean_eps: 0.100000\n","     140903/2000000000: episode: 3778, duration: 3.812s, episode steps:  31, steps per second:   8, episode reward: -65.900, mean reward: -2.126 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 85.539056, mean_q: 30.544187, mean_eps: 0.100000\n","     140935/2000000000: episode: 3779, duration: 3.900s, episode steps:  32, steps per second:   8, episode reward:  4.600, mean reward:  0.144 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.432479, mean_q: 30.653643, mean_eps: 0.100000\n","     140972/2000000000: episode: 3780, duration: 4.595s, episode steps:  37, steps per second:   8, episode reward: 88.900, mean reward:  2.403 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 71.727501, mean_q: 31.060380, mean_eps: 0.100000\n","     141004/2000000000: episode: 3781, duration: 3.974s, episode steps:  32, steps per second:   8, episode reward: 123.500, mean reward:  3.859 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 72.903177, mean_q: 30.037285, mean_eps: 0.100000\n","     141044/2000000000: episode: 3782, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: -57.900, mean reward: -1.448 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.901437, mean_q: 30.597287, mean_eps: 0.100000\n","     141074/2000000000: episode: 3783, duration: 3.594s, episode steps:  30, steps per second:   8, episode reward: -9.500, mean reward: -0.317 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 79.903188, mean_q: 30.771953, mean_eps: 0.100000\n","     141114/2000000000: episode: 3784, duration: 4.829s, episode steps:  40, steps per second:   8, episode reward: -13.000, mean reward: -0.325 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.684004, mean_q: 30.555848, mean_eps: 0.100000\n","     141146/2000000000: episode: 3785, duration: 3.893s, episode steps:  32, steps per second:   8, episode reward: 72.100, mean reward:  2.253 [-20.000, 19.600], mean action: 1.062 [0.000, 2.000],  loss: 75.734911, mean_q: 30.933522, mean_eps: 0.100000\n","     141186/2000000000: episode: 3786, duration: 4.967s, episode steps:  40, steps per second:   8, episode reward: 61.000, mean reward:  1.525 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 73.886216, mean_q: 30.113921, mean_eps: 0.100000\n","     141226/2000000000: episode: 3787, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 16.900, mean reward:  0.423 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.000376, mean_q: 30.229713, mean_eps: 0.100000\n","     141266/2000000000: episode: 3788, duration: 4.829s, episode steps:  40, steps per second:   8, episode reward: -13.100, mean reward: -0.327 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.625064, mean_q: 30.837799, mean_eps: 0.100000\n","     141305/2000000000: episode: 3789, duration: 4.807s, episode steps:  39, steps per second:   8, episode reward: -36.300, mean reward: -0.931 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 78.857592, mean_q: 30.899407, mean_eps: 0.100000\n","     141341/2000000000: episode: 3790, duration: 4.494s, episode steps:  36, steps per second:   8, episode reward: -77.100, mean reward: -2.142 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 69.659765, mean_q: 30.510160, mean_eps: 0.100000\n","     141381/2000000000: episode: 3791, duration: 4.944s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.807253, mean_q: 30.165568, mean_eps: 0.100000\n","     141421/2000000000: episode: 3792, duration: 4.700s, episode steps:  40, steps per second:   9, episode reward: 113.000, mean reward:  2.825 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.030877, mean_q: 30.381368, mean_eps: 0.100000\n","     141461/2000000000: episode: 3793, duration: 4.830s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.970896, mean_q: 30.645613, mean_eps: 0.100000\n","     141501/2000000000: episode: 3794, duration: 4.761s, episode steps:  40, steps per second:   8, episode reward: 138.400, mean reward:  3.460 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.830753, mean_q: 30.777502, mean_eps: 0.100000\n","     141541/2000000000: episode: 3795, duration: 4.700s, episode steps:  40, steps per second:   9, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.042052, mean_q: 30.369813, mean_eps: 0.100000\n","     141581/2000000000: episode: 3796, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: 56.900, mean reward:  1.423 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.423806, mean_q: 30.485058, mean_eps: 0.100000\n","     141621/2000000000: episode: 3797, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: 123.000, mean reward:  3.075 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.626622, mean_q: 30.690541, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     141660/2000000000: episode: 3798, duration: 4.912s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 73.717641, mean_q: 30.379951, mean_eps: 0.100000\n","     141700/2000000000: episode: 3799, duration: 5.154s, episode steps:  40, steps per second:   8, episode reward: -130.900, mean reward: -3.272 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.306420, mean_q: 29.702073, mean_eps: 0.100000\n","     141740/2000000000: episode: 3800, duration: 4.872s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.971823, mean_q: 30.915780, mean_eps: 0.100000\n","     141773/2000000000: episode: 3801, duration: 4.053s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 78.563808, mean_q: 30.164667, mean_eps: 0.100000\n","     141813/2000000000: episode: 3802, duration: 4.975s, episode steps:  40, steps per second:   8, episode reward: -18.200, mean reward: -0.455 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.014123, mean_q: 31.978461, mean_eps: 0.100000\n","     141847/2000000000: episode: 3803, duration: 4.134s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 73.664908, mean_q: 31.404682, mean_eps: 0.100000\n","     141887/2000000000: episode: 3804, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.468742, mean_q: 30.117143, mean_eps: 0.100000\n","     141927/2000000000: episode: 3805, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 100.000, mean reward:  2.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 66.361521, mean_q: 30.325173, mean_eps: 0.100000\n","     141964/2000000000: episode: 3806, duration: 4.323s, episode steps:  37, steps per second:   9, episode reward: -1.900, mean reward: -0.051 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 73.115343, mean_q: 30.969610, mean_eps: 0.100000\n","     142001/2000000000: episode: 3807, duration: 4.329s, episode steps:  37, steps per second:   9, episode reward: 38.500, mean reward:  1.041 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 73.101654, mean_q: 30.801374, mean_eps: 0.100000\n","     142041/2000000000: episode: 3808, duration: 4.882s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.323315, mean_q: 30.499580, mean_eps: 0.100000\n","     142079/2000000000: episode: 3809, duration: 4.454s, episode steps:  38, steps per second:   9, episode reward: 37.600, mean reward:  0.989 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 76.361144, mean_q: 31.008670, mean_eps: 0.100000\n","     142117/2000000000: episode: 3810, duration: 4.691s, episode steps:  38, steps per second:   8, episode reward: 154.500, mean reward:  4.066 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.240313, mean_q: 30.618581, mean_eps: 0.100000\n","     142154/2000000000: episode: 3811, duration: 4.415s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 72.738370, mean_q: 30.355174, mean_eps: 0.100000\n","     142194/2000000000: episode: 3812, duration: 4.920s, episode steps:  40, steps per second:   8, episode reward:  7.600, mean reward:  0.190 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.121827, mean_q: 30.056395, mean_eps: 0.100000\n","     142228/2000000000: episode: 3813, duration: 4.242s, episode steps:  34, steps per second:   8, episode reward: -27.800, mean reward: -0.818 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 70.389426, mean_q: 30.715446, mean_eps: 0.100000\n","     142266/2000000000: episode: 3814, duration: 4.678s, episode steps:  38, steps per second:   8, episode reward: -103.800, mean reward: -2.732 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 77.644827, mean_q: 30.081446, mean_eps: 0.100000\n","     142306/2000000000: episode: 3815, duration: 4.997s, episode steps:  40, steps per second:   8, episode reward: 161.300, mean reward:  4.032 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.854949, mean_q: 30.221431, mean_eps: 0.100000\n","     142346/2000000000: episode: 3816, duration: 5.293s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 85.399577, mean_q: 31.113683, mean_eps: 0.100000\n","     142386/2000000000: episode: 3817, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 68.275777, mean_q: 30.668935, mean_eps: 0.100000\n","     142426/2000000000: episode: 3818, duration: 5.068s, episode steps:  40, steps per second:   8, episode reward: 213.500, mean reward:  5.337 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.653217, mean_q: 30.556416, mean_eps: 0.100000\n","     142466/2000000000: episode: 3819, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: 57.100, mean reward:  1.428 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 74.682241, mean_q: 30.369294, mean_eps: 0.100000\n","     142504/2000000000: episode: 3820, duration: 4.544s, episode steps:  38, steps per second:   8, episode reward: 73.500, mean reward:  1.934 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 70.735750, mean_q: 30.321885, mean_eps: 0.100000\n","     142540/2000000000: episode: 3821, duration: 4.144s, episode steps:  36, steps per second:   9, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 74.244974, mean_q: 31.162540, mean_eps: 0.100000\n","     142580/2000000000: episode: 3822, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: -12.800, mean reward: -0.320 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.945885, mean_q: 31.284413, mean_eps: 0.100000\n","     142610/2000000000: episode: 3823, duration: 3.712s, episode steps:  30, steps per second:   8, episode reward: -38.900, mean reward: -1.297 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 75.632522, mean_q: 31.384048, mean_eps: 0.100000\n","     142640/2000000000: episode: 3824, duration: 3.828s, episode steps:  30, steps per second:   8, episode reward: 26.900, mean reward:  0.897 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.577818, mean_q: 31.123262, mean_eps: 0.100000\n","     142680/2000000000: episode: 3825, duration: 4.897s, episode steps:  40, steps per second:   8, episode reward: 206.900, mean reward:  5.172 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.499551, mean_q: 30.641771, mean_eps: 0.100000\n","     142720/2000000000: episode: 3826, duration: 4.889s, episode steps:  40, steps per second:   8, episode reward: 133.800, mean reward:  3.345 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.880685, mean_q: 30.482398, mean_eps: 0.100000\n","     142756/2000000000: episode: 3827, duration: 4.625s, episode steps:  36, steps per second:   8, episode reward: 54.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 72.394147, mean_q: 31.128622, mean_eps: 0.100000\n","     142789/2000000000: episode: 3828, duration: 4.013s, episode steps:  33, steps per second:   8, episode reward: 36.800, mean reward:  1.115 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 75.575447, mean_q: 30.486146, mean_eps: 0.100000\n","     142822/2000000000: episode: 3829, duration: 4.149s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.654621, mean_q: 30.906356, mean_eps: 0.100000\n","     142850/2000000000: episode: 3830, duration: 3.471s, episode steps:  28, steps per second:   8, episode reward: 178.600, mean reward:  6.379 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 72.940551, mean_q: 31.140939, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     142890/2000000000: episode: 3831, duration: 4.874s, episode steps:  40, steps per second:   8, episode reward: 40.100, mean reward:  1.003 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.258484, mean_q: 30.423169, mean_eps: 0.100000\n","     142929/2000000000: episode: 3832, duration: 4.606s, episode steps:  39, steps per second:   8, episode reward: -42.300, mean reward: -1.085 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 77.795495, mean_q: 30.142875, mean_eps: 0.100000\n","     142969/2000000000: episode: 3833, duration: 4.866s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.886409, mean_q: 30.933009, mean_eps: 0.100000\n","     143007/2000000000: episode: 3834, duration: 4.754s, episode steps:  38, steps per second:   8, episode reward: -67.800, mean reward: -1.784 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 74.319492, mean_q: 30.875298, mean_eps: 0.100000\n","     143047/2000000000: episode: 3835, duration: 4.826s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.675879, mean_q: 30.238447, mean_eps: 0.100000\n","     143080/2000000000: episode: 3836, duration: 4.120s, episode steps:  33, steps per second:   8, episode reward: -66.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 73.435149, mean_q: 30.525754, mean_eps: 0.100000\n","     143115/2000000000: episode: 3837, duration: 4.253s, episode steps:  35, steps per second:   8, episode reward: 139.000, mean reward:  3.971 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.931047, mean_q: 31.006857, mean_eps: 0.100000\n","     143148/2000000000: episode: 3838, duration: 4.035s, episode steps:  33, steps per second:   8, episode reward: -15.200, mean reward: -0.461 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 71.702509, mean_q: 30.073145, mean_eps: 0.100000\n","     143187/2000000000: episode: 3839, duration: 4.774s, episode steps:  39, steps per second:   8, episode reward: 119.200, mean reward:  3.056 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 72.683338, mean_q: 30.109855, mean_eps: 0.100000\n","     143224/2000000000: episode: 3840, duration: 4.567s, episode steps:  37, steps per second:   8, episode reward: 173.300, mean reward:  4.684 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 72.296608, mean_q: 31.434911, mean_eps: 0.100000\n","     143260/2000000000: episode: 3841, duration: 4.385s, episode steps:  36, steps per second:   8, episode reward: -90.900, mean reward: -2.525 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.245748, mean_q: 30.086171, mean_eps: 0.100000\n","     143300/2000000000: episode: 3842, duration: 4.852s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 69.798941, mean_q: 30.725407, mean_eps: 0.100000\n","     143334/2000000000: episode: 3843, duration: 4.484s, episode steps:  34, steps per second:   8, episode reward: 197.200, mean reward:  5.800 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 71.411884, mean_q: 30.078016, mean_eps: 0.100000\n","     143368/2000000000: episode: 3844, duration: 4.207s, episode steps:  34, steps per second:   8, episode reward: 85.400, mean reward:  2.512 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 76.104871, mean_q: 31.406346, mean_eps: 0.100000\n","     143402/2000000000: episode: 3845, duration: 4.215s, episode steps:  34, steps per second:   8, episode reward: 124.700, mean reward:  3.668 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.085625, mean_q: 30.203846, mean_eps: 0.100000\n","     143441/2000000000: episode: 3846, duration: 4.818s, episode steps:  39, steps per second:   8, episode reward: -13.500, mean reward: -0.346 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 69.546169, mean_q: 30.280856, mean_eps: 0.100000\n","     143476/2000000000: episode: 3847, duration: 4.365s, episode steps:  35, steps per second:   8, episode reward: 204.500, mean reward:  5.843 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 73.832251, mean_q: 30.207586, mean_eps: 0.100000\n","     143515/2000000000: episode: 3848, duration: 4.639s, episode steps:  39, steps per second:   8, episode reward: 120.100, mean reward:  3.079 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 78.157303, mean_q: 30.593778, mean_eps: 0.100000\n","     143551/2000000000: episode: 3849, duration: 4.471s, episode steps:  36, steps per second:   8, episode reward: 65.400, mean reward:  1.817 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 70.060448, mean_q: 30.447781, mean_eps: 0.100000\n","     143591/2000000000: episode: 3850, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 66.484257, mean_q: 31.031781, mean_eps: 0.100000\n","     143628/2000000000: episode: 3851, duration: 4.460s, episode steps:  37, steps per second:   8, episode reward: -82.000, mean reward: -2.216 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.759023, mean_q: 30.462908, mean_eps: 0.100000\n","     143662/2000000000: episode: 3852, duration: 4.243s, episode steps:  34, steps per second:   8, episode reward: 10.700, mean reward:  0.315 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.452378, mean_q: 30.640862, mean_eps: 0.100000\n","     143695/2000000000: episode: 3853, duration: 4.010s, episode steps:  33, steps per second:   8, episode reward: -0.500, mean reward: -0.015 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 70.415921, mean_q: 30.878297, mean_eps: 0.100000\n","     143735/2000000000: episode: 3854, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 116.400, mean reward:  2.910 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 71.534491, mean_q: 30.680622, mean_eps: 0.100000\n","     143775/2000000000: episode: 3855, duration: 4.799s, episode steps:  40, steps per second:   8, episode reward: 34.700, mean reward:  0.868 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.530074, mean_q: 30.751063, mean_eps: 0.100000\n","     143815/2000000000: episode: 3856, duration: 5.061s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.984075, mean_q: 30.495309, mean_eps: 0.100000\n","     143847/2000000000: episode: 3857, duration: 3.824s, episode steps:  32, steps per second:   8, episode reward: -52.900, mean reward: -1.653 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 71.217709, mean_q: 31.424777, mean_eps: 0.100000\n","     143885/2000000000: episode: 3858, duration: 4.522s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 68.935341, mean_q: 30.389555, mean_eps: 0.100000\n","     143925/2000000000: episode: 3859, duration: 5.101s, episode steps:  40, steps per second:   8, episode reward: 34.100, mean reward:  0.852 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.208378, mean_q: 31.378341, mean_eps: 0.100000\n","     143965/2000000000: episode: 3860, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward: 17.400, mean reward:  0.435 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.349772, mean_q: 31.468905, mean_eps: 0.100000\n","     144005/2000000000: episode: 3861, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: 62.600, mean reward:  1.565 [-20.000, 18.800], mean action: 1.325 [0.000, 2.000],  loss: 74.728473, mean_q: 31.902105, mean_eps: 0.100000\n","     144037/2000000000: episode: 3862, duration: 4.257s, episode steps:  32, steps per second:   8, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.199760, mean_q: 31.238487, mean_eps: 0.100000\n","     144069/2000000000: episode: 3863, duration: 4.109s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 74.308182, mean_q: 30.444023, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     144103/2000000000: episode: 3864, duration: 4.498s, episode steps:  34, steps per second:   8, episode reward: -5.900, mean reward: -0.174 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 68.809515, mean_q: 31.342352, mean_eps: 0.100000\n","     144133/2000000000: episode: 3865, duration: 3.985s, episode steps:  30, steps per second:   8, episode reward: -2.200, mean reward: -0.073 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.618838, mean_q: 29.922287, mean_eps: 0.100000\n","     144173/2000000000: episode: 3866, duration: 5.045s, episode steps:  40, steps per second:   8, episode reward: 16.300, mean reward:  0.408 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.849727, mean_q: 30.940054, mean_eps: 0.100000\n","     144213/2000000000: episode: 3867, duration: 5.585s, episode steps:  40, steps per second:   7, episode reward: 13.700, mean reward:  0.343 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 69.988272, mean_q: 30.882622, mean_eps: 0.100000\n","     144252/2000000000: episode: 3868, duration: 5.427s, episode steps:  39, steps per second:   7, episode reward: 122.500, mean reward:  3.141 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 72.956240, mean_q: 30.318741, mean_eps: 0.100000\n","     144285/2000000000: episode: 3869, duration: 4.497s, episode steps:  33, steps per second:   7, episode reward: 170.000, mean reward:  5.152 [-20.000, 18.000], mean action: 1.273 [0.000, 2.000],  loss: 69.956794, mean_q: 31.411559, mean_eps: 0.100000\n","     144325/2000000000: episode: 3870, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: 257.100, mean reward:  6.427 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.570170, mean_q: 30.692848, mean_eps: 0.100000\n","     144365/2000000000: episode: 3871, duration: 5.237s, episode steps:  40, steps per second:   8, episode reward: 102.100, mean reward:  2.553 [-20.000, 19.700], mean action: 1.475 [0.000, 2.000],  loss: 76.769979, mean_q: 31.483532, mean_eps: 0.100000\n","     144405/2000000000: episode: 3872, duration: 5.279s, episode steps:  40, steps per second:   8, episode reward: -81.600, mean reward: -2.040 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.449264, mean_q: 30.675476, mean_eps: 0.100000\n","     144445/2000000000: episode: 3873, duration: 5.326s, episode steps:  40, steps per second:   8, episode reward: 46.300, mean reward:  1.157 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.124247, mean_q: 30.170484, mean_eps: 0.100000\n","     144485/2000000000: episode: 3874, duration: 5.272s, episode steps:  40, steps per second:   8, episode reward: 34.200, mean reward:  0.855 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.707698, mean_q: 30.046734, mean_eps: 0.100000\n","     144525/2000000000: episode: 3875, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: -10.700, mean reward: -0.268 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.220415, mean_q: 30.652011, mean_eps: 0.100000\n","     144563/2000000000: episode: 3876, duration: 4.743s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 79.301608, mean_q: 31.083468, mean_eps: 0.100000\n","     144594/2000000000: episode: 3877, duration: 4.100s, episode steps:  31, steps per second:   8, episode reward: -76.100, mean reward: -2.455 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 72.861048, mean_q: 31.028141, mean_eps: 0.100000\n","     144634/2000000000: episode: 3878, duration: 5.137s, episode steps:  40, steps per second:   8, episode reward: 18.200, mean reward:  0.455 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.785140, mean_q: 30.763034, mean_eps: 0.100000\n","     144667/2000000000: episode: 3879, duration: 4.024s, episode steps:  33, steps per second:   8, episode reward: 40.400, mean reward:  1.224 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 73.153616, mean_q: 31.132515, mean_eps: 0.100000\n","     144706/2000000000: episode: 3880, duration: 4.936s, episode steps:  39, steps per second:   8, episode reward: 19.000, mean reward:  0.487 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 80.061629, mean_q: 30.718082, mean_eps: 0.100000\n","     144740/2000000000: episode: 3881, duration: 4.474s, episode steps:  34, steps per second:   8, episode reward: 96.800, mean reward:  2.847 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 72.492956, mean_q: 31.125001, mean_eps: 0.100000\n","     144780/2000000000: episode: 3882, duration: 5.176s, episode steps:  40, steps per second:   8, episode reward: 122.900, mean reward:  3.072 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 72.761425, mean_q: 29.435421, mean_eps: 0.100000\n","     144815/2000000000: episode: 3883, duration: 4.516s, episode steps:  35, steps per second:   8, episode reward: 96.300, mean reward:  2.751 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 74.701436, mean_q: 31.450147, mean_eps: 0.100000\n","     144855/2000000000: episode: 3884, duration: 5.492s, episode steps:  40, steps per second:   7, episode reward: -106.100, mean reward: -2.653 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.042882, mean_q: 30.260055, mean_eps: 0.100000\n","     144891/2000000000: episode: 3885, duration: 4.753s, episode steps:  36, steps per second:   8, episode reward: -70.100, mean reward: -1.947 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 68.222280, mean_q: 30.308984, mean_eps: 0.100000\n","     144931/2000000000: episode: 3886, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: 28.500, mean reward:  0.713 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.984586, mean_q: 29.783863, mean_eps: 0.100000\n","     144957/2000000000: episode: 3887, duration: 3.538s, episode steps:  26, steps per second:   7, episode reward: 120.800, mean reward:  4.646 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 74.815670, mean_q: 30.618733, mean_eps: 0.100000\n","     144995/2000000000: episode: 3888, duration: 4.881s, episode steps:  38, steps per second:   8, episode reward: 90.700, mean reward:  2.387 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 74.614134, mean_q: 30.128280, mean_eps: 0.100000\n","     145035/2000000000: episode: 3889, duration: 5.293s, episode steps:  40, steps per second:   8, episode reward: -72.900, mean reward: -1.823 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.696015, mean_q: 30.244431, mean_eps: 0.100000\n","     145074/2000000000: episode: 3890, duration: 5.430s, episode steps:  39, steps per second:   7, episode reward: 68.700, mean reward:  1.762 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 77.368010, mean_q: 31.909650, mean_eps: 0.100000\n","     145105/2000000000: episode: 3891, duration: 4.180s, episode steps:  31, steps per second:   7, episode reward: 64.500, mean reward:  2.081 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 69.115896, mean_q: 31.322465, mean_eps: 0.100000\n","     145143/2000000000: episode: 3892, duration: 4.966s, episode steps:  38, steps per second:   8, episode reward: -62.400, mean reward: -1.642 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 67.997220, mean_q: 30.865081, mean_eps: 0.100000\n","     145177/2000000000: episode: 3893, duration: 4.688s, episode steps:  34, steps per second:   7, episode reward: 52.800, mean reward:  1.553 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 71.006922, mean_q: 30.315481, mean_eps: 0.100000\n","     145217/2000000000: episode: 3894, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: 41.100, mean reward:  1.027 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.036777, mean_q: 29.635841, mean_eps: 0.100000\n","     145254/2000000000: episode: 3895, duration: 4.619s, episode steps:  37, steps per second:   8, episode reward: -37.900, mean reward: -1.024 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 73.959646, mean_q: 29.990452, mean_eps: 0.100000\n","     145293/2000000000: episode: 3896, duration: 4.801s, episode steps:  39, steps per second:   8, episode reward: 41.600, mean reward:  1.067 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 71.610051, mean_q: 30.351172, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     145333/2000000000: episode: 3897, duration: 4.769s, episode steps:  40, steps per second:   8, episode reward: -15.600, mean reward: -0.390 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.205297, mean_q: 30.009178, mean_eps: 0.100000\n","     145373/2000000000: episode: 3898, duration: 4.984s, episode steps:  40, steps per second:   8, episode reward: 30.500, mean reward:  0.762 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.549826, mean_q: 29.805759, mean_eps: 0.100000\n","     145412/2000000000: episode: 3899, duration: 4.889s, episode steps:  39, steps per second:   8, episode reward: -101.400, mean reward: -2.600 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 78.610396, mean_q: 31.486014, mean_eps: 0.100000\n","     145452/2000000000: episode: 3900, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward: 120.000, mean reward:  3.000 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.596921, mean_q: 30.123993, mean_eps: 0.100000\n","     145492/2000000000: episode: 3901, duration: 5.555s, episode steps:  40, steps per second:   7, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.329330, mean_q: 30.564777, mean_eps: 0.100000\n","     145527/2000000000: episode: 3902, duration: 4.451s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 72.825898, mean_q: 30.549640, mean_eps: 0.100000\n","     145556/2000000000: episode: 3903, duration: 3.723s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.552319, mean_q: 30.569531, mean_eps: 0.100000\n","     145588/2000000000: episode: 3904, duration: 4.099s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 79.641934, mean_q: 30.423962, mean_eps: 0.100000\n","     145628/2000000000: episode: 3905, duration: 4.876s, episode steps:  40, steps per second:   8, episode reward: 70.200, mean reward:  1.755 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.903819, mean_q: 30.857308, mean_eps: 0.100000\n","     145666/2000000000: episode: 3906, duration: 4.710s, episode steps:  38, steps per second:   8, episode reward: -37.100, mean reward: -0.976 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.546573, mean_q: 30.958364, mean_eps: 0.100000\n","     145706/2000000000: episode: 3907, duration: 5.153s, episode steps:  40, steps per second:   8, episode reward: 220.400, mean reward:  5.510 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 78.011659, mean_q: 30.461428, mean_eps: 0.100000\n","     145746/2000000000: episode: 3908, duration: 5.123s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.649342, mean_q: 30.660453, mean_eps: 0.100000\n","     145786/2000000000: episode: 3909, duration: 5.319s, episode steps:  40, steps per second:   8, episode reward: 159.500, mean reward:  3.987 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.977804, mean_q: 30.891466, mean_eps: 0.100000\n","     145825/2000000000: episode: 3910, duration: 5.175s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 69.526792, mean_q: 30.609302, mean_eps: 0.100000\n","     145859/2000000000: episode: 3911, duration: 4.093s, episode steps:  34, steps per second:   8, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 71.959090, mean_q: 30.947102, mean_eps: 0.100000\n","     145892/2000000000: episode: 3912, duration: 3.993s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 67.525242, mean_q: 32.089187, mean_eps: 0.100000\n","     145932/2000000000: episode: 3913, duration: 4.711s, episode steps:  40, steps per second:   8, episode reward: 192.000, mean reward:  4.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.987137, mean_q: 30.015149, mean_eps: 0.100000\n","     145965/2000000000: episode: 3914, duration: 3.873s, episode steps:  33, steps per second:   9, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 70.608894, mean_q: 31.515847, mean_eps: 0.100000\n","     146005/2000000000: episode: 3915, duration: 4.942s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.237154, mean_q: 30.841533, mean_eps: 0.100000\n","     146039/2000000000: episode: 3916, duration: 4.105s, episode steps:  34, steps per second:   8, episode reward: -56.700, mean reward: -1.668 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.833746, mean_q: 31.046915, mean_eps: 0.100000\n","     146078/2000000000: episode: 3917, duration: 4.591s, episode steps:  39, steps per second:   8, episode reward: 12.400, mean reward:  0.318 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 75.791105, mean_q: 30.820041, mean_eps: 0.100000\n","     146106/2000000000: episode: 3918, duration: 3.261s, episode steps:  28, steps per second:   9, episode reward: 195.500, mean reward:  6.982 [-20.000, 18.700], mean action: 0.929 [0.000, 2.000],  loss: 63.420468, mean_q: 32.164942, mean_eps: 0.100000\n","     146145/2000000000: episode: 3919, duration: 4.614s, episode steps:  39, steps per second:   8, episode reward: 81.300, mean reward:  2.085 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 71.882578, mean_q: 30.885858, mean_eps: 0.100000\n","     146184/2000000000: episode: 3920, duration: 4.748s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 68.103396, mean_q: 29.986825, mean_eps: 0.100000\n","     146224/2000000000: episode: 3921, duration: 4.911s, episode steps:  40, steps per second:   8, episode reward: 84.900, mean reward:  2.122 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.945420, mean_q: 30.681925, mean_eps: 0.100000\n","     146264/2000000000: episode: 3922, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: 71.200, mean reward:  1.780 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.012236, mean_q: 30.907027, mean_eps: 0.100000\n","     146304/2000000000: episode: 3923, duration: 4.648s, episode steps:  40, steps per second:   9, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 67.246699, mean_q: 29.929275, mean_eps: 0.100000\n","     146343/2000000000: episode: 3924, duration: 4.540s, episode steps:  39, steps per second:   9, episode reward: -146.200, mean reward: -3.749 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 71.518925, mean_q: 31.469339, mean_eps: 0.100000\n","     146377/2000000000: episode: 3925, duration: 4.039s, episode steps:  34, steps per second:   8, episode reward: -248.000, mean reward: -7.294 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 77.068733, mean_q: 29.951694, mean_eps: 0.100000\n","     146414/2000000000: episode: 3926, duration: 4.359s, episode steps:  37, steps per second:   8, episode reward: 246.000, mean reward:  6.649 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 77.142999, mean_q: 30.836378, mean_eps: 0.100000\n","     146453/2000000000: episode: 3927, duration: 5.145s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 72.757320, mean_q: 30.511653, mean_eps: 0.100000\n","     146490/2000000000: episode: 3928, duration: 4.983s, episode steps:  37, steps per second:   7, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 71.185641, mean_q: 31.223914, mean_eps: 0.100000\n","     146530/2000000000: episode: 3929, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: 125.300, mean reward:  3.133 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.828411, mean_q: 30.475415, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     146560/2000000000: episode: 3930, duration: 3.671s, episode steps:  30, steps per second:   8, episode reward: 46.300, mean reward:  1.543 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 67.488950, mean_q: 30.190069, mean_eps: 0.100000\n","     146592/2000000000: episode: 3931, duration: 3.957s, episode steps:  32, steps per second:   8, episode reward: 30.100, mean reward:  0.941 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 76.033162, mean_q: 30.112357, mean_eps: 0.100000\n","     146626/2000000000: episode: 3932, duration: 4.386s, episode steps:  34, steps per second:   8, episode reward: 91.400, mean reward:  2.688 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 80.078334, mean_q: 31.825388, mean_eps: 0.100000\n","     146655/2000000000: episode: 3933, duration: 3.633s, episode steps:  29, steps per second:   8, episode reward: -104.600, mean reward: -3.607 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 83.252844, mean_q: 29.561807, mean_eps: 0.100000\n","     146692/2000000000: episode: 3934, duration: 4.750s, episode steps:  37, steps per second:   8, episode reward: 57.700, mean reward:  1.559 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 68.822351, mean_q: 30.975071, mean_eps: 0.100000\n","     146732/2000000000: episode: 3935, duration: 5.010s, episode steps:  40, steps per second:   8, episode reward: 103.100, mean reward:  2.577 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.040918, mean_q: 31.270628, mean_eps: 0.100000\n","     146771/2000000000: episode: 3936, duration: 4.777s, episode steps:  39, steps per second:   8, episode reward: 75.300, mean reward:  1.931 [-20.000, 18.000], mean action: 1.385 [0.000, 2.000],  loss: 75.950991, mean_q: 29.893030, mean_eps: 0.100000\n","     146811/2000000000: episode: 3937, duration: 4.784s, episode steps:  40, steps per second:   8, episode reward: 39.300, mean reward:  0.982 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 68.521597, mean_q: 30.574909, mean_eps: 0.100000\n","     146843/2000000000: episode: 3938, duration: 3.870s, episode steps:  32, steps per second:   8, episode reward: -124.800, mean reward: -3.900 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.869111, mean_q: 31.844447, mean_eps: 0.100000\n","     146879/2000000000: episode: 3939, duration: 4.697s, episode steps:  36, steps per second:   8, episode reward: -33.200, mean reward: -0.922 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 71.332949, mean_q: 30.925885, mean_eps: 0.100000\n","     146919/2000000000: episode: 3940, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: 81.700, mean reward:  2.043 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.404833, mean_q: 30.319553, mean_eps: 0.100000\n","     146959/2000000000: episode: 3941, duration: 5.094s, episode steps:  40, steps per second:   8, episode reward: -119.600, mean reward: -2.990 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.674572, mean_q: 30.534997, mean_eps: 0.100000\n","     146999/2000000000: episode: 3942, duration: 5.362s, episode steps:  40, steps per second:   7, episode reward: 33.900, mean reward:  0.848 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.851465, mean_q: 31.084430, mean_eps: 0.100000\n","     147034/2000000000: episode: 3943, duration: 4.467s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 67.922832, mean_q: 30.778815, mean_eps: 0.100000\n","     147073/2000000000: episode: 3944, duration: 4.937s, episode steps:  39, steps per second:   8, episode reward: -91.900, mean reward: -2.356 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 77.636878, mean_q: 30.582820, mean_eps: 0.100000\n","     147111/2000000000: episode: 3945, duration: 5.094s, episode steps:  38, steps per second:   7, episode reward: -39.700, mean reward: -1.045 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 70.601332, mean_q: 30.187978, mean_eps: 0.100000\n","     147142/2000000000: episode: 3946, duration: 3.967s, episode steps:  31, steps per second:   8, episode reward: 23.800, mean reward:  0.768 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.686840, mean_q: 30.531775, mean_eps: 0.100000\n","     147182/2000000000: episode: 3947, duration: 4.830s, episode steps:  40, steps per second:   8, episode reward: 153.900, mean reward:  3.847 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.931525, mean_q: 30.062351, mean_eps: 0.100000\n","     147217/2000000000: episode: 3948, duration: 4.263s, episode steps:  35, steps per second:   8, episode reward: 47.600, mean reward:  1.360 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.274479, mean_q: 31.374002, mean_eps: 0.100000\n","     147247/2000000000: episode: 3949, duration: 3.687s, episode steps:  30, steps per second:   8, episode reward: 39.900, mean reward:  1.330 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 69.368346, mean_q: 31.252180, mean_eps: 0.100000\n","     147284/2000000000: episode: 3950, duration: 4.631s, episode steps:  37, steps per second:   8, episode reward: 200.200, mean reward:  5.411 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 73.995291, mean_q: 30.933453, mean_eps: 0.100000\n","     147323/2000000000: episode: 3951, duration: 4.850s, episode steps:  39, steps per second:   8, episode reward: 40.600, mean reward:  1.041 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 65.265738, mean_q: 30.019175, mean_eps: 0.100000\n","     147354/2000000000: episode: 3952, duration: 3.636s, episode steps:  31, steps per second:   9, episode reward: 143.900, mean reward:  4.642 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 69.600418, mean_q: 31.001092, mean_eps: 0.100000\n","     147382/2000000000: episode: 3953, duration: 3.301s, episode steps:  28, steps per second:   8, episode reward: -61.800, mean reward: -2.207 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 75.736581, mean_q: 30.177915, mean_eps: 0.100000\n","     147421/2000000000: episode: 3954, duration: 4.527s, episode steps:  39, steps per second:   9, episode reward: -78.400, mean reward: -2.010 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 78.607111, mean_q: 31.494644, mean_eps: 0.100000\n","     147454/2000000000: episode: 3955, duration: 4.016s, episode steps:  33, steps per second:   8, episode reward:  7.300, mean reward:  0.221 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 74.601103, mean_q: 31.434432, mean_eps: 0.100000\n","     147494/2000000000: episode: 3956, duration: 4.795s, episode steps:  40, steps per second:   8, episode reward: 17.800, mean reward:  0.445 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.106975, mean_q: 30.235920, mean_eps: 0.100000\n","     147522/2000000000: episode: 3957, duration: 3.446s, episode steps:  28, steps per second:   8, episode reward: -62.000, mean reward: -2.214 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 72.487341, mean_q: 30.734476, mean_eps: 0.100000\n","     147562/2000000000: episode: 3958, duration: 4.714s, episode steps:  40, steps per second:   8, episode reward:  5.500, mean reward:  0.138 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.914755, mean_q: 30.318267, mean_eps: 0.100000\n","     147602/2000000000: episode: 3959, duration: 4.818s, episode steps:  40, steps per second:   8, episode reward: 45.500, mean reward:  1.137 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.104959, mean_q: 30.514322, mean_eps: 0.100000\n","     147638/2000000000: episode: 3960, duration: 4.572s, episode steps:  36, steps per second:   8, episode reward: 28.200, mean reward:  0.783 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 73.147736, mean_q: 30.863279, mean_eps: 0.100000\n","     147672/2000000000: episode: 3961, duration: 4.241s, episode steps:  34, steps per second:   8, episode reward: 87.100, mean reward:  2.562 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 71.425900, mean_q: 31.305621, mean_eps: 0.100000\n","     147710/2000000000: episode: 3962, duration: 4.786s, episode steps:  38, steps per second:   8, episode reward: -132.300, mean reward: -3.482 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 70.527216, mean_q: 30.935586, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     147750/2000000000: episode: 3963, duration: 5.002s, episode steps:  40, steps per second:   8, episode reward: 66.400, mean reward:  1.660 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.407172, mean_q: 30.513954, mean_eps: 0.100000\n","     147775/2000000000: episode: 3964, duration: 3.206s, episode steps:  25, steps per second:   8, episode reward: 76.600, mean reward:  3.064 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 70.442317, mean_q: 30.496931, mean_eps: 0.100000\n","     147812/2000000000: episode: 3965, duration: 4.525s, episode steps:  37, steps per second:   8, episode reward: 68.600, mean reward:  1.854 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 73.648701, mean_q: 30.863041, mean_eps: 0.100000\n","     147843/2000000000: episode: 3966, duration: 4.126s, episode steps:  31, steps per second:   8, episode reward: 69.900, mean reward:  2.255 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 76.685792, mean_q: 30.046410, mean_eps: 0.100000\n","     147882/2000000000: episode: 3967, duration: 4.958s, episode steps:  39, steps per second:   8, episode reward: -106.600, mean reward: -2.733 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 71.985872, mean_q: 30.479989, mean_eps: 0.100000\n","     147922/2000000000: episode: 3968, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.154105, mean_q: 30.714396, mean_eps: 0.100000\n","     147962/2000000000: episode: 3969, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 71.279151, mean_q: 30.114184, mean_eps: 0.100000\n","     148000/2000000000: episode: 3970, duration: 4.667s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 73.112504, mean_q: 30.493695, mean_eps: 0.100000\n","     148040/2000000000: episode: 3971, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: -53.000, mean reward: -1.325 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 65.678609, mean_q: 31.029346, mean_eps: 0.100000\n","     148080/2000000000: episode: 3972, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: 85.700, mean reward:  2.143 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.916964, mean_q: 30.947013, mean_eps: 0.100000\n","     148113/2000000000: episode: 3973, duration: 4.184s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 76.897548, mean_q: 30.610615, mean_eps: 0.100000\n","     148145/2000000000: episode: 3974, duration: 3.893s, episode steps:  32, steps per second:   8, episode reward: 76.000, mean reward:  2.375 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 75.838982, mean_q: 30.696144, mean_eps: 0.100000\n","     148176/2000000000: episode: 3975, duration: 3.776s, episode steps:  31, steps per second:   8, episode reward: 54.400, mean reward:  1.755 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 73.811698, mean_q: 31.517769, mean_eps: 0.100000\n","     148211/2000000000: episode: 3976, duration: 4.222s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 74.906867, mean_q: 31.244005, mean_eps: 0.100000\n","     148251/2000000000: episode: 3977, duration: 4.694s, episode steps:  40, steps per second:   9, episode reward: 84.900, mean reward:  2.122 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 67.729595, mean_q: 30.259495, mean_eps: 0.100000\n","     148289/2000000000: episode: 3978, duration: 4.610s, episode steps:  38, steps per second:   8, episode reward: 196.400, mean reward:  5.168 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 76.253848, mean_q: 30.375486, mean_eps: 0.100000\n","     148329/2000000000: episode: 3979, duration: 4.704s, episode steps:  40, steps per second:   9, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.413544, mean_q: 30.168537, mean_eps: 0.100000\n","     148360/2000000000: episode: 3980, duration: 3.648s, episode steps:  31, steps per second:   8, episode reward: 27.600, mean reward:  0.890 [-20.000, 18.000], mean action: 1.226 [0.000, 2.000],  loss: 73.529992, mean_q: 31.611231, mean_eps: 0.100000\n","     148400/2000000000: episode: 3981, duration: 4.745s, episode steps:  40, steps per second:   8, episode reward: 99.900, mean reward:  2.498 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.563985, mean_q: 29.972898, mean_eps: 0.100000\n","     148440/2000000000: episode: 3982, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: -10.000, mean reward: -0.250 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 79.289514, mean_q: 30.545651, mean_eps: 0.100000\n","     148480/2000000000: episode: 3983, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward: 21.400, mean reward:  0.535 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.907799, mean_q: 30.304461, mean_eps: 0.100000\n","     148511/2000000000: episode: 3984, duration: 3.885s, episode steps:  31, steps per second:   8, episode reward: 152.300, mean reward:  4.913 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 73.372637, mean_q: 30.964316, mean_eps: 0.100000\n","     148543/2000000000: episode: 3985, duration: 3.972s, episode steps:  32, steps per second:   8, episode reward: -76.500, mean reward: -2.391 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 71.547168, mean_q: 30.626005, mean_eps: 0.100000\n","     148582/2000000000: episode: 3986, duration: 4.919s, episode steps:  39, steps per second:   8, episode reward: 28.600, mean reward:  0.733 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 71.233867, mean_q: 30.684408, mean_eps: 0.100000\n","     148622/2000000000: episode: 3987, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: 98.300, mean reward:  2.457 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 69.241232, mean_q: 32.058962, mean_eps: 0.100000\n","     148659/2000000000: episode: 3988, duration: 4.783s, episode steps:  37, steps per second:   8, episode reward: -194.900, mean reward: -5.268 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 67.029224, mean_q: 30.531769, mean_eps: 0.100000\n","     148699/2000000000: episode: 3989, duration: 4.845s, episode steps:  40, steps per second:   8, episode reward: 36.000, mean reward:  0.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.047493, mean_q: 30.424506, mean_eps: 0.100000\n","     148734/2000000000: episode: 3990, duration: 4.359s, episode steps:  35, steps per second:   8, episode reward: -43.100, mean reward: -1.231 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.941289, mean_q: 31.481238, mean_eps: 0.100000\n","     148774/2000000000: episode: 3991, duration: 5.040s, episode steps:  40, steps per second:   8, episode reward: 47.100, mean reward:  1.177 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.930923, mean_q: 30.346970, mean_eps: 0.100000\n","     148814/2000000000: episode: 3992, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: 144.000, mean reward:  3.600 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.278151, mean_q: 30.612069, mean_eps: 0.100000\n","     148848/2000000000: episode: 3993, duration: 4.258s, episode steps:  34, steps per second:   8, episode reward: 98.400, mean reward:  2.894 [-20.000, 18.000], mean action: 1.294 [0.000, 2.000],  loss: 71.708161, mean_q: 31.372477, mean_eps: 0.100000\n","     148880/2000000000: episode: 3994, duration: 4.010s, episode steps:  32, steps per second:   8, episode reward:  1.300, mean reward:  0.041 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 66.048160, mean_q: 31.272987, mean_eps: 0.100000\n","     148920/2000000000: episode: 3995, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: 142.600, mean reward:  3.565 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.283430, mean_q: 31.074150, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     148957/2000000000: episode: 3996, duration: 4.735s, episode steps:  37, steps per second:   8, episode reward: 26.100, mean reward:  0.705 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 79.877190, mean_q: 30.333501, mean_eps: 0.100000\n","     148990/2000000000: episode: 3997, duration: 4.260s, episode steps:  33, steps per second:   8, episode reward: 190.700, mean reward:  5.779 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 63.288501, mean_q: 31.107291, mean_eps: 0.100000\n","     149022/2000000000: episode: 3998, duration: 4.320s, episode steps:  32, steps per second:   7, episode reward:  9.000, mean reward:  0.281 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.920365, mean_q: 30.358499, mean_eps: 0.100000\n","     149055/2000000000: episode: 3999, duration: 4.348s, episode steps:  33, steps per second:   8, episode reward: 73.100, mean reward:  2.215 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.862556, mean_q: 30.672525, mean_eps: 0.100000\n","     149088/2000000000: episode: 4000, duration: 4.241s, episode steps:  33, steps per second:   8, episode reward: -25.300, mean reward: -0.767 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 75.709374, mean_q: 30.963955, mean_eps: 0.100000\n","     149127/2000000000: episode: 4001, duration: 4.847s, episode steps:  39, steps per second:   8, episode reward: 95.100, mean reward:  2.438 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 66.932160, mean_q: 30.090079, mean_eps: 0.100000\n","     149167/2000000000: episode: 4002, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward:  3.500, mean reward:  0.088 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.214034, mean_q: 30.954609, mean_eps: 0.100000\n","     149201/2000000000: episode: 4003, duration: 4.232s, episode steps:  34, steps per second:   8, episode reward: -54.600, mean reward: -1.606 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 67.923028, mean_q: 30.401240, mean_eps: 0.100000\n","     149241/2000000000: episode: 4004, duration: 5.091s, episode steps:  40, steps per second:   8, episode reward: 79.400, mean reward:  1.985 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 70.952735, mean_q: 30.906199, mean_eps: 0.100000\n","     149276/2000000000: episode: 4005, duration: 4.737s, episode steps:  35, steps per second:   7, episode reward: 163.400, mean reward:  4.669 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 74.523569, mean_q: 31.719697, mean_eps: 0.100000\n","     149308/2000000000: episode: 4006, duration: 4.202s, episode steps:  32, steps per second:   8, episode reward: 65.400, mean reward:  2.044 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.921200, mean_q: 31.167754, mean_eps: 0.100000\n","     149338/2000000000: episode: 4007, duration: 3.796s, episode steps:  30, steps per second:   8, episode reward: -12.500, mean reward: -0.417 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 71.854446, mean_q: 29.949786, mean_eps: 0.100000\n","     149378/2000000000: episode: 4008, duration: 5.075s, episode steps:  40, steps per second:   8, episode reward: 74.000, mean reward:  1.850 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.107711, mean_q: 30.033382, mean_eps: 0.100000\n","     149418/2000000000: episode: 4009, duration: 5.120s, episode steps:  40, steps per second:   8, episode reward: -47.000, mean reward: -1.175 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 65.160076, mean_q: 30.367786, mean_eps: 0.100000\n","     149458/2000000000: episode: 4010, duration: 5.335s, episode steps:  40, steps per second:   7, episode reward: -10.000, mean reward: -0.250 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 64.583157, mean_q: 31.623987, mean_eps: 0.100000\n","     149492/2000000000: episode: 4011, duration: 4.190s, episode steps:  34, steps per second:   8, episode reward: 151.600, mean reward:  4.459 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 71.963194, mean_q: 30.613091, mean_eps: 0.100000\n","     149532/2000000000: episode: 4012, duration: 5.134s, episode steps:  40, steps per second:   8, episode reward: 103.300, mean reward:  2.582 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.780302, mean_q: 30.358277, mean_eps: 0.100000\n","     149565/2000000000: episode: 4013, duration: 4.093s, episode steps:  33, steps per second:   8, episode reward: 80.700, mean reward:  2.445 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 75.380190, mean_q: 30.527599, mean_eps: 0.100000\n","     149605/2000000000: episode: 4014, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: 27.200, mean reward:  0.680 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.987168, mean_q: 30.427383, mean_eps: 0.100000\n","     149645/2000000000: episode: 4015, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: -172.000, mean reward: -4.300 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.748905, mean_q: 31.022663, mean_eps: 0.100000\n","     149675/2000000000: episode: 4016, duration: 3.805s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 71.900439, mean_q: 31.131910, mean_eps: 0.100000\n","     149715/2000000000: episode: 4017, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.155764, mean_q: 30.311941, mean_eps: 0.100000\n","     149755/2000000000: episode: 4018, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward: -28.500, mean reward: -0.712 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.650218, mean_q: 30.827006, mean_eps: 0.100000\n","     149793/2000000000: episode: 4019, duration: 4.742s, episode steps:  38, steps per second:   8, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 71.445600, mean_q: 30.869870, mean_eps: 0.100000\n","     149831/2000000000: episode: 4020, duration: 4.616s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 72.317382, mean_q: 31.281891, mean_eps: 0.100000\n","     149870/2000000000: episode: 4021, duration: 4.779s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 79.336804, mean_q: 30.984034, mean_eps: 0.100000\n","     149903/2000000000: episode: 4022, duration: 4.223s, episode steps:  33, steps per second:   8, episode reward: -27.300, mean reward: -0.827 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 73.092404, mean_q: 30.777101, mean_eps: 0.100000\n","     149943/2000000000: episode: 4023, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: -32.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 71.094290, mean_q: 30.701778, mean_eps: 0.100000\n","     149982/2000000000: episode: 4024, duration: 4.888s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 70.081926, mean_q: 30.884260, mean_eps: 0.100000\n","     150022/2000000000: episode: 4025, duration: 4.827s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.899190, mean_q: 30.369006, mean_eps: 0.100000\n","     150059/2000000000: episode: 4026, duration: 4.412s, episode steps:  37, steps per second:   8, episode reward: 128.600, mean reward:  3.476 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 80.974237, mean_q: 31.847374, mean_eps: 0.100000\n","     150092/2000000000: episode: 4027, duration: 3.939s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.048790, mean_q: 31.818308, mean_eps: 0.100000\n","     150132/2000000000: episode: 4028, duration: 4.759s, episode steps:  40, steps per second:   8, episode reward: 129.800, mean reward:  3.245 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.032077, mean_q: 32.524318, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     150172/2000000000: episode: 4029, duration: 4.759s, episode steps:  40, steps per second:   8, episode reward: 96.900, mean reward:  2.423 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.248033, mean_q: 32.010262, mean_eps: 0.100000\n","     150212/2000000000: episode: 4030, duration: 4.881s, episode steps:  40, steps per second:   8, episode reward: -91.200, mean reward: -2.280 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.046000, mean_q: 32.179818, mean_eps: 0.100000\n","     150251/2000000000: episode: 4031, duration: 4.816s, episode steps:  39, steps per second:   8, episode reward: 65.400, mean reward:  1.677 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 83.453863, mean_q: 31.324958, mean_eps: 0.100000\n","     150280/2000000000: episode: 4032, duration: 3.494s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 76.545472, mean_q: 31.935024, mean_eps: 0.100000\n","     150320/2000000000: episode: 4033, duration: 4.864s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.233590, mean_q: 31.742851, mean_eps: 0.100000\n","     150353/2000000000: episode: 4034, duration: 4.157s, episode steps:  33, steps per second:   8, episode reward: 104.400, mean reward:  3.164 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 76.248810, mean_q: 32.197236, mean_eps: 0.100000\n","     150393/2000000000: episode: 4035, duration: 4.949s, episode steps:  40, steps per second:   8, episode reward: -27.600, mean reward: -0.690 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 83.652056, mean_q: 31.832282, mean_eps: 0.100000\n","     150429/2000000000: episode: 4036, duration: 4.508s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 71.142271, mean_q: 32.183686, mean_eps: 0.100000\n","     150469/2000000000: episode: 4037, duration: 4.845s, episode steps:  40, steps per second:   8, episode reward:  8.300, mean reward:  0.208 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.317354, mean_q: 32.325227, mean_eps: 0.100000\n","     150505/2000000000: episode: 4038, duration: 4.331s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 77.980467, mean_q: 31.919636, mean_eps: 0.100000\n","     150536/2000000000: episode: 4039, duration: 3.639s, episode steps:  31, steps per second:   9, episode reward: -60.900, mean reward: -1.965 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 75.864159, mean_q: 32.341360, mean_eps: 0.100000\n","     150571/2000000000: episode: 4040, duration: 4.160s, episode steps:  35, steps per second:   8, episode reward: 66.000, mean reward:  1.886 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 74.849447, mean_q: 32.305325, mean_eps: 0.100000\n","     150611/2000000000: episode: 4041, duration: 4.953s, episode steps:  40, steps per second:   8, episode reward: -57.900, mean reward: -1.447 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 78.929115, mean_q: 32.220081, mean_eps: 0.100000\n","     150651/2000000000: episode: 4042, duration: 4.891s, episode steps:  40, steps per second:   8, episode reward: 25.800, mean reward:  0.645 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.280676, mean_q: 31.471380, mean_eps: 0.100000\n","     150688/2000000000: episode: 4043, duration: 4.406s, episode steps:  37, steps per second:   8, episode reward: 61.100, mean reward:  1.651 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 81.317700, mean_q: 32.232132, mean_eps: 0.100000\n","     150720/2000000000: episode: 4044, duration: 3.927s, episode steps:  32, steps per second:   8, episode reward: 117.000, mean reward:  3.656 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.829217, mean_q: 32.354234, mean_eps: 0.100000\n","     150760/2000000000: episode: 4045, duration: 5.013s, episode steps:  40, steps per second:   8, episode reward: 30.200, mean reward:  0.755 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.286833, mean_q: 31.519256, mean_eps: 0.100000\n","     150794/2000000000: episode: 4046, duration: 4.218s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 78.214353, mean_q: 32.436863, mean_eps: 0.100000\n","     150834/2000000000: episode: 4047, duration: 5.177s, episode steps:  40, steps per second:   8, episode reward: 96.500, mean reward:  2.413 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.346116, mean_q: 32.334518, mean_eps: 0.100000\n","     150874/2000000000: episode: 4048, duration: 5.238s, episode steps:  40, steps per second:   8, episode reward: 82.900, mean reward:  2.072 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.417912, mean_q: 31.797976, mean_eps: 0.100000\n","     150914/2000000000: episode: 4049, duration: 5.288s, episode steps:  40, steps per second:   8, episode reward: 151.300, mean reward:  3.783 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 68.817409, mean_q: 31.935949, mean_eps: 0.100000\n","     150954/2000000000: episode: 4050, duration: 5.250s, episode steps:  40, steps per second:   8, episode reward: -9.100, mean reward: -0.228 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 74.215061, mean_q: 32.075060, mean_eps: 0.100000\n","     150990/2000000000: episode: 4051, duration: 4.540s, episode steps:  36, steps per second:   8, episode reward: -39.800, mean reward: -1.106 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 73.381087, mean_q: 32.718455, mean_eps: 0.100000\n","     151022/2000000000: episode: 4052, duration: 3.931s, episode steps:  32, steps per second:   8, episode reward: 18.400, mean reward:  0.575 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 72.551893, mean_q: 32.386185, mean_eps: 0.100000\n","     151062/2000000000: episode: 4053, duration: 4.975s, episode steps:  40, steps per second:   8, episode reward:  8.600, mean reward:  0.215 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.902701, mean_q: 32.080424, mean_eps: 0.100000\n","     151099/2000000000: episode: 4054, duration: 4.580s, episode steps:  37, steps per second:   8, episode reward: 136.800, mean reward:  3.697 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.573514, mean_q: 31.972429, mean_eps: 0.100000\n","     151139/2000000000: episode: 4055, duration: 5.067s, episode steps:  40, steps per second:   8, episode reward: 55.400, mean reward:  1.385 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 81.745578, mean_q: 32.644027, mean_eps: 0.100000\n","     151176/2000000000: episode: 4056, duration: 4.567s, episode steps:  37, steps per second:   8, episode reward: 95.200, mean reward:  2.573 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 68.935237, mean_q: 32.032103, mean_eps: 0.100000\n","     151212/2000000000: episode: 4057, duration: 4.511s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 65.282863, mean_q: 32.605041, mean_eps: 0.100000\n","     151252/2000000000: episode: 4058, duration: 5.007s, episode steps:  40, steps per second:   8, episode reward: 17.100, mean reward:  0.427 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.343602, mean_q: 32.101193, mean_eps: 0.100000\n","     151292/2000000000: episode: 4059, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: 51.900, mean reward:  1.298 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.322189, mean_q: 31.913089, mean_eps: 0.100000\n","     151332/2000000000: episode: 4060, duration: 5.068s, episode steps:  40, steps per second:   8, episode reward: 54.200, mean reward:  1.355 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.927319, mean_q: 31.571937, mean_eps: 0.100000\n","     151372/2000000000: episode: 4061, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: -33.600, mean reward: -0.840 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.136908, mean_q: 32.244783, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     151412/2000000000: episode: 4062, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: 36.300, mean reward:  0.908 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 69.952556, mean_q: 31.925693, mean_eps: 0.100000\n","     151452/2000000000: episode: 4063, duration: 5.363s, episode steps:  40, steps per second:   7, episode reward: 43.900, mean reward:  1.097 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.892620, mean_q: 32.499597, mean_eps: 0.100000\n","     151491/2000000000: episode: 4064, duration: 4.856s, episode steps:  39, steps per second:   8, episode reward: -27.100, mean reward: -0.695 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 72.670751, mean_q: 31.761643, mean_eps: 0.100000\n","     151521/2000000000: episode: 4065, duration: 4.002s, episode steps:  30, steps per second:   7, episode reward: 58.800, mean reward:  1.960 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 77.375540, mean_q: 31.493993, mean_eps: 0.100000\n","     151556/2000000000: episode: 4066, duration: 4.399s, episode steps:  35, steps per second:   8, episode reward: 142.900, mean reward:  4.083 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 77.145550, mean_q: 31.710029, mean_eps: 0.100000\n","     151596/2000000000: episode: 4067, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 16.500, mean reward:  0.412 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.757804, mean_q: 32.267120, mean_eps: 0.100000\n","     151625/2000000000: episode: 4068, duration: 3.669s, episode steps:  29, steps per second:   8, episode reward: 24.100, mean reward:  0.831 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 79.547634, mean_q: 31.651254, mean_eps: 0.100000\n","     151665/2000000000: episode: 4069, duration: 4.918s, episode steps:  40, steps per second:   8, episode reward: 16.600, mean reward:  0.415 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 79.701220, mean_q: 32.219454, mean_eps: 0.100000\n","     151701/2000000000: episode: 4070, duration: 4.479s, episode steps:  36, steps per second:   8, episode reward: 59.900, mean reward:  1.664 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 73.794047, mean_q: 32.332135, mean_eps: 0.100000\n","     151741/2000000000: episode: 4071, duration: 4.624s, episode steps:  40, steps per second:   9, episode reward: 62.000, mean reward:  1.550 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 71.710851, mean_q: 32.696289, mean_eps: 0.100000\n","     151774/2000000000: episode: 4072, duration: 3.840s, episode steps:  33, steps per second:   9, episode reward: 16.200, mean reward:  0.491 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.726973, mean_q: 32.357030, mean_eps: 0.100000\n","     151799/2000000000: episode: 4073, duration: 3.171s, episode steps:  25, steps per second:   8, episode reward:  1.100, mean reward:  0.044 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.920926, mean_q: 31.688557, mean_eps: 0.100000\n","     151839/2000000000: episode: 4074, duration: 4.837s, episode steps:  40, steps per second:   8, episode reward: 138.400, mean reward:  3.460 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 73.651990, mean_q: 32.099210, mean_eps: 0.100000\n","     151873/2000000000: episode: 4075, duration: 4.114s, episode steps:  34, steps per second:   8, episode reward: 26.000, mean reward:  0.765 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.392380, mean_q: 31.881773, mean_eps: 0.100000\n","     151909/2000000000: episode: 4076, duration: 4.385s, episode steps:  36, steps per second:   8, episode reward: 78.800, mean reward:  2.189 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 83.779121, mean_q: 31.931943, mean_eps: 0.100000\n","     151949/2000000000: episode: 4077, duration: 4.763s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.125696, mean_q: 31.659131, mean_eps: 0.100000\n","     151989/2000000000: episode: 4078, duration: 4.796s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 74.735053, mean_q: 32.796957, mean_eps: 0.100000\n","     152020/2000000000: episode: 4079, duration: 3.920s, episode steps:  31, steps per second:   8, episode reward: -50.700, mean reward: -1.635 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 75.670449, mean_q: 31.850714, mean_eps: 0.100000\n","     152058/2000000000: episode: 4080, duration: 4.531s, episode steps:  38, steps per second:   8, episode reward: -51.500, mean reward: -1.355 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 78.159505, mean_q: 31.227156, mean_eps: 0.100000\n","     152098/2000000000: episode: 4081, duration: 4.835s, episode steps:  40, steps per second:   8, episode reward: 92.400, mean reward:  2.310 [-20.000, 18.100], mean action: 1.400 [0.000, 2.000],  loss: 74.459252, mean_q: 31.709082, mean_eps: 0.100000\n","     152129/2000000000: episode: 4082, duration: 3.623s, episode steps:  31, steps per second:   9, episode reward: -94.600, mean reward: -3.052 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 77.338689, mean_q: 31.975755, mean_eps: 0.100000\n","     152167/2000000000: episode: 4083, duration: 4.471s, episode steps:  38, steps per second:   8, episode reward: 92.200, mean reward:  2.426 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 73.126848, mean_q: 31.997245, mean_eps: 0.100000\n","     152207/2000000000: episode: 4084, duration: 5.014s, episode steps:  40, steps per second:   8, episode reward: -66.300, mean reward: -1.657 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.843301, mean_q: 32.089547, mean_eps: 0.100000\n","     152247/2000000000: episode: 4085, duration: 4.956s, episode steps:  40, steps per second:   8, episode reward: 85.100, mean reward:  2.127 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.977093, mean_q: 32.323144, mean_eps: 0.100000\n","     152278/2000000000: episode: 4086, duration: 3.985s, episode steps:  31, steps per second:   8, episode reward: -54.000, mean reward: -1.742 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.714817, mean_q: 32.490462, mean_eps: 0.100000\n","     152312/2000000000: episode: 4087, duration: 4.310s, episode steps:  34, steps per second:   8, episode reward: 40.700, mean reward:  1.197 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 76.371486, mean_q: 31.973327, mean_eps: 0.100000\n","     152346/2000000000: episode: 4088, duration: 4.155s, episode steps:  34, steps per second:   8, episode reward:  1.300, mean reward:  0.038 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 68.757177, mean_q: 32.013341, mean_eps: 0.100000\n","     152386/2000000000: episode: 4089, duration: 4.758s, episode steps:  40, steps per second:   8, episode reward: 102.600, mean reward:  2.565 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.601712, mean_q: 32.153819, mean_eps: 0.100000\n","     152419/2000000000: episode: 4090, duration: 4.250s, episode steps:  33, steps per second:   8, episode reward: -5.700, mean reward: -0.173 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.786572, mean_q: 32.769888, mean_eps: 0.100000\n","     152459/2000000000: episode: 4091, duration: 4.752s, episode steps:  40, steps per second:   8, episode reward:  1.200, mean reward:  0.030 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.756938, mean_q: 31.898838, mean_eps: 0.100000\n","     152499/2000000000: episode: 4092, duration: 4.743s, episode steps:  40, steps per second:   8, episode reward: -77.600, mean reward: -1.940 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.126614, mean_q: 31.808286, mean_eps: 0.100000\n","     152539/2000000000: episode: 4093, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 21.200, mean reward:  0.530 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 68.715228, mean_q: 32.353492, mean_eps: 0.100000\n","     152574/2000000000: episode: 4094, duration: 4.412s, episode steps:  35, steps per second:   8, episode reward: 151.500, mean reward:  4.329 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 70.563963, mean_q: 31.989363, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     152609/2000000000: episode: 4095, duration: 4.486s, episode steps:  35, steps per second:   8, episode reward: 154.100, mean reward:  4.403 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 77.257633, mean_q: 31.959122, mean_eps: 0.100000\n","     152642/2000000000: episode: 4096, duration: 4.083s, episode steps:  33, steps per second:   8, episode reward: 26.000, mean reward:  0.788 [-20.000, 18.000], mean action: 1.273 [0.000, 2.000],  loss: 73.424723, mean_q: 32.943986, mean_eps: 0.100000\n","     152682/2000000000: episode: 4097, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: 17.000, mean reward:  0.425 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.980872, mean_q: 31.649629, mean_eps: 0.100000\n","     152716/2000000000: episode: 4098, duration: 4.069s, episode steps:  34, steps per second:   8, episode reward: 117.700, mean reward:  3.462 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.492091, mean_q: 32.205485, mean_eps: 0.100000\n","     152745/2000000000: episode: 4099, duration: 3.548s, episode steps:  29, steps per second:   8, episode reward: 149.300, mean reward:  5.148 [-20.000, 18.000], mean action: 0.724 [0.000, 2.000],  loss: 80.346171, mean_q: 31.696059, mean_eps: 0.100000\n","     152785/2000000000: episode: 4100, duration: 5.361s, episode steps:  40, steps per second:   7, episode reward: -29.700, mean reward: -0.742 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.041870, mean_q: 31.968439, mean_eps: 0.100000\n","     152820/2000000000: episode: 4101, duration: 4.608s, episode steps:  35, steps per second:   8, episode reward: 49.900, mean reward:  1.426 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 80.568031, mean_q: 32.270576, mean_eps: 0.100000\n","     152860/2000000000: episode: 4102, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: 66.800, mean reward:  1.670 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 74.019690, mean_q: 32.306343, mean_eps: 0.100000\n","     152895/2000000000: episode: 4103, duration: 4.383s, episode steps:  35, steps per second:   8, episode reward: 165.100, mean reward:  4.717 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 67.143857, mean_q: 31.565382, mean_eps: 0.100000\n","     152935/2000000000: episode: 4104, duration: 4.871s, episode steps:  40, steps per second:   8, episode reward: 104.900, mean reward:  2.623 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.400641, mean_q: 32.797188, mean_eps: 0.100000\n","     152975/2000000000: episode: 4105, duration: 4.792s, episode steps:  40, steps per second:   8, episode reward: -11.700, mean reward: -0.293 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.181209, mean_q: 31.859059, mean_eps: 0.100000\n","     153012/2000000000: episode: 4106, duration: 4.548s, episode steps:  37, steps per second:   8, episode reward: 55.500, mean reward:  1.500 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 73.138491, mean_q: 31.980386, mean_eps: 0.100000\n","     153052/2000000000: episode: 4107, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: 10.400, mean reward:  0.260 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.661615, mean_q: 31.292993, mean_eps: 0.100000\n","     153087/2000000000: episode: 4108, duration: 4.337s, episode steps:  35, steps per second:   8, episode reward: 82.700, mean reward:  2.363 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 72.984022, mean_q: 32.498998, mean_eps: 0.100000\n","     153123/2000000000: episode: 4109, duration: 4.500s, episode steps:  36, steps per second:   8, episode reward: 161.200, mean reward:  4.478 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 73.855719, mean_q: 31.750762, mean_eps: 0.100000\n","     153150/2000000000: episode: 4110, duration: 3.249s, episode steps:  27, steps per second:   8, episode reward: 93.800, mean reward:  3.474 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 70.741794, mean_q: 32.033317, mean_eps: 0.100000\n","     153190/2000000000: episode: 4111, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward: 81.100, mean reward:  2.027 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 66.930799, mean_q: 32.008920, mean_eps: 0.100000\n","     153228/2000000000: episode: 4112, duration: 4.769s, episode steps:  38, steps per second:   8, episode reward: -3.400, mean reward: -0.089 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 73.971406, mean_q: 32.797197, mean_eps: 0.100000\n","     153259/2000000000: episode: 4113, duration: 3.566s, episode steps:  31, steps per second:   9, episode reward: 113.000, mean reward:  3.645 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.805261, mean_q: 32.354580, mean_eps: 0.100000\n","     153299/2000000000: episode: 4114, duration: 4.631s, episode steps:  40, steps per second:   9, episode reward: 38.800, mean reward:  0.970 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 75.821377, mean_q: 32.080634, mean_eps: 0.100000\n","     153339/2000000000: episode: 4115, duration: 4.834s, episode steps:  40, steps per second:   8, episode reward: 104.100, mean reward:  2.602 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.020634, mean_q: 31.197094, mean_eps: 0.100000\n","     153379/2000000000: episode: 4116, duration: 4.807s, episode steps:  40, steps per second:   8, episode reward: 79.100, mean reward:  1.977 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.010050, mean_q: 32.472073, mean_eps: 0.100000\n","     153419/2000000000: episode: 4117, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 41.800, mean reward:  1.045 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.633502, mean_q: 32.055929, mean_eps: 0.100000\n","     153459/2000000000: episode: 4118, duration: 4.919s, episode steps:  40, steps per second:   8, episode reward: 36.800, mean reward:  0.920 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.995380, mean_q: 31.679955, mean_eps: 0.100000\n","     153499/2000000000: episode: 4119, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 79.092513, mean_q: 32.710415, mean_eps: 0.100000\n","     153530/2000000000: episode: 4120, duration: 3.900s, episode steps:  31, steps per second:   8, episode reward: 92.000, mean reward:  2.968 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 83.773454, mean_q: 31.919715, mean_eps: 0.100000\n","     153567/2000000000: episode: 4121, duration: 4.591s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 76.140251, mean_q: 32.663016, mean_eps: 0.100000\n","     153604/2000000000: episode: 4122, duration: 4.491s, episode steps:  37, steps per second:   8, episode reward: 82.600, mean reward:  2.232 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 69.952635, mean_q: 32.619624, mean_eps: 0.100000\n","     153641/2000000000: episode: 4123, duration: 4.443s, episode steps:  37, steps per second:   8, episode reward: -96.800, mean reward: -2.616 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 70.206669, mean_q: 31.644438, mean_eps: 0.100000\n","     153670/2000000000: episode: 4124, duration: 3.617s, episode steps:  29, steps per second:   8, episode reward: 108.000, mean reward:  3.724 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 70.259872, mean_q: 32.928892, mean_eps: 0.100000\n","     153708/2000000000: episode: 4125, duration: 4.477s, episode steps:  38, steps per second:   8, episode reward: -134.000, mean reward: -3.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 76.540233, mean_q: 32.278582, mean_eps: 0.100000\n","     153748/2000000000: episode: 4126, duration: 4.811s, episode steps:  40, steps per second:   8, episode reward: 94.100, mean reward:  2.352 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.219476, mean_q: 31.951955, mean_eps: 0.100000\n","     153788/2000000000: episode: 4127, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward:  5.800, mean reward:  0.145 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.111031, mean_q: 32.169705, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     153821/2000000000: episode: 4128, duration: 4.727s, episode steps:  33, steps per second:   7, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 72.834587, mean_q: 32.843685, mean_eps: 0.100000\n","     153861/2000000000: episode: 4129, duration: 5.329s, episode steps:  40, steps per second:   8, episode reward: 63.000, mean reward:  1.575 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.902968, mean_q: 32.327324, mean_eps: 0.100000\n","     153889/2000000000: episode: 4130, duration: 3.950s, episode steps:  28, steps per second:   7, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.510034, mean_q: 32.469526, mean_eps: 0.100000\n","     153921/2000000000: episode: 4131, duration: 4.100s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 75.814117, mean_q: 31.720979, mean_eps: 0.100000\n","     153961/2000000000: episode: 4132, duration: 5.532s, episode steps:  40, steps per second:   7, episode reward: -48.600, mean reward: -1.215 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 71.817047, mean_q: 32.732930, mean_eps: 0.100000\n","     154001/2000000000: episode: 4133, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.367225, mean_q: 31.864426, mean_eps: 0.100000\n","     154041/2000000000: episode: 4134, duration: 4.965s, episode steps:  40, steps per second:   8, episode reward: 83.100, mean reward:  2.077 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.051619, mean_q: 32.070880, mean_eps: 0.100000\n","     154081/2000000000: episode: 4135, duration: 5.125s, episode steps:  40, steps per second:   8, episode reward: -95.300, mean reward: -2.382 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 77.860420, mean_q: 31.650653, mean_eps: 0.100000\n","     154114/2000000000: episode: 4136, duration: 4.204s, episode steps:  33, steps per second:   8, episode reward: 62.900, mean reward:  1.906 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 72.132655, mean_q: 32.889548, mean_eps: 0.100000\n","     154154/2000000000: episode: 4137, duration: 4.952s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.001781, mean_q: 32.213773, mean_eps: 0.100000\n","     154189/2000000000: episode: 4138, duration: 4.368s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 73.572629, mean_q: 32.905557, mean_eps: 0.100000\n","     154228/2000000000: episode: 4139, duration: 4.763s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 68.814006, mean_q: 32.417388, mean_eps: 0.100000\n","     154268/2000000000: episode: 4140, duration: 4.709s, episode steps:  40, steps per second:   8, episode reward: 49.200, mean reward:  1.230 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.294273, mean_q: 32.272329, mean_eps: 0.100000\n","     154302/2000000000: episode: 4141, duration: 4.111s, episode steps:  34, steps per second:   8, episode reward: -98.900, mean reward: -2.909 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 68.999008, mean_q: 32.895783, mean_eps: 0.100000\n","     154338/2000000000: episode: 4142, duration: 4.924s, episode steps:  36, steps per second:   7, episode reward: 65.300, mean reward:  1.814 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 74.161566, mean_q: 31.644664, mean_eps: 0.100000\n","     154377/2000000000: episode: 4143, duration: 5.483s, episode steps:  39, steps per second:   7, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 76.407350, mean_q: 31.676145, mean_eps: 0.100000\n","     154417/2000000000: episode: 4144, duration: 5.892s, episode steps:  40, steps per second:   7, episode reward: 36.600, mean reward:  0.915 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.209681, mean_q: 31.921852, mean_eps: 0.100000\n","     154456/2000000000: episode: 4145, duration: 5.174s, episode steps:  39, steps per second:   8, episode reward: 32.300, mean reward:  0.828 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 72.737341, mean_q: 32.122757, mean_eps: 0.100000\n","     154487/2000000000: episode: 4146, duration: 4.543s, episode steps:  31, steps per second:   7, episode reward: -22.300, mean reward: -0.719 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 69.575231, mean_q: 32.416253, mean_eps: 0.100000\n","     154526/2000000000: episode: 4147, duration: 6.097s, episode steps:  39, steps per second:   6, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 69.257419, mean_q: 31.858894, mean_eps: 0.100000\n","     154553/2000000000: episode: 4148, duration: 4.713s, episode steps:  27, steps per second:   6, episode reward: -13.000, mean reward: -0.481 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 75.254441, mean_q: 32.239341, mean_eps: 0.100000\n","     154588/2000000000: episode: 4149, duration: 5.215s, episode steps:  35, steps per second:   7, episode reward: 128.600, mean reward:  3.674 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 78.433206, mean_q: 32.536067, mean_eps: 0.100000\n","     154623/2000000000: episode: 4150, duration: 5.265s, episode steps:  35, steps per second:   7, episode reward: -43.800, mean reward: -1.251 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 67.432840, mean_q: 32.578769, mean_eps: 0.100000\n","     154659/2000000000: episode: 4151, duration: 5.584s, episode steps:  36, steps per second:   6, episode reward: -69.400, mean reward: -1.928 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 71.747599, mean_q: 32.124260, mean_eps: 0.100000\n","     154694/2000000000: episode: 4152, duration: 5.217s, episode steps:  35, steps per second:   7, episode reward: -63.700, mean reward: -1.820 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.422138, mean_q: 32.284478, mean_eps: 0.100000\n","     154734/2000000000: episode: 4153, duration: 5.952s, episode steps:  40, steps per second:   7, episode reward: 92.900, mean reward:  2.323 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.197822, mean_q: 32.640555, mean_eps: 0.100000\n","     154767/2000000000: episode: 4154, duration: 5.727s, episode steps:  33, steps per second:   6, episode reward: 168.900, mean reward:  5.118 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 73.837056, mean_q: 31.558359, mean_eps: 0.100000\n","     154793/2000000000: episode: 4155, duration: 4.615s, episode steps:  26, steps per second:   6, episode reward:  3.000, mean reward:  0.115 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 73.437686, mean_q: 32.581428, mean_eps: 0.100000\n","     154819/2000000000: episode: 4156, duration: 4.188s, episode steps:  26, steps per second:   6, episode reward: -66.400, mean reward: -2.554 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 88.765341, mean_q: 31.305522, mean_eps: 0.100000\n","     154859/2000000000: episode: 4157, duration: 6.739s, episode steps:  40, steps per second:   6, episode reward: -38.400, mean reward: -0.960 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.943340, mean_q: 32.005471, mean_eps: 0.100000\n","     154899/2000000000: episode: 4158, duration: 6.267s, episode steps:  40, steps per second:   6, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.242599, mean_q: 32.218966, mean_eps: 0.100000\n","     154939/2000000000: episode: 4159, duration: 6.221s, episode steps:  40, steps per second:   6, episode reward: 22.300, mean reward:  0.557 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.043885, mean_q: 31.596392, mean_eps: 0.100000\n","     154979/2000000000: episode: 4160, duration: 6.428s, episode steps:  40, steps per second:   6, episode reward: 35.700, mean reward:  0.892 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.091159, mean_q: 32.469872, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     155015/2000000000: episode: 4161, duration: 4.964s, episode steps:  36, steps per second:   7, episode reward: -0.100, mean reward: -0.003 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 74.630384, mean_q: 32.070777, mean_eps: 0.100000\n","     155055/2000000000: episode: 4162, duration: 5.752s, episode steps:  40, steps per second:   7, episode reward: 91.400, mean reward:  2.285 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.652719, mean_q: 32.213534, mean_eps: 0.100000\n","     155095/2000000000: episode: 4163, duration: 5.450s, episode steps:  40, steps per second:   7, episode reward: 73.100, mean reward:  1.828 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 73.992410, mean_q: 31.566097, mean_eps: 0.100000\n","     155135/2000000000: episode: 4164, duration: 5.693s, episode steps:  40, steps per second:   7, episode reward: 18.400, mean reward:  0.460 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.563599, mean_q: 31.947225, mean_eps: 0.100000\n","     155175/2000000000: episode: 4165, duration: 5.380s, episode steps:  40, steps per second:   7, episode reward: 228.000, mean reward:  5.700 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 71.557718, mean_q: 31.926253, mean_eps: 0.100000\n","     155215/2000000000: episode: 4166, duration: 5.292s, episode steps:  40, steps per second:   8, episode reward:  0.400, mean reward:  0.010 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.112488, mean_q: 31.738143, mean_eps: 0.100000\n","     155253/2000000000: episode: 4167, duration: 5.032s, episode steps:  38, steps per second:   8, episode reward: -65.800, mean reward: -1.732 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 81.689567, mean_q: 31.282130, mean_eps: 0.100000\n","     155293/2000000000: episode: 4168, duration: 5.515s, episode steps:  40, steps per second:   7, episode reward: -85.700, mean reward: -2.143 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 63.530480, mean_q: 32.621075, mean_eps: 0.100000\n","     155333/2000000000: episode: 4169, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.359252, mean_q: 32.425448, mean_eps: 0.100000\n","     155371/2000000000: episode: 4170, duration: 5.117s, episode steps:  38, steps per second:   7, episode reward: 62.100, mean reward:  1.634 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 69.782357, mean_q: 33.004891, mean_eps: 0.100000\n","     155411/2000000000: episode: 4171, duration: 5.320s, episode steps:  40, steps per second:   8, episode reward:  1.000, mean reward:  0.025 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 63.829185, mean_q: 32.144923, mean_eps: 0.100000\n","     155451/2000000000: episode: 4172, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward:  8.400, mean reward:  0.210 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.633103, mean_q: 32.154764, mean_eps: 0.100000\n","     155484/2000000000: episode: 4173, duration: 4.370s, episode steps:  33, steps per second:   8, episode reward: 35.800, mean reward:  1.085 [-20.000, 18.000], mean action: 1.273 [0.000, 2.000],  loss: 73.328377, mean_q: 32.567369, mean_eps: 0.100000\n","     155523/2000000000: episode: 4174, duration: 4.955s, episode steps:  39, steps per second:   8, episode reward: 73.700, mean reward:  1.890 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 76.382733, mean_q: 31.597209, mean_eps: 0.100000\n","     155554/2000000000: episode: 4175, duration: 3.956s, episode steps:  31, steps per second:   8, episode reward: -1.100, mean reward: -0.035 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 75.048573, mean_q: 32.501093, mean_eps: 0.100000\n","     155589/2000000000: episode: 4176, duration: 4.694s, episode steps:  35, steps per second:   7, episode reward: 226.700, mean reward:  6.477 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 68.577505, mean_q: 32.217095, mean_eps: 0.100000\n","     155622/2000000000: episode: 4177, duration: 4.102s, episode steps:  33, steps per second:   8, episode reward: 43.400, mean reward:  1.315 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 73.319487, mean_q: 32.132100, mean_eps: 0.100000\n","     155662/2000000000: episode: 4178, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.168217, mean_q: 32.241676, mean_eps: 0.100000\n","     155689/2000000000: episode: 4179, duration: 3.390s, episode steps:  27, steps per second:   8, episode reward: 57.100, mean reward:  2.115 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 78.751254, mean_q: 32.922062, mean_eps: 0.100000\n","     155726/2000000000: episode: 4180, duration: 4.809s, episode steps:  37, steps per second:   8, episode reward: 53.200, mean reward:  1.438 [-20.000, 18.200], mean action: 1.324 [0.000, 2.000],  loss: 71.250658, mean_q: 32.001347, mean_eps: 0.100000\n","     155764/2000000000: episode: 4181, duration: 5.118s, episode steps:  38, steps per second:   7, episode reward:  4.400, mean reward:  0.116 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 72.700207, mean_q: 32.057120, mean_eps: 0.100000\n","     155804/2000000000: episode: 4182, duration: 5.433s, episode steps:  40, steps per second:   7, episode reward: 77.400, mean reward:  1.935 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.004979, mean_q: 32.172053, mean_eps: 0.100000\n","     155836/2000000000: episode: 4183, duration: 4.708s, episode steps:  32, steps per second:   7, episode reward: 30.600, mean reward:  0.956 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 72.889961, mean_q: 32.485871, mean_eps: 0.100000\n","     155872/2000000000: episode: 4184, duration: 4.970s, episode steps:  36, steps per second:   7, episode reward: 37.400, mean reward:  1.039 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 77.671044, mean_q: 32.137191, mean_eps: 0.100000\n","     155912/2000000000: episode: 4185, duration: 5.423s, episode steps:  40, steps per second:   7, episode reward: -3.800, mean reward: -0.095 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.234677, mean_q: 31.757570, mean_eps: 0.100000\n","     155939/2000000000: episode: 4186, duration: 4.085s, episode steps:  27, steps per second:   7, episode reward: 85.400, mean reward:  3.163 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 77.781117, mean_q: 32.301790, mean_eps: 0.100000\n","     155979/2000000000: episode: 4187, duration: 6.026s, episode steps:  40, steps per second:   7, episode reward: 168.600, mean reward:  4.215 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.198289, mean_q: 31.572145, mean_eps: 0.100000\n","     156009/2000000000: episode: 4188, duration: 4.853s, episode steps:  30, steps per second:   6, episode reward: -12.300, mean reward: -0.410 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 76.240057, mean_q: 32.819216, mean_eps: 0.100000\n","     156035/2000000000: episode: 4189, duration: 3.834s, episode steps:  26, steps per second:   7, episode reward: 23.900, mean reward:  0.919 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 63.517331, mean_q: 32.504468, mean_eps: 0.100000\n","     156075/2000000000: episode: 4190, duration: 5.322s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.854939, mean_q: 31.870526, mean_eps: 0.100000\n","     156110/2000000000: episode: 4191, duration: 4.814s, episode steps:  35, steps per second:   7, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 75.406927, mean_q: 31.850331, mean_eps: 0.100000\n","     156150/2000000000: episode: 4192, duration: 5.414s, episode steps:  40, steps per second:   7, episode reward: -10.100, mean reward: -0.253 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.167717, mean_q: 31.850037, mean_eps: 0.100000\n","     156190/2000000000: episode: 4193, duration: 5.550s, episode steps:  40, steps per second:   7, episode reward: -11.000, mean reward: -0.275 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.683988, mean_q: 32.402433, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     156230/2000000000: episode: 4194, duration: 5.866s, episode steps:  40, steps per second:   7, episode reward: -64.900, mean reward: -1.622 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.826459, mean_q: 32.730826, mean_eps: 0.100000\n","     156270/2000000000: episode: 4195, duration: 5.468s, episode steps:  40, steps per second:   7, episode reward: 48.500, mean reward:  1.212 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.949975, mean_q: 32.688910, mean_eps: 0.100000\n","     156300/2000000000: episode: 4196, duration: 4.370s, episode steps:  30, steps per second:   7, episode reward: 67.100, mean reward:  2.237 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 82.440851, mean_q: 33.339275, mean_eps: 0.100000\n","     156329/2000000000: episode: 4197, duration: 4.085s, episode steps:  29, steps per second:   7, episode reward: 41.800, mean reward:  1.441 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 74.119432, mean_q: 32.385207, mean_eps: 0.100000\n","     156369/2000000000: episode: 4198, duration: 5.704s, episode steps:  40, steps per second:   7, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.825119, mean_q: 32.269107, mean_eps: 0.100000\n","     156409/2000000000: episode: 4199, duration: 5.567s, episode steps:  40, steps per second:   7, episode reward: 42.200, mean reward:  1.055 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.483763, mean_q: 32.875847, mean_eps: 0.100000\n","     156449/2000000000: episode: 4200, duration: 6.082s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 80.908152, mean_q: 31.711798, mean_eps: 0.100000\n","     156487/2000000000: episode: 4201, duration: 5.910s, episode steps:  38, steps per second:   6, episode reward: 49.600, mean reward:  1.305 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 76.897868, mean_q: 32.457348, mean_eps: 0.100000\n","     156527/2000000000: episode: 4202, duration: 5.817s, episode steps:  40, steps per second:   7, episode reward: -64.600, mean reward: -1.615 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 73.356028, mean_q: 32.339709, mean_eps: 0.100000\n","     156567/2000000000: episode: 4203, duration: 5.506s, episode steps:  40, steps per second:   7, episode reward: -3.700, mean reward: -0.092 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.700346, mean_q: 32.272850, mean_eps: 0.100000\n","     156607/2000000000: episode: 4204, duration: 6.019s, episode steps:  40, steps per second:   7, episode reward: 10.600, mean reward:  0.265 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.242939, mean_q: 32.600034, mean_eps: 0.100000\n","     156637/2000000000: episode: 4205, duration: 4.514s, episode steps:  30, steps per second:   7, episode reward: -132.100, mean reward: -4.403 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 73.272076, mean_q: 31.815867, mean_eps: 0.100000\n","     156677/2000000000: episode: 4206, duration: 5.617s, episode steps:  40, steps per second:   7, episode reward: 100.900, mean reward:  2.523 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.213638, mean_q: 32.451538, mean_eps: 0.100000\n","     156717/2000000000: episode: 4207, duration: 6.021s, episode steps:  40, steps per second:   7, episode reward: 159.900, mean reward:  3.998 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.805382, mean_q: 31.002956, mean_eps: 0.100000\n","     156757/2000000000: episode: 4208, duration: 5.604s, episode steps:  40, steps per second:   7, episode reward: -95.400, mean reward: -2.385 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.770465, mean_q: 32.173304, mean_eps: 0.100000\n","     156792/2000000000: episode: 4209, duration: 4.906s, episode steps:  35, steps per second:   7, episode reward: 188.200, mean reward:  5.377 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.198140, mean_q: 32.166303, mean_eps: 0.100000\n","     156832/2000000000: episode: 4210, duration: 5.524s, episode steps:  40, steps per second:   7, episode reward: 132.100, mean reward:  3.303 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 63.681064, mean_q: 32.596372, mean_eps: 0.100000\n","     156868/2000000000: episode: 4211, duration: 5.024s, episode steps:  36, steps per second:   7, episode reward: -49.400, mean reward: -1.372 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 69.044046, mean_q: 32.272998, mean_eps: 0.100000\n","     156908/2000000000: episode: 4212, duration: 5.386s, episode steps:  40, steps per second:   7, episode reward: 30.500, mean reward:  0.762 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.502372, mean_q: 31.837667, mean_eps: 0.100000\n","     156939/2000000000: episode: 4213, duration: 4.428s, episode steps:  31, steps per second:   7, episode reward: 71.700, mean reward:  2.313 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 66.348791, mean_q: 32.098444, mean_eps: 0.100000\n","     156970/2000000000: episode: 4214, duration: 4.154s, episode steps:  31, steps per second:   7, episode reward: 84.900, mean reward:  2.739 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.599152, mean_q: 31.668843, mean_eps: 0.100000\n","     157010/2000000000: episode: 4215, duration: 5.483s, episode steps:  40, steps per second:   7, episode reward: 14.700, mean reward:  0.367 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 70.188283, mean_q: 31.850374, mean_eps: 0.100000\n","     157046/2000000000: episode: 4216, duration: 4.882s, episode steps:  36, steps per second:   7, episode reward: -107.300, mean reward: -2.981 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 76.066899, mean_q: 32.891912, mean_eps: 0.100000\n","     157084/2000000000: episode: 4217, duration: 5.462s, episode steps:  38, steps per second:   7, episode reward: 107.200, mean reward:  2.821 [-20.000, 18.400], mean action: 1.263 [0.000, 2.000],  loss: 75.738726, mean_q: 32.507108, mean_eps: 0.100000\n","     157117/2000000000: episode: 4218, duration: 4.658s, episode steps:  33, steps per second:   7, episode reward: 153.700, mean reward:  4.658 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 70.582407, mean_q: 31.673386, mean_eps: 0.100000\n","     157147/2000000000: episode: 4219, duration: 4.410s, episode steps:  30, steps per second:   7, episode reward: 20.300, mean reward:  0.677 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 72.467012, mean_q: 32.705199, mean_eps: 0.100000\n","     157179/2000000000: episode: 4220, duration: 4.835s, episode steps:  32, steps per second:   7, episode reward: 108.600, mean reward:  3.394 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.098217, mean_q: 31.700707, mean_eps: 0.100000\n","     157215/2000000000: episode: 4221, duration: 5.069s, episode steps:  36, steps per second:   7, episode reward: 83.000, mean reward:  2.306 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 69.521352, mean_q: 32.021549, mean_eps: 0.100000\n","     157244/2000000000: episode: 4222, duration: 3.915s, episode steps:  29, steps per second:   7, episode reward: 137.700, mean reward:  4.748 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 83.787407, mean_q: 32.724340, mean_eps: 0.100000\n","     157279/2000000000: episode: 4223, duration: 4.483s, episode steps:  35, steps per second:   8, episode reward:  8.700, mean reward:  0.249 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 70.761853, mean_q: 32.572489, mean_eps: 0.100000\n","     157315/2000000000: episode: 4224, duration: 4.319s, episode steps:  36, steps per second:   8, episode reward: 115.100, mean reward:  3.197 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 72.401908, mean_q: 32.901667, mean_eps: 0.100000\n","     157355/2000000000: episode: 4225, duration: 4.993s, episode steps:  40, steps per second:   8, episode reward: 19.600, mean reward:  0.490 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 68.588641, mean_q: 31.452487, mean_eps: 0.100000\n","     157391/2000000000: episode: 4226, duration: 4.741s, episode steps:  36, steps per second:   8, episode reward: 36.500, mean reward:  1.014 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 70.966199, mean_q: 32.267681, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     157424/2000000000: episode: 4227, duration: 4.267s, episode steps:  33, steps per second:   8, episode reward: 73.300, mean reward:  2.221 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 73.685510, mean_q: 31.853864, mean_eps: 0.100000\n","     157459/2000000000: episode: 4228, duration: 4.393s, episode steps:  35, steps per second:   8, episode reward: 20.800, mean reward:  0.594 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 76.258279, mean_q: 31.290534, mean_eps: 0.100000\n","     157499/2000000000: episode: 4229, duration: 5.070s, episode steps:  40, steps per second:   8, episode reward: 53.100, mean reward:  1.327 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.567145, mean_q: 32.880946, mean_eps: 0.100000\n","     157539/2000000000: episode: 4230, duration: 4.933s, episode steps:  40, steps per second:   8, episode reward: -27.700, mean reward: -0.693 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.821023, mean_q: 32.096276, mean_eps: 0.100000\n","     157579/2000000000: episode: 4231, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: -72.000, mean reward: -1.800 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 86.043619, mean_q: 32.279027, mean_eps: 0.100000\n","     157609/2000000000: episode: 4232, duration: 3.689s, episode steps:  30, steps per second:   8, episode reward: 80.400, mean reward:  2.680 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 72.824253, mean_q: 32.214207, mean_eps: 0.100000\n","     157649/2000000000: episode: 4233, duration: 4.873s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.613518, mean_q: 32.209737, mean_eps: 0.100000\n","     157688/2000000000: episode: 4234, duration: 4.724s, episode steps:  39, steps per second:   8, episode reward: 154.600, mean reward:  3.964 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 77.040876, mean_q: 32.374289, mean_eps: 0.100000\n","     157726/2000000000: episode: 4235, duration: 4.674s, episode steps:  38, steps per second:   8, episode reward: -92.000, mean reward: -2.421 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.725679, mean_q: 32.206884, mean_eps: 0.100000\n","     157753/2000000000: episode: 4236, duration: 3.446s, episode steps:  27, steps per second:   8, episode reward: 218.500, mean reward:  8.093 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 82.501974, mean_q: 32.329875, mean_eps: 0.100000\n","     157792/2000000000: episode: 4237, duration: 5.067s, episode steps:  39, steps per second:   8, episode reward: 59.900, mean reward:  1.536 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 68.999381, mean_q: 32.189270, mean_eps: 0.100000\n","     157826/2000000000: episode: 4238, duration: 4.162s, episode steps:  34, steps per second:   8, episode reward: -134.000, mean reward: -3.941 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 74.190167, mean_q: 32.646687, mean_eps: 0.100000\n","     157866/2000000000: episode: 4239, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.303607, mean_q: 31.786470, mean_eps: 0.100000\n","     157897/2000000000: episode: 4240, duration: 3.858s, episode steps:  31, steps per second:   8, episode reward: 34.400, mean reward:  1.110 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 75.320997, mean_q: 32.631587, mean_eps: 0.100000\n","     157926/2000000000: episode: 4241, duration: 3.664s, episode steps:  29, steps per second:   8, episode reward: -139.800, mean reward: -4.821 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 73.881711, mean_q: 32.662447, mean_eps: 0.100000\n","     157966/2000000000: episode: 4242, duration: 4.722s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.456026, mean_q: 32.399441, mean_eps: 0.100000\n","     157999/2000000000: episode: 4243, duration: 3.934s, episode steps:  33, steps per second:   8, episode reward: -96.000, mean reward: -2.909 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.933328, mean_q: 32.270591, mean_eps: 0.100000\n","     158029/2000000000: episode: 4244, duration: 3.701s, episode steps:  30, steps per second:   8, episode reward: 42.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 70.279706, mean_q: 32.279398, mean_eps: 0.100000\n","     158069/2000000000: episode: 4245, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 76.114132, mean_q: 32.010516, mean_eps: 0.100000\n","     158097/2000000000: episode: 4246, duration: 3.542s, episode steps:  28, steps per second:   8, episode reward: -58.000, mean reward: -2.071 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 80.178892, mean_q: 31.963390, mean_eps: 0.100000\n","     158137/2000000000: episode: 4247, duration: 4.888s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.900482, mean_q: 32.105013, mean_eps: 0.100000\n","     158177/2000000000: episode: 4248, duration: 5.150s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.791708, mean_q: 32.417934, mean_eps: 0.100000\n","     158208/2000000000: episode: 4249, duration: 3.962s, episode steps:  31, steps per second:   8, episode reward: -102.000, mean reward: -3.290 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 73.943449, mean_q: 32.650757, mean_eps: 0.100000\n","     158248/2000000000: episode: 4250, duration: 5.251s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.757215, mean_q: 32.294883, mean_eps: 0.100000\n","     158288/2000000000: episode: 4251, duration: 4.869s, episode steps:  40, steps per second:   8, episode reward: -22.900, mean reward: -0.573 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.568826, mean_q: 31.718047, mean_eps: 0.100000\n","     158328/2000000000: episode: 4252, duration: 5.124s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.969809, mean_q: 32.363474, mean_eps: 0.100000\n","     158362/2000000000: episode: 4253, duration: 4.265s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 75.225226, mean_q: 32.735093, mean_eps: 0.100000\n","     158390/2000000000: episode: 4254, duration: 3.615s, episode steps:  28, steps per second:   8, episode reward: 37.900, mean reward:  1.354 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 81.131643, mean_q: 31.064815, mean_eps: 0.100000\n","     158426/2000000000: episode: 4255, duration: 4.389s, episode steps:  36, steps per second:   8, episode reward:  9.700, mean reward:  0.269 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 84.759174, mean_q: 31.688110, mean_eps: 0.100000\n","     158466/2000000000: episode: 4256, duration: 5.035s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.466272, mean_q: 32.055676, mean_eps: 0.100000\n","     158505/2000000000: episode: 4257, duration: 4.842s, episode steps:  39, steps per second:   8, episode reward: -76.700, mean reward: -1.967 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 77.469503, mean_q: 32.338246, mean_eps: 0.100000\n","     158532/2000000000: episode: 4258, duration: 3.533s, episode steps:  27, steps per second:   8, episode reward: 123.200, mean reward:  4.563 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 76.327124, mean_q: 31.708146, mean_eps: 0.100000\n","     158572/2000000000: episode: 4259, duration: 5.155s, episode steps:  40, steps per second:   8, episode reward: -22.700, mean reward: -0.567 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.522425, mean_q: 31.796522, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     158612/2000000000: episode: 4260, duration: 5.226s, episode steps:  40, steps per second:   8, episode reward: 97.900, mean reward:  2.447 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.336634, mean_q: 32.176982, mean_eps: 0.100000\n","     158652/2000000000: episode: 4261, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: -28.000, mean reward: -0.700 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.467039, mean_q: 32.807678, mean_eps: 0.100000\n","     158685/2000000000: episode: 4262, duration: 4.229s, episode steps:  33, steps per second:   8, episode reward: -68.800, mean reward: -2.085 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 73.077758, mean_q: 32.603274, mean_eps: 0.100000\n","     158718/2000000000: episode: 4263, duration: 4.231s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 72.614101, mean_q: 32.259240, mean_eps: 0.100000\n","     158749/2000000000: episode: 4264, duration: 3.861s, episode steps:  31, steps per second:   8, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 69.748479, mean_q: 32.448694, mean_eps: 0.100000\n","     158789/2000000000: episode: 4265, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.274423, mean_q: 31.676092, mean_eps: 0.100000\n","     158829/2000000000: episode: 4266, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: 43.900, mean reward:  1.097 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.369291, mean_q: 32.316193, mean_eps: 0.100000\n","     158868/2000000000: episode: 4267, duration: 4.785s, episode steps:  39, steps per second:   8, episode reward: -13.700, mean reward: -0.351 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 73.995053, mean_q: 32.191204, mean_eps: 0.100000\n","     158902/2000000000: episode: 4268, duration: 4.504s, episode steps:  34, steps per second:   8, episode reward: 33.900, mean reward:  0.997 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 69.593474, mean_q: 31.999975, mean_eps: 0.100000\n","     158935/2000000000: episode: 4269, duration: 4.483s, episode steps:  33, steps per second:   7, episode reward: -9.200, mean reward: -0.279 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 77.220718, mean_q: 32.390585, mean_eps: 0.100000\n","     158963/2000000000: episode: 4270, duration: 3.731s, episode steps:  28, steps per second:   8, episode reward: -29.500, mean reward: -1.054 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 67.787503, mean_q: 32.725439, mean_eps: 0.100000\n","     159003/2000000000: episode: 4271, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.835826, mean_q: 32.170057, mean_eps: 0.100000\n","     159033/2000000000: episode: 4272, duration: 3.555s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 72.845384, mean_q: 32.751178, mean_eps: 0.100000\n","     159072/2000000000: episode: 4273, duration: 4.876s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 70.074138, mean_q: 32.180937, mean_eps: 0.100000\n","     159112/2000000000: episode: 4274, duration: 4.849s, episode steps:  40, steps per second:   8, episode reward: -11.600, mean reward: -0.290 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.670950, mean_q: 32.223448, mean_eps: 0.100000\n","     159150/2000000000: episode: 4275, duration: 4.775s, episode steps:  38, steps per second:   8, episode reward: 157.400, mean reward:  4.142 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 76.809582, mean_q: 31.710699, mean_eps: 0.100000\n","     159183/2000000000: episode: 4276, duration: 4.139s, episode steps:  33, steps per second:   8, episode reward: 35.000, mean reward:  1.061 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.651170, mean_q: 32.295999, mean_eps: 0.100000\n","     159221/2000000000: episode: 4277, duration: 4.730s, episode steps:  38, steps per second:   8, episode reward: 71.200, mean reward:  1.874 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 83.682510, mean_q: 32.338686, mean_eps: 0.100000\n","     159255/2000000000: episode: 4278, duration: 4.125s, episode steps:  34, steps per second:   8, episode reward: -22.000, mean reward: -0.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 74.145927, mean_q: 32.013607, mean_eps: 0.100000\n","     159294/2000000000: episode: 4279, duration: 4.840s, episode steps:  39, steps per second:   8, episode reward: 16.500, mean reward:  0.423 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 74.403939, mean_q: 32.445470, mean_eps: 0.100000\n","     159326/2000000000: episode: 4280, duration: 4.024s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 75.249387, mean_q: 31.815804, mean_eps: 0.100000\n","     159364/2000000000: episode: 4281, duration: 4.638s, episode steps:  38, steps per second:   8, episode reward: 88.000, mean reward:  2.316 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 74.480174, mean_q: 31.594787, mean_eps: 0.100000\n","     159398/2000000000: episode: 4282, duration: 4.155s, episode steps:  34, steps per second:   8, episode reward: -24.900, mean reward: -0.732 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 67.606265, mean_q: 32.528325, mean_eps: 0.100000\n","     159438/2000000000: episode: 4283, duration: 5.055s, episode steps:  40, steps per second:   8, episode reward: 120.000, mean reward:  3.000 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 75.431138, mean_q: 32.604520, mean_eps: 0.100000\n","     159478/2000000000: episode: 4284, duration: 4.646s, episode steps:  40, steps per second:   9, episode reward: 28.600, mean reward:  0.715 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 68.426250, mean_q: 31.833779, mean_eps: 0.100000\n","     159507/2000000000: episode: 4285, duration: 3.392s, episode steps:  29, steps per second:   9, episode reward: 123.100, mean reward:  4.245 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 73.271260, mean_q: 31.552789, mean_eps: 0.100000\n","     159547/2000000000: episode: 4286, duration: 4.796s, episode steps:  40, steps per second:   8, episode reward: -55.900, mean reward: -1.398 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.666910, mean_q: 32.135125, mean_eps: 0.100000\n","     159587/2000000000: episode: 4287, duration: 4.608s, episode steps:  40, steps per second:   9, episode reward: 60.600, mean reward:  1.515 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 73.884321, mean_q: 32.034144, mean_eps: 0.100000\n","     159619/2000000000: episode: 4288, duration: 3.857s, episode steps:  32, steps per second:   8, episode reward:  9.700, mean reward:  0.303 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 68.789003, mean_q: 32.591898, mean_eps: 0.100000\n","     159659/2000000000: episode: 4289, duration: 4.816s, episode steps:  40, steps per second:   8, episode reward: 79.900, mean reward:  1.997 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.035113, mean_q: 32.283142, mean_eps: 0.100000\n","     159690/2000000000: episode: 4290, duration: 3.794s, episode steps:  31, steps per second:   8, episode reward: 60.200, mean reward:  1.942 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 75.080949, mean_q: 32.438518, mean_eps: 0.100000\n","     159721/2000000000: episode: 4291, duration: 3.685s, episode steps:  31, steps per second:   8, episode reward: 111.200, mean reward:  3.587 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 76.796624, mean_q: 31.921278, mean_eps: 0.100000\n","     159761/2000000000: episode: 4292, duration: 4.901s, episode steps:  40, steps per second:   8, episode reward: 97.700, mean reward:  2.443 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.369559, mean_q: 31.943225, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     159801/2000000000: episode: 4293, duration: 5.049s, episode steps:  40, steps per second:   8, episode reward: 46.500, mean reward:  1.162 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 74.962136, mean_q: 32.114817, mean_eps: 0.100000\n","     159839/2000000000: episode: 4294, duration: 4.930s, episode steps:  38, steps per second:   8, episode reward: 74.400, mean reward:  1.958 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 71.092789, mean_q: 31.644389, mean_eps: 0.100000\n","     159875/2000000000: episode: 4295, duration: 4.815s, episode steps:  36, steps per second:   7, episode reward: -94.500, mean reward: -2.625 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 72.579732, mean_q: 32.891116, mean_eps: 0.100000\n","     159909/2000000000: episode: 4296, duration: 4.394s, episode steps:  34, steps per second:   8, episode reward: 34.400, mean reward:  1.012 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 76.504497, mean_q: 32.635232, mean_eps: 0.100000\n","     159943/2000000000: episode: 4297, duration: 4.414s, episode steps:  34, steps per second:   8, episode reward: 76.800, mean reward:  2.259 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.302016, mean_q: 31.834488, mean_eps: 0.100000\n","     159977/2000000000: episode: 4298, duration: 4.366s, episode steps:  34, steps per second:   8, episode reward: -4.900, mean reward: -0.144 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.404385, mean_q: 32.452948, mean_eps: 0.100000\n","     160016/2000000000: episode: 4299, duration: 4.857s, episode steps:  39, steps per second:   8, episode reward: 96.600, mean reward:  2.477 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 67.901570, mean_q: 32.824719, mean_eps: 0.100000\n","     160056/2000000000: episode: 4300, duration: 5.262s, episode steps:  40, steps per second:   8, episode reward: -5.800, mean reward: -0.145 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.697070, mean_q: 34.061979, mean_eps: 0.100000\n","     160088/2000000000: episode: 4301, duration: 4.222s, episode steps:  32, steps per second:   8, episode reward:  9.600, mean reward:  0.300 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.740397, mean_q: 33.836115, mean_eps: 0.100000\n","     160119/2000000000: episode: 4302, duration: 4.119s, episode steps:  31, steps per second:   8, episode reward: 27.700, mean reward:  0.894 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 81.479055, mean_q: 34.105119, mean_eps: 0.100000\n","     160154/2000000000: episode: 4303, duration: 4.847s, episode steps:  35, steps per second:   7, episode reward: -100.900, mean reward: -2.883 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 90.692042, mean_q: 33.636510, mean_eps: 0.100000\n","     160191/2000000000: episode: 4304, duration: 4.899s, episode steps:  37, steps per second:   8, episode reward: 65.700, mean reward:  1.776 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 77.964215, mean_q: 33.281079, mean_eps: 0.100000\n","     160225/2000000000: episode: 4305, duration: 4.690s, episode steps:  34, steps per second:   7, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 75.753778, mean_q: 34.112490, mean_eps: 0.100000\n","     160264/2000000000: episode: 4306, duration: 5.289s, episode steps:  39, steps per second:   7, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 83.944106, mean_q: 33.315105, mean_eps: 0.100000\n","     160303/2000000000: episode: 4307, duration: 5.314s, episode steps:  39, steps per second:   7, episode reward: 31.900, mean reward:  0.818 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 81.232456, mean_q: 34.335609, mean_eps: 0.100000\n","     160342/2000000000: episode: 4308, duration: 5.264s, episode steps:  39, steps per second:   7, episode reward: 84.600, mean reward:  2.169 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 82.536393, mean_q: 34.256000, mean_eps: 0.100000\n","     160382/2000000000: episode: 4309, duration: 5.323s, episode steps:  40, steps per second:   8, episode reward:  6.700, mean reward:  0.168 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 88.814695, mean_q: 34.195891, mean_eps: 0.100000\n","     160422/2000000000: episode: 4310, duration: 5.080s, episode steps:  40, steps per second:   8, episode reward: 10.200, mean reward:  0.255 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.671240, mean_q: 33.961995, mean_eps: 0.100000\n","     160457/2000000000: episode: 4311, duration: 4.634s, episode steps:  35, steps per second:   8, episode reward: 38.400, mean reward:  1.097 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 79.668819, mean_q: 33.809370, mean_eps: 0.100000\n","     160492/2000000000: episode: 4312, duration: 4.739s, episode steps:  35, steps per second:   7, episode reward: 79.600, mean reward:  2.274 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.478716, mean_q: 33.407185, mean_eps: 0.100000\n","     160532/2000000000: episode: 4313, duration: 5.311s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.072958, mean_q: 33.978569, mean_eps: 0.100000\n","     160558/2000000000: episode: 4314, duration: 3.427s, episode steps:  26, steps per second:   8, episode reward: 25.200, mean reward:  0.969 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 77.789943, mean_q: 34.683698, mean_eps: 0.100000\n","     160591/2000000000: episode: 4315, duration: 4.281s, episode steps:  33, steps per second:   8, episode reward: 43.000, mean reward:  1.303 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.640268, mean_q: 33.942315, mean_eps: 0.100000\n","     160629/2000000000: episode: 4316, duration: 5.213s, episode steps:  38, steps per second:   7, episode reward: 125.100, mean reward:  3.292 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 73.330063, mean_q: 33.427046, mean_eps: 0.100000\n","     160658/2000000000: episode: 4317, duration: 3.851s, episode steps:  29, steps per second:   8, episode reward: 165.500, mean reward:  5.707 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 82.199519, mean_q: 33.961141, mean_eps: 0.100000\n","     160696/2000000000: episode: 4318, duration: 5.228s, episode steps:  38, steps per second:   7, episode reward: 50.400, mean reward:  1.326 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 80.075701, mean_q: 34.889076, mean_eps: 0.100000\n","     160733/2000000000: episode: 4319, duration: 4.904s, episode steps:  37, steps per second:   8, episode reward: 95.800, mean reward:  2.589 [-20.000, 18.200], mean action: 1.216 [0.000, 2.000],  loss: 75.947323, mean_q: 34.244184, mean_eps: 0.100000\n","     160771/2000000000: episode: 4320, duration: 5.254s, episode steps:  38, steps per second:   7, episode reward: 40.500, mean reward:  1.066 [-20.000, 18.200], mean action: 1.263 [0.000, 2.000],  loss: 80.721845, mean_q: 33.981290, mean_eps: 0.100000\n","     160807/2000000000: episode: 4321, duration: 4.915s, episode steps:  36, steps per second:   7, episode reward: 157.900, mean reward:  4.386 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 75.863123, mean_q: 33.652765, mean_eps: 0.100000\n","     160841/2000000000: episode: 4322, duration: 4.598s, episode steps:  34, steps per second:   7, episode reward: -44.100, mean reward: -1.297 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 71.371570, mean_q: 34.547194, mean_eps: 0.100000\n","     160879/2000000000: episode: 4323, duration: 4.985s, episode steps:  38, steps per second:   8, episode reward: -46.500, mean reward: -1.224 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 82.695463, mean_q: 33.409227, mean_eps: 0.100000\n","     160907/2000000000: episode: 4324, duration: 3.896s, episode steps:  28, steps per second:   7, episode reward: -69.500, mean reward: -2.482 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.697880, mean_q: 34.542219, mean_eps: 0.100000\n","     160938/2000000000: episode: 4325, duration: 3.875s, episode steps:  31, steps per second:   8, episode reward: -119.400, mean reward: -3.852 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 79.190201, mean_q: 34.682667, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     160978/2000000000: episode: 4326, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: 46.000, mean reward:  1.150 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 79.804996, mean_q: 33.798511, mean_eps: 0.100000\n","     161000/2000000000: episode: 4327, duration: 3.050s, episode steps:  22, steps per second:   7, episode reward: 142.600, mean reward:  6.482 [-20.000, 18.000], mean action: 0.682 [0.000, 2.000],  loss: 74.972874, mean_q: 33.741675, mean_eps: 0.100000\n","     161033/2000000000: episode: 4328, duration: 4.455s, episode steps:  33, steps per second:   7, episode reward: 49.200, mean reward:  1.491 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 84.476964, mean_q: 34.547054, mean_eps: 0.100000\n","     161072/2000000000: episode: 4329, duration: 5.361s, episode steps:  39, steps per second:   7, episode reward: 69.200, mean reward:  1.774 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 76.453822, mean_q: 33.687187, mean_eps: 0.100000\n","     161108/2000000000: episode: 4330, duration: 4.789s, episode steps:  36, steps per second:   8, episode reward: 162.300, mean reward:  4.508 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 80.191410, mean_q: 34.257209, mean_eps: 0.100000\n","     161148/2000000000: episode: 4331, duration: 5.709s, episode steps:  40, steps per second:   7, episode reward: 91.900, mean reward:  2.297 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.928140, mean_q: 33.997541, mean_eps: 0.100000\n","     161181/2000000000: episode: 4332, duration: 4.382s, episode steps:  33, steps per second:   8, episode reward: 101.300, mean reward:  3.070 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 78.776398, mean_q: 33.983864, mean_eps: 0.100000\n","     161221/2000000000: episode: 4333, duration: 5.463s, episode steps:  40, steps per second:   7, episode reward: -27.900, mean reward: -0.698 [-20.000, 18.100], mean action: 1.275 [0.000, 2.000],  loss: 79.931254, mean_q: 33.998556, mean_eps: 0.100000\n","     161260/2000000000: episode: 4334, duration: 5.383s, episode steps:  39, steps per second:   7, episode reward: 39.100, mean reward:  1.003 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 72.108007, mean_q: 33.745925, mean_eps: 0.100000\n","     161300/2000000000: episode: 4335, duration: 5.466s, episode steps:  40, steps per second:   7, episode reward: 81.400, mean reward:  2.035 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.903675, mean_q: 34.531547, mean_eps: 0.100000\n","     161333/2000000000: episode: 4336, duration: 4.875s, episode steps:  33, steps per second:   7, episode reward:  3.700, mean reward:  0.112 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.663900, mean_q: 34.498963, mean_eps: 0.100000\n","     161365/2000000000: episode: 4337, duration: 4.595s, episode steps:  32, steps per second:   7, episode reward: -5.800, mean reward: -0.181 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 77.469056, mean_q: 34.860387, mean_eps: 0.100000\n","     161398/2000000000: episode: 4338, duration: 4.695s, episode steps:  33, steps per second:   7, episode reward: 22.100, mean reward:  0.670 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 74.551204, mean_q: 33.579120, mean_eps: 0.100000\n","     161431/2000000000: episode: 4339, duration: 4.247s, episode steps:  33, steps per second:   8, episode reward: 175.200, mean reward:  5.309 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 72.387080, mean_q: 34.922598, mean_eps: 0.100000\n","     161465/2000000000: episode: 4340, duration: 4.468s, episode steps:  34, steps per second:   8, episode reward: -103.900, mean reward: -3.056 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 70.128263, mean_q: 33.454569, mean_eps: 0.100000\n","     161492/2000000000: episode: 4341, duration: 3.535s, episode steps:  27, steps per second:   8, episode reward:  7.800, mean reward:  0.289 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 70.882112, mean_q: 34.257612, mean_eps: 0.100000\n","     161532/2000000000: episode: 4342, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward:  8.500, mean reward:  0.213 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.641514, mean_q: 34.685063, mean_eps: 0.100000\n","     161572/2000000000: episode: 4343, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward:  0.300, mean reward:  0.008 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.491862, mean_q: 34.044217, mean_eps: 0.100000\n","     161606/2000000000: episode: 4344, duration: 4.772s, episode steps:  34, steps per second:   7, episode reward: -10.500, mean reward: -0.309 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 73.746973, mean_q: 34.324060, mean_eps: 0.100000\n","     161644/2000000000: episode: 4345, duration: 5.310s, episode steps:  38, steps per second:   7, episode reward: 155.800, mean reward:  4.100 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 89.858202, mean_q: 33.135347, mean_eps: 0.100000\n","     161684/2000000000: episode: 4346, duration: 5.287s, episode steps:  40, steps per second:   8, episode reward: 62.700, mean reward:  1.568 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.662922, mean_q: 34.074085, mean_eps: 0.100000\n","     161714/2000000000: episode: 4347, duration: 4.245s, episode steps:  30, steps per second:   7, episode reward: 16.000, mean reward:  0.533 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 75.759699, mean_q: 33.754490, mean_eps: 0.100000\n","     161754/2000000000: episode: 4348, duration: 5.842s, episode steps:  40, steps per second:   7, episode reward: 27.100, mean reward:  0.677 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.788877, mean_q: 34.489156, mean_eps: 0.100000\n","     161790/2000000000: episode: 4349, duration: 5.370s, episode steps:  36, steps per second:   7, episode reward:  8.700, mean reward:  0.242 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 74.623823, mean_q: 33.893760, mean_eps: 0.100000\n","     161817/2000000000: episode: 4350, duration: 3.991s, episode steps:  27, steps per second:   7, episode reward: 22.400, mean reward:  0.830 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 78.935455, mean_q: 34.343911, mean_eps: 0.100000\n","     161857/2000000000: episode: 4351, duration: 5.437s, episode steps:  40, steps per second:   7, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.235603, mean_q: 34.756899, mean_eps: 0.100000\n","     161897/2000000000: episode: 4352, duration: 5.846s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.649836, mean_q: 34.185599, mean_eps: 0.100000\n","     161930/2000000000: episode: 4353, duration: 4.820s, episode steps:  33, steps per second:   7, episode reward: 61.200, mean reward:  1.855 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.352965, mean_q: 33.830019, mean_eps: 0.100000\n","     161962/2000000000: episode: 4354, duration: 4.912s, episode steps:  32, steps per second:   7, episode reward: 45.000, mean reward:  1.406 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.405065, mean_q: 33.538117, mean_eps: 0.100000\n","     161991/2000000000: episode: 4355, duration: 4.178s, episode steps:  29, steps per second:   7, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 74.039112, mean_q: 34.292368, mean_eps: 0.100000\n","     162025/2000000000: episode: 4356, duration: 4.795s, episode steps:  34, steps per second:   7, episode reward: -134.000, mean reward: -3.941 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 83.553939, mean_q: 34.454062, mean_eps: 0.100000\n","     162060/2000000000: episode: 4357, duration: 5.261s, episode steps:  35, steps per second:   7, episode reward: 22.500, mean reward:  0.643 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 77.031142, mean_q: 33.889092, mean_eps: 0.100000\n","     162100/2000000000: episode: 4358, duration: 6.068s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.648590, mean_q: 33.542063, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     162137/2000000000: episode: 4359, duration: 5.105s, episode steps:  37, steps per second:   7, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 76.808260, mean_q: 33.982149, mean_eps: 0.100000\n","     162177/2000000000: episode: 4360, duration: 5.315s, episode steps:  40, steps per second:   8, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.930057, mean_q: 33.394677, mean_eps: 0.100000\n","     162210/2000000000: episode: 4361, duration: 3.985s, episode steps:  33, steps per second:   8, episode reward: 75.700, mean reward:  2.294 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 74.012054, mean_q: 34.020945, mean_eps: 0.100000\n","     162245/2000000000: episode: 4362, duration: 4.594s, episode steps:  35, steps per second:   8, episode reward: -44.200, mean reward: -1.263 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 75.678707, mean_q: 34.597128, mean_eps: 0.100000\n","     162283/2000000000: episode: 4363, duration: 4.813s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 74.331588, mean_q: 34.026385, mean_eps: 0.100000\n","     162323/2000000000: episode: 4364, duration: 4.842s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.503363, mean_q: 34.364424, mean_eps: 0.100000\n","     162351/2000000000: episode: 4365, duration: 3.545s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 77.817040, mean_q: 33.798735, mean_eps: 0.100000\n","     162388/2000000000: episode: 4366, duration: 4.664s, episode steps:  37, steps per second:   8, episode reward: 12.600, mean reward:  0.341 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 77.917294, mean_q: 34.705625, mean_eps: 0.100000\n","     162423/2000000000: episode: 4367, duration: 4.270s, episode steps:  35, steps per second:   8, episode reward: -134.000, mean reward: -3.829 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 77.492039, mean_q: 33.790565, mean_eps: 0.100000\n","     162461/2000000000: episode: 4368, duration: 4.649s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 76.183859, mean_q: 33.694464, mean_eps: 0.100000\n","     162501/2000000000: episode: 4369, duration: 4.825s, episode steps:  40, steps per second:   8, episode reward: -49.300, mean reward: -1.233 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.274141, mean_q: 34.109793, mean_eps: 0.100000\n","     162537/2000000000: episode: 4370, duration: 4.466s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.114630, mean_q: 33.803977, mean_eps: 0.100000\n","     162567/2000000000: episode: 4371, duration: 3.911s, episode steps:  30, steps per second:   8, episode reward: 42.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 82.321468, mean_q: 33.719201, mean_eps: 0.100000\n","     162601/2000000000: episode: 4372, duration: 4.613s, episode steps:  34, steps per second:   7, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 78.460532, mean_q: 33.787084, mean_eps: 0.100000\n","     162641/2000000000: episode: 4373, duration: 5.215s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.572651, mean_q: 34.314447, mean_eps: 0.100000\n","     162681/2000000000: episode: 4374, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: -14.500, mean reward: -0.363 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.731183, mean_q: 34.071155, mean_eps: 0.100000\n","     162721/2000000000: episode: 4375, duration: 4.807s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.390215, mean_q: 34.299606, mean_eps: 0.100000\n","     162757/2000000000: episode: 4376, duration: 4.556s, episode steps:  36, steps per second:   8, episode reward: 11.900, mean reward:  0.331 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.041550, mean_q: 34.850223, mean_eps: 0.100000\n","     162796/2000000000: episode: 4377, duration: 5.255s, episode steps:  39, steps per second:   7, episode reward: -38.100, mean reward: -0.977 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 76.095790, mean_q: 33.736460, mean_eps: 0.100000\n","     162833/2000000000: episode: 4378, duration: 4.699s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 80.689484, mean_q: 33.822185, mean_eps: 0.100000\n","     162866/2000000000: episode: 4379, duration: 4.064s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 83.043028, mean_q: 34.209606, mean_eps: 0.100000\n","     162905/2000000000: episode: 4380, duration: 4.823s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 84.597307, mean_q: 34.404798, mean_eps: 0.100000\n","     162940/2000000000: episode: 4381, duration: 4.390s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 74.231125, mean_q: 34.577643, mean_eps: 0.100000\n","     162980/2000000000: episode: 4382, duration: 5.205s, episode steps:  40, steps per second:   8, episode reward: -163.600, mean reward: -4.090 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.107861, mean_q: 34.714208, mean_eps: 0.100000\n","     163020/2000000000: episode: 4383, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 69.700, mean reward:  1.742 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 77.976387, mean_q: 34.134539, mean_eps: 0.100000\n","     163060/2000000000: episode: 4384, duration: 5.242s, episode steps:  40, steps per second:   8, episode reward: 149.500, mean reward:  3.737 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 82.767888, mean_q: 33.577624, mean_eps: 0.100000\n","     163100/2000000000: episode: 4385, duration: 5.355s, episode steps:  40, steps per second:   7, episode reward: 32.100, mean reward:  0.803 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.801722, mean_q: 33.062148, mean_eps: 0.100000\n","     163134/2000000000: episode: 4386, duration: 4.221s, episode steps:  34, steps per second:   8, episode reward: -142.800, mean reward: -4.200 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 64.338350, mean_q: 34.484212, mean_eps: 0.100000\n","     163174/2000000000: episode: 4387, duration: 4.809s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 77.588111, mean_q: 34.756833, mean_eps: 0.100000\n","     163210/2000000000: episode: 4388, duration: 4.304s, episode steps:  36, steps per second:   8, episode reward: 25.700, mean reward:  0.714 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 75.386126, mean_q: 34.245615, mean_eps: 0.100000\n","     163249/2000000000: episode: 4389, duration: 4.769s, episode steps:  39, steps per second:   8, episode reward: -38.800, mean reward: -0.995 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 75.780213, mean_q: 34.603982, mean_eps: 0.100000\n","     163286/2000000000: episode: 4390, duration: 4.425s, episode steps:  37, steps per second:   8, episode reward: 74.400, mean reward:  2.011 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 78.452802, mean_q: 33.764640, mean_eps: 0.100000\n","     163326/2000000000: episode: 4391, duration: 4.794s, episode steps:  40, steps per second:   8, episode reward: 139.600, mean reward:  3.490 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.606874, mean_q: 34.533769, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     163366/2000000000: episode: 4392, duration: 4.795s, episode steps:  40, steps per second:   8, episode reward: 66.700, mean reward:  1.667 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 70.576358, mean_q: 34.033039, mean_eps: 0.100000\n","     163402/2000000000: episode: 4393, duration: 4.407s, episode steps:  36, steps per second:   8, episode reward: -126.700, mean reward: -3.519 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 69.771465, mean_q: 33.669165, mean_eps: 0.100000\n","     163435/2000000000: episode: 4394, duration: 4.130s, episode steps:  33, steps per second:   8, episode reward: 117.400, mean reward:  3.558 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 74.113350, mean_q: 34.419969, mean_eps: 0.100000\n","     163463/2000000000: episode: 4395, duration: 3.438s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 78.591128, mean_q: 34.492191, mean_eps: 0.100000\n","     163495/2000000000: episode: 4396, duration: 3.719s, episode steps:  32, steps per second:   9, episode reward: -15.100, mean reward: -0.472 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.570603, mean_q: 34.365962, mean_eps: 0.100000\n","     163535/2000000000: episode: 4397, duration: 4.573s, episode steps:  40, steps per second:   9, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.797033, mean_q: 33.401816, mean_eps: 0.100000\n","     163571/2000000000: episode: 4398, duration: 4.403s, episode steps:  36, steps per second:   8, episode reward: 200.300, mean reward:  5.564 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.313412, mean_q: 33.567307, mean_eps: 0.100000\n","     163607/2000000000: episode: 4399, duration: 4.576s, episode steps:  36, steps per second:   8, episode reward: -48.100, mean reward: -1.336 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 72.372078, mean_q: 33.302712, mean_eps: 0.100000\n","     163637/2000000000: episode: 4400, duration: 3.836s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.650576, mean_q: 34.261332, mean_eps: 0.100000\n","     163668/2000000000: episode: 4401, duration: 3.981s, episode steps:  31, steps per second:   8, episode reward: 37.400, mean reward:  1.206 [-20.000, 18.000], mean action: 0.839 [0.000, 2.000],  loss: 72.482072, mean_q: 33.798260, mean_eps: 0.100000\n","     163707/2000000000: episode: 4402, duration: 5.065s, episode steps:  39, steps per second:   8, episode reward: -15.400, mean reward: -0.395 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 81.585816, mean_q: 33.008102, mean_eps: 0.100000\n","     163747/2000000000: episode: 4403, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: -11.400, mean reward: -0.285 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.540726, mean_q: 34.248270, mean_eps: 0.100000\n","     163787/2000000000: episode: 4404, duration: 5.310s, episode steps:  40, steps per second:   8, episode reward: 49.200, mean reward:  1.230 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.069254, mean_q: 33.482252, mean_eps: 0.100000\n","     163822/2000000000: episode: 4405, duration: 4.818s, episode steps:  35, steps per second:   7, episode reward: 13.600, mean reward:  0.389 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.715299, mean_q: 34.464592, mean_eps: 0.100000\n","     163853/2000000000: episode: 4406, duration: 4.189s, episode steps:  31, steps per second:   7, episode reward: 105.200, mean reward:  3.394 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 71.562200, mean_q: 34.468705, mean_eps: 0.100000\n","     163893/2000000000: episode: 4407, duration: 4.834s, episode steps:  40, steps per second:   8, episode reward: -0.300, mean reward: -0.008 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.143857, mean_q: 33.716946, mean_eps: 0.100000\n","     163933/2000000000: episode: 4408, duration: 4.891s, episode steps:  40, steps per second:   8, episode reward: 101.500, mean reward:  2.538 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.922173, mean_q: 34.219803, mean_eps: 0.100000\n","     163970/2000000000: episode: 4409, duration: 4.533s, episode steps:  37, steps per second:   8, episode reward: 28.300, mean reward:  0.765 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 79.426331, mean_q: 33.705304, mean_eps: 0.100000\n","     164010/2000000000: episode: 4410, duration: 5.128s, episode steps:  40, steps per second:   8, episode reward: -109.600, mean reward: -2.740 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.309520, mean_q: 33.414746, mean_eps: 0.100000\n","     164047/2000000000: episode: 4411, duration: 4.716s, episode steps:  37, steps per second:   8, episode reward: 74.100, mean reward:  2.003 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 78.094369, mean_q: 33.874149, mean_eps: 0.100000\n","     164087/2000000000: episode: 4412, duration: 5.010s, episode steps:  40, steps per second:   8, episode reward: 41.900, mean reward:  1.047 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 83.637381, mean_q: 33.475101, mean_eps: 0.100000\n","     164123/2000000000: episode: 4413, duration: 4.424s, episode steps:  36, steps per second:   8, episode reward: 150.900, mean reward:  4.192 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 83.213934, mean_q: 34.077516, mean_eps: 0.100000\n","     164163/2000000000: episode: 4414, duration: 5.090s, episode steps:  40, steps per second:   8, episode reward: -27.700, mean reward: -0.693 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 76.625328, mean_q: 34.320787, mean_eps: 0.100000\n","     164203/2000000000: episode: 4415, duration: 5.200s, episode steps:  40, steps per second:   8, episode reward: 23.300, mean reward:  0.582 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.378512, mean_q: 33.637433, mean_eps: 0.100000\n","     164240/2000000000: episode: 4416, duration: 4.646s, episode steps:  37, steps per second:   8, episode reward: -25.700, mean reward: -0.695 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 69.185318, mean_q: 34.074332, mean_eps: 0.100000\n","     164269/2000000000: episode: 4417, duration: 3.752s, episode steps:  29, steps per second:   8, episode reward: 83.800, mean reward:  2.890 [-20.000, 18.000], mean action: 1.138 [0.000, 2.000],  loss: 73.762828, mean_q: 34.063079, mean_eps: 0.100000\n","     164302/2000000000: episode: 4418, duration: 4.255s, episode steps:  33, steps per second:   8, episode reward: 98.100, mean reward:  2.973 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 72.050599, mean_q: 34.426018, mean_eps: 0.100000\n","     164334/2000000000: episode: 4419, duration: 4.095s, episode steps:  32, steps per second:   8, episode reward: -57.300, mean reward: -1.791 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 74.406801, mean_q: 34.285116, mean_eps: 0.100000\n","     164373/2000000000: episode: 4420, duration: 5.034s, episode steps:  39, steps per second:   8, episode reward: 139.600, mean reward:  3.579 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 80.330555, mean_q: 33.666469, mean_eps: 0.100000\n","     164411/2000000000: episode: 4421, duration: 4.835s, episode steps:  38, steps per second:   8, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 75.342674, mean_q: 34.195672, mean_eps: 0.100000\n","     164440/2000000000: episode: 4422, duration: 3.756s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.002671, mean_q: 34.730190, mean_eps: 0.100000\n","     164476/2000000000: episode: 4423, duration: 4.426s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 81.322642, mean_q: 34.810758, mean_eps: 0.100000\n","     164512/2000000000: episode: 4424, duration: 4.525s, episode steps:  36, steps per second:   8, episode reward: 48.900, mean reward:  1.358 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 75.096731, mean_q: 34.111836, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     164552/2000000000: episode: 4425, duration: 5.510s, episode steps:  40, steps per second:   7, episode reward: 12.100, mean reward:  0.303 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 73.518254, mean_q: 34.155697, mean_eps: 0.100000\n","     164587/2000000000: episode: 4426, duration: 4.660s, episode steps:  35, steps per second:   8, episode reward: -4.100, mean reward: -0.117 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 80.053176, mean_q: 33.920851, mean_eps: 0.100000\n","     164627/2000000000: episode: 4427, duration: 4.861s, episode steps:  40, steps per second:   8, episode reward: 44.300, mean reward:  1.107 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.503423, mean_q: 34.247226, mean_eps: 0.100000\n","     164666/2000000000: episode: 4428, duration: 4.939s, episode steps:  39, steps per second:   8, episode reward: 134.600, mean reward:  3.451 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 76.545699, mean_q: 33.898278, mean_eps: 0.100000\n","     164706/2000000000: episode: 4429, duration: 5.004s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.344610, mean_q: 33.999710, mean_eps: 0.100000\n","     164736/2000000000: episode: 4430, duration: 4.002s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 80.135176, mean_q: 34.330476, mean_eps: 0.100000\n","     164776/2000000000: episode: 4431, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: 109.300, mean reward:  2.732 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.020060, mean_q: 33.656843, mean_eps: 0.100000\n","     164814/2000000000: episode: 4432, duration: 4.623s, episode steps:  38, steps per second:   8, episode reward: 42.300, mean reward:  1.113 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 76.714049, mean_q: 33.836082, mean_eps: 0.100000\n","     164854/2000000000: episode: 4433, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.968179, mean_q: 33.962186, mean_eps: 0.100000\n","     164892/2000000000: episode: 4434, duration: 4.693s, episode steps:  38, steps per second:   8, episode reward: 45.900, mean reward:  1.208 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 78.684754, mean_q: 34.234325, mean_eps: 0.100000\n","     164931/2000000000: episode: 4435, duration: 5.223s, episode steps:  39, steps per second:   7, episode reward: -134.000, mean reward: -3.436 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 83.634402, mean_q: 33.852189, mean_eps: 0.100000\n","     164959/2000000000: episode: 4436, duration: 3.671s, episode steps:  28, steps per second:   8, episode reward: 73.000, mean reward:  2.607 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.639061, mean_q: 33.730186, mean_eps: 0.100000\n","     164999/2000000000: episode: 4437, duration: 5.127s, episode steps:  40, steps per second:   8, episode reward: 17.500, mean reward:  0.438 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.381737, mean_q: 33.775503, mean_eps: 0.100000\n","     165039/2000000000: episode: 4438, duration: 5.163s, episode steps:  40, steps per second:   8, episode reward: 213.700, mean reward:  5.343 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 79.557842, mean_q: 33.831591, mean_eps: 0.100000\n","     165079/2000000000: episode: 4439, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.578932, mean_q: 34.353834, mean_eps: 0.100000\n","     165119/2000000000: episode: 4440, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: -25.500, mean reward: -0.637 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 79.419392, mean_q: 34.705925, mean_eps: 0.100000\n","     165156/2000000000: episode: 4441, duration: 4.878s, episode steps:  37, steps per second:   8, episode reward: 11.800, mean reward:  0.319 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 77.475085, mean_q: 34.900035, mean_eps: 0.100000\n","     165196/2000000000: episode: 4442, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: -18.400, mean reward: -0.460 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.665223, mean_q: 34.421535, mean_eps: 0.100000\n","     165228/2000000000: episode: 4443, duration: 4.168s, episode steps:  32, steps per second:   8, episode reward: -168.800, mean reward: -5.275 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 75.923048, mean_q: 34.050068, mean_eps: 0.100000\n","     165265/2000000000: episode: 4444, duration: 4.831s, episode steps:  37, steps per second:   8, episode reward: 138.400, mean reward:  3.741 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 75.902547, mean_q: 34.270752, mean_eps: 0.100000\n","     165297/2000000000: episode: 4445, duration: 4.193s, episode steps:  32, steps per second:   8, episode reward: 137.000, mean reward:  4.281 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.257347, mean_q: 33.803649, mean_eps: 0.100000\n","     165327/2000000000: episode: 4446, duration: 3.818s, episode steps:  30, steps per second:   8, episode reward: -15.100, mean reward: -0.503 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 73.827798, mean_q: 34.009604, mean_eps: 0.100000\n","     165355/2000000000: episode: 4447, duration: 3.600s, episode steps:  28, steps per second:   8, episode reward: 175.000, mean reward:  6.250 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 77.287372, mean_q: 33.610325, mean_eps: 0.100000\n","     165387/2000000000: episode: 4448, duration: 3.963s, episode steps:  32, steps per second:   8, episode reward: -15.900, mean reward: -0.497 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 70.319498, mean_q: 34.729348, mean_eps: 0.100000\n","     165421/2000000000: episode: 4449, duration: 4.254s, episode steps:  34, steps per second:   8, episode reward: 120.200, mean reward:  3.535 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 74.172360, mean_q: 34.130534, mean_eps: 0.100000\n","     165458/2000000000: episode: 4450, duration: 4.740s, episode steps:  37, steps per second:   8, episode reward: -17.800, mean reward: -0.481 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 80.251872, mean_q: 33.998010, mean_eps: 0.100000\n","     165494/2000000000: episode: 4451, duration: 3.977s, episode steps:  36, steps per second:   9, episode reward: -3.500, mean reward: -0.097 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.156344, mean_q: 33.830477, mean_eps: 0.100000\n","     165527/2000000000: episode: 4452, duration: 3.770s, episode steps:  33, steps per second:   9, episode reward: 132.500, mean reward:  4.015 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 72.275225, mean_q: 34.289457, mean_eps: 0.100000\n","     165567/2000000000: episode: 4453, duration: 4.877s, episode steps:  40, steps per second:   8, episode reward: 80.600, mean reward:  2.015 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.277352, mean_q: 34.037350, mean_eps: 0.100000\n","     165604/2000000000: episode: 4454, duration: 4.571s, episode steps:  37, steps per second:   8, episode reward: 164.200, mean reward:  4.438 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 77.456311, mean_q: 33.704610, mean_eps: 0.100000\n","     165637/2000000000: episode: 4455, duration: 3.959s, episode steps:  33, steps per second:   8, episode reward: 88.100, mean reward:  2.670 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 69.915429, mean_q: 34.107715, mean_eps: 0.100000\n","     165677/2000000000: episode: 4456, duration: 4.880s, episode steps:  40, steps per second:   8, episode reward: 124.800, mean reward:  3.120 [-10.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 78.280315, mean_q: 34.051623, mean_eps: 0.100000\n","     165717/2000000000: episode: 4457, duration: 4.975s, episode steps:  40, steps per second:   8, episode reward: -37.300, mean reward: -0.932 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.210342, mean_q: 34.162187, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     165750/2000000000: episode: 4458, duration: 4.178s, episode steps:  33, steps per second:   8, episode reward: 99.800, mean reward:  3.024 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 79.266448, mean_q: 34.559174, mean_eps: 0.100000\n","     165790/2000000000: episode: 4459, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: 181.000, mean reward:  4.525 [-20.000, 18.400], mean action: 1.275 [0.000, 2.000],  loss: 77.906215, mean_q: 33.679737, mean_eps: 0.100000\n","     165819/2000000000: episode: 4460, duration: 3.722s, episode steps:  29, steps per second:   8, episode reward: 74.800, mean reward:  2.579 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 74.094964, mean_q: 34.547245, mean_eps: 0.100000\n","     165855/2000000000: episode: 4461, duration: 4.397s, episode steps:  36, steps per second:   8, episode reward: 39.900, mean reward:  1.108 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.109964, mean_q: 34.437880, mean_eps: 0.100000\n","     165895/2000000000: episode: 4462, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.407752, mean_q: 33.634551, mean_eps: 0.100000\n","     165935/2000000000: episode: 4463, duration: 5.771s, episode steps:  40, steps per second:   7, episode reward: 21.000, mean reward:  0.525 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 75.665157, mean_q: 33.766053, mean_eps: 0.100000\n","     165971/2000000000: episode: 4464, duration: 5.049s, episode steps:  36, steps per second:   7, episode reward: -64.600, mean reward: -1.794 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 87.881782, mean_q: 34.115012, mean_eps: 0.100000\n","     166011/2000000000: episode: 4465, duration: 5.640s, episode steps:  40, steps per second:   7, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.875782, mean_q: 33.904975, mean_eps: 0.100000\n","     166037/2000000000: episode: 4466, duration: 3.585s, episode steps:  26, steps per second:   7, episode reward: 170.000, mean reward:  6.538 [-20.000, 18.000], mean action: 0.615 [0.000, 2.000],  loss: 74.480792, mean_q: 34.607255, mean_eps: 0.100000\n","     166065/2000000000: episode: 4467, duration: 3.693s, episode steps:  28, steps per second:   8, episode reward: -14.500, mean reward: -0.518 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 71.004862, mean_q: 35.300255, mean_eps: 0.100000\n","     166099/2000000000: episode: 4468, duration: 4.282s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 76.020233, mean_q: 33.521731, mean_eps: 0.100000\n","     166129/2000000000: episode: 4469, duration: 3.707s, episode steps:  30, steps per second:   8, episode reward: 221.300, mean reward:  7.377 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 74.418750, mean_q: 33.909459, mean_eps: 0.100000\n","     166163/2000000000: episode: 4470, duration: 4.490s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 72.912647, mean_q: 34.529719, mean_eps: 0.100000\n","     166203/2000000000: episode: 4471, duration: 4.893s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.912532, mean_q: 34.100499, mean_eps: 0.100000\n","     166243/2000000000: episode: 4472, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.844906, mean_q: 34.078961, mean_eps: 0.100000\n","     166269/2000000000: episode: 4473, duration: 3.499s, episode steps:  26, steps per second:   7, episode reward:  9.300, mean reward:  0.358 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 74.838738, mean_q: 34.386571, mean_eps: 0.100000\n","     166298/2000000000: episode: 4474, duration: 3.739s, episode steps:  29, steps per second:   8, episode reward: 39.800, mean reward:  1.372 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 69.061916, mean_q: 32.940075, mean_eps: 0.100000\n","     166331/2000000000: episode: 4475, duration: 4.103s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 75.944793, mean_q: 33.776620, mean_eps: 0.100000\n","     166364/2000000000: episode: 4476, duration: 4.052s, episode steps:  33, steps per second:   8, episode reward: -50.600, mean reward: -1.533 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 76.909089, mean_q: 33.953339, mean_eps: 0.100000\n","     166391/2000000000: episode: 4477, duration: 3.450s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 72.889554, mean_q: 34.772510, mean_eps: 0.100000\n","     166431/2000000000: episode: 4478, duration: 5.040s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.127489, mean_q: 34.276213, mean_eps: 0.100000\n","     166471/2000000000: episode: 4479, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.709034, mean_q: 33.636439, mean_eps: 0.100000\n","     166503/2000000000: episode: 4480, duration: 4.069s, episode steps:  32, steps per second:   8, episode reward: -73.900, mean reward: -2.309 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 76.561674, mean_q: 34.248296, mean_eps: 0.100000\n","     166543/2000000000: episode: 4481, duration: 4.979s, episode steps:  40, steps per second:   8, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.723463, mean_q: 33.956137, mean_eps: 0.100000\n","     166583/2000000000: episode: 4482, duration: 5.030s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 76.353175, mean_q: 33.703027, mean_eps: 0.100000\n","     166620/2000000000: episode: 4483, duration: 4.788s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 79.352964, mean_q: 34.770767, mean_eps: 0.100000\n","     166658/2000000000: episode: 4484, duration: 5.116s, episode steps:  38, steps per second:   7, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.079 [0.000, 2.000],  loss: 67.083417, mean_q: 34.265611, mean_eps: 0.100000\n","     166694/2000000000: episode: 4485, duration: 4.606s, episode steps:  36, steps per second:   8, episode reward: -71.400, mean reward: -1.983 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 78.825957, mean_q: 33.993248, mean_eps: 0.100000\n","     166733/2000000000: episode: 4486, duration: 4.893s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 74.143483, mean_q: 34.218757, mean_eps: 0.100000\n","     166773/2000000000: episode: 4487, duration: 4.864s, episode steps:  40, steps per second:   8, episode reward: -27.500, mean reward: -0.688 [-20.000, 18.000], mean action: 1.625 [0.000, 2.000],  loss: 75.972033, mean_q: 33.870094, mean_eps: 0.100000\n","     166805/2000000000: episode: 4488, duration: 4.002s, episode steps:  32, steps per second:   8, episode reward: -2.200, mean reward: -0.069 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 70.839334, mean_q: 34.363557, mean_eps: 0.100000\n","     166838/2000000000: episode: 4489, duration: 4.169s, episode steps:  33, steps per second:   8, episode reward: -76.500, mean reward: -2.318 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 69.317324, mean_q: 34.247911, mean_eps: 0.100000\n","     166871/2000000000: episode: 4490, duration: 4.159s, episode steps:  33, steps per second:   8, episode reward:  0.800, mean reward:  0.024 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 75.103215, mean_q: 34.684166, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     166910/2000000000: episode: 4491, duration: 5.345s, episode steps:  39, steps per second:   7, episode reward: 121.900, mean reward:  3.126 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 77.626145, mean_q: 34.184353, mean_eps: 0.100000\n","     166946/2000000000: episode: 4492, duration: 4.448s, episode steps:  36, steps per second:   8, episode reward: 85.700, mean reward:  2.381 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 74.354014, mean_q: 34.536148, mean_eps: 0.100000\n","     166984/2000000000: episode: 4493, duration: 4.657s, episode steps:  38, steps per second:   8, episode reward: -36.900, mean reward: -0.971 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 73.474000, mean_q: 33.979158, mean_eps: 0.100000\n","     167024/2000000000: episode: 4494, duration: 4.845s, episode steps:  40, steps per second:   8, episode reward: -11.300, mean reward: -0.283 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.971197, mean_q: 33.319088, mean_eps: 0.100000\n","     167064/2000000000: episode: 4495, duration: 4.868s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.713367, mean_q: 34.218837, mean_eps: 0.100000\n","     167104/2000000000: episode: 4496, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.585263, mean_q: 33.280775, mean_eps: 0.100000\n","     167142/2000000000: episode: 4497, duration: 4.851s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.305995, mean_q: 33.724121, mean_eps: 0.100000\n","     167173/2000000000: episode: 4498, duration: 3.766s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 75.858721, mean_q: 33.901737, mean_eps: 0.100000\n","     167213/2000000000: episode: 4499, duration: 4.995s, episode steps:  40, steps per second:   8, episode reward: -87.000, mean reward: -2.175 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.344481, mean_q: 34.331610, mean_eps: 0.100000\n","     167249/2000000000: episode: 4500, duration: 4.328s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 77.112554, mean_q: 34.170932, mean_eps: 0.100000\n","     167289/2000000000: episode: 4501, duration: 4.871s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 71.765332, mean_q: 34.088345, mean_eps: 0.100000\n","     167326/2000000000: episode: 4502, duration: 4.462s, episode steps:  37, steps per second:   8, episode reward: 112.900, mean reward:  3.051 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 73.453307, mean_q: 33.957595, mean_eps: 0.100000\n","     167358/2000000000: episode: 4503, duration: 3.890s, episode steps:  32, steps per second:   8, episode reward: -47.500, mean reward: -1.484 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.090261, mean_q: 33.906884, mean_eps: 0.100000\n","     167398/2000000000: episode: 4504, duration: 4.964s, episode steps:  40, steps per second:   8, episode reward: -90.700, mean reward: -2.268 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.453624, mean_q: 33.770675, mean_eps: 0.100000\n","     167430/2000000000: episode: 4505, duration: 3.979s, episode steps:  32, steps per second:   8, episode reward: 191.800, mean reward:  5.994 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.218930, mean_q: 34.372016, mean_eps: 0.100000\n","     167469/2000000000: episode: 4506, duration: 4.976s, episode steps:  39, steps per second:   8, episode reward: -199.200, mean reward: -5.108 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 81.368802, mean_q: 34.306942, mean_eps: 0.100000\n","     167505/2000000000: episode: 4507, duration: 4.813s, episode steps:  36, steps per second:   7, episode reward: -68.200, mean reward: -1.894 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 74.843235, mean_q: 34.011532, mean_eps: 0.100000\n","     167545/2000000000: episode: 4508, duration: 5.301s, episode steps:  40, steps per second:   8, episode reward: 78.400, mean reward:  1.960 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 74.698374, mean_q: 33.624894, mean_eps: 0.100000\n","     167577/2000000000: episode: 4509, duration: 4.898s, episode steps:  32, steps per second:   7, episode reward: 17.300, mean reward:  0.541 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 69.653850, mean_q: 34.491799, mean_eps: 0.100000\n","     167608/2000000000: episode: 4510, duration: 4.307s, episode steps:  31, steps per second:   7, episode reward: 36.200, mean reward:  1.168 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 75.121651, mean_q: 33.860424, mean_eps: 0.100000\n","     167648/2000000000: episode: 4511, duration: 5.364s, episode steps:  40, steps per second:   7, episode reward: -10.200, mean reward: -0.255 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 73.764079, mean_q: 34.286480, mean_eps: 0.100000\n","     167688/2000000000: episode: 4512, duration: 5.378s, episode steps:  40, steps per second:   7, episode reward: -41.900, mean reward: -1.047 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.094947, mean_q: 33.669384, mean_eps: 0.100000\n","     167720/2000000000: episode: 4513, duration: 4.466s, episode steps:  32, steps per second:   7, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 77.372560, mean_q: 33.990980, mean_eps: 0.100000\n","     167754/2000000000: episode: 4514, duration: 4.578s, episode steps:  34, steps per second:   7, episode reward: -81.600, mean reward: -2.400 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.704303, mean_q: 33.774671, mean_eps: 0.100000\n","     167794/2000000000: episode: 4515, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: 181.000, mean reward:  4.525 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 79.016191, mean_q: 33.978904, mean_eps: 0.100000\n","     167834/2000000000: episode: 4516, duration: 5.194s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.196384, mean_q: 34.042136, mean_eps: 0.100000\n","     167874/2000000000: episode: 4517, duration: 5.009s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.533716, mean_q: 34.388408, mean_eps: 0.100000\n","     167903/2000000000: episode: 4518, duration: 3.740s, episode steps:  29, steps per second:   8, episode reward: 98.300, mean reward:  3.390 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 71.373523, mean_q: 33.632603, mean_eps: 0.100000\n","     167942/2000000000: episode: 4519, duration: 5.118s, episode steps:  39, steps per second:   8, episode reward: 137.700, mean reward:  3.531 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 78.812545, mean_q: 33.337222, mean_eps: 0.100000\n","     167982/2000000000: episode: 4520, duration: 5.315s, episode steps:  40, steps per second:   8, episode reward: 203.600, mean reward:  5.090 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 77.197737, mean_q: 33.200992, mean_eps: 0.100000\n","     168019/2000000000: episode: 4521, duration: 5.339s, episode steps:  37, steps per second:   7, episode reward: -121.300, mean reward: -3.278 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 71.445553, mean_q: 34.386280, mean_eps: 0.100000\n","     168052/2000000000: episode: 4522, duration: 4.339s, episode steps:  33, steps per second:   8, episode reward: -65.300, mean reward: -1.979 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 70.195641, mean_q: 34.040722, mean_eps: 0.100000\n","     168085/2000000000: episode: 4523, duration: 4.210s, episode steps:  33, steps per second:   8, episode reward: -12.800, mean reward: -0.388 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 70.332624, mean_q: 34.404780, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     168119/2000000000: episode: 4524, duration: 4.334s, episode steps:  34, steps per second:   8, episode reward: 59.800, mean reward:  1.759 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 75.690288, mean_q: 34.264538, mean_eps: 0.100000\n","     168152/2000000000: episode: 4525, duration: 4.251s, episode steps:  33, steps per second:   8, episode reward: 174.700, mean reward:  5.294 [-20.000, 19.100], mean action: 1.061 [0.000, 2.000],  loss: 68.238771, mean_q: 35.129434, mean_eps: 0.100000\n","     168181/2000000000: episode: 4526, duration: 3.732s, episode steps:  29, steps per second:   8, episode reward: -99.200, mean reward: -3.421 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 74.997399, mean_q: 33.783220, mean_eps: 0.100000\n","     168212/2000000000: episode: 4527, duration: 3.757s, episode steps:  31, steps per second:   8, episode reward: 158.100, mean reward:  5.100 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 73.764126, mean_q: 34.260306, mean_eps: 0.100000\n","     168252/2000000000: episode: 4528, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: 74.200, mean reward:  1.855 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.639502, mean_q: 33.962621, mean_eps: 0.100000\n","     168282/2000000000: episode: 4529, duration: 3.800s, episode steps:  30, steps per second:   8, episode reward: -83.200, mean reward: -2.773 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 70.958470, mean_q: 33.692033, mean_eps: 0.100000\n","     168314/2000000000: episode: 4530, duration: 4.010s, episode steps:  32, steps per second:   8, episode reward: 49.100, mean reward:  1.534 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 73.927580, mean_q: 34.072687, mean_eps: 0.100000\n","     168341/2000000000: episode: 4531, duration: 3.626s, episode steps:  27, steps per second:   7, episode reward: 175.400, mean reward:  6.496 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 69.451939, mean_q: 34.381470, mean_eps: 0.100000\n","     168378/2000000000: episode: 4532, duration: 4.890s, episode steps:  37, steps per second:   8, episode reward: -27.100, mean reward: -0.732 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 85.416019, mean_q: 33.916706, mean_eps: 0.100000\n","     168414/2000000000: episode: 4533, duration: 4.281s, episode steps:  36, steps per second:   8, episode reward: 115.200, mean reward:  3.200 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 78.185782, mean_q: 33.802960, mean_eps: 0.100000\n","     168454/2000000000: episode: 4534, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: 102.900, mean reward:  2.573 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.219569, mean_q: 34.004373, mean_eps: 0.100000\n","     168491/2000000000: episode: 4535, duration: 4.510s, episode steps:  37, steps per second:   8, episode reward: -1.700, mean reward: -0.046 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.328146, mean_q: 34.172579, mean_eps: 0.100000\n","     168531/2000000000: episode: 4536, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.663445, mean_q: 33.774470, mean_eps: 0.100000\n","     168571/2000000000: episode: 4537, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: -50.500, mean reward: -1.263 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.088701, mean_q: 33.648769, mean_eps: 0.100000\n","     168611/2000000000: episode: 4538, duration: 5.368s, episode steps:  40, steps per second:   7, episode reward: 107.000, mean reward:  2.675 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.176711, mean_q: 34.325293, mean_eps: 0.100000\n","     168647/2000000000: episode: 4539, duration: 4.838s, episode steps:  36, steps per second:   7, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 78.517007, mean_q: 34.100204, mean_eps: 0.100000\n","     168687/2000000000: episode: 4540, duration: 5.105s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.408543, mean_q: 33.941251, mean_eps: 0.100000\n","     168727/2000000000: episode: 4541, duration: 4.944s, episode steps:  40, steps per second:   8, episode reward: 115.000, mean reward:  2.875 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.125649, mean_q: 34.336362, mean_eps: 0.100000\n","     168765/2000000000: episode: 4542, duration: 4.679s, episode steps:  38, steps per second:   8, episode reward: -25.600, mean reward: -0.674 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 71.456989, mean_q: 33.694807, mean_eps: 0.100000\n","     168799/2000000000: episode: 4543, duration: 4.073s, episode steps:  34, steps per second:   8, episode reward: -24.700, mean reward: -0.726 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 68.580628, mean_q: 34.523901, mean_eps: 0.100000\n","     168839/2000000000: episode: 4544, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.765453, mean_q: 34.063462, mean_eps: 0.100000\n","     168869/2000000000: episode: 4545, duration: 3.779s, episode steps:  30, steps per second:   8, episode reward: 55.500, mean reward:  1.850 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 71.323551, mean_q: 33.824796, mean_eps: 0.100000\n","     168909/2000000000: episode: 4546, duration: 4.976s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 77.566613, mean_q: 33.303496, mean_eps: 0.100000\n","     168949/2000000000: episode: 4547, duration: 5.038s, episode steps:  40, steps per second:   8, episode reward: 16.000, mean reward:  0.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.013066, mean_q: 34.166250, mean_eps: 0.100000\n","     168989/2000000000: episode: 4548, duration: 5.042s, episode steps:  40, steps per second:   8, episode reward: 85.900, mean reward:  2.147 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 67.559505, mean_q: 33.468494, mean_eps: 0.100000\n","     169027/2000000000: episode: 4549, duration: 4.558s, episode steps:  38, steps per second:   8, episode reward: 190.800, mean reward:  5.021 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 81.302594, mean_q: 33.931250, mean_eps: 0.100000\n","     169057/2000000000: episode: 4550, duration: 3.645s, episode steps:  30, steps per second:   8, episode reward: 62.900, mean reward:  2.097 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.604907, mean_q: 34.791436, mean_eps: 0.100000\n","     169092/2000000000: episode: 4551, duration: 4.398s, episode steps:  35, steps per second:   8, episode reward: -35.600, mean reward: -1.017 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.913415, mean_q: 34.003034, mean_eps: 0.100000\n","     169129/2000000000: episode: 4552, duration: 4.686s, episode steps:  37, steps per second:   8, episode reward: 81.200, mean reward:  2.195 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 77.383935, mean_q: 33.972473, mean_eps: 0.100000\n","     169169/2000000000: episode: 4553, duration: 5.014s, episode steps:  40, steps per second:   8, episode reward: 121.700, mean reward:  3.043 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 72.335194, mean_q: 34.611065, mean_eps: 0.100000\n","     169209/2000000000: episode: 4554, duration: 5.142s, episode steps:  40, steps per second:   8, episode reward: 164.300, mean reward:  4.107 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.729617, mean_q: 33.610237, mean_eps: 0.100000\n","     169249/2000000000: episode: 4555, duration: 4.810s, episode steps:  40, steps per second:   8, episode reward: 28.000, mean reward:  0.700 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 75.285645, mean_q: 33.801316, mean_eps: 0.100000\n","     169276/2000000000: episode: 4556, duration: 3.397s, episode steps:  27, steps per second:   8, episode reward: 75.600, mean reward:  2.800 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 86.690984, mean_q: 34.436454, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     169316/2000000000: episode: 4557, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: -59.700, mean reward: -1.493 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.884852, mean_q: 34.650366, mean_eps: 0.100000\n","     169355/2000000000: episode: 4558, duration: 4.748s, episode steps:  39, steps per second:   8, episode reward: 27.800, mean reward:  0.713 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 71.177614, mean_q: 34.621125, mean_eps: 0.100000\n","     169390/2000000000: episode: 4559, duration: 4.364s, episode steps:  35, steps per second:   8, episode reward: 24.300, mean reward:  0.694 [-20.000, 18.100], mean action: 1.229 [0.000, 2.000],  loss: 76.379175, mean_q: 34.749284, mean_eps: 0.100000\n","     169430/2000000000: episode: 4560, duration: 4.889s, episode steps:  40, steps per second:   8, episode reward: 40.300, mean reward:  1.007 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.553209, mean_q: 33.753343, mean_eps: 0.100000\n","     169454/2000000000: episode: 4561, duration: 3.158s, episode steps:  24, steps per second:   8, episode reward: 105.500, mean reward:  4.396 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 83.366688, mean_q: 33.314242, mean_eps: 0.100000\n","     169483/2000000000: episode: 4562, duration: 4.022s, episode steps:  29, steps per second:   7, episode reward: 58.100, mean reward:  2.003 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 73.410716, mean_q: 33.556712, mean_eps: 0.100000\n","     169516/2000000000: episode: 4563, duration: 4.459s, episode steps:  33, steps per second:   7, episode reward: 95.800, mean reward:  2.903 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.757600, mean_q: 34.460516, mean_eps: 0.100000\n","     169556/2000000000: episode: 4564, duration: 5.379s, episode steps:  40, steps per second:   7, episode reward:  3.500, mean reward:  0.088 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 74.369496, mean_q: 34.201572, mean_eps: 0.100000\n","     169591/2000000000: episode: 4565, duration: 4.639s, episode steps:  35, steps per second:   8, episode reward: -7.600, mean reward: -0.217 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 80.364548, mean_q: 33.707088, mean_eps: 0.100000\n","     169630/2000000000: episode: 4566, duration: 5.098s, episode steps:  39, steps per second:   8, episode reward: 151.200, mean reward:  3.877 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 70.301199, mean_q: 33.477980, mean_eps: 0.100000\n","     169658/2000000000: episode: 4567, duration: 3.769s, episode steps:  28, steps per second:   7, episode reward: 88.800, mean reward:  3.171 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 72.938363, mean_q: 34.750017, mean_eps: 0.100000\n","     169689/2000000000: episode: 4568, duration: 4.259s, episode steps:  31, steps per second:   7, episode reward: 82.200, mean reward:  2.652 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.474060, mean_q: 34.135607, mean_eps: 0.100000\n","     169721/2000000000: episode: 4569, duration: 4.343s, episode steps:  32, steps per second:   7, episode reward: 10.400, mean reward:  0.325 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 81.247335, mean_q: 34.500967, mean_eps: 0.100000\n","     169761/2000000000: episode: 4570, duration: 5.227s, episode steps:  40, steps per second:   8, episode reward: 129.400, mean reward:  3.235 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.767874, mean_q: 33.930518, mean_eps: 0.100000\n","     169792/2000000000: episode: 4571, duration: 4.493s, episode steps:  31, steps per second:   7, episode reward:  6.800, mean reward:  0.219 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 69.348523, mean_q: 34.054114, mean_eps: 0.100000\n","     169825/2000000000: episode: 4572, duration: 4.731s, episode steps:  33, steps per second:   7, episode reward: 19.600, mean reward:  0.594 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 75.533977, mean_q: 34.396402, mean_eps: 0.100000\n","     169865/2000000000: episode: 4573, duration: 5.642s, episode steps:  40, steps per second:   7, episode reward: 69.800, mean reward:  1.745 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 80.132180, mean_q: 33.612740, mean_eps: 0.100000\n","     169905/2000000000: episode: 4574, duration: 5.628s, episode steps:  40, steps per second:   7, episode reward: 48.200, mean reward:  1.205 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.026872, mean_q: 33.993215, mean_eps: 0.100000\n","     169939/2000000000: episode: 4575, duration: 4.582s, episode steps:  34, steps per second:   7, episode reward: 55.300, mean reward:  1.626 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.308015, mean_q: 34.423821, mean_eps: 0.100000\n","     169979/2000000000: episode: 4576, duration: 5.476s, episode steps:  40, steps per second:   7, episode reward: -63.100, mean reward: -1.578 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 82.751547, mean_q: 34.326003, mean_eps: 0.100000\n","     170019/2000000000: episode: 4577, duration: 5.068s, episode steps:  40, steps per second:   8, episode reward: 38.500, mean reward:  0.962 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.151665, mean_q: 33.206571, mean_eps: 0.100000\n","     170055/2000000000: episode: 4578, duration: 4.534s, episode steps:  36, steps per second:   8, episode reward: -73.600, mean reward: -2.044 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.967352, mean_q: 34.794019, mean_eps: 0.100000\n","     170087/2000000000: episode: 4579, duration: 4.072s, episode steps:  32, steps per second:   8, episode reward: 139.400, mean reward:  4.356 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 80.631824, mean_q: 33.521308, mean_eps: 0.100000\n","     170116/2000000000: episode: 4580, duration: 3.780s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 68.109570, mean_q: 34.208277, mean_eps: 0.100000\n","     170144/2000000000: episode: 4581, duration: 3.764s, episode steps:  28, steps per second:   7, episode reward: 183.800, mean reward:  6.564 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 86.184147, mean_q: 34.931366, mean_eps: 0.100000\n","     170173/2000000000: episode: 4582, duration: 3.656s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.558968, mean_q: 34.944531, mean_eps: 0.100000\n","     170206/2000000000: episode: 4583, duration: 4.241s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.291532, mean_q: 34.428470, mean_eps: 0.100000\n","     170238/2000000000: episode: 4584, duration: 4.113s, episode steps:  32, steps per second:   8, episode reward: 54.300, mean reward:  1.697 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 71.742568, mean_q: 33.929569, mean_eps: 0.100000\n","     170278/2000000000: episode: 4585, duration: 5.140s, episode steps:  40, steps per second:   8, episode reward: 126.100, mean reward:  3.153 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 73.834861, mean_q: 33.919193, mean_eps: 0.100000\n","     170318/2000000000: episode: 4586, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: 18.900, mean reward:  0.472 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.555505, mean_q: 33.864634, mean_eps: 0.100000\n","     170355/2000000000: episode: 4587, duration: 4.769s, episode steps:  37, steps per second:   8, episode reward: 13.300, mean reward:  0.359 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 74.694976, mean_q: 34.253700, mean_eps: 0.100000\n","     170395/2000000000: episode: 4588, duration: 5.538s, episode steps:  40, steps per second:   7, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.766892, mean_q: 33.855914, mean_eps: 0.100000\n","     170430/2000000000: episode: 4589, duration: 4.393s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 73.649707, mean_q: 34.021184, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     170468/2000000000: episode: 4590, duration: 4.824s, episode steps:  38, steps per second:   8, episode reward: -8.100, mean reward: -0.213 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 74.125283, mean_q: 33.525520, mean_eps: 0.100000\n","     170508/2000000000: episode: 4591, duration: 4.857s, episode steps:  40, steps per second:   8, episode reward: 178.600, mean reward:  4.465 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.068313, mean_q: 34.900725, mean_eps: 0.100000\n","     170541/2000000000: episode: 4592, duration: 3.978s, episode steps:  33, steps per second:   8, episode reward: 161.300, mean reward:  4.888 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 73.806001, mean_q: 33.991423, mean_eps: 0.100000\n","     170581/2000000000: episode: 4593, duration: 4.916s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 73.600972, mean_q: 33.457048, mean_eps: 0.100000\n","     170621/2000000000: episode: 4594, duration: 4.835s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.918089, mean_q: 34.188449, mean_eps: 0.100000\n","     170659/2000000000: episode: 4595, duration: 4.796s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 82.264577, mean_q: 33.705532, mean_eps: 0.100000\n","     170699/2000000000: episode: 4596, duration: 5.402s, episode steps:  40, steps per second:   7, episode reward: -74.600, mean reward: -1.865 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.510635, mean_q: 33.412543, mean_eps: 0.100000\n","     170730/2000000000: episode: 4597, duration: 4.063s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 74.794295, mean_q: 33.544164, mean_eps: 0.100000\n","     170767/2000000000: episode: 4598, duration: 4.610s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 69.972678, mean_q: 34.621043, mean_eps: 0.100000\n","     170792/2000000000: episode: 4599, duration: 3.082s, episode steps:  25, steps per second:   8, episode reward: 18.000, mean reward:  0.720 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 76.687332, mean_q: 34.746797, mean_eps: 0.100000\n","     170832/2000000000: episode: 4600, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward: 107.800, mean reward:  2.695 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.244722, mean_q: 34.152441, mean_eps: 0.100000\n","     170872/2000000000: episode: 4601, duration: 5.268s, episode steps:  40, steps per second:   8, episode reward: 30.100, mean reward:  0.752 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.387668, mean_q: 34.126356, mean_eps: 0.100000\n","     170902/2000000000: episode: 4602, duration: 3.955s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 82.397421, mean_q: 34.765023, mean_eps: 0.100000\n","     170936/2000000000: episode: 4603, duration: 4.261s, episode steps:  34, steps per second:   8, episode reward: 106.300, mean reward:  3.126 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 75.257404, mean_q: 34.281363, mean_eps: 0.100000\n","     170964/2000000000: episode: 4604, duration: 3.626s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 77.975282, mean_q: 34.284779, mean_eps: 0.100000\n","     171004/2000000000: episode: 4605, duration: 5.059s, episode steps:  40, steps per second:   8, episode reward: 39.900, mean reward:  0.998 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.072712, mean_q: 34.382942, mean_eps: 0.100000\n","     171044/2000000000: episode: 4606, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: -41.800, mean reward: -1.045 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.390087, mean_q: 34.116616, mean_eps: 0.100000\n","     171084/2000000000: episode: 4607, duration: 5.344s, episode steps:  40, steps per second:   7, episode reward: -42.600, mean reward: -1.065 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.701698, mean_q: 33.542175, mean_eps: 0.100000\n","     171124/2000000000: episode: 4608, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward: 111.500, mean reward:  2.787 [-20.000, 18.900], mean action: 1.300 [0.000, 2.000],  loss: 77.721312, mean_q: 34.107255, mean_eps: 0.100000\n","     171164/2000000000: episode: 4609, duration: 5.829s, episode steps:  40, steps per second:   7, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.776686, mean_q: 34.380194, mean_eps: 0.100000\n","     171203/2000000000: episode: 4610, duration: 5.744s, episode steps:  39, steps per second:   7, episode reward: 179.500, mean reward:  4.603 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 84.076168, mean_q: 33.780711, mean_eps: 0.100000\n","     171232/2000000000: episode: 4611, duration: 3.970s, episode steps:  29, steps per second:   7, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 72.245491, mean_q: 33.971378, mean_eps: 0.100000\n","     171267/2000000000: episode: 4612, duration: 4.729s, episode steps:  35, steps per second:   7, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 78.688335, mean_q: 34.286027, mean_eps: 0.100000\n","     171304/2000000000: episode: 4613, duration: 4.950s, episode steps:  37, steps per second:   7, episode reward: 98.700, mean reward:  2.668 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 78.636504, mean_q: 34.030678, mean_eps: 0.100000\n","     171339/2000000000: episode: 4614, duration: 4.592s, episode steps:  35, steps per second:   8, episode reward: -113.500, mean reward: -3.243 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 70.021101, mean_q: 34.134331, mean_eps: 0.100000\n","     171370/2000000000: episode: 4615, duration: 3.909s, episode steps:  31, steps per second:   8, episode reward: 27.700, mean reward:  0.894 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 83.875286, mean_q: 33.793323, mean_eps: 0.100000\n","     171410/2000000000: episode: 4616, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: -51.900, mean reward: -1.298 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.186724, mean_q: 34.402974, mean_eps: 0.100000\n","     171441/2000000000: episode: 4617, duration: 4.188s, episode steps:  31, steps per second:   7, episode reward: 50.900, mean reward:  1.642 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 81.463473, mean_q: 33.037878, mean_eps: 0.100000\n","     171473/2000000000: episode: 4618, duration: 4.064s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 79.653281, mean_q: 34.984129, mean_eps: 0.100000\n","     171510/2000000000: episode: 4619, duration: 5.187s, episode steps:  37, steps per second:   7, episode reward: 19.900, mean reward:  0.538 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 77.234225, mean_q: 33.337858, mean_eps: 0.100000\n","     171547/2000000000: episode: 4620, duration: 4.771s, episode steps:  37, steps per second:   8, episode reward: 81.000, mean reward:  2.189 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 75.020250, mean_q: 33.254304, mean_eps: 0.100000\n","     171587/2000000000: episode: 4621, duration: 5.569s, episode steps:  40, steps per second:   7, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.920740, mean_q: 34.453841, mean_eps: 0.100000\n","     171627/2000000000: episode: 4622, duration: 5.367s, episode steps:  40, steps per second:   7, episode reward: 52.100, mean reward:  1.302 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.992895, mean_q: 33.438685, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     171666/2000000000: episode: 4623, duration: 5.415s, episode steps:  39, steps per second:   7, episode reward: 98.800, mean reward:  2.533 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 81.895325, mean_q: 34.343230, mean_eps: 0.100000\n","     171706/2000000000: episode: 4624, duration: 5.145s, episode steps:  40, steps per second:   8, episode reward: 72.300, mean reward:  1.807 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.170040, mean_q: 34.063013, mean_eps: 0.100000\n","     171739/2000000000: episode: 4625, duration: 4.189s, episode steps:  33, steps per second:   8, episode reward: -64.000, mean reward: -1.939 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 82.455961, mean_q: 33.064708, mean_eps: 0.100000\n","     171776/2000000000: episode: 4626, duration: 4.763s, episode steps:  37, steps per second:   8, episode reward: 151.200, mean reward:  4.086 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.742131, mean_q: 34.758095, mean_eps: 0.100000\n","     171815/2000000000: episode: 4627, duration: 5.335s, episode steps:  39, steps per second:   7, episode reward: 159.000, mean reward:  4.077 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 80.060203, mean_q: 33.973815, mean_eps: 0.100000\n","     171855/2000000000: episode: 4628, duration: 5.113s, episode steps:  40, steps per second:   8, episode reward: 96.800, mean reward:  2.420 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.434611, mean_q: 34.476968, mean_eps: 0.100000\n","     171892/2000000000: episode: 4629, duration: 4.779s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 81.940647, mean_q: 34.569157, mean_eps: 0.100000\n","     171921/2000000000: episode: 4630, duration: 3.742s, episode steps:  29, steps per second:   8, episode reward: -4.600, mean reward: -0.159 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 73.009290, mean_q: 34.469438, mean_eps: 0.100000\n","     171957/2000000000: episode: 4631, duration: 4.724s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 78.924577, mean_q: 34.538440, mean_eps: 0.100000\n","     171997/2000000000: episode: 4632, duration: 5.655s, episode steps:  40, steps per second:   7, episode reward: -90.000, mean reward: -2.250 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 71.689762, mean_q: 34.405692, mean_eps: 0.100000\n","     172037/2000000000: episode: 4633, duration: 5.541s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.245773, mean_q: 34.446042, mean_eps: 0.100000\n","     172077/2000000000: episode: 4634, duration: 5.629s, episode steps:  40, steps per second:   7, episode reward: 30.100, mean reward:  0.752 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.656009, mean_q: 33.835297, mean_eps: 0.100000\n","     172117/2000000000: episode: 4635, duration: 6.320s, episode steps:  40, steps per second:   6, episode reward:  8.500, mean reward:  0.213 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 78.209584, mean_q: 33.634376, mean_eps: 0.100000\n","     172149/2000000000: episode: 4636, duration: 4.800s, episode steps:  32, steps per second:   7, episode reward: 78.600, mean reward:  2.456 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 75.870263, mean_q: 33.225904, mean_eps: 0.100000\n","     172180/2000000000: episode: 4637, duration: 4.653s, episode steps:  31, steps per second:   7, episode reward: 35.600, mean reward:  1.148 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 78.799192, mean_q: 34.401692, mean_eps: 0.100000\n","     172220/2000000000: episode: 4638, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 80.347462, mean_q: 33.786561, mean_eps: 0.100000\n","     172260/2000000000: episode: 4639, duration: 5.537s, episode steps:  40, steps per second:   7, episode reward: 16.500, mean reward:  0.412 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.788583, mean_q: 34.605744, mean_eps: 0.100000\n","     172300/2000000000: episode: 4640, duration: 5.590s, episode steps:  40, steps per second:   7, episode reward: 75.200, mean reward:  1.880 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 72.809139, mean_q: 34.411422, mean_eps: 0.100000\n","     172331/2000000000: episode: 4641, duration: 4.195s, episode steps:  31, steps per second:   7, episode reward: 74.200, mean reward:  2.394 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 73.085622, mean_q: 33.624624, mean_eps: 0.100000\n","     172358/2000000000: episode: 4642, duration: 3.790s, episode steps:  27, steps per second:   7, episode reward: 40.900, mean reward:  1.515 [-20.000, 19.000], mean action: 0.852 [0.000, 2.000],  loss: 76.956900, mean_q: 34.717760, mean_eps: 0.100000\n","     172398/2000000000: episode: 4643, duration: 5.591s, episode steps:  40, steps per second:   7, episode reward: 73.000, mean reward:  1.825 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 75.127933, mean_q: 33.962495, mean_eps: 0.100000\n","     172430/2000000000: episode: 4644, duration: 4.289s, episode steps:  32, steps per second:   7, episode reward: 82.800, mean reward:  2.588 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 75.653232, mean_q: 33.333889, mean_eps: 0.100000\n","     172461/2000000000: episode: 4645, duration: 4.273s, episode steps:  31, steps per second:   7, episode reward:  0.500, mean reward:  0.016 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 77.720515, mean_q: 34.260865, mean_eps: 0.100000\n","     172501/2000000000: episode: 4646, duration: 5.161s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 72.838687, mean_q: 34.094171, mean_eps: 0.100000\n","     172533/2000000000: episode: 4647, duration: 4.000s, episode steps:  32, steps per second:   8, episode reward: -112.700, mean reward: -3.522 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 77.876567, mean_q: 34.240276, mean_eps: 0.100000\n","     172571/2000000000: episode: 4648, duration: 4.726s, episode steps:  38, steps per second:   8, episode reward: -36.400, mean reward: -0.958 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 83.710543, mean_q: 34.053289, mean_eps: 0.100000\n","     172609/2000000000: episode: 4649, duration: 4.750s, episode steps:  38, steps per second:   8, episode reward: -51.200, mean reward: -1.347 [-20.000, 18.000], mean action: 1.368 [0.000, 2.000],  loss: 74.574809, mean_q: 34.600795, mean_eps: 0.100000\n","     172649/2000000000: episode: 4650, duration: 5.112s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.627006, mean_q: 33.804278, mean_eps: 0.100000\n","     172689/2000000000: episode: 4651, duration: 5.215s, episode steps:  40, steps per second:   8, episode reward: 18.800, mean reward:  0.470 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.423821, mean_q: 33.190707, mean_eps: 0.100000\n","     172722/2000000000: episode: 4652, duration: 4.242s, episode steps:  33, steps per second:   8, episode reward: 109.900, mean reward:  3.330 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.645666, mean_q: 34.855502, mean_eps: 0.100000\n","     172762/2000000000: episode: 4653, duration: 5.167s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.089433, mean_q: 33.978220, mean_eps: 0.100000\n","     172799/2000000000: episode: 4654, duration: 4.515s, episode steps:  37, steps per second:   8, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 74.026794, mean_q: 35.370643, mean_eps: 0.100000\n","     172832/2000000000: episode: 4655, duration: 4.250s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 84.604899, mean_q: 33.773024, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     172871/2000000000: episode: 4656, duration: 5.138s, episode steps:  39, steps per second:   8, episode reward: 19.400, mean reward:  0.497 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 77.573510, mean_q: 34.078621, mean_eps: 0.100000\n","     172911/2000000000: episode: 4657, duration: 5.392s, episode steps:  40, steps per second:   7, episode reward: -1.700, mean reward: -0.042 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 70.095734, mean_q: 34.150005, mean_eps: 0.100000\n","     172940/2000000000: episode: 4658, duration: 4.043s, episode steps:  29, steps per second:   7, episode reward: 94.300, mean reward:  3.252 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 71.732682, mean_q: 33.881504, mean_eps: 0.100000\n","     172980/2000000000: episode: 4659, duration: 5.417s, episode steps:  40, steps per second:   7, episode reward: 99.100, mean reward:  2.478 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 71.236714, mean_q: 33.838482, mean_eps: 0.100000\n","     173020/2000000000: episode: 4660, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.303789, mean_q: 34.522733, mean_eps: 0.100000\n","     173052/2000000000: episode: 4661, duration: 3.908s, episode steps:  32, steps per second:   8, episode reward: -58.100, mean reward: -1.816 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 80.428007, mean_q: 34.479150, mean_eps: 0.100000\n","     173092/2000000000: episode: 4662, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: -16.100, mean reward: -0.403 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.520378, mean_q: 33.941745, mean_eps: 0.100000\n","     173129/2000000000: episode: 4663, duration: 4.570s, episode steps:  37, steps per second:   8, episode reward: -31.900, mean reward: -0.862 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 81.457967, mean_q: 33.971106, mean_eps: 0.100000\n","     173162/2000000000: episode: 4664, duration: 4.112s, episode steps:  33, steps per second:   8, episode reward: 97.900, mean reward:  2.967 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 70.297526, mean_q: 34.531202, mean_eps: 0.100000\n","     173202/2000000000: episode: 4665, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: -86.500, mean reward: -2.162 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 74.690608, mean_q: 33.574577, mean_eps: 0.100000\n","     173240/2000000000: episode: 4666, duration: 4.910s, episode steps:  38, steps per second:   8, episode reward: -14.000, mean reward: -0.368 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 85.504015, mean_q: 33.825822, mean_eps: 0.100000\n","     173280/2000000000: episode: 4667, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: 93.000, mean reward:  2.325 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.371334, mean_q: 33.675150, mean_eps: 0.100000\n","     173313/2000000000: episode: 4668, duration: 4.247s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 70.454660, mean_q: 34.988221, mean_eps: 0.100000\n","     173353/2000000000: episode: 4669, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: 48.600, mean reward:  1.215 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 74.707215, mean_q: 33.166108, mean_eps: 0.100000\n","     173393/2000000000: episode: 4670, duration: 5.171s, episode steps:  40, steps per second:   8, episode reward: -16.700, mean reward: -0.418 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.265320, mean_q: 33.397561, mean_eps: 0.100000\n","     173431/2000000000: episode: 4671, duration: 5.075s, episode steps:  38, steps per second:   7, episode reward: 166.700, mean reward:  4.387 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 79.250463, mean_q: 33.683726, mean_eps: 0.100000\n","     173469/2000000000: episode: 4672, duration: 5.196s, episode steps:  38, steps per second:   7, episode reward: 109.400, mean reward:  2.879 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.527644, mean_q: 33.454318, mean_eps: 0.100000\n","     173509/2000000000: episode: 4673, duration: 5.035s, episode steps:  40, steps per second:   8, episode reward: -10.300, mean reward: -0.258 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.060511, mean_q: 33.865949, mean_eps: 0.100000\n","     173541/2000000000: episode: 4674, duration: 4.534s, episode steps:  32, steps per second:   7, episode reward: 147.700, mean reward:  4.616 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.926140, mean_q: 35.119326, mean_eps: 0.100000\n","     173573/2000000000: episode: 4675, duration: 4.356s, episode steps:  32, steps per second:   7, episode reward: -12.400, mean reward: -0.387 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.221522, mean_q: 33.536924, mean_eps: 0.100000\n","     173613/2000000000: episode: 4676, duration: 5.189s, episode steps:  40, steps per second:   8, episode reward: -34.100, mean reward: -0.852 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.418507, mean_q: 34.034543, mean_eps: 0.100000\n","     173647/2000000000: episode: 4677, duration: 4.336s, episode steps:  34, steps per second:   8, episode reward: 74.900, mean reward:  2.203 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 78.453632, mean_q: 33.632155, mean_eps: 0.100000\n","     173687/2000000000: episode: 4678, duration: 5.409s, episode steps:  40, steps per second:   7, episode reward: 52.000, mean reward:  1.300 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 72.087212, mean_q: 33.925146, mean_eps: 0.100000\n","     173727/2000000000: episode: 4679, duration: 4.948s, episode steps:  40, steps per second:   8, episode reward: 67.400, mean reward:  1.685 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 71.789209, mean_q: 34.615690, mean_eps: 0.100000\n","     173764/2000000000: episode: 4680, duration: 4.703s, episode steps:  37, steps per second:   8, episode reward: -60.400, mean reward: -1.632 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 72.442263, mean_q: 34.231761, mean_eps: 0.100000\n","     173800/2000000000: episode: 4681, duration: 4.321s, episode steps:  36, steps per second:   8, episode reward: 49.300, mean reward:  1.369 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 76.698416, mean_q: 34.260527, mean_eps: 0.100000\n","     173829/2000000000: episode: 4682, duration: 3.835s, episode steps:  29, steps per second:   8, episode reward: 80.900, mean reward:  2.790 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 75.474332, mean_q: 34.249268, mean_eps: 0.100000\n","     173865/2000000000: episode: 4683, duration: 4.973s, episode steps:  36, steps per second:   7, episode reward: -49.300, mean reward: -1.369 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 72.683885, mean_q: 34.688892, mean_eps: 0.100000\n","     173897/2000000000: episode: 4684, duration: 4.239s, episode steps:  32, steps per second:   8, episode reward: 155.200, mean reward:  4.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 65.541861, mean_q: 34.282774, mean_eps: 0.100000\n","     173937/2000000000: episode: 4685, duration: 5.089s, episode steps:  40, steps per second:   8, episode reward: 126.600, mean reward:  3.165 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.232047, mean_q: 33.968234, mean_eps: 0.100000\n","     173975/2000000000: episode: 4686, duration: 4.924s, episode steps:  38, steps per second:   8, episode reward: -9.000, mean reward: -0.237 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 71.511168, mean_q: 34.669343, mean_eps: 0.100000\n","     174015/2000000000: episode: 4687, duration: 5.265s, episode steps:  40, steps per second:   8, episode reward: 60.400, mean reward:  1.510 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.647298, mean_q: 33.369718, mean_eps: 0.100000\n","     174046/2000000000: episode: 4688, duration: 3.978s, episode steps:  31, steps per second:   8, episode reward: -15.300, mean reward: -0.494 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 76.348649, mean_q: 33.202085, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     174081/2000000000: episode: 4689, duration: 4.607s, episode steps:  35, steps per second:   8, episode reward: -18.500, mean reward: -0.529 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 71.886604, mean_q: 34.439819, mean_eps: 0.100000\n","     174121/2000000000: episode: 4690, duration: 5.628s, episode steps:  40, steps per second:   7, episode reward: -6.800, mean reward: -0.170 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.029443, mean_q: 34.657280, mean_eps: 0.100000\n","     174152/2000000000: episode: 4691, duration: 4.280s, episode steps:  31, steps per second:   7, episode reward: 183.400, mean reward:  5.916 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 70.686635, mean_q: 34.671330, mean_eps: 0.100000\n","     174190/2000000000: episode: 4692, duration: 5.237s, episode steps:  38, steps per second:   7, episode reward: 79.900, mean reward:  2.103 [-20.000, 18.600], mean action: 1.211 [0.000, 2.000],  loss: 79.519133, mean_q: 34.407469, mean_eps: 0.100000\n","     174230/2000000000: episode: 4693, duration: 5.471s, episode steps:  40, steps per second:   7, episode reward: -51.400, mean reward: -1.285 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.317316, mean_q: 34.017451, mean_eps: 0.100000\n","     174262/2000000000: episode: 4694, duration: 4.201s, episode steps:  32, steps per second:   8, episode reward: 68.500, mean reward:  2.141 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.571383, mean_q: 34.235933, mean_eps: 0.100000\n","     174301/2000000000: episode: 4695, duration: 5.256s, episode steps:  39, steps per second:   7, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 71.659518, mean_q: 33.930028, mean_eps: 0.100000\n","     174340/2000000000: episode: 4696, duration: 4.945s, episode steps:  39, steps per second:   8, episode reward: 12.800, mean reward:  0.328 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 70.600686, mean_q: 34.712956, mean_eps: 0.100000\n","     174372/2000000000: episode: 4697, duration: 4.100s, episode steps:  32, steps per second:   8, episode reward: 222.300, mean reward:  6.947 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.687952, mean_q: 34.783583, mean_eps: 0.100000\n","     174412/2000000000: episode: 4698, duration: 4.916s, episode steps:  40, steps per second:   8, episode reward: 63.800, mean reward:  1.595 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.301355, mean_q: 34.426163, mean_eps: 0.100000\n","     174452/2000000000: episode: 4699, duration: 5.031s, episode steps:  40, steps per second:   8, episode reward: -139.700, mean reward: -3.492 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.808177, mean_q: 34.532599, mean_eps: 0.100000\n","     174490/2000000000: episode: 4700, duration: 4.790s, episode steps:  38, steps per second:   8, episode reward: 212.100, mean reward:  5.582 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 74.745947, mean_q: 33.576284, mean_eps: 0.100000\n","     174518/2000000000: episode: 4701, duration: 3.579s, episode steps:  28, steps per second:   8, episode reward: -146.200, mean reward: -5.221 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.139396, mean_q: 33.856279, mean_eps: 0.100000\n","     174548/2000000000: episode: 4702, duration: 3.751s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 72.783206, mean_q: 33.679579, mean_eps: 0.100000\n","     174587/2000000000: episode: 4703, duration: 4.867s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 78.303429, mean_q: 34.242173, mean_eps: 0.100000\n","     174616/2000000000: episode: 4704, duration: 3.815s, episode steps:  29, steps per second:   8, episode reward: 218.900, mean reward:  7.548 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 76.613595, mean_q: 34.409105, mean_eps: 0.100000\n","     174650/2000000000: episode: 4705, duration: 4.649s, episode steps:  34, steps per second:   7, episode reward: 68.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 77.509053, mean_q: 34.564640, mean_eps: 0.100000\n","     174680/2000000000: episode: 4706, duration: 4.210s, episode steps:  30, steps per second:   7, episode reward: -45.100, mean reward: -1.503 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.682890, mean_q: 33.485878, mean_eps: 0.100000\n","     174705/2000000000: episode: 4707, duration: 3.419s, episode steps:  25, steps per second:   7, episode reward: 56.000, mean reward:  2.240 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 77.005184, mean_q: 34.054453, mean_eps: 0.100000\n","     174745/2000000000: episode: 4708, duration: 4.887s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 70.798026, mean_q: 33.969575, mean_eps: 0.100000\n","     174785/2000000000: episode: 4709, duration: 5.273s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 77.250853, mean_q: 34.512461, mean_eps: 0.100000\n","     174825/2000000000: episode: 4710, duration: 5.409s, episode steps:  40, steps per second:   7, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 82.041739, mean_q: 33.793312, mean_eps: 0.100000\n","     174861/2000000000: episode: 4711, duration: 4.649s, episode steps:  36, steps per second:   8, episode reward: -96.000, mean reward: -2.667 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 78.711638, mean_q: 33.942344, mean_eps: 0.100000\n","     174894/2000000000: episode: 4712, duration: 4.394s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 73.215631, mean_q: 34.698575, mean_eps: 0.100000\n","     174927/2000000000: episode: 4713, duration: 4.731s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 71.698924, mean_q: 34.737704, mean_eps: 0.100000\n","     174961/2000000000: episode: 4714, duration: 4.953s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 68.015386, mean_q: 34.658096, mean_eps: 0.100000\n","     174990/2000000000: episode: 4715, duration: 4.459s, episode steps:  29, steps per second:   7, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.588360, mean_q: 34.323627, mean_eps: 0.100000\n","     175030/2000000000: episode: 4716, duration: 5.614s, episode steps:  40, steps per second:   7, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 73.584514, mean_q: 34.583677, mean_eps: 0.100000\n","     175059/2000000000: episode: 4717, duration: 3.882s, episode steps:  29, steps per second:   7, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 82.851064, mean_q: 34.432526, mean_eps: 0.100000\n","     175097/2000000000: episode: 4718, duration: 5.058s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.277931, mean_q: 34.049558, mean_eps: 0.100000\n","     175137/2000000000: episode: 4719, duration: 5.581s, episode steps:  40, steps per second:   7, episode reward: 50.600, mean reward:  1.265 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 69.749292, mean_q: 34.218423, mean_eps: 0.100000\n","     175170/2000000000: episode: 4720, duration: 4.534s, episode steps:  33, steps per second:   7, episode reward: 157.800, mean reward:  4.782 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 75.485780, mean_q: 33.659405, mean_eps: 0.100000\n","     175205/2000000000: episode: 4721, duration: 4.934s, episode steps:  35, steps per second:   7, episode reward: 115.900, mean reward:  3.311 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 83.446385, mean_q: 33.903662, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     175245/2000000000: episode: 4722, duration: 5.932s, episode steps:  40, steps per second:   7, episode reward: 115.100, mean reward:  2.878 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.248674, mean_q: 34.112520, mean_eps: 0.100000\n","     175285/2000000000: episode: 4723, duration: 5.384s, episode steps:  40, steps per second:   7, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.515283, mean_q: 33.775311, mean_eps: 0.100000\n","     175324/2000000000: episode: 4724, duration: 5.324s, episode steps:  39, steps per second:   7, episode reward: 180.700, mean reward:  4.633 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 76.421206, mean_q: 35.418963, mean_eps: 0.100000\n","     175353/2000000000: episode: 4725, duration: 3.914s, episode steps:  29, steps per second:   7, episode reward: -124.200, mean reward: -4.283 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 73.686608, mean_q: 33.645979, mean_eps: 0.100000\n","     175390/2000000000: episode: 4726, duration: 4.827s, episode steps:  37, steps per second:   8, episode reward: -60.100, mean reward: -1.624 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 76.365919, mean_q: 33.425079, mean_eps: 0.100000\n","     175430/2000000000: episode: 4727, duration: 5.278s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.777245, mean_q: 34.848599, mean_eps: 0.100000\n","     175470/2000000000: episode: 4728, duration: 5.465s, episode steps:  40, steps per second:   7, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 70.761930, mean_q: 34.116830, mean_eps: 0.100000\n","     175509/2000000000: episode: 4729, duration: 5.191s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 83.166514, mean_q: 34.157625, mean_eps: 0.100000\n","     175548/2000000000: episode: 4730, duration: 5.171s, episode steps:  39, steps per second:   8, episode reward: -94.900, mean reward: -2.433 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 78.632386, mean_q: 34.895148, mean_eps: 0.100000\n","     175582/2000000000: episode: 4731, duration: 4.592s, episode steps:  34, steps per second:   7, episode reward: -107.400, mean reward: -3.159 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 74.825620, mean_q: 33.878006, mean_eps: 0.100000\n","     175615/2000000000: episode: 4732, duration: 4.457s, episode steps:  33, steps per second:   7, episode reward: 126.800, mean reward:  3.842 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.347602, mean_q: 34.552210, mean_eps: 0.100000\n","     175653/2000000000: episode: 4733, duration: 5.042s, episode steps:  38, steps per second:   8, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 70.157111, mean_q: 33.987412, mean_eps: 0.100000\n","     175681/2000000000: episode: 4734, duration: 3.782s, episode steps:  28, steps per second:   7, episode reward: 53.100, mean reward:  1.896 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 79.057071, mean_q: 34.535789, mean_eps: 0.100000\n","     175707/2000000000: episode: 4735, duration: 3.527s, episode steps:  26, steps per second:   7, episode reward: 48.800, mean reward:  1.877 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 73.009303, mean_q: 33.931676, mean_eps: 0.100000\n","     175747/2000000000: episode: 4736, duration: 5.250s, episode steps:  40, steps per second:   8, episode reward: -68.700, mean reward: -1.718 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.935524, mean_q: 33.959637, mean_eps: 0.100000\n","     175783/2000000000: episode: 4737, duration: 4.836s, episode steps:  36, steps per second:   7, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.621978, mean_q: 35.311145, mean_eps: 0.100000\n","     175823/2000000000: episode: 4738, duration: 5.471s, episode steps:  40, steps per second:   7, episode reward: 103.800, mean reward:  2.595 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.292794, mean_q: 34.402941, mean_eps: 0.100000\n","     175860/2000000000: episode: 4739, duration: 5.113s, episode steps:  37, steps per second:   7, episode reward: -99.700, mean reward: -2.695 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 69.721481, mean_q: 35.669169, mean_eps: 0.100000\n","     175899/2000000000: episode: 4740, duration: 4.978s, episode steps:  39, steps per second:   8, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 79.842609, mean_q: 34.027987, mean_eps: 0.100000\n","     175939/2000000000: episode: 4741, duration: 5.147s, episode steps:  40, steps per second:   8, episode reward: 14.300, mean reward:  0.358 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 73.105326, mean_q: 34.612322, mean_eps: 0.100000\n","     175979/2000000000: episode: 4742, duration: 5.382s, episode steps:  40, steps per second:   7, episode reward: 119.400, mean reward:  2.985 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 72.880562, mean_q: 34.140423, mean_eps: 0.100000\n","     176019/2000000000: episode: 4743, duration: 5.424s, episode steps:  40, steps per second:   7, episode reward: -39.400, mean reward: -0.985 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.709786, mean_q: 34.165713, mean_eps: 0.100000\n","     176051/2000000000: episode: 4744, duration: 4.391s, episode steps:  32, steps per second:   7, episode reward: -14.800, mean reward: -0.463 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 71.533642, mean_q: 34.348745, mean_eps: 0.100000\n","     176091/2000000000: episode: 4745, duration: 5.264s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.830570, mean_q: 34.051009, mean_eps: 0.100000\n","     176131/2000000000: episode: 4746, duration: 5.192s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.824041, mean_q: 33.758009, mean_eps: 0.100000\n","     176171/2000000000: episode: 4747, duration: 5.204s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.266079, mean_q: 34.159632, mean_eps: 0.100000\n","     176211/2000000000: episode: 4748, duration: 5.443s, episode steps:  40, steps per second:   7, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.916825, mean_q: 33.955820, mean_eps: 0.100000\n","     176242/2000000000: episode: 4749, duration: 4.258s, episode steps:  31, steps per second:   7, episode reward: 73.000, mean reward:  2.355 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 74.847576, mean_q: 33.374927, mean_eps: 0.100000\n","     176274/2000000000: episode: 4750, duration: 4.159s, episode steps:  32, steps per second:   8, episode reward: 123.000, mean reward:  3.844 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 74.190923, mean_q: 34.323595, mean_eps: 0.100000\n","     176311/2000000000: episode: 4751, duration: 5.088s, episode steps:  37, steps per second:   7, episode reward: 50.600, mean reward:  1.368 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 68.954750, mean_q: 34.473641, mean_eps: 0.100000\n","     176351/2000000000: episode: 4752, duration: 5.436s, episode steps:  40, steps per second:   7, episode reward: 75.500, mean reward:  1.887 [-20.000, 18.500], mean action: 1.400 [0.000, 2.000],  loss: 75.774679, mean_q: 33.771713, mean_eps: 0.100000\n","     176387/2000000000: episode: 4753, duration: 4.947s, episode steps:  36, steps per second:   7, episode reward: 63.200, mean reward:  1.756 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 72.137845, mean_q: 33.603888, mean_eps: 0.100000\n","     176422/2000000000: episode: 4754, duration: 4.847s, episode steps:  35, steps per second:   7, episode reward:  3.200, mean reward:  0.091 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 77.147483, mean_q: 34.163002, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     176452/2000000000: episode: 4755, duration: 4.185s, episode steps:  30, steps per second:   7, episode reward: -71.500, mean reward: -2.383 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 72.249322, mean_q: 34.994032, mean_eps: 0.100000\n","     176486/2000000000: episode: 4756, duration: 4.408s, episode steps:  34, steps per second:   8, episode reward: 85.100, mean reward:  2.503 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 76.206065, mean_q: 33.949423, mean_eps: 0.100000\n","     176526/2000000000: episode: 4757, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: 65.700, mean reward:  1.642 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.158477, mean_q: 33.926967, mean_eps: 0.100000\n","     176558/2000000000: episode: 4758, duration: 4.309s, episode steps:  32, steps per second:   7, episode reward: 59.100, mean reward:  1.847 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 69.820926, mean_q: 34.345442, mean_eps: 0.100000\n","     176598/2000000000: episode: 4759, duration: 5.606s, episode steps:  40, steps per second:   7, episode reward: 77.100, mean reward:  1.928 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 69.007226, mean_q: 34.257902, mean_eps: 0.100000\n","     176637/2000000000: episode: 4760, duration: 5.325s, episode steps:  39, steps per second:   7, episode reward: -3.500, mean reward: -0.090 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 73.444453, mean_q: 35.062506, mean_eps: 0.100000\n","     176670/2000000000: episode: 4761, duration: 4.575s, episode steps:  33, steps per second:   7, episode reward:  3.600, mean reward:  0.109 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 74.249126, mean_q: 34.326823, mean_eps: 0.100000\n","     176701/2000000000: episode: 4762, duration: 4.352s, episode steps:  31, steps per second:   7, episode reward: 63.200, mean reward:  2.039 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 83.821312, mean_q: 34.018994, mean_eps: 0.100000\n","     176725/2000000000: episode: 4763, duration: 3.434s, episode steps:  24, steps per second:   7, episode reward: 82.500, mean reward:  3.437 [-20.000, 18.000], mean action: 0.917 [0.000, 2.000],  loss: 80.083734, mean_q: 33.863610, mean_eps: 0.100000\n","     176765/2000000000: episode: 4764, duration: 5.589s, episode steps:  40, steps per second:   7, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.368957, mean_q: 34.745997, mean_eps: 0.100000\n","     176802/2000000000: episode: 4765, duration: 5.026s, episode steps:  37, steps per second:   7, episode reward: 172.000, mean reward:  4.649 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 74.000190, mean_q: 33.500315, mean_eps: 0.100000\n","     176842/2000000000: episode: 4766, duration: 5.624s, episode steps:  40, steps per second:   7, episode reward: 91.500, mean reward:  2.287 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.543802, mean_q: 33.867913, mean_eps: 0.100000\n","     176882/2000000000: episode: 4767, duration: 5.540s, episode steps:  40, steps per second:   7, episode reward: 56.100, mean reward:  1.402 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.434866, mean_q: 33.313977, mean_eps: 0.100000\n","     176914/2000000000: episode: 4768, duration: 4.591s, episode steps:  32, steps per second:   7, episode reward: 20.800, mean reward:  0.650 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 73.165059, mean_q: 34.458325, mean_eps: 0.100000\n","     176941/2000000000: episode: 4769, duration: 3.801s, episode steps:  27, steps per second:   7, episode reward: -33.400, mean reward: -1.237 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 72.355894, mean_q: 34.427079, mean_eps: 0.100000\n","     176976/2000000000: episode: 4770, duration: 4.973s, episode steps:  35, steps per second:   7, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 78.203941, mean_q: 34.415827, mean_eps: 0.100000\n","     177016/2000000000: episode: 4771, duration: 5.578s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.715362, mean_q: 34.012839, mean_eps: 0.100000\n","     177046/2000000000: episode: 4772, duration: 4.323s, episode steps:  30, steps per second:   7, episode reward: 22.000, mean reward:  0.733 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 82.398691, mean_q: 33.538273, mean_eps: 0.100000\n","     177086/2000000000: episode: 4773, duration: 5.640s, episode steps:  40, steps per second:   7, episode reward: 10.600, mean reward:  0.265 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 70.324701, mean_q: 34.167959, mean_eps: 0.100000\n","     177126/2000000000: episode: 4774, duration: 5.679s, episode steps:  40, steps per second:   7, episode reward: -74.500, mean reward: -1.863 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.746620, mean_q: 34.101442, mean_eps: 0.100000\n","     177158/2000000000: episode: 4775, duration: 4.411s, episode steps:  32, steps per second:   7, episode reward: -52.600, mean reward: -1.644 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 69.643231, mean_q: 34.497344, mean_eps: 0.100000\n","     177193/2000000000: episode: 4776, duration: 4.709s, episode steps:  35, steps per second:   7, episode reward: 49.400, mean reward:  1.411 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 75.952865, mean_q: 34.368903, mean_eps: 0.100000\n","     177232/2000000000: episode: 4777, duration: 5.230s, episode steps:  39, steps per second:   7, episode reward: 97.400, mean reward:  2.497 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 75.458343, mean_q: 35.370652, mean_eps: 0.100000\n","     177267/2000000000: episode: 4778, duration: 4.755s, episode steps:  35, steps per second:   7, episode reward: 53.500, mean reward:  1.529 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 80.584664, mean_q: 34.136690, mean_eps: 0.100000\n","     177307/2000000000: episode: 4779, duration: 5.227s, episode steps:  40, steps per second:   8, episode reward: 107.600, mean reward:  2.690 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.106242, mean_q: 34.423008, mean_eps: 0.100000\n","     177347/2000000000: episode: 4780, duration: 5.234s, episode steps:  40, steps per second:   8, episode reward: 114.600, mean reward:  2.865 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 68.536255, mean_q: 34.617171, mean_eps: 0.100000\n","     177381/2000000000: episode: 4781, duration: 4.384s, episode steps:  34, steps per second:   8, episode reward: 165.600, mean reward:  4.871 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 71.182594, mean_q: 34.214419, mean_eps: 0.100000\n","     177410/2000000000: episode: 4782, duration: 3.921s, episode steps:  29, steps per second:   7, episode reward: 98.000, mean reward:  3.379 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 72.778241, mean_q: 33.689149, mean_eps: 0.100000\n","     177439/2000000000: episode: 4783, duration: 3.893s, episode steps:  29, steps per second:   7, episode reward: -28.900, mean reward: -0.997 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 70.738989, mean_q: 35.191190, mean_eps: 0.100000\n","     177470/2000000000: episode: 4784, duration: 4.141s, episode steps:  31, steps per second:   7, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 68.012447, mean_q: 34.370699, mean_eps: 0.100000\n","     177509/2000000000: episode: 4785, duration: 5.708s, episode steps:  39, steps per second:   7, episode reward: -119.700, mean reward: -3.069 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 82.320482, mean_q: 33.831238, mean_eps: 0.100000\n","     177542/2000000000: episode: 4786, duration: 4.708s, episode steps:  33, steps per second:   7, episode reward: -37.400, mean reward: -1.133 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.180987, mean_q: 34.092046, mean_eps: 0.100000\n","     177581/2000000000: episode: 4787, duration: 5.299s, episode steps:  39, steps per second:   7, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 77.032257, mean_q: 34.026387, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     177621/2000000000: episode: 4788, duration: 5.288s, episode steps:  40, steps per second:   8, episode reward:  4.700, mean reward:  0.117 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.196876, mean_q: 33.641413, mean_eps: 0.100000\n","     177657/2000000000: episode: 4789, duration: 4.870s, episode steps:  36, steps per second:   7, episode reward: 188.500, mean reward:  5.236 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.716435, mean_q: 34.193784, mean_eps: 0.100000\n","     177694/2000000000: episode: 4790, duration: 5.135s, episode steps:  37, steps per second:   7, episode reward: -4.600, mean reward: -0.124 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 78.699208, mean_q: 34.008348, mean_eps: 0.100000\n","     177734/2000000000: episode: 4791, duration: 5.652s, episode steps:  40, steps per second:   7, episode reward: -68.600, mean reward: -1.715 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 71.207837, mean_q: 34.035713, mean_eps: 0.100000\n","     177770/2000000000: episode: 4792, duration: 4.883s, episode steps:  36, steps per second:   7, episode reward: 142.500, mean reward:  3.958 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 78.282488, mean_q: 34.005718, mean_eps: 0.100000\n","     177807/2000000000: episode: 4793, duration: 5.011s, episode steps:  37, steps per second:   7, episode reward:  1.600, mean reward:  0.043 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 75.140950, mean_q: 34.701721, mean_eps: 0.100000\n","     177847/2000000000: episode: 4794, duration: 5.595s, episode steps:  40, steps per second:   7, episode reward: 101.600, mean reward:  2.540 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.744658, mean_q: 34.231206, mean_eps: 0.100000\n","     177876/2000000000: episode: 4795, duration: 4.165s, episode steps:  29, steps per second:   7, episode reward: 131.100, mean reward:  4.521 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.286938, mean_q: 35.015435, mean_eps: 0.100000\n","     177916/2000000000: episode: 4796, duration: 5.533s, episode steps:  40, steps per second:   7, episode reward: 19.600, mean reward:  0.490 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.490307, mean_q: 34.809946, mean_eps: 0.100000\n","     177951/2000000000: episode: 4797, duration: 5.067s, episode steps:  35, steps per second:   7, episode reward: 64.200, mean reward:  1.834 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 74.330103, mean_q: 34.778732, mean_eps: 0.100000\n","     177990/2000000000: episode: 4798, duration: 5.446s, episode steps:  39, steps per second:   7, episode reward:  6.900, mean reward:  0.177 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 75.560889, mean_q: 33.654659, mean_eps: 0.100000\n","     178016/2000000000: episode: 4799, duration: 3.806s, episode steps:  26, steps per second:   7, episode reward: 78.800, mean reward:  3.031 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.763090, mean_q: 34.001777, mean_eps: 0.100000\n","     178056/2000000000: episode: 4800, duration: 5.530s, episode steps:  40, steps per second:   7, episode reward: 133.600, mean reward:  3.340 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.414917, mean_q: 34.146334, mean_eps: 0.100000\n","     178090/2000000000: episode: 4801, duration: 4.148s, episode steps:  34, steps per second:   8, episode reward: 199.300, mean reward:  5.862 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 73.768759, mean_q: 33.955390, mean_eps: 0.100000\n","     178117/2000000000: episode: 4802, duration: 3.393s, episode steps:  27, steps per second:   8, episode reward: -55.600, mean reward: -2.059 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 72.925743, mean_q: 34.750570, mean_eps: 0.100000\n","     178152/2000000000: episode: 4803, duration: 4.345s, episode steps:  35, steps per second:   8, episode reward: 166.500, mean reward:  4.757 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 77.259510, mean_q: 34.609546, mean_eps: 0.100000\n","     178187/2000000000: episode: 4804, duration: 4.312s, episode steps:  35, steps per second:   8, episode reward: 121.700, mean reward:  3.477 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 73.776295, mean_q: 33.571890, mean_eps: 0.100000\n","     178220/2000000000: episode: 4805, duration: 4.064s, episode steps:  33, steps per second:   8, episode reward:  8.900, mean reward:  0.270 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 73.016629, mean_q: 34.723552, mean_eps: 0.100000\n","     178250/2000000000: episode: 4806, duration: 3.777s, episode steps:  30, steps per second:   8, episode reward: 39.800, mean reward:  1.327 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 72.711873, mean_q: 34.358198, mean_eps: 0.100000\n","     178287/2000000000: episode: 4807, duration: 4.443s, episode steps:  37, steps per second:   8, episode reward: -66.400, mean reward: -1.795 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 72.096017, mean_q: 34.153299, mean_eps: 0.100000\n","     178327/2000000000: episode: 4808, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward: 38.600, mean reward:  0.965 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.364910, mean_q: 33.723432, mean_eps: 0.100000\n","     178365/2000000000: episode: 4809, duration: 4.770s, episode steps:  38, steps per second:   8, episode reward: 73.200, mean reward:  1.926 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 76.982272, mean_q: 33.703096, mean_eps: 0.100000\n","     178397/2000000000: episode: 4810, duration: 3.993s, episode steps:  32, steps per second:   8, episode reward: 124.800, mean reward:  3.900 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 76.386834, mean_q: 34.472074, mean_eps: 0.100000\n","     178430/2000000000: episode: 4811, duration: 4.244s, episode steps:  33, steps per second:   8, episode reward: 119.100, mean reward:  3.609 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 74.828663, mean_q: 34.632573, mean_eps: 0.100000\n","     178460/2000000000: episode: 4812, duration: 3.750s, episode steps:  30, steps per second:   8, episode reward: 115.700, mean reward:  3.857 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 73.680595, mean_q: 34.372809, mean_eps: 0.100000\n","     178500/2000000000: episode: 4813, duration: 4.894s, episode steps:  40, steps per second:   8, episode reward: 89.000, mean reward:  2.225 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.804715, mean_q: 34.154260, mean_eps: 0.100000\n","     178540/2000000000: episode: 4814, duration: 4.959s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.361777, mean_q: 33.947491, mean_eps: 0.100000\n","     178572/2000000000: episode: 4815, duration: 4.152s, episode steps:  32, steps per second:   8, episode reward: -109.100, mean reward: -3.409 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 65.643337, mean_q: 34.495392, mean_eps: 0.100000\n","     178605/2000000000: episode: 4816, duration: 4.210s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.108001, mean_q: 34.189722, mean_eps: 0.100000\n","     178637/2000000000: episode: 4817, duration: 4.105s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.956035, mean_q: 34.449360, mean_eps: 0.100000\n","     178677/2000000000: episode: 4818, duration: 5.005s, episode steps:  40, steps per second:   8, episode reward: 46.200, mean reward:  1.155 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 87.570628, mean_q: 34.285053, mean_eps: 0.100000\n","     178715/2000000000: episode: 4819, duration: 4.742s, episode steps:  38, steps per second:   8, episode reward: 78.200, mean reward:  2.058 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 73.781197, mean_q: 33.846483, mean_eps: 0.100000\n","     178755/2000000000: episode: 4820, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.765016, mean_q: 33.644684, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     178791/2000000000: episode: 4821, duration: 4.522s, episode steps:  36, steps per second:   8, episode reward:  7.900, mean reward:  0.219 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.866824, mean_q: 34.379304, mean_eps: 0.100000\n","     178831/2000000000: episode: 4822, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: -112.000, mean reward: -2.800 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.537677, mean_q: 34.585739, mean_eps: 0.100000\n","     178869/2000000000: episode: 4823, duration: 4.798s, episode steps:  38, steps per second:   8, episode reward: -45.900, mean reward: -1.208 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 74.413123, mean_q: 35.056808, mean_eps: 0.100000\n","     178893/2000000000: episode: 4824, duration: 3.102s, episode steps:  24, steps per second:   8, episode reward: -96.000, mean reward: -4.000 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 74.218933, mean_q: 33.627951, mean_eps: 0.100000\n","     178923/2000000000: episode: 4825, duration: 3.737s, episode steps:  30, steps per second:   8, episode reward: -134.000, mean reward: -4.467 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 76.569007, mean_q: 34.281503, mean_eps: 0.100000\n","     178963/2000000000: episode: 4826, duration: 4.832s, episode steps:  40, steps per second:   8, episode reward: 144.800, mean reward:  3.620 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.261722, mean_q: 33.987523, mean_eps: 0.100000\n","     178999/2000000000: episode: 4827, duration: 4.664s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 74.097008, mean_q: 33.476211, mean_eps: 0.100000\n","     179031/2000000000: episode: 4828, duration: 4.075s, episode steps:  32, steps per second:   8, episode reward: 32.100, mean reward:  1.003 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 78.095997, mean_q: 34.046923, mean_eps: 0.100000\n","     179066/2000000000: episode: 4829, duration: 4.462s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.879000, mean_q: 33.782487, mean_eps: 0.100000\n","     179106/2000000000: episode: 4830, duration: 4.893s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.822348, mean_q: 34.027755, mean_eps: 0.100000\n","     179140/2000000000: episode: 4831, duration: 4.299s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 71.245289, mean_q: 34.265896, mean_eps: 0.100000\n","     179180/2000000000: episode: 4832, duration: 4.967s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.733446, mean_q: 33.996596, mean_eps: 0.100000\n","     179215/2000000000: episode: 4833, duration: 4.326s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 79.931582, mean_q: 33.477296, mean_eps: 0.100000\n","     179252/2000000000: episode: 4834, duration: 4.591s, episode steps:  37, steps per second:   8, episode reward: 75.400, mean reward:  2.038 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.113140, mean_q: 34.618880, mean_eps: 0.100000\n","     179291/2000000000: episode: 4835, duration: 4.971s, episode steps:  39, steps per second:   8, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 73.793635, mean_q: 34.708573, mean_eps: 0.100000\n","     179328/2000000000: episode: 4836, duration: 4.576s, episode steps:  37, steps per second:   8, episode reward: 63.000, mean reward:  1.703 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 75.645543, mean_q: 34.357517, mean_eps: 0.100000\n","     179365/2000000000: episode: 4837, duration: 4.747s, episode steps:  37, steps per second:   8, episode reward: 71.200, mean reward:  1.924 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 76.235942, mean_q: 33.473133, mean_eps: 0.100000\n","     179400/2000000000: episode: 4838, duration: 4.388s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 73.143914, mean_q: 34.645367, mean_eps: 0.100000\n","     179440/2000000000: episode: 4839, duration: 4.856s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 72.603278, mean_q: 33.915561, mean_eps: 0.100000\n","     179474/2000000000: episode: 4840, duration: 4.309s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 79.840618, mean_q: 35.063012, mean_eps: 0.100000\n","     179514/2000000000: episode: 4841, duration: 4.911s, episode steps:  40, steps per second:   8, episode reward: 26.000, mean reward:  0.650 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 81.610726, mean_q: 34.115986, mean_eps: 0.100000\n","     179549/2000000000: episode: 4842, duration: 4.424s, episode steps:  35, steps per second:   8, episode reward: 52.300, mean reward:  1.494 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.645719, mean_q: 34.427659, mean_eps: 0.100000\n","     179589/2000000000: episode: 4843, duration: 5.105s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.597441, mean_q: 34.584788, mean_eps: 0.100000\n","     179623/2000000000: episode: 4844, duration: 4.255s, episode steps:  34, steps per second:   8, episode reward: 100.200, mean reward:  2.947 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 76.869431, mean_q: 33.059288, mean_eps: 0.100000\n","     179660/2000000000: episode: 4845, duration: 5.053s, episode steps:  37, steps per second:   7, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 74.417535, mean_q: 34.983004, mean_eps: 0.100000\n","     179689/2000000000: episode: 4846, duration: 4.021s, episode steps:  29, steps per second:   7, episode reward: -39.200, mean reward: -1.352 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 74.333632, mean_q: 33.645143, mean_eps: 0.100000\n","     179729/2000000000: episode: 4847, duration: 5.014s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.653338, mean_q: 34.267295, mean_eps: 0.100000\n","     179766/2000000000: episode: 4848, duration: 4.601s, episode steps:  37, steps per second:   8, episode reward: 186.400, mean reward:  5.038 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 77.412255, mean_q: 34.134771, mean_eps: 0.100000\n","     179806/2000000000: episode: 4849, duration: 5.158s, episode steps:  40, steps per second:   8, episode reward: 67.600, mean reward:  1.690 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.942072, mean_q: 33.903311, mean_eps: 0.100000\n","     179845/2000000000: episode: 4850, duration: 4.927s, episode steps:  39, steps per second:   8, episode reward: 34.700, mean reward:  0.890 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 76.755753, mean_q: 33.994665, mean_eps: 0.100000\n","     179885/2000000000: episode: 4851, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: 93.700, mean reward:  2.343 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 70.380262, mean_q: 34.042565, mean_eps: 0.100000\n","     179914/2000000000: episode: 4852, duration: 3.608s, episode steps:  29, steps per second:   8, episode reward: 49.200, mean reward:  1.697 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 72.396569, mean_q: 34.973993, mean_eps: 0.100000\n","     179954/2000000000: episode: 4853, duration: 4.858s, episode steps:  40, steps per second:   8, episode reward: 133.600, mean reward:  3.340 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.635923, mean_q: 33.115882, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     179994/2000000000: episode: 4854, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.418578, mean_q: 33.488960, mean_eps: 0.100000\n","     180034/2000000000: episode: 4855, duration: 5.058s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.696658, mean_q: 36.355597, mean_eps: 0.100000\n","     180066/2000000000: episode: 4856, duration: 4.121s, episode steps:  32, steps per second:   8, episode reward: -3.600, mean reward: -0.112 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.123575, mean_q: 36.764409, mean_eps: 0.100000\n","     180101/2000000000: episode: 4857, duration: 4.462s, episode steps:  35, steps per second:   8, episode reward: 151.900, mean reward:  4.340 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 82.132453, mean_q: 36.573422, mean_eps: 0.100000\n","     180139/2000000000: episode: 4858, duration: 4.741s, episode steps:  38, steps per second:   8, episode reward: 27.800, mean reward:  0.732 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 90.903829, mean_q: 37.256450, mean_eps: 0.100000\n","     180171/2000000000: episode: 4859, duration: 4.048s, episode steps:  32, steps per second:   8, episode reward: 70.200, mean reward:  2.194 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 78.371057, mean_q: 36.313658, mean_eps: 0.100000\n","     180209/2000000000: episode: 4860, duration: 4.744s, episode steps:  38, steps per second:   8, episode reward: 115.100, mean reward:  3.029 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 84.956944, mean_q: 35.850142, mean_eps: 0.100000\n","     180243/2000000000: episode: 4861, duration: 4.356s, episode steps:  34, steps per second:   8, episode reward: -8.900, mean reward: -0.262 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 79.747590, mean_q: 36.664034, mean_eps: 0.100000\n","     180282/2000000000: episode: 4862, duration: 4.705s, episode steps:  39, steps per second:   8, episode reward: -5.000, mean reward: -0.128 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 86.163568, mean_q: 36.104918, mean_eps: 0.100000\n","     180320/2000000000: episode: 4863, duration: 4.609s, episode steps:  38, steps per second:   8, episode reward: 124.200, mean reward:  3.268 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 73.862872, mean_q: 36.304248, mean_eps: 0.100000\n","     180350/2000000000: episode: 4864, duration: 3.772s, episode steps:  30, steps per second:   8, episode reward: 17.900, mean reward:  0.597 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 81.379168, mean_q: 37.160237, mean_eps: 0.100000\n","     180390/2000000000: episode: 4865, duration: 4.903s, episode steps:  40, steps per second:   8, episode reward: -72.200, mean reward: -1.805 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 81.158670, mean_q: 36.153259, mean_eps: 0.100000\n","     180430/2000000000: episode: 4866, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward:  4.300, mean reward:  0.107 [-20.000, 18.300], mean action: 1.500 [0.000, 2.000],  loss: 81.798310, mean_q: 36.909813, mean_eps: 0.100000\n","     180464/2000000000: episode: 4867, duration: 4.200s, episode steps:  34, steps per second:   8, episode reward: 140.400, mean reward:  4.129 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 82.912265, mean_q: 36.535322, mean_eps: 0.100000\n","     180494/2000000000: episode: 4868, duration: 3.782s, episode steps:  30, steps per second:   8, episode reward: -46.300, mean reward: -1.543 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 77.244520, mean_q: 36.043778, mean_eps: 0.100000\n","     180531/2000000000: episode: 4869, duration: 4.695s, episode steps:  37, steps per second:   8, episode reward:  7.800, mean reward:  0.211 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 76.973271, mean_q: 35.602763, mean_eps: 0.100000\n","     180571/2000000000: episode: 4870, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward:  4.900, mean reward:  0.122 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.550910, mean_q: 37.003875, mean_eps: 0.100000\n","     180611/2000000000: episode: 4871, duration: 5.300s, episode steps:  40, steps per second:   8, episode reward: 58.100, mean reward:  1.453 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.391678, mean_q: 37.271437, mean_eps: 0.100000\n","     180646/2000000000: episode: 4872, duration: 4.699s, episode steps:  35, steps per second:   7, episode reward: 60.700, mean reward:  1.734 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.605270, mean_q: 36.113753, mean_eps: 0.100000\n","     180685/2000000000: episode: 4873, duration: 5.178s, episode steps:  39, steps per second:   8, episode reward: 94.500, mean reward:  2.423 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 82.293423, mean_q: 35.640469, mean_eps: 0.100000\n","     180721/2000000000: episode: 4874, duration: 4.640s, episode steps:  36, steps per second:   8, episode reward: 79.600, mean reward:  2.211 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 80.371611, mean_q: 36.040489, mean_eps: 0.100000\n","     180750/2000000000: episode: 4875, duration: 3.831s, episode steps:  29, steps per second:   8, episode reward: 181.600, mean reward:  6.262 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 77.270249, mean_q: 36.444198, mean_eps: 0.100000\n","     180779/2000000000: episode: 4876, duration: 3.790s, episode steps:  29, steps per second:   8, episode reward:  1.400, mean reward:  0.048 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 86.241214, mean_q: 36.201852, mean_eps: 0.100000\n","     180819/2000000000: episode: 4877, duration: 5.162s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.546667, mean_q: 36.865916, mean_eps: 0.100000\n","     180854/2000000000: episode: 4878, duration: 4.671s, episode steps:  35, steps per second:   7, episode reward: -12.600, mean reward: -0.360 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 71.315208, mean_q: 37.790629, mean_eps: 0.100000\n","     180887/2000000000: episode: 4879, duration: 4.362s, episode steps:  33, steps per second:   8, episode reward: 66.900, mean reward:  2.027 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.514983, mean_q: 36.144034, mean_eps: 0.100000\n","     180919/2000000000: episode: 4880, duration: 4.329s, episode steps:  32, steps per second:   7, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.630247, mean_q: 36.794966, mean_eps: 0.100000\n","     180959/2000000000: episode: 4881, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: -35.500, mean reward: -0.888 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.839897, mean_q: 36.013887, mean_eps: 0.100000\n","     180989/2000000000: episode: 4882, duration: 3.793s, episode steps:  30, steps per second:   8, episode reward: 155.700, mean reward:  5.190 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.459892, mean_q: 36.492641, mean_eps: 0.100000\n","     181018/2000000000: episode: 4883, duration: 3.668s, episode steps:  29, steps per second:   8, episode reward: 40.800, mean reward:  1.407 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 83.955838, mean_q: 36.992145, mean_eps: 0.100000\n","     181057/2000000000: episode: 4884, duration: 5.008s, episode steps:  39, steps per second:   8, episode reward: 35.200, mean reward:  0.903 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 76.302966, mean_q: 36.531959, mean_eps: 0.100000\n","     181097/2000000000: episode: 4885, duration: 5.291s, episode steps:  40, steps per second:   8, episode reward: 91.500, mean reward:  2.288 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.096326, mean_q: 36.503629, mean_eps: 0.100000\n","     181137/2000000000: episode: 4886, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward: 121.400, mean reward:  3.035 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.100248, mean_q: 35.704763, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     181168/2000000000: episode: 4887, duration: 3.890s, episode steps:  31, steps per second:   8, episode reward: 129.500, mean reward:  4.177 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 82.258482, mean_q: 36.346436, mean_eps: 0.100000\n","     181207/2000000000: episode: 4888, duration: 4.943s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 80.990457, mean_q: 36.878183, mean_eps: 0.100000\n","     181247/2000000000: episode: 4889, duration: 5.257s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 73.968849, mean_q: 37.038288, mean_eps: 0.100000\n","     181282/2000000000: episode: 4890, duration: 4.580s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 75.515702, mean_q: 36.835789, mean_eps: 0.100000\n","     181319/2000000000: episode: 4891, duration: 4.732s, episode steps:  37, steps per second:   8, episode reward: 231.100, mean reward:  6.246 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 77.691809, mean_q: 37.255719, mean_eps: 0.100000\n","     181351/2000000000: episode: 4892, duration: 4.152s, episode steps:  32, steps per second:   8, episode reward: -54.800, mean reward: -1.713 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.369689, mean_q: 37.143436, mean_eps: 0.100000\n","     181389/2000000000: episode: 4893, duration: 5.081s, episode steps:  38, steps per second:   7, episode reward: -35.400, mean reward: -0.932 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 77.870987, mean_q: 36.871731, mean_eps: 0.100000\n","     181425/2000000000: episode: 4894, duration: 4.732s, episode steps:  36, steps per second:   8, episode reward: 139.500, mean reward:  3.875 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 80.103226, mean_q: 36.872348, mean_eps: 0.100000\n","     181457/2000000000: episode: 4895, duration: 4.561s, episode steps:  32, steps per second:   7, episode reward: 114.200, mean reward:  3.569 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 81.881711, mean_q: 36.872657, mean_eps: 0.100000\n","     181497/2000000000: episode: 4896, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward:  3.400, mean reward:  0.085 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 95.425826, mean_q: 36.243944, mean_eps: 0.100000\n","     181537/2000000000: episode: 4897, duration: 5.038s, episode steps:  40, steps per second:   8, episode reward: 45.400, mean reward:  1.135 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.551849, mean_q: 36.127483, mean_eps: 0.100000\n","     181577/2000000000: episode: 4898, duration: 5.174s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.312610, mean_q: 36.719048, mean_eps: 0.100000\n","     181614/2000000000: episode: 4899, duration: 4.424s, episode steps:  37, steps per second:   8, episode reward: 22.800, mean reward:  0.616 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 77.174653, mean_q: 36.741350, mean_eps: 0.100000\n","     181652/2000000000: episode: 4900, duration: 4.641s, episode steps:  38, steps per second:   8, episode reward: 55.600, mean reward:  1.463 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 72.280093, mean_q: 35.900672, mean_eps: 0.100000\n","     181690/2000000000: episode: 4901, duration: 4.721s, episode steps:  38, steps per second:   8, episode reward: -15.500, mean reward: -0.408 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 71.826105, mean_q: 36.092612, mean_eps: 0.100000\n","     181726/2000000000: episode: 4902, duration: 4.335s, episode steps:  36, steps per second:   8, episode reward: 128.000, mean reward:  3.556 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 74.676859, mean_q: 36.252070, mean_eps: 0.100000\n","     181766/2000000000: episode: 4903, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 74.823743, mean_q: 36.691951, mean_eps: 0.100000\n","     181797/2000000000: episode: 4904, duration: 3.666s, episode steps:  31, steps per second:   8, episode reward: 98.900, mean reward:  3.190 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 87.191420, mean_q: 35.925002, mean_eps: 0.100000\n","     181827/2000000000: episode: 4905, duration: 3.677s, episode steps:  30, steps per second:   8, episode reward: 104.500, mean reward:  3.483 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 75.897916, mean_q: 36.587368, mean_eps: 0.100000\n","     181867/2000000000: episode: 4906, duration: 5.130s, episode steps:  40, steps per second:   8, episode reward: -37.400, mean reward: -0.935 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 76.383601, mean_q: 36.955978, mean_eps: 0.100000\n","     181902/2000000000: episode: 4907, duration: 4.477s, episode steps:  35, steps per second:   8, episode reward: 27.000, mean reward:  0.771 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 79.311414, mean_q: 37.103359, mean_eps: 0.100000\n","     181932/2000000000: episode: 4908, duration: 4.230s, episode steps:  30, steps per second:   7, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.824676, mean_q: 36.101783, mean_eps: 0.100000\n","     181972/2000000000: episode: 4909, duration: 5.275s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.082030, mean_q: 37.001993, mean_eps: 0.100000\n","     182011/2000000000: episode: 4910, duration: 5.152s, episode steps:  39, steps per second:   8, episode reward: 133.900, mean reward:  3.433 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 82.261507, mean_q: 36.657480, mean_eps: 0.100000\n","     182051/2000000000: episode: 4911, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: -16.100, mean reward: -0.403 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.779313, mean_q: 36.677403, mean_eps: 0.100000\n","     182091/2000000000: episode: 4912, duration: 5.391s, episode steps:  40, steps per second:   7, episode reward: 45.800, mean reward:  1.145 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.116822, mean_q: 37.493739, mean_eps: 0.100000\n","     182124/2000000000: episode: 4913, duration: 4.377s, episode steps:  33, steps per second:   8, episode reward: 73.300, mean reward:  2.221 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 73.835420, mean_q: 35.975524, mean_eps: 0.100000\n","     182154/2000000000: episode: 4914, duration: 3.959s, episode steps:  30, steps per second:   8, episode reward: 143.200, mean reward:  4.773 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.593298, mean_q: 36.799117, mean_eps: 0.100000\n","     182194/2000000000: episode: 4915, duration: 4.945s, episode steps:  40, steps per second:   8, episode reward: 115.100, mean reward:  2.877 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.808189, mean_q: 36.850132, mean_eps: 0.100000\n","     182234/2000000000: episode: 4916, duration: 5.393s, episode steps:  40, steps per second:   7, episode reward: 40.700, mean reward:  1.017 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.018266, mean_q: 36.817049, mean_eps: 0.100000\n","     182271/2000000000: episode: 4917, duration: 4.500s, episode steps:  37, steps per second:   8, episode reward: 87.800, mean reward:  2.373 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 83.649234, mean_q: 36.570460, mean_eps: 0.100000\n","     182306/2000000000: episode: 4918, duration: 4.482s, episode steps:  35, steps per second:   8, episode reward: 37.200, mean reward:  1.063 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 73.329323, mean_q: 36.924529, mean_eps: 0.100000\n","     182335/2000000000: episode: 4919, duration: 3.852s, episode steps:  29, steps per second:   8, episode reward: 82.300, mean reward:  2.838 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 82.225117, mean_q: 37.038071, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     182370/2000000000: episode: 4920, duration: 4.370s, episode steps:  35, steps per second:   8, episode reward: 132.600, mean reward:  3.789 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.338441, mean_q: 37.023047, mean_eps: 0.100000\n","     182400/2000000000: episode: 4921, duration: 3.680s, episode steps:  30, steps per second:   8, episode reward: -35.500, mean reward: -1.183 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.300864, mean_q: 37.067693, mean_eps: 0.100000\n","     182440/2000000000: episode: 4922, duration: 5.304s, episode steps:  40, steps per second:   8, episode reward: -29.800, mean reward: -0.745 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.697756, mean_q: 36.357229, mean_eps: 0.100000\n","     182480/2000000000: episode: 4923, duration: 4.905s, episode steps:  40, steps per second:   8, episode reward: 112.300, mean reward:  2.807 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.377192, mean_q: 36.793857, mean_eps: 0.100000\n","     182520/2000000000: episode: 4924, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: 175.300, mean reward:  4.383 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.698214, mean_q: 36.746549, mean_eps: 0.100000\n","     182552/2000000000: episode: 4925, duration: 4.042s, episode steps:  32, steps per second:   8, episode reward:  2.000, mean reward:  0.062 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 83.007001, mean_q: 35.609165, mean_eps: 0.100000\n","     182584/2000000000: episode: 4926, duration: 4.036s, episode steps:  32, steps per second:   8, episode reward: -39.500, mean reward: -1.234 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 77.415842, mean_q: 36.919539, mean_eps: 0.100000\n","     182618/2000000000: episode: 4927, duration: 4.233s, episode steps:  34, steps per second:   8, episode reward: -10.100, mean reward: -0.297 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 83.406071, mean_q: 35.925710, mean_eps: 0.100000\n","     182658/2000000000: episode: 4928, duration: 5.149s, episode steps:  40, steps per second:   8, episode reward: 226.200, mean reward:  5.655 [-14.100, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 82.073797, mean_q: 36.918865, mean_eps: 0.100000\n","     182698/2000000000: episode: 4929, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 79.000, mean reward:  1.975 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.343102, mean_q: 36.551451, mean_eps: 0.100000\n","     182734/2000000000: episode: 4930, duration: 4.602s, episode steps:  36, steps per second:   8, episode reward: 95.800, mean reward:  2.661 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 79.516543, mean_q: 36.770414, mean_eps: 0.100000\n","     182774/2000000000: episode: 4931, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 76.523715, mean_q: 36.588700, mean_eps: 0.100000\n","     182812/2000000000: episode: 4932, duration: 4.833s, episode steps:  38, steps per second:   8, episode reward: 70.000, mean reward:  1.842 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.579173, mean_q: 36.524843, mean_eps: 0.100000\n","     182849/2000000000: episode: 4933, duration: 4.866s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 88.054001, mean_q: 36.766037, mean_eps: 0.100000\n","     182889/2000000000: episode: 4934, duration: 5.143s, episode steps:  40, steps per second:   8, episode reward: 22.400, mean reward:  0.560 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.661038, mean_q: 36.467288, mean_eps: 0.100000\n","     182929/2000000000: episode: 4935, duration: 5.101s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.811056, mean_q: 37.199903, mean_eps: 0.100000\n","     182969/2000000000: episode: 4936, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.553773, mean_q: 35.974745, mean_eps: 0.100000\n","     183006/2000000000: episode: 4937, duration: 4.650s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 78.784875, mean_q: 36.155643, mean_eps: 0.100000\n","     183045/2000000000: episode: 4938, duration: 5.118s, episode steps:  39, steps per second:   8, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 78.110510, mean_q: 36.610129, mean_eps: 0.100000\n","     183084/2000000000: episode: 4939, duration: 5.023s, episode steps:  39, steps per second:   8, episode reward: 231.200, mean reward:  5.928 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 78.545752, mean_q: 35.924187, mean_eps: 0.100000\n","     183121/2000000000: episode: 4940, duration: 4.558s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 80.424671, mean_q: 36.743610, mean_eps: 0.100000\n","     183161/2000000000: episode: 4941, duration: 4.992s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 81.998635, mean_q: 36.496711, mean_eps: 0.100000\n","     183195/2000000000: episode: 4942, duration: 4.168s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 68.738243, mean_q: 37.202862, mean_eps: 0.100000\n","     183228/2000000000: episode: 4943, duration: 4.065s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.458239, mean_q: 36.664781, mean_eps: 0.100000\n","     183268/2000000000: episode: 4944, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 72.151756, mean_q: 36.647102, mean_eps: 0.100000\n","     183305/2000000000: episode: 4945, duration: 4.693s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 83.337163, mean_q: 36.240059, mean_eps: 0.100000\n","     183340/2000000000: episode: 4946, duration: 4.341s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 81.700887, mean_q: 36.920163, mean_eps: 0.100000\n","     183375/2000000000: episode: 4947, duration: 4.275s, episode steps:  35, steps per second:   8, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 89.507110, mean_q: 37.694962, mean_eps: 0.100000\n","     183413/2000000000: episode: 4948, duration: 4.661s, episode steps:  38, steps per second:   8, episode reward: 32.300, mean reward:  0.850 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 75.672510, mean_q: 36.547682, mean_eps: 0.100000\n","     183453/2000000000: episode: 4949, duration: 4.814s, episode steps:  40, steps per second:   8, episode reward:  2.700, mean reward:  0.068 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.082117, mean_q: 35.480972, mean_eps: 0.100000\n","     183487/2000000000: episode: 4950, duration: 4.310s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 83.106344, mean_q: 36.679868, mean_eps: 0.100000\n","     183526/2000000000: episode: 4951, duration: 5.102s, episode steps:  39, steps per second:   8, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 80.230545, mean_q: 36.132413, mean_eps: 0.100000\n","     183566/2000000000: episode: 4952, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: -114.000, mean reward: -2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.628126, mean_q: 36.507216, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     183594/2000000000: episode: 4953, duration: 3.642s, episode steps:  28, steps per second:   8, episode reward: 182.300, mean reward:  6.511 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 78.634516, mean_q: 36.483416, mean_eps: 0.100000\n","     183634/2000000000: episode: 4954, duration: 4.950s, episode steps:  40, steps per second:   8, episode reward:  7.300, mean reward:  0.183 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 82.513267, mean_q: 36.286016, mean_eps: 0.100000\n","     183664/2000000000: episode: 4955, duration: 3.994s, episode steps:  30, steps per second:   8, episode reward: -109.500, mean reward: -3.650 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 85.539980, mean_q: 36.982628, mean_eps: 0.100000\n","     183692/2000000000: episode: 4956, duration: 3.680s, episode steps:  28, steps per second:   8, episode reward: -16.600, mean reward: -0.593 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 74.110146, mean_q: 36.529982, mean_eps: 0.100000\n","     183720/2000000000: episode: 4957, duration: 3.774s, episode steps:  28, steps per second:   7, episode reward: 24.300, mean reward:  0.868 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 83.507675, mean_q: 37.065636, mean_eps: 0.100000\n","     183760/2000000000: episode: 4958, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.075 [0.000, 2.000],  loss: 77.305842, mean_q: 37.045534, mean_eps: 0.100000\n","     183791/2000000000: episode: 4959, duration: 3.882s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.398597, mean_q: 36.958862, mean_eps: 0.100000\n","     183827/2000000000: episode: 4960, duration: 4.458s, episode steps:  36, steps per second:   8, episode reward: -142.800, mean reward: -3.967 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.849845, mean_q: 36.401284, mean_eps: 0.100000\n","     183859/2000000000: episode: 4961, duration: 4.223s, episode steps:  32, steps per second:   8, episode reward: 19.500, mean reward:  0.609 [-20.000, 19.500], mean action: 1.219 [0.000, 2.000],  loss: 76.258257, mean_q: 37.163272, mean_eps: 0.100000\n","     183895/2000000000: episode: 4962, duration: 4.860s, episode steps:  36, steps per second:   7, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 82.799966, mean_q: 36.105929, mean_eps: 0.100000\n","     183930/2000000000: episode: 4963, duration: 4.523s, episode steps:  35, steps per second:   8, episode reward: -1.700, mean reward: -0.049 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.273642, mean_q: 36.336904, mean_eps: 0.100000\n","     183970/2000000000: episode: 4964, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.754279, mean_q: 37.564714, mean_eps: 0.100000\n","     184005/2000000000: episode: 4965, duration: 4.573s, episode steps:  35, steps per second:   8, episode reward: -23.500, mean reward: -0.671 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 70.370969, mean_q: 36.832277, mean_eps: 0.100000\n","     184041/2000000000: episode: 4966, duration: 4.673s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 75.216636, mean_q: 36.586474, mean_eps: 0.100000\n","     184081/2000000000: episode: 4967, duration: 5.458s, episode steps:  40, steps per second:   7, episode reward: -45.300, mean reward: -1.132 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.989731, mean_q: 36.060962, mean_eps: 0.100000\n","     184121/2000000000: episode: 4968, duration: 4.803s, episode steps:  40, steps per second:   8, episode reward: 13.600, mean reward:  0.340 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.182925, mean_q: 35.980590, mean_eps: 0.100000\n","     184161/2000000000: episode: 4969, duration: 4.934s, episode steps:  40, steps per second:   8, episode reward: -36.400, mean reward: -0.910 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.938781, mean_q: 36.256090, mean_eps: 0.100000\n","     184192/2000000000: episode: 4970, duration: 3.989s, episode steps:  31, steps per second:   8, episode reward: -82.600, mean reward: -2.665 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 74.671785, mean_q: 35.649799, mean_eps: 0.100000\n","     184232/2000000000: episode: 4971, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: 42.600, mean reward:  1.065 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 79.672692, mean_q: 36.619398, mean_eps: 0.100000\n","     184271/2000000000: episode: 4972, duration: 4.954s, episode steps:  39, steps per second:   8, episode reward: 99.900, mean reward:  2.562 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 83.245520, mean_q: 36.688034, mean_eps: 0.100000\n","     184305/2000000000: episode: 4973, duration: 4.426s, episode steps:  34, steps per second:   8, episode reward: 154.400, mean reward:  4.541 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 82.202499, mean_q: 36.857681, mean_eps: 0.100000\n","     184341/2000000000: episode: 4974, duration: 4.685s, episode steps:  36, steps per second:   8, episode reward: 87.900, mean reward:  2.442 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 84.145840, mean_q: 36.077701, mean_eps: 0.100000\n","     184381/2000000000: episode: 4975, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: 68.600, mean reward:  1.715 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.469701, mean_q: 36.856637, mean_eps: 0.100000\n","     184421/2000000000: episode: 4976, duration: 5.101s, episode steps:  40, steps per second:   8, episode reward: 123.700, mean reward:  3.092 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.978979, mean_q: 36.687389, mean_eps: 0.100000\n","     184460/2000000000: episode: 4977, duration: 5.214s, episode steps:  39, steps per second:   7, episode reward: -22.900, mean reward: -0.587 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 75.813646, mean_q: 36.635467, mean_eps: 0.100000\n","     184491/2000000000: episode: 4978, duration: 4.041s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 86.638670, mean_q: 37.145286, mean_eps: 0.100000\n","     184531/2000000000: episode: 4979, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.303773, mean_q: 37.342579, mean_eps: 0.100000\n","     184571/2000000000: episode: 4980, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: 14.400, mean reward:  0.360 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.440157, mean_q: 37.124851, mean_eps: 0.100000\n","     184605/2000000000: episode: 4981, duration: 4.471s, episode steps:  34, steps per second:   8, episode reward: 100.600, mean reward:  2.959 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 73.414180, mean_q: 35.608341, mean_eps: 0.100000\n","     184640/2000000000: episode: 4982, duration: 4.174s, episode steps:  35, steps per second:   8, episode reward: -80.700, mean reward: -2.306 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 72.962288, mean_q: 36.529736, mean_eps: 0.100000\n","     184680/2000000000: episode: 4983, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: 117.400, mean reward:  2.935 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.344841, mean_q: 35.761135, mean_eps: 0.100000\n","     184718/2000000000: episode: 4984, duration: 4.634s, episode steps:  38, steps per second:   8, episode reward: 134.200, mean reward:  3.532 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 83.702850, mean_q: 36.849088, mean_eps: 0.100000\n","     184754/2000000000: episode: 4985, duration: 4.613s, episode steps:  36, steps per second:   8, episode reward: 89.300, mean reward:  2.481 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 85.901917, mean_q: 36.741838, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     184794/2000000000: episode: 4986, duration: 5.143s, episode steps:  40, steps per second:   8, episode reward: -32.300, mean reward: -0.808 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.404571, mean_q: 35.953908, mean_eps: 0.100000\n","     184827/2000000000: episode: 4987, duration: 4.361s, episode steps:  33, steps per second:   8, episode reward: 133.400, mean reward:  4.042 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.454021, mean_q: 37.092232, mean_eps: 0.100000\n","     184855/2000000000: episode: 4988, duration: 3.654s, episode steps:  28, steps per second:   8, episode reward: -61.400, mean reward: -2.193 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 85.079990, mean_q: 36.074681, mean_eps: 0.100000\n","     184895/2000000000: episode: 4989, duration: 5.383s, episode steps:  40, steps per second:   7, episode reward: 147.400, mean reward:  3.685 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 71.282071, mean_q: 36.928072, mean_eps: 0.100000\n","     184935/2000000000: episode: 4990, duration: 5.075s, episode steps:  40, steps per second:   8, episode reward: 58.300, mean reward:  1.457 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 86.312130, mean_q: 36.257648, mean_eps: 0.100000\n","     184975/2000000000: episode: 4991, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: -141.700, mean reward: -3.542 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.863215, mean_q: 37.163903, mean_eps: 0.100000\n","     185000/2000000000: episode: 4992, duration: 3.314s, episode steps:  25, steps per second:   8, episode reward: 197.100, mean reward:  7.884 [-20.000, 18.000], mean action: 0.960 [0.000, 2.000],  loss: 79.659545, mean_q: 36.670484, mean_eps: 0.100000\n","     185035/2000000000: episode: 4993, duration: 4.803s, episode steps:  35, steps per second:   7, episode reward: 93.200, mean reward:  2.663 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 77.065639, mean_q: 35.389484, mean_eps: 0.100000\n","     185073/2000000000: episode: 4994, duration: 5.105s, episode steps:  38, steps per second:   7, episode reward: -39.700, mean reward: -1.045 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 71.928143, mean_q: 36.833279, mean_eps: 0.100000\n","     185103/2000000000: episode: 4995, duration: 4.320s, episode steps:  30, steps per second:   7, episode reward: 114.800, mean reward:  3.827 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 80.088458, mean_q: 37.141448, mean_eps: 0.100000\n","     185138/2000000000: episode: 4996, duration: 4.989s, episode steps:  35, steps per second:   7, episode reward: 208.000, mean reward:  5.943 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 77.425319, mean_q: 35.805638, mean_eps: 0.100000\n","     185178/2000000000: episode: 4997, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: 50.100, mean reward:  1.252 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.284679, mean_q: 37.728661, mean_eps: 0.100000\n","     185215/2000000000: episode: 4998, duration: 4.591s, episode steps:  37, steps per second:   8, episode reward: 127.300, mean reward:  3.441 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 82.176148, mean_q: 36.533596, mean_eps: 0.100000\n","     185251/2000000000: episode: 4999, duration: 4.620s, episode steps:  36, steps per second:   8, episode reward: -48.800, mean reward: -1.356 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.781158, mean_q: 36.115190, mean_eps: 0.100000\n","     185289/2000000000: episode: 5000, duration: 4.741s, episode steps:  38, steps per second:   8, episode reward: -53.200, mean reward: -1.400 [-20.000, 18.000], mean action: 1.026 [0.000, 2.000],  loss: 74.109683, mean_q: 36.367419, mean_eps: 0.100000\n","     185324/2000000000: episode: 5001, duration: 4.216s, episode steps:  35, steps per second:   8, episode reward: 179.100, mean reward:  5.117 [-20.000, 18.000], mean action: 1.314 [0.000, 2.000],  loss: 79.311444, mean_q: 36.658540, mean_eps: 0.100000\n","     185358/2000000000: episode: 5002, duration: 4.411s, episode steps:  34, steps per second:   8, episode reward: 168.300, mean reward:  4.950 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 78.416482, mean_q: 36.525623, mean_eps: 0.100000\n","     185398/2000000000: episode: 5003, duration: 4.993s, episode steps:  40, steps per second:   8, episode reward: -95.400, mean reward: -2.385 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.999113, mean_q: 37.305238, mean_eps: 0.100000\n","     185429/2000000000: episode: 5004, duration: 4.042s, episode steps:  31, steps per second:   8, episode reward: 208.000, mean reward:  6.710 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.907357, mean_q: 36.615003, mean_eps: 0.100000\n","     185469/2000000000: episode: 5005, duration: 5.169s, episode steps:  40, steps per second:   8, episode reward: -59.400, mean reward: -1.485 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 68.829177, mean_q: 37.124920, mean_eps: 0.100000\n","     185506/2000000000: episode: 5006, duration: 4.705s, episode steps:  37, steps per second:   8, episode reward: -8.500, mean reward: -0.230 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 86.371957, mean_q: 36.156774, mean_eps: 0.100000\n","     185542/2000000000: episode: 5007, duration: 4.358s, episode steps:  36, steps per second:   8, episode reward: 133.400, mean reward:  3.706 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 79.887262, mean_q: 36.981811, mean_eps: 0.100000\n","     185570/2000000000: episode: 5008, duration: 3.421s, episode steps:  28, steps per second:   8, episode reward: 18.900, mean reward:  0.675 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 77.099931, mean_q: 37.329176, mean_eps: 0.100000\n","     185604/2000000000: episode: 5009, duration: 4.276s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.464575, mean_q: 37.032547, mean_eps: 0.100000\n","     185644/2000000000: episode: 5010, duration: 5.018s, episode steps:  40, steps per second:   8, episode reward: -44.000, mean reward: -1.100 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.632232, mean_q: 36.354793, mean_eps: 0.100000\n","     185678/2000000000: episode: 5011, duration: 4.298s, episode steps:  34, steps per second:   8, episode reward: -38.000, mean reward: -1.118 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.847219, mean_q: 36.485044, mean_eps: 0.100000\n","     185716/2000000000: episode: 5012, duration: 4.623s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.789483, mean_q: 37.105238, mean_eps: 0.100000\n","     185747/2000000000: episode: 5013, duration: 3.800s, episode steps:  31, steps per second:   8, episode reward: 106.500, mean reward:  3.435 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 76.292810, mean_q: 36.575112, mean_eps: 0.100000\n","     185786/2000000000: episode: 5014, duration: 4.692s, episode steps:  39, steps per second:   8, episode reward: 110.500, mean reward:  2.833 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 76.205552, mean_q: 36.428128, mean_eps: 0.100000\n","     185813/2000000000: episode: 5015, duration: 3.374s, episode steps:  27, steps per second:   8, episode reward: 208.000, mean reward:  7.704 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 75.100124, mean_q: 37.864157, mean_eps: 0.100000\n","     185853/2000000000: episode: 5016, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.913264, mean_q: 36.115453, mean_eps: 0.100000\n","     185893/2000000000: episode: 5017, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: 164.200, mean reward:  4.105 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.928872, mean_q: 36.512812, mean_eps: 0.100000\n","     185924/2000000000: episode: 5018, duration: 4.053s, episode steps:  31, steps per second:   8, episode reward: -6.500, mean reward: -0.210 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 76.679551, mean_q: 36.307642, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     185964/2000000000: episode: 5019, duration: 5.134s, episode steps:  40, steps per second:   8, episode reward: -72.000, mean reward: -1.800 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 82.705070, mean_q: 37.385577, mean_eps: 0.100000\n","     186002/2000000000: episode: 5020, duration: 4.701s, episode steps:  38, steps per second:   8, episode reward: 89.000, mean reward:  2.342 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 77.169357, mean_q: 37.238918, mean_eps: 0.100000\n","     186037/2000000000: episode: 5021, duration: 4.235s, episode steps:  35, steps per second:   8, episode reward: 100.300, mean reward:  2.866 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 72.877151, mean_q: 38.234941, mean_eps: 0.100000\n","     186068/2000000000: episode: 5022, duration: 3.781s, episode steps:  31, steps per second:   8, episode reward: -53.600, mean reward: -1.729 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 78.477767, mean_q: 36.215449, mean_eps: 0.100000\n","     186108/2000000000: episode: 5023, duration: 4.772s, episode steps:  40, steps per second:   8, episode reward: -44.400, mean reward: -1.110 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 81.046030, mean_q: 37.554222, mean_eps: 0.100000\n","     186148/2000000000: episode: 5024, duration: 4.817s, episode steps:  40, steps per second:   8, episode reward: 25.600, mean reward:  0.640 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.192401, mean_q: 36.599755, mean_eps: 0.100000\n","     186188/2000000000: episode: 5025, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 19.600], mean action: 1.325 [0.000, 2.000],  loss: 78.209069, mean_q: 36.565028, mean_eps: 0.100000\n","     186220/2000000000: episode: 5026, duration: 4.524s, episode steps:  32, steps per second:   7, episode reward: -4.600, mean reward: -0.144 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 77.354345, mean_q: 36.517817, mean_eps: 0.100000\n","     186260/2000000000: episode: 5027, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 76.100, mean reward:  1.903 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.685566, mean_q: 36.832437, mean_eps: 0.100000\n","     186299/2000000000: episode: 5028, duration: 5.058s, episode steps:  39, steps per second:   8, episode reward: 18.400, mean reward:  0.472 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 76.883641, mean_q: 36.527713, mean_eps: 0.100000\n","     186321/2000000000: episode: 5029, duration: 2.905s, episode steps:  22, steps per second:   8, episode reward: 72.400, mean reward:  3.291 [-20.000, 18.000], mean action: 0.727 [0.000, 2.000],  loss: 87.154346, mean_q: 37.849584, mean_eps: 0.100000\n","     186354/2000000000: episode: 5030, duration: 4.069s, episode steps:  33, steps per second:   8, episode reward: -16.200, mean reward: -0.491 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 76.683682, mean_q: 37.071456, mean_eps: 0.100000\n","     186383/2000000000: episode: 5031, duration: 3.686s, episode steps:  29, steps per second:   8, episode reward: -54.200, mean reward: -1.869 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 76.421401, mean_q: 37.351133, mean_eps: 0.100000\n","     186423/2000000000: episode: 5032, duration: 5.090s, episode steps:  40, steps per second:   8, episode reward: 15.500, mean reward:  0.387 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.936960, mean_q: 36.824297, mean_eps: 0.100000\n","     186459/2000000000: episode: 5033, duration: 4.569s, episode steps:  36, steps per second:   8, episode reward: 40.200, mean reward:  1.117 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 76.364914, mean_q: 36.053288, mean_eps: 0.100000\n","     186485/2000000000: episode: 5034, duration: 3.534s, episode steps:  26, steps per second:   7, episode reward: 33.000, mean reward:  1.269 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 82.214046, mean_q: 36.635595, mean_eps: 0.100000\n","     186525/2000000000: episode: 5035, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: -71.400, mean reward: -1.785 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.582893, mean_q: 37.324945, mean_eps: 0.100000\n","     186562/2000000000: episode: 5036, duration: 4.716s, episode steps:  37, steps per second:   8, episode reward:  0.500, mean reward:  0.014 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 77.705649, mean_q: 35.861286, mean_eps: 0.100000\n","     186599/2000000000: episode: 5037, duration: 4.477s, episode steps:  37, steps per second:   8, episode reward: 165.200, mean reward:  4.465 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 75.539302, mean_q: 37.023925, mean_eps: 0.100000\n","     186639/2000000000: episode: 5038, duration: 4.947s, episode steps:  40, steps per second:   8, episode reward: 25.300, mean reward:  0.632 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.109195, mean_q: 36.663365, mean_eps: 0.100000\n","     186674/2000000000: episode: 5039, duration: 4.245s, episode steps:  35, steps per second:   8, episode reward: -53.300, mean reward: -1.523 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 73.725435, mean_q: 36.432837, mean_eps: 0.100000\n","     186707/2000000000: episode: 5040, duration: 4.382s, episode steps:  33, steps per second:   8, episode reward: -105.800, mean reward: -3.206 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 79.750363, mean_q: 36.748552, mean_eps: 0.100000\n","     186740/2000000000: episode: 5041, duration: 4.096s, episode steps:  33, steps per second:   8, episode reward: 188.800, mean reward:  5.721 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 72.989741, mean_q: 36.530454, mean_eps: 0.100000\n","     186778/2000000000: episode: 5042, duration: 4.788s, episode steps:  38, steps per second:   8, episode reward: -58.500, mean reward: -1.539 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 70.025535, mean_q: 36.479357, mean_eps: 0.100000\n","     186816/2000000000: episode: 5043, duration: 4.629s, episode steps:  38, steps per second:   8, episode reward: 63.500, mean reward:  1.671 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 77.955184, mean_q: 37.046147, mean_eps: 0.100000\n","     186845/2000000000: episode: 5044, duration: 3.677s, episode steps:  29, steps per second:   8, episode reward: 131.500, mean reward:  4.534 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 73.455788, mean_q: 37.369135, mean_eps: 0.100000\n","     186878/2000000000: episode: 5045, duration: 4.142s, episode steps:  33, steps per second:   8, episode reward: 184.400, mean reward:  5.588 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 72.059180, mean_q: 37.083918, mean_eps: 0.100000\n","     186914/2000000000: episode: 5046, duration: 4.651s, episode steps:  36, steps per second:   8, episode reward: 79.000, mean reward:  2.194 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 80.945688, mean_q: 35.916832, mean_eps: 0.100000\n","     186951/2000000000: episode: 5047, duration: 4.691s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 81.195327, mean_q: 36.453321, mean_eps: 0.100000\n","     186984/2000000000: episode: 5048, duration: 4.177s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 75.727393, mean_q: 36.186784, mean_eps: 0.100000\n","     187018/2000000000: episode: 5049, duration: 4.413s, episode steps:  34, steps per second:   8, episode reward: -39.900, mean reward: -1.174 [-20.000, 18.000], mean action: 0.912 [0.000, 2.000],  loss: 85.732448, mean_q: 36.347413, mean_eps: 0.100000\n","     187054/2000000000: episode: 5050, duration: 4.908s, episode steps:  36, steps per second:   7, episode reward: -6.100, mean reward: -0.169 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 75.499548, mean_q: 35.568274, mean_eps: 0.100000\n","     187093/2000000000: episode: 5051, duration: 5.106s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 79.042815, mean_q: 36.468731, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     187133/2000000000: episode: 5052, duration: 5.184s, episode steps:  40, steps per second:   8, episode reward: -0.400, mean reward: -0.010 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 85.181281, mean_q: 36.799456, mean_eps: 0.100000\n","     187170/2000000000: episode: 5053, duration: 4.692s, episode steps:  37, steps per second:   8, episode reward: 62.200, mean reward:  1.681 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 77.288674, mean_q: 36.651524, mean_eps: 0.100000\n","     187210/2000000000: episode: 5054, duration: 4.893s, episode steps:  40, steps per second:   8, episode reward: 42.000, mean reward:  1.050 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 71.735588, mean_q: 36.405326, mean_eps: 0.100000\n","     187247/2000000000: episode: 5055, duration: 4.540s, episode steps:  37, steps per second:   8, episode reward: 60.900, mean reward:  1.646 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 79.056304, mean_q: 37.106915, mean_eps: 0.100000\n","     187287/2000000000: episode: 5056, duration: 5.072s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.097952, mean_q: 36.827991, mean_eps: 0.100000\n","     187318/2000000000: episode: 5057, duration: 4.128s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.381657, mean_q: 36.248252, mean_eps: 0.100000\n","     187356/2000000000: episode: 5058, duration: 5.170s, episode steps:  38, steps per second:   7, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 73.431587, mean_q: 36.865849, mean_eps: 0.100000\n","     187396/2000000000: episode: 5059, duration: 5.422s, episode steps:  40, steps per second:   7, episode reward: -65.400, mean reward: -1.635 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.036124, mean_q: 36.889219, mean_eps: 0.100000\n","     187436/2000000000: episode: 5060, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: 125.200, mean reward:  3.130 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.524636, mean_q: 36.867937, mean_eps: 0.100000\n","     187471/2000000000: episode: 5061, duration: 4.581s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 82.193481, mean_q: 36.623201, mean_eps: 0.100000\n","     187511/2000000000: episode: 5062, duration: 4.892s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.346333, mean_q: 37.165468, mean_eps: 0.100000\n","     187551/2000000000: episode: 5063, duration: 4.938s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 75.554893, mean_q: 36.703444, mean_eps: 0.100000\n","     187582/2000000000: episode: 5064, duration: 3.982s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.756842, mean_q: 36.573970, mean_eps: 0.100000\n","     187619/2000000000: episode: 5065, duration: 4.804s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 72.049657, mean_q: 35.986330, mean_eps: 0.100000\n","     187652/2000000000: episode: 5066, duration: 4.217s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 73.698327, mean_q: 38.086788, mean_eps: 0.100000\n","     187684/2000000000: episode: 5067, duration: 4.143s, episode steps:  32, steps per second:   8, episode reward: 116.900, mean reward:  3.653 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 74.670321, mean_q: 37.459856, mean_eps: 0.100000\n","     187724/2000000000: episode: 5068, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.609446, mean_q: 37.020840, mean_eps: 0.100000\n","     187762/2000000000: episode: 5069, duration: 4.903s, episode steps:  38, steps per second:   8, episode reward: 76.700, mean reward:  2.018 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 79.659741, mean_q: 36.493258, mean_eps: 0.100000\n","     187801/2000000000: episode: 5070, duration: 5.016s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 73.071446, mean_q: 37.476008, mean_eps: 0.100000\n","     187836/2000000000: episode: 5071, duration: 4.301s, episode steps:  35, steps per second:   8, episode reward: 101.000, mean reward:  2.886 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 72.191227, mean_q: 37.355524, mean_eps: 0.100000\n","     187876/2000000000: episode: 5072, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward: -1.900, mean reward: -0.048 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.545233, mean_q: 36.833870, mean_eps: 0.100000\n","     187909/2000000000: episode: 5073, duration: 4.280s, episode steps:  33, steps per second:   8, episode reward: 23.100, mean reward:  0.700 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 71.767518, mean_q: 36.577365, mean_eps: 0.100000\n","     187946/2000000000: episode: 5074, duration: 4.557s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 78.520573, mean_q: 36.122857, mean_eps: 0.100000\n","     187986/2000000000: episode: 5075, duration: 5.210s, episode steps:  40, steps per second:   8, episode reward: -34.100, mean reward: -0.853 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.120552, mean_q: 37.099414, mean_eps: 0.100000\n","     188020/2000000000: episode: 5076, duration: 4.450s, episode steps:  34, steps per second:   8, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 80.218983, mean_q: 36.288683, mean_eps: 0.100000\n","     188045/2000000000: episode: 5077, duration: 3.252s, episode steps:  25, steps per second:   8, episode reward:  1.900, mean reward:  0.076 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 81.299850, mean_q: 37.264063, mean_eps: 0.100000\n","     188078/2000000000: episode: 5078, duration: 4.311s, episode steps:  33, steps per second:   8, episode reward: 152.300, mean reward:  4.615 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 78.962380, mean_q: 37.373873, mean_eps: 0.100000\n","     188112/2000000000: episode: 5079, duration: 4.506s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 78.585823, mean_q: 36.410304, mean_eps: 0.100000\n","     188152/2000000000: episode: 5080, duration: 5.112s, episode steps:  40, steps per second:   8, episode reward: -12.200, mean reward: -0.305 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.587852, mean_q: 36.866652, mean_eps: 0.100000\n","     188192/2000000000: episode: 5081, duration: 5.120s, episode steps:  40, steps per second:   8, episode reward: 213.900, mean reward:  5.348 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.158694, mean_q: 36.377406, mean_eps: 0.100000\n","     188222/2000000000: episode: 5082, duration: 3.820s, episode steps:  30, steps per second:   8, episode reward: -76.100, mean reward: -2.537 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 75.559039, mean_q: 36.901258, mean_eps: 0.100000\n","     188255/2000000000: episode: 5083, duration: 4.334s, episode steps:  33, steps per second:   8, episode reward: 171.000, mean reward:  5.182 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 74.277299, mean_q: 36.600616, mean_eps: 0.100000\n","     188285/2000000000: episode: 5084, duration: 3.952s, episode steps:  30, steps per second:   8, episode reward: 51.400, mean reward:  1.713 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 78.676787, mean_q: 36.204080, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     188320/2000000000: episode: 5085, duration: 4.679s, episode steps:  35, steps per second:   7, episode reward: 11.500, mean reward:  0.329 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 79.866163, mean_q: 36.871789, mean_eps: 0.100000\n","     188357/2000000000: episode: 5086, duration: 4.723s, episode steps:  37, steps per second:   8, episode reward: 46.200, mean reward:  1.249 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 75.871182, mean_q: 36.426357, mean_eps: 0.100000\n","     188389/2000000000: episode: 5087, duration: 4.064s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.641962, mean_q: 36.869924, mean_eps: 0.100000\n","     188413/2000000000: episode: 5088, duration: 3.062s, episode steps:  24, steps per second:   8, episode reward: 94.000, mean reward:  3.917 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 70.188046, mean_q: 35.745128, mean_eps: 0.100000\n","     188453/2000000000: episode: 5089, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 53.300, mean reward:  1.333 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.490789, mean_q: 36.024084, mean_eps: 0.100000\n","     188485/2000000000: episode: 5090, duration: 4.236s, episode steps:  32, steps per second:   8, episode reward: -26.300, mean reward: -0.822 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.460047, mean_q: 36.527219, mean_eps: 0.100000\n","     188522/2000000000: episode: 5091, duration: 5.043s, episode steps:  37, steps per second:   7, episode reward: 148.800, mean reward:  4.022 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 76.566561, mean_q: 36.710901, mean_eps: 0.100000\n","     188560/2000000000: episode: 5092, duration: 5.003s, episode steps:  38, steps per second:   8, episode reward: 171.600, mean reward:  4.516 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 73.056711, mean_q: 36.404302, mean_eps: 0.100000\n","     188593/2000000000: episode: 5093, duration: 4.047s, episode steps:  33, steps per second:   8, episode reward: -64.700, mean reward: -1.961 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 79.251948, mean_q: 37.007320, mean_eps: 0.100000\n","     188633/2000000000: episode: 5094, duration: 5.047s, episode steps:  40, steps per second:   8, episode reward: 27.100, mean reward:  0.678 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.077816, mean_q: 36.542232, mean_eps: 0.100000\n","     188672/2000000000: episode: 5095, duration: 5.142s, episode steps:  39, steps per second:   8, episode reward: 136.700, mean reward:  3.505 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 78.615702, mean_q: 36.584413, mean_eps: 0.100000\n","     188712/2000000000: episode: 5096, duration: 5.328s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 84.511465, mean_q: 36.719614, mean_eps: 0.100000\n","     188752/2000000000: episode: 5097, duration: 5.395s, episode steps:  40, steps per second:   7, episode reward: -34.100, mean reward: -0.853 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.266196, mean_q: 36.991474, mean_eps: 0.100000\n","     188791/2000000000: episode: 5098, duration: 5.342s, episode steps:  39, steps per second:   7, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 87.860599, mean_q: 37.065593, mean_eps: 0.100000\n","     188831/2000000000: episode: 5099, duration: 5.556s, episode steps:  40, steps per second:   7, episode reward: 114.900, mean reward:  2.872 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.936843, mean_q: 36.534868, mean_eps: 0.100000\n","     188866/2000000000: episode: 5100, duration: 5.255s, episode steps:  35, steps per second:   7, episode reward: 112.100, mean reward:  3.203 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 74.246895, mean_q: 36.861167, mean_eps: 0.100000\n","     188894/2000000000: episode: 5101, duration: 3.810s, episode steps:  28, steps per second:   7, episode reward: 112.100, mean reward:  4.004 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 85.686457, mean_q: 37.092273, mean_eps: 0.100000\n","     188921/2000000000: episode: 5102, duration: 3.493s, episode steps:  27, steps per second:   8, episode reward: 210.700, mean reward:  7.804 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 74.596635, mean_q: 37.233531, mean_eps: 0.100000\n","     188950/2000000000: episode: 5103, duration: 3.801s, episode steps:  29, steps per second:   8, episode reward:  7.600, mean reward:  0.262 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 78.684867, mean_q: 36.212875, mean_eps: 0.100000\n","     188990/2000000000: episode: 5104, duration: 5.214s, episode steps:  40, steps per second:   8, episode reward: 173.200, mean reward:  4.330 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.194597, mean_q: 37.221062, mean_eps: 0.100000\n","     189030/2000000000: episode: 5105, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: 27.600, mean reward:  0.690 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.935018, mean_q: 36.166101, mean_eps: 0.100000\n","     189064/2000000000: episode: 5106, duration: 4.537s, episode steps:  34, steps per second:   7, episode reward: 116.300, mean reward:  3.421 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.196915, mean_q: 36.479813, mean_eps: 0.100000\n","     189092/2000000000: episode: 5107, duration: 3.643s, episode steps:  28, steps per second:   8, episode reward: 88.400, mean reward:  3.157 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.341459, mean_q: 37.047180, mean_eps: 0.100000\n","     189129/2000000000: episode: 5108, duration: 4.939s, episode steps:  37, steps per second:   7, episode reward: 23.800, mean reward:  0.643 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 74.308450, mean_q: 36.354575, mean_eps: 0.100000\n","     189167/2000000000: episode: 5109, duration: 5.013s, episode steps:  38, steps per second:   8, episode reward: -98.400, mean reward: -2.589 [-20.000, 18.400], mean action: 1.263 [0.000, 2.000],  loss: 78.427694, mean_q: 36.342124, mean_eps: 0.100000\n","     189201/2000000000: episode: 5110, duration: 4.258s, episode steps:  34, steps per second:   8, episode reward: -79.900, mean reward: -2.350 [-20.000, 18.900], mean action: 1.118 [0.000, 2.000],  loss: 82.610386, mean_q: 36.598440, mean_eps: 0.100000\n","     189237/2000000000: episode: 5111, duration: 4.449s, episode steps:  36, steps per second:   8, episode reward: 117.000, mean reward:  3.250 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 72.823518, mean_q: 35.966153, mean_eps: 0.100000\n","     189266/2000000000: episode: 5112, duration: 3.644s, episode steps:  29, steps per second:   8, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 73.483777, mean_q: 36.117993, mean_eps: 0.100000\n","     189306/2000000000: episode: 5113, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: -165.700, mean reward: -4.143 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.942077, mean_q: 37.229656, mean_eps: 0.100000\n","     189344/2000000000: episode: 5114, duration: 4.934s, episode steps:  38, steps per second:   8, episode reward:  9.400, mean reward:  0.247 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.325406, mean_q: 37.063650, mean_eps: 0.100000\n","     189373/2000000000: episode: 5115, duration: 3.831s, episode steps:  29, steps per second:   8, episode reward: 166.500, mean reward:  5.741 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 81.977097, mean_q: 36.898162, mean_eps: 0.100000\n","     189400/2000000000: episode: 5116, duration: 3.523s, episode steps:  27, steps per second:   8, episode reward: 55.400, mean reward:  2.052 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 77.172407, mean_q: 37.061113, mean_eps: 0.100000\n","     189424/2000000000: episode: 5117, duration: 3.119s, episode steps:  24, steps per second:   8, episode reward: 18.600, mean reward:  0.775 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 81.310548, mean_q: 36.942284, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     189464/2000000000: episode: 5118, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: 70.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.222046, mean_q: 36.450686, mean_eps: 0.100000\n","     189497/2000000000: episode: 5119, duration: 4.328s, episode steps:  33, steps per second:   8, episode reward: -46.500, mean reward: -1.409 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 75.567410, mean_q: 37.175677, mean_eps: 0.100000\n","     189537/2000000000: episode: 5120, duration: 4.970s, episode steps:  40, steps per second:   8, episode reward: 132.500, mean reward:  3.313 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 80.060799, mean_q: 36.141359, mean_eps: 0.100000\n","     189577/2000000000: episode: 5121, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: -12.800, mean reward: -0.320 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.490982, mean_q: 37.360509, mean_eps: 0.100000\n","     189617/2000000000: episode: 5122, duration: 5.028s, episode steps:  40, steps per second:   8, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 69.494147, mean_q: 37.263522, mean_eps: 0.100000\n","     189649/2000000000: episode: 5123, duration: 4.149s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 78.503184, mean_q: 36.028431, mean_eps: 0.100000\n","     189689/2000000000: episode: 5124, duration: 5.175s, episode steps:  40, steps per second:   8, episode reward: 44.000, mean reward:  1.100 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 75.820104, mean_q: 36.396454, mean_eps: 0.100000\n","     189727/2000000000: episode: 5125, duration: 4.851s, episode steps:  38, steps per second:   8, episode reward: -19.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 86.043458, mean_q: 36.847590, mean_eps: 0.100000\n","     189760/2000000000: episode: 5126, duration: 4.264s, episode steps:  33, steps per second:   8, episode reward: 118.400, mean reward:  3.588 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.694803, mean_q: 37.792477, mean_eps: 0.100000\n","     189791/2000000000: episode: 5127, duration: 3.950s, episode steps:  31, steps per second:   8, episode reward: 49.800, mean reward:  1.606 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 72.522997, mean_q: 37.081930, mean_eps: 0.100000\n","     189818/2000000000: episode: 5128, duration: 3.495s, episode steps:  27, steps per second:   8, episode reward:  7.700, mean reward:  0.285 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 79.833195, mean_q: 37.570968, mean_eps: 0.100000\n","     189855/2000000000: episode: 5129, duration: 4.821s, episode steps:  37, steps per second:   8, episode reward: 149.100, mean reward:  4.030 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 78.682209, mean_q: 36.977338, mean_eps: 0.100000\n","     189891/2000000000: episode: 5130, duration: 4.438s, episode steps:  36, steps per second:   8, episode reward: -188.700, mean reward: -5.242 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 76.809501, mean_q: 37.602876, mean_eps: 0.100000\n","     189919/2000000000: episode: 5131, duration: 3.628s, episode steps:  28, steps per second:   8, episode reward: 86.100, mean reward:  3.075 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 82.691663, mean_q: 36.455043, mean_eps: 0.100000\n","     189956/2000000000: episode: 5132, duration: 4.937s, episode steps:  37, steps per second:   7, episode reward: 83.600, mean reward:  2.259 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 80.168831, mean_q: 36.951398, mean_eps: 0.100000\n","     189982/2000000000: episode: 5133, duration: 3.400s, episode steps:  26, steps per second:   8, episode reward: 290.100, mean reward: 11.158 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 82.041504, mean_q: 37.643433, mean_eps: 0.100000\n","     190021/2000000000: episode: 5134, duration: 4.957s, episode steps:  39, steps per second:   8, episode reward: 78.100, mean reward:  2.003 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 83.064613, mean_q: 37.190731, mean_eps: 0.100000\n","     190052/2000000000: episode: 5135, duration: 3.915s, episode steps:  31, steps per second:   8, episode reward: 65.500, mean reward:  2.113 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 87.027428, mean_q: 38.352217, mean_eps: 0.100000\n","     190080/2000000000: episode: 5136, duration: 3.511s, episode steps:  28, steps per second:   8, episode reward: -90.000, mean reward: -3.214 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 92.605564, mean_q: 37.798887, mean_eps: 0.100000\n","     190116/2000000000: episode: 5137, duration: 4.584s, episode steps:  36, steps per second:   8, episode reward: 136.600, mean reward:  3.794 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 90.026298, mean_q: 38.694098, mean_eps: 0.100000\n","     190154/2000000000: episode: 5138, duration: 4.940s, episode steps:  38, steps per second:   8, episode reward: -9.100, mean reward: -0.239 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 81.653841, mean_q: 38.388567, mean_eps: 0.100000\n","     190189/2000000000: episode: 5139, duration: 4.842s, episode steps:  35, steps per second:   7, episode reward: 124.700, mean reward:  3.563 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 80.092770, mean_q: 38.774454, mean_eps: 0.100000\n","     190221/2000000000: episode: 5140, duration: 4.337s, episode steps:  32, steps per second:   7, episode reward: -50.500, mean reward: -1.578 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 84.372348, mean_q: 38.840279, mean_eps: 0.100000\n","     190254/2000000000: episode: 5141, duration: 4.418s, episode steps:  33, steps per second:   7, episode reward: -90.500, mean reward: -2.742 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 84.487791, mean_q: 37.616622, mean_eps: 0.100000\n","     190287/2000000000: episode: 5142, duration: 4.395s, episode steps:  33, steps per second:   8, episode reward: -85.400, mean reward: -2.588 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.137420, mean_q: 38.222146, mean_eps: 0.100000\n","     190325/2000000000: episode: 5143, duration: 4.793s, episode steps:  38, steps per second:   8, episode reward: 65.300, mean reward:  1.718 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 83.352346, mean_q: 38.205838, mean_eps: 0.100000\n","     190357/2000000000: episode: 5144, duration: 4.061s, episode steps:  32, steps per second:   8, episode reward: -13.800, mean reward: -0.431 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 75.561553, mean_q: 38.207717, mean_eps: 0.100000\n","     190397/2000000000: episode: 5145, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.233766, mean_q: 37.990973, mean_eps: 0.100000\n","     190425/2000000000: episode: 5146, duration: 3.609s, episode steps:  28, steps per second:   8, episode reward: 16.500, mean reward:  0.589 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 84.263951, mean_q: 38.900490, mean_eps: 0.100000\n","     190462/2000000000: episode: 5147, duration: 4.826s, episode steps:  37, steps per second:   8, episode reward: 140.100, mean reward:  3.786 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 80.109547, mean_q: 37.923636, mean_eps: 0.100000\n","     190502/2000000000: episode: 5148, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: 70.400, mean reward:  1.760 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.046281, mean_q: 38.385512, mean_eps: 0.100000\n","     190532/2000000000: episode: 5149, duration: 4.009s, episode steps:  30, steps per second:   7, episode reward: 36.900, mean reward:  1.230 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 82.461399, mean_q: 37.977943, mean_eps: 0.100000\n","     190571/2000000000: episode: 5150, duration: 5.141s, episode steps:  39, steps per second:   8, episode reward: -32.300, mean reward: -0.828 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 80.279894, mean_q: 39.216505, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     190600/2000000000: episode: 5151, duration: 3.925s, episode steps:  29, steps per second:   7, episode reward: -10.500, mean reward: -0.362 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 86.999785, mean_q: 38.250439, mean_eps: 0.100000\n","     190635/2000000000: episode: 5152, duration: 4.614s, episode steps:  35, steps per second:   8, episode reward: 105.900, mean reward:  3.026 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 79.591378, mean_q: 38.358717, mean_eps: 0.100000\n","     190675/2000000000: episode: 5153, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward: 211.500, mean reward:  5.287 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 82.514044, mean_q: 38.592818, mean_eps: 0.100000\n","     190706/2000000000: episode: 5154, duration: 3.961s, episode steps:  31, steps per second:   8, episode reward: 191.500, mean reward:  6.177 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 77.035858, mean_q: 38.712554, mean_eps: 0.100000\n","     190746/2000000000: episode: 5155, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 224.000, mean reward:  5.600 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 91.267297, mean_q: 38.407394, mean_eps: 0.100000\n","     190780/2000000000: episode: 5156, duration: 4.279s, episode steps:  34, steps per second:   8, episode reward: 121.100, mean reward:  3.562 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 78.072777, mean_q: 37.815413, mean_eps: 0.100000\n","     190820/2000000000: episode: 5157, duration: 5.082s, episode steps:  40, steps per second:   8, episode reward: 44.300, mean reward:  1.107 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.871636, mean_q: 38.136385, mean_eps: 0.100000\n","     190852/2000000000: episode: 5158, duration: 4.114s, episode steps:  32, steps per second:   8, episode reward: 29.700, mean reward:  0.928 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.005649, mean_q: 39.711015, mean_eps: 0.100000\n","     190889/2000000000: episode: 5159, duration: 4.910s, episode steps:  37, steps per second:   8, episode reward: -9.500, mean reward: -0.257 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 79.096691, mean_q: 38.336070, mean_eps: 0.100000\n","     190929/2000000000: episode: 5160, duration: 5.501s, episode steps:  40, steps per second:   7, episode reward: -1.600, mean reward: -0.040 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 84.865271, mean_q: 38.276928, mean_eps: 0.100000\n","     190969/2000000000: episode: 5161, duration: 5.332s, episode steps:  40, steps per second:   8, episode reward: 55.500, mean reward:  1.387 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.638273, mean_q: 37.927447, mean_eps: 0.100000\n","     191009/2000000000: episode: 5162, duration: 5.409s, episode steps:  40, steps per second:   7, episode reward: 61.300, mean reward:  1.533 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 82.944034, mean_q: 38.252131, mean_eps: 0.100000\n","     191043/2000000000: episode: 5163, duration: 4.322s, episode steps:  34, steps per second:   8, episode reward: 141.500, mean reward:  4.162 [-20.000, 18.400], mean action: 1.029 [0.000, 2.000],  loss: 76.793586, mean_q: 37.775886, mean_eps: 0.100000\n","     191077/2000000000: episode: 5164, duration: 4.250s, episode steps:  34, steps per second:   8, episode reward: 28.200, mean reward:  0.829 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 85.334312, mean_q: 38.772211, mean_eps: 0.100000\n","     191117/2000000000: episode: 5165, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: 118.800, mean reward:  2.970 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.224193, mean_q: 37.895121, mean_eps: 0.100000\n","     191157/2000000000: episode: 5166, duration: 4.793s, episode steps:  40, steps per second:   8, episode reward: -20.100, mean reward: -0.503 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 77.571533, mean_q: 38.821248, mean_eps: 0.100000\n","     191197/2000000000: episode: 5167, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 77.271790, mean_q: 38.286023, mean_eps: 0.100000\n","     191237/2000000000: episode: 5168, duration: 5.062s, episode steps:  40, steps per second:   8, episode reward: 76.300, mean reward:  1.908 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 87.170687, mean_q: 37.698664, mean_eps: 0.100000\n","     191271/2000000000: episode: 5169, duration: 4.384s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 77.117106, mean_q: 38.300503, mean_eps: 0.100000\n","     191311/2000000000: episode: 5170, duration: 5.192s, episode steps:  40, steps per second:   8, episode reward:  2.200, mean reward:  0.055 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 74.982955, mean_q: 37.972004, mean_eps: 0.100000\n","     191351/2000000000: episode: 5171, duration: 5.018s, episode steps:  40, steps per second:   8, episode reward: 85.600, mean reward:  2.140 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.794519, mean_q: 37.622631, mean_eps: 0.100000\n","     191383/2000000000: episode: 5172, duration: 4.009s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.479236, mean_q: 38.064302, mean_eps: 0.100000\n","     191421/2000000000: episode: 5173, duration: 4.907s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 80.200425, mean_q: 38.549257, mean_eps: 0.100000\n","     191461/2000000000: episode: 5174, duration: 4.823s, episode steps:  40, steps per second:   8, episode reward: -109.000, mean reward: -2.725 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 90.751911, mean_q: 37.867773, mean_eps: 0.100000\n","     191496/2000000000: episode: 5175, duration: 4.289s, episode steps:  35, steps per second:   8, episode reward: -6.900, mean reward: -0.197 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.899504, mean_q: 37.861846, mean_eps: 0.100000\n","     191536/2000000000: episode: 5176, duration: 5.252s, episode steps:  40, steps per second:   8, episode reward: 30.400, mean reward:  0.760 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.382261, mean_q: 38.524533, mean_eps: 0.100000\n","     191576/2000000000: episode: 5177, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 89.379839, mean_q: 37.781266, mean_eps: 0.100000\n","     191602/2000000000: episode: 5178, duration: 3.209s, episode steps:  26, steps per second:   8, episode reward: 158.800, mean reward:  6.108 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 79.862088, mean_q: 37.992070, mean_eps: 0.100000\n","     191639/2000000000: episode: 5179, duration: 4.561s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 88.457093, mean_q: 38.216981, mean_eps: 0.100000\n","     191674/2000000000: episode: 5180, duration: 4.538s, episode steps:  35, steps per second:   8, episode reward: -134.000, mean reward: -3.829 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 79.566382, mean_q: 37.608158, mean_eps: 0.100000\n","     191714/2000000000: episode: 5181, duration: 5.109s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.117166, mean_q: 38.455192, mean_eps: 0.100000\n","     191749/2000000000: episode: 5182, duration: 4.505s, episode steps:  35, steps per second:   8, episode reward: -13.000, mean reward: -0.371 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 82.013635, mean_q: 39.027888, mean_eps: 0.100000\n","     191784/2000000000: episode: 5183, duration: 4.542s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 83.949709, mean_q: 37.474520, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     191824/2000000000: episode: 5184, duration: 4.719s, episode steps:  40, steps per second:   8, episode reward: 228.000, mean reward:  5.700 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.495516, mean_q: 38.018053, mean_eps: 0.100000\n","     191860/2000000000: episode: 5185, duration: 4.512s, episode steps:  36, steps per second:   8, episode reward: 42.700, mean reward:  1.186 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 86.136783, mean_q: 38.310008, mean_eps: 0.100000\n","     191894/2000000000: episode: 5186, duration: 4.406s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 81.178281, mean_q: 38.540098, mean_eps: 0.100000\n","     191934/2000000000: episode: 5187, duration: 5.647s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.692063, mean_q: 38.149067, mean_eps: 0.100000\n","     191960/2000000000: episode: 5188, duration: 3.324s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 82.004649, mean_q: 39.129515, mean_eps: 0.100000\n","     191986/2000000000: episode: 5189, duration: 3.209s, episode steps:  26, steps per second:   8, episode reward: -40.400, mean reward: -1.554 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 79.738243, mean_q: 37.795059, mean_eps: 0.100000\n","     192018/2000000000: episode: 5190, duration: 4.016s, episode steps:  32, steps per second:   8, episode reward: 231.600, mean reward:  7.237 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 79.635183, mean_q: 39.101292, mean_eps: 0.100000\n","     192054/2000000000: episode: 5191, duration: 4.382s, episode steps:  36, steps per second:   8, episode reward: 37.200, mean reward:  1.033 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 78.769265, mean_q: 38.832780, mean_eps: 0.100000\n","     192088/2000000000: episode: 5192, duration: 4.166s, episode steps:  34, steps per second:   8, episode reward: -30.700, mean reward: -0.903 [-20.000, 18.000], mean action: 0.941 [0.000, 2.000],  loss: 75.418305, mean_q: 38.604325, mean_eps: 0.100000\n","     192128/2000000000: episode: 5193, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: -71.700, mean reward: -1.793 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 85.800587, mean_q: 38.404383, mean_eps: 0.100000\n","     192168/2000000000: episode: 5194, duration: 5.284s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 82.931759, mean_q: 38.167500, mean_eps: 0.100000\n","     192197/2000000000: episode: 5195, duration: 4.235s, episode steps:  29, steps per second:   7, episode reward: 49.800, mean reward:  1.717 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 77.602456, mean_q: 37.961124, mean_eps: 0.100000\n","     192237/2000000000: episode: 5196, duration: 5.431s, episode steps:  40, steps per second:   7, episode reward: -76.400, mean reward: -1.910 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 73.341271, mean_q: 38.188195, mean_eps: 0.100000\n","     192277/2000000000: episode: 5197, duration: 5.541s, episode steps:  40, steps per second:   7, episode reward: 208.000, mean reward:  5.200 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.293826, mean_q: 38.713927, mean_eps: 0.100000\n","     192315/2000000000: episode: 5198, duration: 5.092s, episode steps:  38, steps per second:   7, episode reward: 155.000, mean reward:  4.079 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 77.971327, mean_q: 37.189036, mean_eps: 0.100000\n","     192348/2000000000: episode: 5199, duration: 4.652s, episode steps:  33, steps per second:   7, episode reward: -92.500, mean reward: -2.803 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 84.202882, mean_q: 38.697518, mean_eps: 0.100000\n","     192383/2000000000: episode: 5200, duration: 4.897s, episode steps:  35, steps per second:   7, episode reward: -63.300, mean reward: -1.809 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.770321, mean_q: 38.821536, mean_eps: 0.100000\n","     192411/2000000000: episode: 5201, duration: 3.816s, episode steps:  28, steps per second:   7, episode reward: 21.300, mean reward:  0.761 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 86.435225, mean_q: 38.281309, mean_eps: 0.100000\n","     192447/2000000000: episode: 5202, duration: 4.940s, episode steps:  36, steps per second:   7, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 77.191102, mean_q: 37.886629, mean_eps: 0.100000\n","     192487/2000000000: episode: 5203, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: -55.800, mean reward: -1.395 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 88.191669, mean_q: 37.364444, mean_eps: 0.100000\n","     192527/2000000000: episode: 5204, duration: 5.362s, episode steps:  40, steps per second:   7, episode reward: -127.500, mean reward: -3.187 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 89.091866, mean_q: 37.506670, mean_eps: 0.100000\n","     192567/2000000000: episode: 5205, duration: 5.372s, episode steps:  40, steps per second:   7, episode reward: -12.700, mean reward: -0.317 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 72.673107, mean_q: 38.709831, mean_eps: 0.100000\n","     192607/2000000000: episode: 5206, duration: 5.251s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.723360, mean_q: 38.151086, mean_eps: 0.100000\n","     192647/2000000000: episode: 5207, duration: 5.193s, episode steps:  40, steps per second:   8, episode reward: -200.300, mean reward: -5.008 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.204646, mean_q: 38.706578, mean_eps: 0.100000\n","     192676/2000000000: episode: 5208, duration: 3.912s, episode steps:  29, steps per second:   7, episode reward: 12.700, mean reward:  0.438 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 82.882664, mean_q: 38.805182, mean_eps: 0.100000\n","     192712/2000000000: episode: 5209, duration: 4.925s, episode steps:  36, steps per second:   7, episode reward: 90.900, mean reward:  2.525 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.138615, mean_q: 38.330098, mean_eps: 0.100000\n","     192742/2000000000: episode: 5210, duration: 4.037s, episode steps:  30, steps per second:   7, episode reward: 32.900, mean reward:  1.097 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 78.353111, mean_q: 37.222393, mean_eps: 0.100000\n","     192782/2000000000: episode: 5211, duration: 5.318s, episode steps:  40, steps per second:   8, episode reward: 35.400, mean reward:  0.885 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.941049, mean_q: 38.989267, mean_eps: 0.100000\n","     192815/2000000000: episode: 5212, duration: 5.177s, episode steps:  33, steps per second:   6, episode reward: 64.700, mean reward:  1.961 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.648227, mean_q: 38.111137, mean_eps: 0.100000\n","     192852/2000000000: episode: 5213, duration: 5.750s, episode steps:  37, steps per second:   6, episode reward: 279.600, mean reward:  7.557 [-12.500, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 80.129844, mean_q: 38.525890, mean_eps: 0.100000\n","     192892/2000000000: episode: 5214, duration: 6.002s, episode steps:  40, steps per second:   7, episode reward: 76.200, mean reward:  1.905 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.801597, mean_q: 38.185070, mean_eps: 0.100000\n","     192932/2000000000: episode: 5215, duration: 5.741s, episode steps:  40, steps per second:   7, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.347092, mean_q: 37.748582, mean_eps: 0.100000\n","     192968/2000000000: episode: 5216, duration: 4.887s, episode steps:  36, steps per second:   7, episode reward: 144.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.824402, mean_q: 37.527437, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     193002/2000000000: episode: 5217, duration: 4.568s, episode steps:  34, steps per second:   7, episode reward: -28.800, mean reward: -0.847 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 87.583849, mean_q: 37.872802, mean_eps: 0.100000\n","     193042/2000000000: episode: 5218, duration: 5.396s, episode steps:  40, steps per second:   7, episode reward: 76.400, mean reward:  1.910 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.408810, mean_q: 38.627477, mean_eps: 0.100000\n","     193079/2000000000: episode: 5219, duration: 5.030s, episode steps:  37, steps per second:   7, episode reward: -6.500, mean reward: -0.176 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 75.446734, mean_q: 38.044375, mean_eps: 0.100000\n","     193119/2000000000: episode: 5220, duration: 5.600s, episode steps:  40, steps per second:   7, episode reward: 72.800, mean reward:  1.820 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.256649, mean_q: 38.259514, mean_eps: 0.100000\n","     193155/2000000000: episode: 5221, duration: 4.783s, episode steps:  36, steps per second:   8, episode reward: 55.400, mean reward:  1.539 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.062361, mean_q: 37.974258, mean_eps: 0.100000\n","     193191/2000000000: episode: 5222, duration: 4.803s, episode steps:  36, steps per second:   7, episode reward: -29.800, mean reward: -0.828 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 79.481551, mean_q: 38.895474, mean_eps: 0.100000\n","     193225/2000000000: episode: 5223, duration: 4.775s, episode steps:  34, steps per second:   7, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 81.302195, mean_q: 38.438094, mean_eps: 0.100000\n","     193265/2000000000: episode: 5224, duration: 5.584s, episode steps:  40, steps per second:   7, episode reward: 64.400, mean reward:  1.610 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.192508, mean_q: 37.729472, mean_eps: 0.100000\n","     193300/2000000000: episode: 5225, duration: 4.960s, episode steps:  35, steps per second:   7, episode reward: -23.700, mean reward: -0.677 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 75.028544, mean_q: 38.897122, mean_eps: 0.100000\n","     193337/2000000000: episode: 5226, duration: 5.014s, episode steps:  37, steps per second:   7, episode reward: 125.600, mean reward:  3.395 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 81.491467, mean_q: 38.076267, mean_eps: 0.100000\n","     193372/2000000000: episode: 5227, duration: 4.944s, episode steps:  35, steps per second:   7, episode reward: 75.700, mean reward:  2.163 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 76.653433, mean_q: 37.452855, mean_eps: 0.100000\n","     193404/2000000000: episode: 5228, duration: 4.363s, episode steps:  32, steps per second:   7, episode reward: 44.400, mean reward:  1.388 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 81.083100, mean_q: 38.654429, mean_eps: 0.100000\n","     193443/2000000000: episode: 5229, duration: 6.080s, episode steps:  39, steps per second:   6, episode reward: 70.900, mean reward:  1.818 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 78.506865, mean_q: 38.951399, mean_eps: 0.100000\n","     193481/2000000000: episode: 5230, duration: 5.345s, episode steps:  38, steps per second:   7, episode reward:  0.300, mean reward:  0.008 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 80.379118, mean_q: 38.180409, mean_eps: 0.100000\n","     193520/2000000000: episode: 5231, duration: 5.607s, episode steps:  39, steps per second:   7, episode reward: -20.000, mean reward: -0.513 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 73.108341, mean_q: 38.608869, mean_eps: 0.100000\n","     193549/2000000000: episode: 5232, duration: 4.188s, episode steps:  29, steps per second:   7, episode reward:  9.900, mean reward:  0.341 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 79.427114, mean_q: 39.009621, mean_eps: 0.100000\n","     193589/2000000000: episode: 5233, duration: 5.678s, episode steps:  40, steps per second:   7, episode reward: -10.400, mean reward: -0.260 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.879898, mean_q: 38.375446, mean_eps: 0.100000\n","     193629/2000000000: episode: 5234, duration: 5.490s, episode steps:  40, steps per second:   7, episode reward: -37.100, mean reward: -0.928 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.548020, mean_q: 38.289601, mean_eps: 0.100000\n","     193669/2000000000: episode: 5235, duration: 5.476s, episode steps:  40, steps per second:   7, episode reward: 16.900, mean reward:  0.422 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.962301, mean_q: 38.047504, mean_eps: 0.100000\n","     193708/2000000000: episode: 5236, duration: 5.556s, episode steps:  39, steps per second:   7, episode reward: 171.300, mean reward:  4.392 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 79.086789, mean_q: 38.232743, mean_eps: 0.100000\n","     193748/2000000000: episode: 5237, duration: 5.547s, episode steps:  40, steps per second:   7, episode reward: -53.300, mean reward: -1.332 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 84.762133, mean_q: 38.337153, mean_eps: 0.100000\n","     193786/2000000000: episode: 5238, duration: 5.658s, episode steps:  38, steps per second:   7, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 85.040271, mean_q: 37.925293, mean_eps: 0.100000\n","     193814/2000000000: episode: 5239, duration: 3.982s, episode steps:  28, steps per second:   7, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 78.139030, mean_q: 38.672333, mean_eps: 0.100000\n","     193853/2000000000: episode: 5240, duration: 5.476s, episode steps:  39, steps per second:   7, episode reward: -42.000, mean reward: -1.077 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 79.642079, mean_q: 37.731452, mean_eps: 0.100000\n","     193892/2000000000: episode: 5241, duration: 5.624s, episode steps:  39, steps per second:   7, episode reward: 144.700, mean reward:  3.710 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 74.615438, mean_q: 39.502228, mean_eps: 0.100000\n","     193932/2000000000: episode: 5242, duration: 6.029s, episode steps:  40, steps per second:   7, episode reward: 96.200, mean reward:  2.405 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.794768, mean_q: 37.921545, mean_eps: 0.100000\n","     193969/2000000000: episode: 5243, duration: 5.611s, episode steps:  37, steps per second:   7, episode reward: 10.600, mean reward:  0.286 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 79.053808, mean_q: 38.848204, mean_eps: 0.100000\n","     193994/2000000000: episode: 5244, duration: 3.627s, episode steps:  25, steps per second:   7, episode reward: 42.700, mean reward:  1.708 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 83.811288, mean_q: 39.045245, mean_eps: 0.100000\n","     194029/2000000000: episode: 5245, duration: 4.874s, episode steps:  35, steps per second:   7, episode reward: -69.000, mean reward: -1.971 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 79.256688, mean_q: 38.137084, mean_eps: 0.100000\n","     194068/2000000000: episode: 5246, duration: 5.446s, episode steps:  39, steps per second:   7, episode reward: 170.000, mean reward:  4.359 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 83.716315, mean_q: 38.378538, mean_eps: 0.100000\n","     194108/2000000000: episode: 5247, duration: 5.740s, episode steps:  40, steps per second:   7, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.872468, mean_q: 38.908557, mean_eps: 0.100000\n","     194148/2000000000: episode: 5248, duration: 6.637s, episode steps:  40, steps per second:   6, episode reward: 84.300, mean reward:  2.107 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.910104, mean_q: 38.131808, mean_eps: 0.100000\n","     194188/2000000000: episode: 5249, duration: 5.739s, episode steps:  40, steps per second:   7, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 88.259453, mean_q: 37.956489, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     194228/2000000000: episode: 5250, duration: 5.660s, episode steps:  40, steps per second:   7, episode reward: -14.600, mean reward: -0.365 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.771606, mean_q: 38.235191, mean_eps: 0.100000\n","     194263/2000000000: episode: 5251, duration: 5.192s, episode steps:  35, steps per second:   7, episode reward: 246.000, mean reward:  7.029 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 84.106943, mean_q: 38.238799, mean_eps: 0.100000\n","     194303/2000000000: episode: 5252, duration: 5.917s, episode steps:  40, steps per second:   7, episode reward: 84.300, mean reward:  2.107 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.604510, mean_q: 37.981302, mean_eps: 0.100000\n","     194339/2000000000: episode: 5253, duration: 4.757s, episode steps:  36, steps per second:   8, episode reward: 160.600, mean reward:  4.461 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.925738, mean_q: 39.192031, mean_eps: 0.100000\n","     194369/2000000000: episode: 5254, duration: 3.965s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 75.743541, mean_q: 38.786195, mean_eps: 0.100000\n","     194402/2000000000: episode: 5255, duration: 5.084s, episode steps:  33, steps per second:   6, episode reward: 77.100, mean reward:  2.336 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 83.891358, mean_q: 38.278353, mean_eps: 0.100000\n","     194432/2000000000: episode: 5256, duration: 4.105s, episode steps:  30, steps per second:   7, episode reward: 23.800, mean reward:  0.793 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.197130, mean_q: 39.078099, mean_eps: 0.100000\n","     194472/2000000000: episode: 5257, duration: 5.474s, episode steps:  40, steps per second:   7, episode reward: 72.100, mean reward:  1.802 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.643444, mean_q: 38.182024, mean_eps: 0.100000\n","     194511/2000000000: episode: 5258, duration: 5.263s, episode steps:  39, steps per second:   7, episode reward:  0.300, mean reward:  0.008 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 73.165303, mean_q: 38.840551, mean_eps: 0.100000\n","     194551/2000000000: episode: 5259, duration: 5.319s, episode steps:  40, steps per second:   8, episode reward: -45.500, mean reward: -1.137 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 84.702718, mean_q: 37.850666, mean_eps: 0.100000\n","     194591/2000000000: episode: 5260, duration: 5.755s, episode steps:  40, steps per second:   7, episode reward: -44.900, mean reward: -1.123 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.648927, mean_q: 38.386785, mean_eps: 0.100000\n","     194621/2000000000: episode: 5261, duration: 4.504s, episode steps:  30, steps per second:   7, episode reward: 46.700, mean reward:  1.557 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 77.681479, mean_q: 38.925524, mean_eps: 0.100000\n","     194649/2000000000: episode: 5262, duration: 4.212s, episode steps:  28, steps per second:   7, episode reward: 24.800, mean reward:  0.886 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 81.641462, mean_q: 38.362492, mean_eps: 0.100000\n","     194689/2000000000: episode: 5263, duration: 5.619s, episode steps:  40, steps per second:   7, episode reward: -25.700, mean reward: -0.642 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.576914, mean_q: 38.866860, mean_eps: 0.100000\n","     194714/2000000000: episode: 5264, duration: 3.422s, episode steps:  25, steps per second:   7, episode reward: 75.900, mean reward:  3.036 [-20.000, 19.100], mean action: 0.840 [0.000, 2.000],  loss: 79.289969, mean_q: 37.958616, mean_eps: 0.100000\n","     194754/2000000000: episode: 5265, duration: 5.572s, episode steps:  40, steps per second:   7, episode reward: 96.300, mean reward:  2.408 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.399127, mean_q: 37.902873, mean_eps: 0.100000\n","     194794/2000000000: episode: 5266, duration: 5.464s, episode steps:  40, steps per second:   7, episode reward: 113.500, mean reward:  2.838 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 80.565644, mean_q: 38.691278, mean_eps: 0.100000\n","     194828/2000000000: episode: 5267, duration: 4.687s, episode steps:  34, steps per second:   7, episode reward: 84.400, mean reward:  2.482 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 92.312780, mean_q: 37.065287, mean_eps: 0.100000\n","     194864/2000000000: episode: 5268, duration: 4.984s, episode steps:  36, steps per second:   7, episode reward: 194.900, mean reward:  5.414 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 77.926805, mean_q: 37.693012, mean_eps: 0.100000\n","     194899/2000000000: episode: 5269, duration: 4.693s, episode steps:  35, steps per second:   7, episode reward: 120.700, mean reward:  3.449 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 78.881100, mean_q: 38.022760, mean_eps: 0.100000\n","     194931/2000000000: episode: 5270, duration: 4.923s, episode steps:  32, steps per second:   7, episode reward: 144.000, mean reward:  4.500 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 81.210050, mean_q: 38.605616, mean_eps: 0.100000\n","     194971/2000000000: episode: 5271, duration: 6.403s, episode steps:  40, steps per second:   6, episode reward: 191.900, mean reward:  4.797 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.287027, mean_q: 38.188763, mean_eps: 0.100000\n","     195010/2000000000: episode: 5272, duration: 5.960s, episode steps:  39, steps per second:   7, episode reward: -7.900, mean reward: -0.203 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 71.228998, mean_q: 38.667298, mean_eps: 0.100000\n","     195039/2000000000: episode: 5273, duration: 4.394s, episode steps:  29, steps per second:   7, episode reward: -20.600, mean reward: -0.710 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 77.909749, mean_q: 38.832714, mean_eps: 0.100000\n","     195074/2000000000: episode: 5274, duration: 5.364s, episode steps:  35, steps per second:   7, episode reward: 108.400, mean reward:  3.097 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 80.496748, mean_q: 37.814513, mean_eps: 0.100000\n","     195107/2000000000: episode: 5275, duration: 5.115s, episode steps:  33, steps per second:   6, episode reward: -99.800, mean reward: -3.024 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 80.087714, mean_q: 38.273239, mean_eps: 0.100000\n","     195147/2000000000: episode: 5276, duration: 6.060s, episode steps:  40, steps per second:   7, episode reward: 61.200, mean reward:  1.530 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.002038, mean_q: 39.091311, mean_eps: 0.100000\n","     195182/2000000000: episode: 5277, duration: 5.235s, episode steps:  35, steps per second:   7, episode reward: 170.300, mean reward:  4.866 [-20.000, 19.500], mean action: 1.143 [0.000, 2.000],  loss: 76.581442, mean_q: 38.860442, mean_eps: 0.100000\n","     195221/2000000000: episode: 5278, duration: 5.869s, episode steps:  39, steps per second:   7, episode reward: 22.000, mean reward:  0.564 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.841231, mean_q: 38.099446, mean_eps: 0.100000\n","     195261/2000000000: episode: 5279, duration: 5.443s, episode steps:  40, steps per second:   7, episode reward: 119.000, mean reward:  2.975 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 79.405897, mean_q: 38.815923, mean_eps: 0.100000\n","     195298/2000000000: episode: 5280, duration: 5.343s, episode steps:  37, steps per second:   7, episode reward: -14.400, mean reward: -0.389 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 78.342773, mean_q: 37.799256, mean_eps: 0.100000\n","     195337/2000000000: episode: 5281, duration: 5.527s, episode steps:  39, steps per second:   7, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 85.856474, mean_q: 37.901456, mean_eps: 0.100000\n","     195377/2000000000: episode: 5282, duration: 5.793s, episode steps:  40, steps per second:   7, episode reward: 165.400, mean reward:  4.135 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.269482, mean_q: 38.912362, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     195406/2000000000: episode: 5283, duration: 4.337s, episode steps:  29, steps per second:   7, episode reward: -18.800, mean reward: -0.648 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 77.003238, mean_q: 39.202697, mean_eps: 0.100000\n","     195443/2000000000: episode: 5284, duration: 5.458s, episode steps:  37, steps per second:   7, episode reward: 93.200, mean reward:  2.519 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 80.926544, mean_q: 38.369121, mean_eps: 0.100000\n","     195483/2000000000: episode: 5285, duration: 5.646s, episode steps:  40, steps per second:   7, episode reward: -37.900, mean reward: -0.947 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 86.414035, mean_q: 37.795217, mean_eps: 0.100000\n","     195523/2000000000: episode: 5286, duration: 5.873s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.256633, mean_q: 38.073299, mean_eps: 0.100000\n","     195557/2000000000: episode: 5287, duration: 4.790s, episode steps:  34, steps per second:   7, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 77.406139, mean_q: 38.029426, mean_eps: 0.100000\n","     195596/2000000000: episode: 5288, duration: 5.455s, episode steps:  39, steps per second:   7, episode reward: -58.000, mean reward: -1.487 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 77.490955, mean_q: 37.908799, mean_eps: 0.100000\n","     195636/2000000000: episode: 5289, duration: 5.646s, episode steps:  40, steps per second:   7, episode reward: 228.000, mean reward:  5.700 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.470050, mean_q: 38.590843, mean_eps: 0.100000\n","     195669/2000000000: episode: 5290, duration: 4.784s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 90.748826, mean_q: 38.297721, mean_eps: 0.100000\n","     195709/2000000000: episode: 5291, duration: 5.387s, episode steps:  40, steps per second:   7, episode reward: 155.300, mean reward:  3.882 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.357969, mean_q: 38.017010, mean_eps: 0.100000\n","     195749/2000000000: episode: 5292, duration: 6.088s, episode steps:  40, steps per second:   7, episode reward: 230.000, mean reward:  5.750 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.826398, mean_q: 37.993471, mean_eps: 0.100000\n","     195788/2000000000: episode: 5293, duration: 5.529s, episode steps:  39, steps per second:   7, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 72.452344, mean_q: 38.755331, mean_eps: 0.100000\n","     195826/2000000000: episode: 5294, duration: 5.162s, episode steps:  38, steps per second:   7, episode reward: 71.800, mean reward:  1.889 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 74.851112, mean_q: 38.443236, mean_eps: 0.100000\n","     195865/2000000000: episode: 5295, duration: 5.618s, episode steps:  39, steps per second:   7, episode reward: -96.000, mean reward: -2.462 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 85.319787, mean_q: 38.344622, mean_eps: 0.100000\n","     195896/2000000000: episode: 5296, duration: 4.354s, episode steps:  31, steps per second:   7, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.809183, mean_q: 39.050821, mean_eps: 0.100000\n","     195936/2000000000: episode: 5297, duration: 5.717s, episode steps:  40, steps per second:   7, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.711828, mean_q: 37.579805, mean_eps: 0.100000\n","     195976/2000000000: episode: 5298, duration: 5.988s, episode steps:  40, steps per second:   7, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.068316, mean_q: 37.601713, mean_eps: 0.100000\n","     196008/2000000000: episode: 5299, duration: 4.506s, episode steps:  32, steps per second:   7, episode reward: 120.200, mean reward:  3.756 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 84.804057, mean_q: 37.499684, mean_eps: 0.100000\n","     196048/2000000000: episode: 5300, duration: 5.746s, episode steps:  40, steps per second:   7, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 85.494708, mean_q: 37.668996, mean_eps: 0.100000\n","     196088/2000000000: episode: 5301, duration: 5.487s, episode steps:  40, steps per second:   7, episode reward: 58.400, mean reward:  1.460 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.116212, mean_q: 37.683124, mean_eps: 0.100000\n","     196128/2000000000: episode: 5302, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: -21.800, mean reward: -0.545 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.007528, mean_q: 38.006174, mean_eps: 0.100000\n","     196167/2000000000: episode: 5303, duration: 4.925s, episode steps:  39, steps per second:   8, episode reward: -35.900, mean reward: -0.921 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 82.757813, mean_q: 37.517892, mean_eps: 0.100000\n","     196197/2000000000: episode: 5304, duration: 3.818s, episode steps:  30, steps per second:   8, episode reward: -134.000, mean reward: -4.467 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.263179, mean_q: 38.150040, mean_eps: 0.100000\n","     196227/2000000000: episode: 5305, duration: 3.859s, episode steps:  30, steps per second:   8, episode reward: 46.800, mean reward:  1.560 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 83.222672, mean_q: 38.039482, mean_eps: 0.100000\n","     196267/2000000000: episode: 5306, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: -12.700, mean reward: -0.318 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.411082, mean_q: 38.040645, mean_eps: 0.100000\n","     196305/2000000000: episode: 5307, duration: 4.915s, episode steps:  38, steps per second:   8, episode reward: 57.300, mean reward:  1.508 [-20.000, 19.300], mean action: 1.158 [0.000, 2.000],  loss: 83.686902, mean_q: 37.620526, mean_eps: 0.100000\n","     196344/2000000000: episode: 5308, duration: 5.403s, episode steps:  39, steps per second:   7, episode reward: 91.700, mean reward:  2.351 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 87.169991, mean_q: 38.557030, mean_eps: 0.100000\n","     196376/2000000000: episode: 5309, duration: 4.179s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 78.263283, mean_q: 38.408820, mean_eps: 0.100000\n","     196408/2000000000: episode: 5310, duration: 4.334s, episode steps:  32, steps per second:   7, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 80.977396, mean_q: 38.387308, mean_eps: 0.100000\n","     196437/2000000000: episode: 5311, duration: 3.823s, episode steps:  29, steps per second:   8, episode reward: 208.000, mean reward:  7.172 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 82.690390, mean_q: 37.573797, mean_eps: 0.100000\n","     196473/2000000000: episode: 5312, duration: 4.475s, episode steps:  36, steps per second:   8, episode reward: 36.300, mean reward:  1.008 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 82.482882, mean_q: 37.710265, mean_eps: 0.100000\n","     196512/2000000000: episode: 5313, duration: 4.866s, episode steps:  39, steps per second:   8, episode reward: 89.200, mean reward:  2.287 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 77.684323, mean_q: 38.171663, mean_eps: 0.100000\n","     196550/2000000000: episode: 5314, duration: 4.931s, episode steps:  38, steps per second:   8, episode reward:  3.800, mean reward:  0.100 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.435135, mean_q: 38.510211, mean_eps: 0.100000\n","     196590/2000000000: episode: 5315, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: 105.700, mean reward:  2.643 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.897159, mean_q: 38.930911, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     196625/2000000000: episode: 5316, duration: 4.644s, episode steps:  35, steps per second:   8, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 84.164896, mean_q: 37.948546, mean_eps: 0.100000\n","     196662/2000000000: episode: 5317, duration: 5.083s, episode steps:  37, steps per second:   7, episode reward: 23.600, mean reward:  0.638 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 81.061074, mean_q: 37.642351, mean_eps: 0.100000\n","     196695/2000000000: episode: 5318, duration: 4.417s, episode steps:  33, steps per second:   7, episode reward: 125.500, mean reward:  3.803 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.837685, mean_q: 38.389391, mean_eps: 0.100000\n","     196726/2000000000: episode: 5319, duration: 4.020s, episode steps:  31, steps per second:   8, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 75.025440, mean_q: 38.287294, mean_eps: 0.100000\n","     196757/2000000000: episode: 5320, duration: 4.093s, episode steps:  31, steps per second:   8, episode reward: 208.000, mean reward:  6.710 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 70.347466, mean_q: 38.774377, mean_eps: 0.100000\n","     196787/2000000000: episode: 5321, duration: 3.759s, episode steps:  30, steps per second:   8, episode reward: 136.900, mean reward:  4.563 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 87.162373, mean_q: 38.228322, mean_eps: 0.100000\n","     196827/2000000000: episode: 5322, duration: 4.978s, episode steps:  40, steps per second:   8, episode reward: 42.500, mean reward:  1.063 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.955036, mean_q: 37.793281, mean_eps: 0.100000\n","     196855/2000000000: episode: 5323, duration: 3.576s, episode steps:  28, steps per second:   8, episode reward: 149.300, mean reward:  5.332 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 68.887706, mean_q: 38.410971, mean_eps: 0.100000\n","     196890/2000000000: episode: 5324, duration: 4.571s, episode steps:  35, steps per second:   8, episode reward: 34.300, mean reward:  0.980 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.390500, mean_q: 38.661722, mean_eps: 0.100000\n","     196926/2000000000: episode: 5325, duration: 4.537s, episode steps:  36, steps per second:   8, episode reward: 56.800, mean reward:  1.578 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 80.451571, mean_q: 38.590821, mean_eps: 0.100000\n","     196966/2000000000: episode: 5326, duration: 5.133s, episode steps:  40, steps per second:   8, episode reward: -12.400, mean reward: -0.310 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.413039, mean_q: 38.551656, mean_eps: 0.100000\n","     197000/2000000000: episode: 5327, duration: 4.366s, episode steps:  34, steps per second:   8, episode reward: -5.100, mean reward: -0.150 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 88.270132, mean_q: 37.609911, mean_eps: 0.100000\n","     197033/2000000000: episode: 5328, duration: 4.280s, episode steps:  33, steps per second:   8, episode reward: -148.500, mean reward: -4.500 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 78.899546, mean_q: 37.908017, mean_eps: 0.100000\n","     197063/2000000000: episode: 5329, duration: 4.054s, episode steps:  30, steps per second:   7, episode reward: 246.600, mean reward:  8.220 [-20.000, 18.600], mean action: 1.067 [0.000, 2.000],  loss: 81.015524, mean_q: 37.831491, mean_eps: 0.100000\n","     197095/2000000000: episode: 5330, duration: 4.046s, episode steps:  32, steps per second:   8, episode reward: 107.300, mean reward:  3.353 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.799277, mean_q: 38.638165, mean_eps: 0.100000\n","     197132/2000000000: episode: 5331, duration: 4.612s, episode steps:  37, steps per second:   8, episode reward:  7.300, mean reward:  0.197 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 79.859027, mean_q: 38.341475, mean_eps: 0.100000\n","     197172/2000000000: episode: 5332, duration: 4.860s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.957696, mean_q: 38.201508, mean_eps: 0.100000\n","     197211/2000000000: episode: 5333, duration: 4.824s, episode steps:  39, steps per second:   8, episode reward: 75.500, mean reward:  1.936 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 84.256356, mean_q: 37.486556, mean_eps: 0.100000\n","     197237/2000000000: episode: 5334, duration: 3.270s, episode steps:  26, steps per second:   8, episode reward: 115.200, mean reward:  4.431 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 81.308503, mean_q: 38.373968, mean_eps: 0.100000\n","     197274/2000000000: episode: 5335, duration: 4.597s, episode steps:  37, steps per second:   8, episode reward: 65.700, mean reward:  1.776 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 73.253702, mean_q: 38.605170, mean_eps: 0.100000\n","     197314/2000000000: episode: 5336, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: -148.800, mean reward: -3.720 [-20.000, 19.400], mean action: 1.250 [0.000, 2.000],  loss: 80.229772, mean_q: 38.448141, mean_eps: 0.100000\n","     197352/2000000000: episode: 5337, duration: 4.905s, episode steps:  38, steps per second:   8, episode reward: -32.400, mean reward: -0.853 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 80.668729, mean_q: 38.276409, mean_eps: 0.100000\n","     197391/2000000000: episode: 5338, duration: 5.088s, episode steps:  39, steps per second:   8, episode reward: 172.600, mean reward:  4.426 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 83.783224, mean_q: 38.051105, mean_eps: 0.100000\n","     197430/2000000000: episode: 5339, duration: 5.303s, episode steps:  39, steps per second:   7, episode reward: 163.800, mean reward:  4.200 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 81.121557, mean_q: 38.351115, mean_eps: 0.100000\n","     197469/2000000000: episode: 5340, duration: 5.261s, episode steps:  39, steps per second:   7, episode reward: 64.800, mean reward:  1.662 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 78.122704, mean_q: 38.327798, mean_eps: 0.100000\n","     197501/2000000000: episode: 5341, duration: 4.123s, episode steps:  32, steps per second:   8, episode reward: 223.300, mean reward:  6.978 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 75.993161, mean_q: 38.912945, mean_eps: 0.100000\n","     197529/2000000000: episode: 5342, duration: 3.547s, episode steps:  28, steps per second:   8, episode reward: 110.000, mean reward:  3.929 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 81.698848, mean_q: 38.375251, mean_eps: 0.100000\n","     197568/2000000000: episode: 5343, duration: 5.127s, episode steps:  39, steps per second:   8, episode reward: 28.700, mean reward:  0.736 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 78.648164, mean_q: 38.743753, mean_eps: 0.100000\n","     197595/2000000000: episode: 5344, duration: 3.419s, episode steps:  27, steps per second:   8, episode reward: -32.400, mean reward: -1.200 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 79.993423, mean_q: 38.010207, mean_eps: 0.100000\n","     197633/2000000000: episode: 5345, duration: 5.218s, episode steps:  38, steps per second:   7, episode reward: 27.800, mean reward:  0.732 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 76.324376, mean_q: 38.572358, mean_eps: 0.100000\n","     197672/2000000000: episode: 5346, duration: 5.041s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 80.404484, mean_q: 38.245349, mean_eps: 0.100000\n","     197712/2000000000: episode: 5347, duration: 5.196s, episode steps:  40, steps per second:   8, episode reward: 49.300, mean reward:  1.232 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.989326, mean_q: 38.130714, mean_eps: 0.100000\n","     197752/2000000000: episode: 5348, duration: 5.238s, episode steps:  40, steps per second:   8, episode reward: -30.300, mean reward: -0.757 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.101129, mean_q: 38.418502, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     197792/2000000000: episode: 5349, duration: 5.275s, episode steps:  40, steps per second:   8, episode reward:  5.200, mean reward:  0.130 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.782148, mean_q: 36.914022, mean_eps: 0.100000\n","     197832/2000000000: episode: 5350, duration: 5.425s, episode steps:  40, steps per second:   7, episode reward: 49.600, mean reward:  1.240 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 76.885207, mean_q: 38.239390, mean_eps: 0.100000\n","     197872/2000000000: episode: 5351, duration: 5.282s, episode steps:  40, steps per second:   8, episode reward: 160.800, mean reward:  4.020 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 90.191899, mean_q: 38.230334, mean_eps: 0.100000\n","     197911/2000000000: episode: 5352, duration: 5.208s, episode steps:  39, steps per second:   7, episode reward: 35.000, mean reward:  0.897 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 80.292120, mean_q: 37.955804, mean_eps: 0.100000\n","     197949/2000000000: episode: 5353, duration: 4.800s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 77.288171, mean_q: 38.234508, mean_eps: 0.100000\n","     197985/2000000000: episode: 5354, duration: 4.637s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.799520, mean_q: 38.364525, mean_eps: 0.100000\n","     198025/2000000000: episode: 5355, duration: 5.658s, episode steps:  40, steps per second:   7, episode reward: -23.800, mean reward: -0.595 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.171032, mean_q: 38.464310, mean_eps: 0.100000\n","     198062/2000000000: episode: 5356, duration: 4.951s, episode steps:  37, steps per second:   7, episode reward: 162.500, mean reward:  4.392 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 79.031130, mean_q: 38.570761, mean_eps: 0.100000\n","     198092/2000000000: episode: 5357, duration: 3.751s, episode steps:  30, steps per second:   8, episode reward: 65.100, mean reward:  2.170 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.560884, mean_q: 39.067935, mean_eps: 0.100000\n","     198127/2000000000: episode: 5358, duration: 4.293s, episode steps:  35, steps per second:   8, episode reward: 37.100, mean reward:  1.060 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 75.674547, mean_q: 39.282002, mean_eps: 0.100000\n","     198158/2000000000: episode: 5359, duration: 3.839s, episode steps:  31, steps per second:   8, episode reward: 53.500, mean reward:  1.726 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 86.165432, mean_q: 37.963007, mean_eps: 0.100000\n","     198198/2000000000: episode: 5360, duration: 4.906s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.664948, mean_q: 38.519759, mean_eps: 0.100000\n","     198229/2000000000: episode: 5361, duration: 3.943s, episode steps:  31, steps per second:   8, episode reward: 162.700, mean reward:  5.248 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 74.987717, mean_q: 39.469484, mean_eps: 0.100000\n","     198256/2000000000: episode: 5362, duration: 3.498s, episode steps:  27, steps per second:   8, episode reward: -97.500, mean reward: -3.611 [-20.000, 18.000], mean action: 1.074 [0.000, 2.000],  loss: 78.712775, mean_q: 37.736898, mean_eps: 0.100000\n","     198296/2000000000: episode: 5363, duration: 5.327s, episode steps:  40, steps per second:   8, episode reward: -3.100, mean reward: -0.077 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 85.518863, mean_q: 38.221986, mean_eps: 0.100000\n","     198333/2000000000: episode: 5364, duration: 4.633s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 79.035752, mean_q: 38.105027, mean_eps: 0.100000\n","     198372/2000000000: episode: 5365, duration: 4.857s, episode steps:  39, steps per second:   8, episode reward: 141.900, mean reward:  3.638 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 75.218188, mean_q: 38.067604, mean_eps: 0.100000\n","     198405/2000000000: episode: 5366, duration: 4.158s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.306562, mean_q: 38.942273, mean_eps: 0.100000\n","     198442/2000000000: episode: 5367, duration: 4.767s, episode steps:  37, steps per second:   8, episode reward: 18.500, mean reward:  0.500 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 73.142652, mean_q: 38.577714, mean_eps: 0.100000\n","     198482/2000000000: episode: 5368, duration: 5.171s, episode steps:  40, steps per second:   8, episode reward: 26.100, mean reward:  0.653 [-20.000, 18.100], mean action: 1.500 [0.000, 2.000],  loss: 78.589606, mean_q: 38.368983, mean_eps: 0.100000\n","     198521/2000000000: episode: 5369, duration: 5.040s, episode steps:  39, steps per second:   8, episode reward: 98.600, mean reward:  2.528 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 81.065733, mean_q: 38.110815, mean_eps: 0.100000\n","     198561/2000000000: episode: 5370, duration: 5.069s, episode steps:  40, steps per second:   8, episode reward: 75.600, mean reward:  1.890 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.828713, mean_q: 37.181006, mean_eps: 0.100000\n","     198601/2000000000: episode: 5371, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: 65.100, mean reward:  1.627 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 83.522590, mean_q: 37.698449, mean_eps: 0.100000\n","     198641/2000000000: episode: 5372, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: -55.300, mean reward: -1.382 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.666033, mean_q: 38.442685, mean_eps: 0.100000\n","     198678/2000000000: episode: 5373, duration: 4.785s, episode steps:  37, steps per second:   8, episode reward: 43.200, mean reward:  1.168 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 85.353592, mean_q: 37.948891, mean_eps: 0.100000\n","     198714/2000000000: episode: 5374, duration: 4.536s, episode steps:  36, steps per second:   8, episode reward: 84.500, mean reward:  2.347 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 79.051517, mean_q: 38.480218, mean_eps: 0.100000\n","     198754/2000000000: episode: 5375, duration: 5.043s, episode steps:  40, steps per second:   8, episode reward: 40.400, mean reward:  1.010 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.423786, mean_q: 38.149398, mean_eps: 0.100000\n","     198794/2000000000: episode: 5376, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 198.600, mean reward:  4.965 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 88.927165, mean_q: 38.235954, mean_eps: 0.100000\n","     198834/2000000000: episode: 5377, duration: 5.166s, episode steps:  40, steps per second:   8, episode reward: 50.500, mean reward:  1.262 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.949838, mean_q: 38.120833, mean_eps: 0.100000\n","     198874/2000000000: episode: 5378, duration: 5.123s, episode steps:  40, steps per second:   8, episode reward: 131.200, mean reward:  3.280 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.852270, mean_q: 38.026281, mean_eps: 0.100000\n","     198910/2000000000: episode: 5379, duration: 4.679s, episode steps:  36, steps per second:   8, episode reward: 80.600, mean reward:  2.239 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.240777, mean_q: 38.217562, mean_eps: 0.100000\n","     198950/2000000000: episode: 5380, duration: 5.359s, episode steps:  40, steps per second:   7, episode reward: 80.700, mean reward:  2.018 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.478505, mean_q: 37.585432, mean_eps: 0.100000\n","     198987/2000000000: episode: 5381, duration: 5.008s, episode steps:  37, steps per second:   7, episode reward: -2.800, mean reward: -0.076 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 85.222911, mean_q: 38.145670, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     199018/2000000000: episode: 5382, duration: 4.168s, episode steps:  31, steps per second:   7, episode reward: 129.300, mean reward:  4.171 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 71.544004, mean_q: 38.528120, mean_eps: 0.100000\n","     199052/2000000000: episode: 5383, duration: 4.244s, episode steps:  34, steps per second:   8, episode reward: 130.300, mean reward:  3.832 [-20.000, 18.900], mean action: 1.000 [0.000, 2.000],  loss: 73.077920, mean_q: 38.458011, mean_eps: 0.100000\n","     199087/2000000000: episode: 5384, duration: 4.374s, episode steps:  35, steps per second:   8, episode reward: 39.600, mean reward:  1.131 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 78.917005, mean_q: 38.447124, mean_eps: 0.100000\n","     199118/2000000000: episode: 5385, duration: 3.881s, episode steps:  31, steps per second:   8, episode reward: -103.400, mean reward: -3.335 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 82.022042, mean_q: 38.375683, mean_eps: 0.100000\n","     199147/2000000000: episode: 5386, duration: 3.591s, episode steps:  29, steps per second:   8, episode reward: 110.200, mean reward:  3.800 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.570274, mean_q: 38.306847, mean_eps: 0.100000\n","     199176/2000000000: episode: 5387, duration: 3.844s, episode steps:  29, steps per second:   8, episode reward: -49.000, mean reward: -1.690 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.995001, mean_q: 38.703939, mean_eps: 0.100000\n","     199210/2000000000: episode: 5388, duration: 4.432s, episode steps:  34, steps per second:   8, episode reward: 29.900, mean reward:  0.879 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 69.463317, mean_q: 38.974947, mean_eps: 0.100000\n","     199249/2000000000: episode: 5389, duration: 5.128s, episode steps:  39, steps per second:   8, episode reward: 92.300, mean reward:  2.367 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 75.163944, mean_q: 38.267454, mean_eps: 0.100000\n","     199289/2000000000: episode: 5390, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: -25.700, mean reward: -0.642 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.096267, mean_q: 38.276854, mean_eps: 0.100000\n","     199326/2000000000: episode: 5391, duration: 4.755s, episode steps:  37, steps per second:   8, episode reward: 92.600, mean reward:  2.503 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 74.563378, mean_q: 38.893790, mean_eps: 0.100000\n","     199354/2000000000: episode: 5392, duration: 3.457s, episode steps:  28, steps per second:   8, episode reward: 148.900, mean reward:  5.318 [-20.000, 18.400], mean action: 0.893 [0.000, 2.000],  loss: 74.114906, mean_q: 38.663303, mean_eps: 0.100000\n","     199394/2000000000: episode: 5393, duration: 4.786s, episode steps:  40, steps per second:   8, episode reward: 26.400, mean reward:  0.660 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 80.956996, mean_q: 37.384352, mean_eps: 0.100000\n","     199429/2000000000: episode: 5394, duration: 4.457s, episode steps:  35, steps per second:   8, episode reward: 45.800, mean reward:  1.309 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 76.434326, mean_q: 38.226072, mean_eps: 0.100000\n","     199467/2000000000: episode: 5395, duration: 4.965s, episode steps:  38, steps per second:   8, episode reward: 85.300, mean reward:  2.245 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 79.242061, mean_q: 38.042711, mean_eps: 0.100000\n","     199507/2000000000: episode: 5396, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.066780, mean_q: 38.457922, mean_eps: 0.100000\n","     199538/2000000000: episode: 5397, duration: 4.050s, episode steps:  31, steps per second:   8, episode reward: -8.700, mean reward: -0.281 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 79.245762, mean_q: 38.130571, mean_eps: 0.100000\n","     199573/2000000000: episode: 5398, duration: 4.876s, episode steps:  35, steps per second:   7, episode reward: 29.200, mean reward:  0.834 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 77.465574, mean_q: 38.278845, mean_eps: 0.100000\n","     199601/2000000000: episode: 5399, duration: 3.641s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 80.664660, mean_q: 38.084879, mean_eps: 0.100000\n","     199640/2000000000: episode: 5400, duration: 5.298s, episode steps:  39, steps per second:   7, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 83.818694, mean_q: 38.209374, mean_eps: 0.100000\n","     199677/2000000000: episode: 5401, duration: 4.696s, episode steps:  37, steps per second:   8, episode reward: 119.900, mean reward:  3.241 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 77.250159, mean_q: 38.590902, mean_eps: 0.100000\n","     199717/2000000000: episode: 5402, duration: 5.203s, episode steps:  40, steps per second:   8, episode reward: 147.200, mean reward:  3.680 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.538171, mean_q: 37.375206, mean_eps: 0.100000\n","     199745/2000000000: episode: 5403, duration: 3.670s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 79.644424, mean_q: 38.727217, mean_eps: 0.100000\n","     199785/2000000000: episode: 5404, duration: 5.247s, episode steps:  40, steps per second:   8, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 73.142242, mean_q: 38.543196, mean_eps: 0.100000\n","     199825/2000000000: episode: 5405, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward: 96.800, mean reward:  2.420 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.413635, mean_q: 38.382098, mean_eps: 0.100000\n","     199865/2000000000: episode: 5406, duration: 5.293s, episode steps:  40, steps per second:   8, episode reward:  4.000, mean reward:  0.100 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.382356, mean_q: 38.162564, mean_eps: 0.100000\n","     199905/2000000000: episode: 5407, duration: 5.451s, episode steps:  40, steps per second:   7, episode reward: 174.000, mean reward:  4.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 71.876864, mean_q: 37.948360, mean_eps: 0.100000\n","     199940/2000000000: episode: 5408, duration: 4.715s, episode steps:  35, steps per second:   7, episode reward: -77.700, mean reward: -2.220 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 76.184619, mean_q: 38.554164, mean_eps: 0.100000\n","     199980/2000000000: episode: 5409, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.619552, mean_q: 38.143358, mean_eps: 0.100000\n","     200020/2000000000: episode: 5410, duration: 4.805s, episode steps:  40, steps per second:   8, episode reward: 106.500, mean reward:  2.662 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.934700, mean_q: 37.986068, mean_eps: 0.100000\n","     200054/2000000000: episode: 5411, duration: 4.291s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 87.355315, mean_q: 38.699451, mean_eps: 0.100000\n","     200092/2000000000: episode: 5412, duration: 4.712s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 83.524620, mean_q: 38.092088, mean_eps: 0.100000\n","     200120/2000000000: episode: 5413, duration: 3.602s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 74.889357, mean_q: 38.539801, mean_eps: 0.100000\n","     200156/2000000000: episode: 5414, duration: 4.458s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 79.602159, mean_q: 37.768844, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     200192/2000000000: episode: 5415, duration: 4.666s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 81.993973, mean_q: 38.440318, mean_eps: 0.100000\n","     200230/2000000000: episode: 5416, duration: 5.172s, episode steps:  38, steps per second:   7, episode reward: -18.300, mean reward: -0.482 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 88.199723, mean_q: 38.316554, mean_eps: 0.100000\n","     200270/2000000000: episode: 5417, duration: 5.319s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.349010, mean_q: 38.338373, mean_eps: 0.100000\n","     200305/2000000000: episode: 5418, duration: 4.601s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 76.975836, mean_q: 38.406736, mean_eps: 0.100000\n","     200337/2000000000: episode: 5419, duration: 4.046s, episode steps:  32, steps per second:   8, episode reward: -29.200, mean reward: -0.913 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 75.290956, mean_q: 39.512992, mean_eps: 0.100000\n","     200362/2000000000: episode: 5420, duration: 3.100s, episode steps:  25, steps per second:   8, episode reward: -6.200, mean reward: -0.248 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 82.052711, mean_q: 38.945639, mean_eps: 0.100000\n","     200397/2000000000: episode: 5421, duration: 4.578s, episode steps:  35, steps per second:   8, episode reward: 14.600, mean reward:  0.417 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 83.366295, mean_q: 38.948174, mean_eps: 0.100000\n","     200432/2000000000: episode: 5422, duration: 4.234s, episode steps:  35, steps per second:   8, episode reward: 100.000, mean reward:  2.857 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 81.266243, mean_q: 38.538390, mean_eps: 0.100000\n","     200470/2000000000: episode: 5423, duration: 4.788s, episode steps:  38, steps per second:   8, episode reward: -73.100, mean reward: -1.924 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 75.493153, mean_q: 39.163563, mean_eps: 0.100000\n","     200504/2000000000: episode: 5424, duration: 4.259s, episode steps:  34, steps per second:   8, episode reward: 169.300, mean reward:  4.979 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 78.125851, mean_q: 38.747856, mean_eps: 0.100000\n","     200544/2000000000: episode: 5425, duration: 4.770s, episode steps:  40, steps per second:   8, episode reward: -75.900, mean reward: -1.897 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.547821, mean_q: 37.945548, mean_eps: 0.100000\n","     200576/2000000000: episode: 5426, duration: 4.156s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 84.098615, mean_q: 38.841073, mean_eps: 0.100000\n","     200613/2000000000: episode: 5427, duration: 4.951s, episode steps:  37, steps per second:   7, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 78.123494, mean_q: 37.876643, mean_eps: 0.100000\n","     200651/2000000000: episode: 5428, duration: 4.954s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 79.670140, mean_q: 38.877206, mean_eps: 0.100000\n","     200677/2000000000: episode: 5429, duration: 3.242s, episode steps:  26, steps per second:   8, episode reward: -58.000, mean reward: -2.231 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.069511, mean_q: 38.701143, mean_eps: 0.100000\n","     200717/2000000000: episode: 5430, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward: -77.400, mean reward: -1.935 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.853173, mean_q: 37.972713, mean_eps: 0.100000\n","     200748/2000000000: episode: 5431, duration: 3.982s, episode steps:  31, steps per second:   8, episode reward: -58.800, mean reward: -1.897 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.371896, mean_q: 38.667743, mean_eps: 0.100000\n","     200788/2000000000: episode: 5432, duration: 5.145s, episode steps:  40, steps per second:   8, episode reward: 136.200, mean reward:  3.405 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.289472, mean_q: 38.413916, mean_eps: 0.100000\n","     200828/2000000000: episode: 5433, duration: 5.100s, episode steps:  40, steps per second:   8, episode reward: 155.900, mean reward:  3.898 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.510787, mean_q: 38.213823, mean_eps: 0.100000\n","     200863/2000000000: episode: 5434, duration: 4.544s, episode steps:  35, steps per second:   8, episode reward: 32.500, mean reward:  0.929 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 69.748660, mean_q: 38.505662, mean_eps: 0.100000\n","     200895/2000000000: episode: 5435, duration: 3.958s, episode steps:  32, steps per second:   8, episode reward: 89.900, mean reward:  2.809 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.230732, mean_q: 39.455361, mean_eps: 0.100000\n","     200927/2000000000: episode: 5436, duration: 4.244s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 79.259207, mean_q: 39.018627, mean_eps: 0.100000\n","     200963/2000000000: episode: 5437, duration: 4.754s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.290379, mean_q: 38.843844, mean_eps: 0.100000\n","     200998/2000000000: episode: 5438, duration: 4.653s, episode steps:  35, steps per second:   8, episode reward: 190.400, mean reward:  5.440 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 85.886683, mean_q: 38.890715, mean_eps: 0.100000\n","     201029/2000000000: episode: 5439, duration: 3.949s, episode steps:  31, steps per second:   8, episode reward: -48.300, mean reward: -1.558 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.587764, mean_q: 39.402343, mean_eps: 0.100000\n","     201059/2000000000: episode: 5440, duration: 4.029s, episode steps:  30, steps per second:   7, episode reward: 80.800, mean reward:  2.693 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 76.106245, mean_q: 38.994260, mean_eps: 0.100000\n","     201094/2000000000: episode: 5441, duration: 4.873s, episode steps:  35, steps per second:   7, episode reward: 141.600, mean reward:  4.046 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 74.833237, mean_q: 39.181745, mean_eps: 0.100000\n","     201127/2000000000: episode: 5442, duration: 4.299s, episode steps:  33, steps per second:   8, episode reward: -35.200, mean reward: -1.067 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 77.182180, mean_q: 39.004799, mean_eps: 0.100000\n","     201164/2000000000: episode: 5443, duration: 4.953s, episode steps:  37, steps per second:   7, episode reward: 69.700, mean reward:  1.884 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 84.582360, mean_q: 38.330894, mean_eps: 0.100000\n","     201204/2000000000: episode: 5444, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.536467, mean_q: 38.599057, mean_eps: 0.100000\n","     201227/2000000000: episode: 5445, duration: 2.960s, episode steps:  23, steps per second:   8, episode reward:  3.500, mean reward:  0.152 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 74.762072, mean_q: 39.821095, mean_eps: 0.100000\n","     201257/2000000000: episode: 5446, duration: 3.982s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 84.692560, mean_q: 37.487918, mean_eps: 0.100000\n","     201296/2000000000: episode: 5447, duration: 4.899s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 79.713835, mean_q: 38.662470, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     201336/2000000000: episode: 5448, duration: 5.291s, episode steps:  40, steps per second:   8, episode reward: 96.100, mean reward:  2.403 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.440487, mean_q: 38.041667, mean_eps: 0.100000\n","     201372/2000000000: episode: 5449, duration: 4.868s, episode steps:  36, steps per second:   7, episode reward: -36.400, mean reward: -1.011 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 81.409436, mean_q: 38.532344, mean_eps: 0.100000\n","     201405/2000000000: episode: 5450, duration: 4.311s, episode steps:  33, steps per second:   8, episode reward: 126.000, mean reward:  3.818 [-20.000, 19.800], mean action: 1.121 [0.000, 2.000],  loss: 86.761422, mean_q: 39.089190, mean_eps: 0.100000\n","     201443/2000000000: episode: 5451, duration: 4.653s, episode steps:  38, steps per second:   8, episode reward: -36.900, mean reward: -0.971 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 80.708317, mean_q: 37.669498, mean_eps: 0.100000\n","     201483/2000000000: episode: 5452, duration: 5.002s, episode steps:  40, steps per second:   8, episode reward: 251.800, mean reward:  6.295 [-20.000, 19.800], mean action: 1.350 [0.000, 2.000],  loss: 72.147224, mean_q: 38.907586, mean_eps: 0.100000\n","     201515/2000000000: episode: 5453, duration: 4.009s, episode steps:  32, steps per second:   8, episode reward: 87.300, mean reward:  2.728 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.580885, mean_q: 39.785036, mean_eps: 0.100000\n","     201546/2000000000: episode: 5454, duration: 3.987s, episode steps:  31, steps per second:   8, episode reward: 114.100, mean reward:  3.681 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 76.462122, mean_q: 38.610192, mean_eps: 0.100000\n","     201584/2000000000: episode: 5455, duration: 5.075s, episode steps:  38, steps per second:   7, episode reward: 46.700, mean reward:  1.229 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 71.796166, mean_q: 38.866592, mean_eps: 0.100000\n","     201618/2000000000: episode: 5456, duration: 4.559s, episode steps:  34, steps per second:   7, episode reward: 148.200, mean reward:  4.359 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 85.980149, mean_q: 38.691286, mean_eps: 0.100000\n","     201658/2000000000: episode: 5457, duration: 4.931s, episode steps:  40, steps per second:   8, episode reward: -108.100, mean reward: -2.702 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.853933, mean_q: 38.383993, mean_eps: 0.100000\n","     201695/2000000000: episode: 5458, duration: 4.540s, episode steps:  37, steps per second:   8, episode reward: 14.600, mean reward:  0.395 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 83.636491, mean_q: 37.735070, mean_eps: 0.100000\n","     201724/2000000000: episode: 5459, duration: 3.687s, episode steps:  29, steps per second:   8, episode reward: 58.800, mean reward:  2.028 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 75.965619, mean_q: 38.393260, mean_eps: 0.100000\n","     201764/2000000000: episode: 5460, duration: 4.924s, episode steps:  40, steps per second:   8, episode reward: -36.800, mean reward: -0.920 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.678565, mean_q: 38.252289, mean_eps: 0.100000\n","     201795/2000000000: episode: 5461, duration: 3.918s, episode steps:  31, steps per second:   8, episode reward: -26.700, mean reward: -0.861 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 78.878441, mean_q: 38.189485, mean_eps: 0.100000\n","     201835/2000000000: episode: 5462, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward:  0.700, mean reward:  0.018 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.892974, mean_q: 37.812414, mean_eps: 0.100000\n","     201865/2000000000: episode: 5463, duration: 4.296s, episode steps:  30, steps per second:   7, episode reward: -37.700, mean reward: -1.257 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 80.372099, mean_q: 39.152376, mean_eps: 0.100000\n","     201891/2000000000: episode: 5464, duration: 3.374s, episode steps:  26, steps per second:   8, episode reward: 111.500, mean reward:  4.288 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 75.502936, mean_q: 38.329498, mean_eps: 0.100000\n","     201926/2000000000: episode: 5465, duration: 4.750s, episode steps:  35, steps per second:   7, episode reward: 70.200, mean reward:  2.006 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 78.548128, mean_q: 38.454101, mean_eps: 0.100000\n","     201963/2000000000: episode: 5466, duration: 4.847s, episode steps:  37, steps per second:   8, episode reward:  6.200, mean reward:  0.168 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 77.196631, mean_q: 38.195317, mean_eps: 0.100000\n","     202003/2000000000: episode: 5467, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward: 64.200, mean reward:  1.605 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 86.815188, mean_q: 38.398120, mean_eps: 0.100000\n","     202032/2000000000: episode: 5468, duration: 3.648s, episode steps:  29, steps per second:   8, episode reward: 197.400, mean reward:  6.807 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 77.639122, mean_q: 38.657680, mean_eps: 0.100000\n","     202065/2000000000: episode: 5469, duration: 4.093s, episode steps:  33, steps per second:   8, episode reward: -4.100, mean reward: -0.124 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 80.168387, mean_q: 38.485908, mean_eps: 0.100000\n","     202105/2000000000: episode: 5470, duration: 5.350s, episode steps:  40, steps per second:   7, episode reward: -58.400, mean reward: -1.460 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.099367, mean_q: 38.741265, mean_eps: 0.100000\n","     202136/2000000000: episode: 5471, duration: 4.081s, episode steps:  31, steps per second:   8, episode reward: 112.400, mean reward:  3.626 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 79.639460, mean_q: 38.083230, mean_eps: 0.100000\n","     202172/2000000000: episode: 5472, duration: 4.788s, episode steps:  36, steps per second:   8, episode reward: 43.700, mean reward:  1.214 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.547100, mean_q: 38.733034, mean_eps: 0.100000\n","     202210/2000000000: episode: 5473, duration: 5.061s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 75.043750, mean_q: 38.835254, mean_eps: 0.100000\n","     202248/2000000000: episode: 5474, duration: 4.686s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 81.421929, mean_q: 38.399776, mean_eps: 0.100000\n","     202277/2000000000: episode: 5475, duration: 3.792s, episode steps:  29, steps per second:   8, episode reward: -26.700, mean reward: -0.921 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 75.562896, mean_q: 38.593855, mean_eps: 0.100000\n","     202312/2000000000: episode: 5476, duration: 4.548s, episode steps:  35, steps per second:   8, episode reward: 15.200, mean reward:  0.434 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 82.725495, mean_q: 38.353379, mean_eps: 0.100000\n","     202352/2000000000: episode: 5477, duration: 5.094s, episode steps:  40, steps per second:   8, episode reward: 182.200, mean reward:  4.555 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.869659, mean_q: 38.544509, mean_eps: 0.100000\n","     202386/2000000000: episode: 5478, duration: 4.303s, episode steps:  34, steps per second:   8, episode reward: -2.000, mean reward: -0.059 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 74.207508, mean_q: 38.289430, mean_eps: 0.100000\n","     202424/2000000000: episode: 5479, duration: 4.795s, episode steps:  38, steps per second:   8, episode reward: 208.300, mean reward:  5.482 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 80.508797, mean_q: 37.923721, mean_eps: 0.100000\n","     202458/2000000000: episode: 5480, duration: 4.203s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 74.735549, mean_q: 38.010118, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     202495/2000000000: episode: 5481, duration: 5.001s, episode steps:  37, steps per second:   7, episode reward: -38.200, mean reward: -1.032 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 74.378608, mean_q: 38.257940, mean_eps: 0.100000\n","     202535/2000000000: episode: 5482, duration: 4.907s, episode steps:  40, steps per second:   8, episode reward: 184.300, mean reward:  4.607 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 73.486802, mean_q: 38.017268, mean_eps: 0.100000\n","     202564/2000000000: episode: 5483, duration: 3.782s, episode steps:  29, steps per second:   8, episode reward: -32.600, mean reward: -1.124 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.649823, mean_q: 38.243307, mean_eps: 0.100000\n","     202601/2000000000: episode: 5484, duration: 4.728s, episode steps:  37, steps per second:   8, episode reward: 30.000, mean reward:  0.811 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 79.706613, mean_q: 38.317807, mean_eps: 0.100000\n","     202641/2000000000: episode: 5485, duration: 5.095s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 80.146912, mean_q: 38.188665, mean_eps: 0.100000\n","     202681/2000000000: episode: 5486, duration: 5.351s, episode steps:  40, steps per second:   7, episode reward: 190.000, mean reward:  4.750 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 82.189524, mean_q: 38.149435, mean_eps: 0.100000\n","     202719/2000000000: episode: 5487, duration: 4.993s, episode steps:  38, steps per second:   8, episode reward: 35.300, mean reward:  0.929 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 82.741433, mean_q: 37.744293, mean_eps: 0.100000\n","     202759/2000000000: episode: 5488, duration: 4.848s, episode steps:  40, steps per second:   8, episode reward: 108.000, mean reward:  2.700 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.389238, mean_q: 38.493207, mean_eps: 0.100000\n","     202798/2000000000: episode: 5489, duration: 4.947s, episode steps:  39, steps per second:   8, episode reward: -100.400, mean reward: -2.574 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 79.776308, mean_q: 38.450222, mean_eps: 0.100000\n","     202838/2000000000: episode: 5490, duration: 4.982s, episode steps:  40, steps per second:   8, episode reward: 92.800, mean reward:  2.320 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.697306, mean_q: 38.259224, mean_eps: 0.100000\n","     202877/2000000000: episode: 5491, duration: 5.211s, episode steps:  39, steps per second:   7, episode reward: 106.500, mean reward:  2.731 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 77.912418, mean_q: 38.914222, mean_eps: 0.100000\n","     202917/2000000000: episode: 5492, duration: 5.384s, episode steps:  40, steps per second:   7, episode reward: -81.000, mean reward: -2.025 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.595215, mean_q: 38.604079, mean_eps: 0.100000\n","     202951/2000000000: episode: 5493, duration: 4.254s, episode steps:  34, steps per second:   8, episode reward: 171.600, mean reward:  5.047 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 73.556495, mean_q: 38.722586, mean_eps: 0.100000\n","     202988/2000000000: episode: 5494, duration: 4.653s, episode steps:  37, steps per second:   8, episode reward: 169.800, mean reward:  4.589 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 81.200105, mean_q: 38.725610, mean_eps: 0.100000\n","     203023/2000000000: episode: 5495, duration: 4.622s, episode steps:  35, steps per second:   8, episode reward: 89.600, mean reward:  2.560 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 84.099287, mean_q: 39.117393, mean_eps: 0.100000\n","     203062/2000000000: episode: 5496, duration: 4.895s, episode steps:  39, steps per second:   8, episode reward: -25.800, mean reward: -0.662 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 77.500011, mean_q: 38.077778, mean_eps: 0.100000\n","     203102/2000000000: episode: 5497, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: 68.500, mean reward:  1.713 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.115639, mean_q: 38.710196, mean_eps: 0.100000\n","     203135/2000000000: episode: 5498, duration: 4.277s, episode steps:  33, steps per second:   8, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.746467, mean_q: 39.740772, mean_eps: 0.100000\n","     203172/2000000000: episode: 5499, duration: 4.805s, episode steps:  37, steps per second:   8, episode reward: -42.000, mean reward: -1.135 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 82.937817, mean_q: 39.205630, mean_eps: 0.100000\n","     203203/2000000000: episode: 5500, duration: 4.147s, episode steps:  31, steps per second:   7, episode reward: 97.600, mean reward:  3.148 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 70.533162, mean_q: 38.203360, mean_eps: 0.100000\n","     203232/2000000000: episode: 5501, duration: 4.050s, episode steps:  29, steps per second:   7, episode reward: 82.600, mean reward:  2.848 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 79.779524, mean_q: 38.702649, mean_eps: 0.100000\n","     203272/2000000000: episode: 5502, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: 175.100, mean reward:  4.377 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.372345, mean_q: 38.819948, mean_eps: 0.100000\n","     203309/2000000000: episode: 5503, duration: 4.421s, episode steps:  37, steps per second:   8, episode reward: 91.000, mean reward:  2.459 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 79.521016, mean_q: 38.198833, mean_eps: 0.100000\n","     203349/2000000000: episode: 5504, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: 60.500, mean reward:  1.512 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.020869, mean_q: 38.374074, mean_eps: 0.100000\n","     203380/2000000000: episode: 5505, duration: 4.055s, episode steps:  31, steps per second:   8, episode reward: 162.600, mean reward:  5.245 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.575484, mean_q: 39.257811, mean_eps: 0.100000\n","     203420/2000000000: episode: 5506, duration: 4.980s, episode steps:  40, steps per second:   8, episode reward: 84.500, mean reward:  2.112 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.164398, mean_q: 37.818924, mean_eps: 0.100000\n","     203449/2000000000: episode: 5507, duration: 3.646s, episode steps:  29, steps per second:   8, episode reward: 55.600, mean reward:  1.917 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 73.927515, mean_q: 38.171599, mean_eps: 0.100000\n","     203483/2000000000: episode: 5508, duration: 4.254s, episode steps:  34, steps per second:   8, episode reward: 101.400, mean reward:  2.982 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 75.884666, mean_q: 38.340756, mean_eps: 0.100000\n","     203523/2000000000: episode: 5509, duration: 5.070s, episode steps:  40, steps per second:   8, episode reward: 42.800, mean reward:  1.070 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.726271, mean_q: 38.706376, mean_eps: 0.100000\n","     203563/2000000000: episode: 5510, duration: 5.196s, episode steps:  40, steps per second:   8, episode reward: -3.700, mean reward: -0.092 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.922407, mean_q: 37.915264, mean_eps: 0.100000\n","     203599/2000000000: episode: 5511, duration: 4.900s, episode steps:  36, steps per second:   7, episode reward: 46.300, mean reward:  1.286 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 79.841128, mean_q: 38.356273, mean_eps: 0.100000\n","     203639/2000000000: episode: 5512, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward: -88.300, mean reward: -2.207 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 88.974282, mean_q: 38.330838, mean_eps: 0.100000\n","     203679/2000000000: episode: 5513, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: 47.600, mean reward:  1.190 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 75.293365, mean_q: 39.603692, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     203717/2000000000: episode: 5514, duration: 4.935s, episode steps:  38, steps per second:   8, episode reward: -46.400, mean reward: -1.221 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 74.055062, mean_q: 38.668591, mean_eps: 0.100000\n","     203757/2000000000: episode: 5515, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: -118.400, mean reward: -2.960 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.421088, mean_q: 38.458368, mean_eps: 0.100000\n","     203787/2000000000: episode: 5516, duration: 3.818s, episode steps:  30, steps per second:   8, episode reward: 260.300, mean reward:  8.677 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 71.842428, mean_q: 38.749620, mean_eps: 0.100000\n","     203820/2000000000: episode: 5517, duration: 4.351s, episode steps:  33, steps per second:   8, episode reward: 88.000, mean reward:  2.667 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 74.876372, mean_q: 38.312963, mean_eps: 0.100000\n","     203855/2000000000: episode: 5518, duration: 4.301s, episode steps:  35, steps per second:   8, episode reward:  7.500, mean reward:  0.214 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 76.465314, mean_q: 38.613255, mean_eps: 0.100000\n","     203894/2000000000: episode: 5519, duration: 4.907s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.077 [0.000, 2.000],  loss: 72.676115, mean_q: 38.635163, mean_eps: 0.100000\n","     203934/2000000000: episode: 5520, duration: 4.989s, episode steps:  40, steps per second:   8, episode reward: -134.000, mean reward: -3.350 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.476314, mean_q: 38.204098, mean_eps: 0.100000\n","     203959/2000000000: episode: 5521, duration: 3.405s, episode steps:  25, steps per second:   7, episode reward: -20.000, mean reward: -0.800 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 69.592007, mean_q: 37.300779, mean_eps: 0.100000\n","     203990/2000000000: episode: 5522, duration: 4.061s, episode steps:  31, steps per second:   8, episode reward: 30.900, mean reward:  0.997 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 74.613508, mean_q: 39.331016, mean_eps: 0.100000\n","     204024/2000000000: episode: 5523, duration: 4.443s, episode steps:  34, steps per second:   8, episode reward: -190.300, mean reward: -5.597 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 70.807028, mean_q: 39.275656, mean_eps: 0.100000\n","     204051/2000000000: episode: 5524, duration: 3.520s, episode steps:  27, steps per second:   8, episode reward: 34.500, mean reward:  1.278 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 87.365791, mean_q: 38.462551, mean_eps: 0.100000\n","     204084/2000000000: episode: 5525, duration: 4.513s, episode steps:  33, steps per second:   7, episode reward: -134.000, mean reward: -4.061 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 81.080296, mean_q: 39.953468, mean_eps: 0.100000\n","     204110/2000000000: episode: 5526, duration: 3.546s, episode steps:  26, steps per second:   7, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 80.943524, mean_q: 39.718415, mean_eps: 0.100000\n","     204150/2000000000: episode: 5527, duration: 5.548s, episode steps:  40, steps per second:   7, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.629426, mean_q: 38.080893, mean_eps: 0.100000\n","     204187/2000000000: episode: 5528, duration: 4.627s, episode steps:  37, steps per second:   8, episode reward: -85.900, mean reward: -2.322 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 77.572165, mean_q: 38.264411, mean_eps: 0.100000\n","     204227/2000000000: episode: 5529, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: -92.000, mean reward: -2.300 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 75.063879, mean_q: 38.337757, mean_eps: 0.100000\n","     204267/2000000000: episode: 5530, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward:  5.100, mean reward:  0.128 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.934084, mean_q: 38.581926, mean_eps: 0.100000\n","     204302/2000000000: episode: 5531, duration: 4.482s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 84.089546, mean_q: 39.145440, mean_eps: 0.100000\n","     204342/2000000000: episode: 5532, duration: 5.047s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 78.355651, mean_q: 39.329500, mean_eps: 0.100000\n","     204372/2000000000: episode: 5533, duration: 3.719s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 72.796086, mean_q: 38.056973, mean_eps: 0.100000\n","     204412/2000000000: episode: 5534, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 22.000, mean reward:  0.550 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.218343, mean_q: 38.448742, mean_eps: 0.100000\n","     204441/2000000000: episode: 5535, duration: 3.697s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 69.086095, mean_q: 39.446658, mean_eps: 0.100000\n","     204476/2000000000: episode: 5536, duration: 4.567s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 76.325850, mean_q: 38.675137, mean_eps: 0.100000\n","     204512/2000000000: episode: 5537, duration: 4.580s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 85.794908, mean_q: 37.882091, mean_eps: 0.100000\n","     204535/2000000000: episode: 5538, duration: 2.998s, episode steps:  23, steps per second:   8, episode reward: 25.700, mean reward:  1.117 [-20.000, 18.000], mean action: 0.783 [0.000, 2.000],  loss: 77.552222, mean_q: 39.853927, mean_eps: 0.100000\n","     204558/2000000000: episode: 5539, duration: 3.018s, episode steps:  23, steps per second:   8, episode reward: -134.000, mean reward: -5.826 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 77.746767, mean_q: 37.887847, mean_eps: 0.100000\n","     204598/2000000000: episode: 5540, duration: 5.296s, episode steps:  40, steps per second:   8, episode reward: 53.900, mean reward:  1.348 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.535693, mean_q: 38.197348, mean_eps: 0.100000\n","     204638/2000000000: episode: 5541, duration: 5.182s, episode steps:  40, steps per second:   8, episode reward: 90.200, mean reward:  2.255 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 78.783723, mean_q: 38.342823, mean_eps: 0.100000\n","     204673/2000000000: episode: 5542, duration: 4.558s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.270012, mean_q: 38.944353, mean_eps: 0.100000\n","     204708/2000000000: episode: 5543, duration: 4.487s, episode steps:  35, steps per second:   8, episode reward: 150.900, mean reward:  4.311 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 73.575588, mean_q: 39.218427, mean_eps: 0.100000\n","     204735/2000000000: episode: 5544, duration: 3.547s, episode steps:  27, steps per second:   8, episode reward:  3.800, mean reward:  0.141 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 78.672833, mean_q: 38.458109, mean_eps: 0.100000\n","     204775/2000000000: episode: 5545, duration: 5.297s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.942390, mean_q: 38.756124, mean_eps: 0.100000\n","     204815/2000000000: episode: 5546, duration: 5.270s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.132089, mean_q: 38.252623, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     204855/2000000000: episode: 5547, duration: 5.330s, episode steps:  40, steps per second:   8, episode reward: 64.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 86.692719, mean_q: 38.404719, mean_eps: 0.100000\n","     204889/2000000000: episode: 5548, duration: 4.293s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 81.735428, mean_q: 37.849058, mean_eps: 0.100000\n","     204929/2000000000: episode: 5549, duration: 5.113s, episode steps:  40, steps per second:   8, episode reward: 127.700, mean reward:  3.193 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.084507, mean_q: 38.143878, mean_eps: 0.100000\n","     204962/2000000000: episode: 5550, duration: 4.143s, episode steps:  33, steps per second:   8, episode reward: 11.700, mean reward:  0.355 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 78.315949, mean_q: 38.092930, mean_eps: 0.100000\n","     204994/2000000000: episode: 5551, duration: 4.218s, episode steps:  32, steps per second:   8, episode reward: 11.000, mean reward:  0.344 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 79.800465, mean_q: 39.097970, mean_eps: 0.100000\n","     205024/2000000000: episode: 5552, duration: 4.000s, episode steps:  30, steps per second:   7, episode reward: 148.500, mean reward:  4.950 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.396577, mean_q: 39.225318, mean_eps: 0.100000\n","     205062/2000000000: episode: 5553, duration: 5.088s, episode steps:  38, steps per second:   7, episode reward: 86.700, mean reward:  2.282 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 74.627138, mean_q: 39.116896, mean_eps: 0.100000\n","     205099/2000000000: episode: 5554, duration: 4.769s, episode steps:  37, steps per second:   8, episode reward:  4.700, mean reward:  0.127 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 74.870718, mean_q: 38.846742, mean_eps: 0.100000\n","     205127/2000000000: episode: 5555, duration: 3.589s, episode steps:  28, steps per second:   8, episode reward: 166.200, mean reward:  5.936 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 80.197527, mean_q: 38.643661, mean_eps: 0.100000\n","     205167/2000000000: episode: 5556, duration: 5.114s, episode steps:  40, steps per second:   8, episode reward: 97.000, mean reward:  2.425 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.862281, mean_q: 39.127000, mean_eps: 0.100000\n","     205202/2000000000: episode: 5557, duration: 4.529s, episode steps:  35, steps per second:   8, episode reward: 67.200, mean reward:  1.920 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.111066, mean_q: 38.616048, mean_eps: 0.100000\n","     205228/2000000000: episode: 5558, duration: 3.518s, episode steps:  26, steps per second:   7, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 80.971453, mean_q: 38.512011, mean_eps: 0.100000\n","     205263/2000000000: episode: 5559, duration: 4.407s, episode steps:  35, steps per second:   8, episode reward: 54.700, mean reward:  1.563 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 72.837657, mean_q: 38.414771, mean_eps: 0.100000\n","     205293/2000000000: episode: 5560, duration: 3.950s, episode steps:  30, steps per second:   8, episode reward: 44.900, mean reward:  1.497 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 75.888819, mean_q: 38.484372, mean_eps: 0.100000\n","     205329/2000000000: episode: 5561, duration: 4.807s, episode steps:  36, steps per second:   7, episode reward: 119.700, mean reward:  3.325 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 75.780743, mean_q: 38.469760, mean_eps: 0.100000\n","     205369/2000000000: episode: 5562, duration: 5.348s, episode steps:  40, steps per second:   7, episode reward: -22.000, mean reward: -0.550 [-20.000, 18.600], mean action: 1.300 [0.000, 2.000],  loss: 73.659138, mean_q: 38.253064, mean_eps: 0.100000\n","     205397/2000000000: episode: 5563, duration: 3.608s, episode steps:  28, steps per second:   8, episode reward: 32.100, mean reward:  1.146 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 75.881420, mean_q: 38.676300, mean_eps: 0.100000\n","     205437/2000000000: episode: 5564, duration: 4.952s, episode steps:  40, steps per second:   8, episode reward: -32.700, mean reward: -0.818 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 79.015767, mean_q: 37.578576, mean_eps: 0.100000\n","     205477/2000000000: episode: 5565, duration: 4.968s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.240672, mean_q: 38.169208, mean_eps: 0.100000\n","     205514/2000000000: episode: 5566, duration: 5.111s, episode steps:  37, steps per second:   7, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 77.206825, mean_q: 38.037110, mean_eps: 0.100000\n","     205554/2000000000: episode: 5567, duration: 5.479s, episode steps:  40, steps per second:   7, episode reward: 41.400, mean reward:  1.035 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.197680, mean_q: 38.643822, mean_eps: 0.100000\n","     205594/2000000000: episode: 5568, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.307869, mean_q: 38.387158, mean_eps: 0.100000\n","     205626/2000000000: episode: 5569, duration: 4.226s, episode steps:  32, steps per second:   8, episode reward: 20.800, mean reward:  0.650 [-20.000, 18.300], mean action: 1.000 [0.000, 2.000],  loss: 73.032314, mean_q: 38.498467, mean_eps: 0.100000\n","     205666/2000000000: episode: 5570, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: 269.200, mean reward:  6.730 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 74.749649, mean_q: 38.098031, mean_eps: 0.100000\n","     205698/2000000000: episode: 5571, duration: 4.128s, episode steps:  32, steps per second:   8, episode reward: 28.100, mean reward:  0.878 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 83.145066, mean_q: 38.802547, mean_eps: 0.100000\n","     205723/2000000000: episode: 5572, duration: 3.335s, episode steps:  25, steps per second:   7, episode reward: 148.500, mean reward:  5.940 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 78.369802, mean_q: 38.498070, mean_eps: 0.100000\n","     205763/2000000000: episode: 5573, duration: 5.279s, episode steps:  40, steps per second:   8, episode reward: 59.400, mean reward:  1.485 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.762108, mean_q: 37.903898, mean_eps: 0.100000\n","     205802/2000000000: episode: 5574, duration: 5.223s, episode steps:  39, steps per second:   7, episode reward: 38.100, mean reward:  0.977 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 80.418614, mean_q: 38.209686, mean_eps: 0.100000\n","     205836/2000000000: episode: 5575, duration: 4.490s, episode steps:  34, steps per second:   8, episode reward:  3.500, mean reward:  0.103 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 83.240503, mean_q: 38.123498, mean_eps: 0.100000\n","     205872/2000000000: episode: 5576, duration: 4.717s, episode steps:  36, steps per second:   8, episode reward: 171.900, mean reward:  4.775 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 80.832218, mean_q: 38.684125, mean_eps: 0.100000\n","     205899/2000000000: episode: 5577, duration: 3.455s, episode steps:  27, steps per second:   8, episode reward: 40.300, mean reward:  1.493 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 84.817398, mean_q: 39.341419, mean_eps: 0.100000\n","     205930/2000000000: episode: 5578, duration: 3.878s, episode steps:  31, steps per second:   8, episode reward: 99.000, mean reward:  3.194 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 71.648542, mean_q: 37.865821, mean_eps: 0.100000\n","     205964/2000000000: episode: 5579, duration: 4.631s, episode steps:  34, steps per second:   7, episode reward: 155.600, mean reward:  4.576 [-20.000, 18.300], mean action: 1.000 [0.000, 2.000],  loss: 81.650227, mean_q: 38.678429, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     206001/2000000000: episode: 5580, duration: 4.947s, episode steps:  37, steps per second:   7, episode reward: 25.000, mean reward:  0.676 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 79.697123, mean_q: 38.520530, mean_eps: 0.100000\n","     206034/2000000000: episode: 5581, duration: 4.366s, episode steps:  33, steps per second:   8, episode reward: -0.700, mean reward: -0.021 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 78.464928, mean_q: 38.675358, mean_eps: 0.100000\n","     206064/2000000000: episode: 5582, duration: 3.965s, episode steps:  30, steps per second:   8, episode reward: 160.300, mean reward:  5.343 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 83.873949, mean_q: 38.828165, mean_eps: 0.100000\n","     206104/2000000000: episode: 5583, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: 29.300, mean reward:  0.733 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.190539, mean_q: 38.855262, mean_eps: 0.100000\n","     206132/2000000000: episode: 5584, duration: 3.525s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 79.660533, mean_q: 39.009347, mean_eps: 0.100000\n","     206163/2000000000: episode: 5585, duration: 3.681s, episode steps:  31, steps per second:   8, episode reward: 26.800, mean reward:  0.865 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 77.599311, mean_q: 38.117981, mean_eps: 0.100000\n","     206199/2000000000: episode: 5586, duration: 4.264s, episode steps:  36, steps per second:   8, episode reward: 98.800, mean reward:  2.744 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 76.976818, mean_q: 38.322685, mean_eps: 0.100000\n","     206234/2000000000: episode: 5587, duration: 4.410s, episode steps:  35, steps per second:   8, episode reward: -118.600, mean reward: -3.389 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.094155, mean_q: 38.609929, mean_eps: 0.100000\n","     206265/2000000000: episode: 5588, duration: 4.074s, episode steps:  31, steps per second:   8, episode reward: 17.000, mean reward:  0.548 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 71.984470, mean_q: 38.815707, mean_eps: 0.100000\n","     206292/2000000000: episode: 5589, duration: 3.672s, episode steps:  27, steps per second:   7, episode reward: 191.300, mean reward:  7.085 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 77.836620, mean_q: 38.403264, mean_eps: 0.100000\n","     206321/2000000000: episode: 5590, duration: 3.854s, episode steps:  29, steps per second:   8, episode reward: 131.900, mean reward:  4.548 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 84.165056, mean_q: 38.168073, mean_eps: 0.100000\n","     206361/2000000000: episode: 5591, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 14.600, mean reward:  0.365 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 76.172789, mean_q: 38.424698, mean_eps: 0.100000\n","     206393/2000000000: episode: 5592, duration: 4.182s, episode steps:  32, steps per second:   8, episode reward: -21.500, mean reward: -0.672 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 81.290071, mean_q: 38.233937, mean_eps: 0.100000\n","     206433/2000000000: episode: 5593, duration: 5.456s, episode steps:  40, steps per second:   7, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 81.881248, mean_q: 38.763078, mean_eps: 0.100000\n","     206458/2000000000: episode: 5594, duration: 3.250s, episode steps:  25, steps per second:   8, episode reward: 225.900, mean reward:  9.036 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 82.446187, mean_q: 38.812986, mean_eps: 0.100000\n","     206498/2000000000: episode: 5595, duration: 5.241s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.614524, mean_q: 38.913975, mean_eps: 0.100000\n","     206532/2000000000: episode: 5596, duration: 4.309s, episode steps:  34, steps per second:   8, episode reward: 16.600, mean reward:  0.488 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 72.537998, mean_q: 38.831735, mean_eps: 0.100000\n","     206572/2000000000: episode: 5597, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward:  7.500, mean reward:  0.188 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.531804, mean_q: 38.675378, mean_eps: 0.100000\n","     206609/2000000000: episode: 5598, duration: 4.908s, episode steps:  37, steps per second:   8, episode reward: 30.600, mean reward:  0.827 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 73.277649, mean_q: 38.608336, mean_eps: 0.100000\n","     206646/2000000000: episode: 5599, duration: 4.569s, episode steps:  37, steps per second:   8, episode reward: 178.100, mean reward:  4.814 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 79.578068, mean_q: 38.873810, mean_eps: 0.100000\n","     206686/2000000000: episode: 5600, duration: 5.089s, episode steps:  40, steps per second:   8, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.486603, mean_q: 38.084253, mean_eps: 0.100000\n","     206726/2000000000: episode: 5601, duration: 5.223s, episode steps:  40, steps per second:   8, episode reward: -52.000, mean reward: -1.300 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 77.107800, mean_q: 38.199321, mean_eps: 0.100000\n","     206758/2000000000: episode: 5602, duration: 4.116s, episode steps:  32, steps per second:   8, episode reward: -146.500, mean reward: -4.578 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 80.355733, mean_q: 38.965027, mean_eps: 0.100000\n","     206788/2000000000: episode: 5603, duration: 3.809s, episode steps:  30, steps per second:   8, episode reward: 120.700, mean reward:  4.023 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 71.876038, mean_q: 38.740127, mean_eps: 0.100000\n","     206822/2000000000: episode: 5604, duration: 4.501s, episode steps:  34, steps per second:   8, episode reward: -23.700, mean reward: -0.697 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 82.813599, mean_q: 38.118877, mean_eps: 0.100000\n","     206856/2000000000: episode: 5605, duration: 4.403s, episode steps:  34, steps per second:   8, episode reward: 53.900, mean reward:  1.585 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 72.315444, mean_q: 38.813524, mean_eps: 0.100000\n","     206896/2000000000: episode: 5606, duration: 5.353s, episode steps:  40, steps per second:   7, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.058324, mean_q: 38.530135, mean_eps: 0.100000\n","     206930/2000000000: episode: 5607, duration: 4.689s, episode steps:  34, steps per second:   7, episode reward: 211.900, mean reward:  6.232 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 79.059776, mean_q: 39.267457, mean_eps: 0.100000\n","     206969/2000000000: episode: 5608, duration: 5.260s, episode steps:  39, steps per second:   7, episode reward: 198.900, mean reward:  5.100 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 69.430260, mean_q: 38.288257, mean_eps: 0.100000\n","     207009/2000000000: episode: 5609, duration: 5.255s, episode steps:  40, steps per second:   8, episode reward: 35.900, mean reward:  0.898 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.700038, mean_q: 38.720373, mean_eps: 0.100000\n","     207044/2000000000: episode: 5610, duration: 4.818s, episode steps:  35, steps per second:   7, episode reward: 80.700, mean reward:  2.306 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 81.423492, mean_q: 38.070926, mean_eps: 0.100000\n","     207077/2000000000: episode: 5611, duration: 4.464s, episode steps:  33, steps per second:   7, episode reward: 176.600, mean reward:  5.352 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 75.150343, mean_q: 38.503400, mean_eps: 0.100000\n","     207117/2000000000: episode: 5612, duration: 5.270s, episode steps:  40, steps per second:   8, episode reward: 87.400, mean reward:  2.185 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.125044, mean_q: 38.226683, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     207154/2000000000: episode: 5613, duration: 4.572s, episode steps:  37, steps per second:   8, episode reward: -31.500, mean reward: -0.851 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 83.969468, mean_q: 38.819721, mean_eps: 0.100000\n","     207194/2000000000: episode: 5614, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: 38.400, mean reward:  0.960 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 79.971961, mean_q: 38.042407, mean_eps: 0.100000\n","     207234/2000000000: episode: 5615, duration: 5.094s, episode steps:  40, steps per second:   8, episode reward: -23.700, mean reward: -0.593 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 85.000218, mean_q: 38.014028, mean_eps: 0.100000\n","     207266/2000000000: episode: 5616, duration: 4.203s, episode steps:  32, steps per second:   8, episode reward: 48.000, mean reward:  1.500 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 79.202693, mean_q: 38.417482, mean_eps: 0.100000\n","     207290/2000000000: episode: 5617, duration: 3.389s, episode steps:  24, steps per second:   7, episode reward: -88.800, mean reward: -3.700 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 80.818814, mean_q: 38.133484, mean_eps: 0.100000\n","     207314/2000000000: episode: 5618, duration: 3.325s, episode steps:  24, steps per second:   7, episode reward: 107.300, mean reward:  4.471 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 75.669342, mean_q: 37.664642, mean_eps: 0.100000\n","     207351/2000000000: episode: 5619, duration: 4.701s, episode steps:  37, steps per second:   8, episode reward: 175.100, mean reward:  4.732 [-14.200, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 82.227058, mean_q: 38.885892, mean_eps: 0.100000\n","     207391/2000000000: episode: 5620, duration: 5.175s, episode steps:  40, steps per second:   8, episode reward: 103.500, mean reward:  2.587 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.718186, mean_q: 38.680746, mean_eps: 0.100000\n","     207431/2000000000: episode: 5621, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: -56.400, mean reward: -1.410 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.787887, mean_q: 38.577104, mean_eps: 0.100000\n","     207460/2000000000: episode: 5622, duration: 3.619s, episode steps:  29, steps per second:   8, episode reward: 247.800, mean reward:  8.545 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.093484, mean_q: 38.277362, mean_eps: 0.100000\n","     207499/2000000000: episode: 5623, duration: 5.125s, episode steps:  39, steps per second:   8, episode reward: 164.100, mean reward:  4.208 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 80.636682, mean_q: 38.296859, mean_eps: 0.100000\n","     207539/2000000000: episode: 5624, duration: 5.011s, episode steps:  40, steps per second:   8, episode reward: 41.000, mean reward:  1.025 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 72.229460, mean_q: 38.079002, mean_eps: 0.100000\n","     207573/2000000000: episode: 5625, duration: 4.219s, episode steps:  34, steps per second:   8, episode reward: 33.100, mean reward:  0.974 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 72.412964, mean_q: 38.690871, mean_eps: 0.100000\n","     207613/2000000000: episode: 5626, duration: 5.038s, episode steps:  40, steps per second:   8, episode reward: -27.000, mean reward: -0.675 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.745846, mean_q: 38.247566, mean_eps: 0.100000\n","     207637/2000000000: episode: 5627, duration: 3.151s, episode steps:  24, steps per second:   8, episode reward: 70.000, mean reward:  2.917 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 82.241774, mean_q: 38.236866, mean_eps: 0.100000\n","     207671/2000000000: episode: 5628, duration: 4.435s, episode steps:  34, steps per second:   8, episode reward: 66.900, mean reward:  1.968 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 75.019360, mean_q: 38.714049, mean_eps: 0.100000\n","     207703/2000000000: episode: 5629, duration: 4.073s, episode steps:  32, steps per second:   8, episode reward: 33.400, mean reward:  1.044 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 70.610762, mean_q: 39.582577, mean_eps: 0.100000\n","     207734/2000000000: episode: 5630, duration: 4.072s, episode steps:  31, steps per second:   8, episode reward: 39.100, mean reward:  1.261 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 75.890511, mean_q: 38.363657, mean_eps: 0.100000\n","     207774/2000000000: episode: 5631, duration: 5.227s, episode steps:  40, steps per second:   8, episode reward: 36.300, mean reward:  0.908 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 79.639429, mean_q: 38.432777, mean_eps: 0.100000\n","     207814/2000000000: episode: 5632, duration: 5.288s, episode steps:  40, steps per second:   8, episode reward: -42.900, mean reward: -1.073 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.754349, mean_q: 38.812632, mean_eps: 0.100000\n","     207851/2000000000: episode: 5633, duration: 4.832s, episode steps:  37, steps per second:   8, episode reward: 158.400, mean reward:  4.281 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 83.819261, mean_q: 39.318872, mean_eps: 0.100000\n","     207891/2000000000: episode: 5634, duration: 5.356s, episode steps:  40, steps per second:   7, episode reward: 80.500, mean reward:  2.012 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.871183, mean_q: 38.513083, mean_eps: 0.100000\n","     207931/2000000000: episode: 5635, duration: 5.167s, episode steps:  40, steps per second:   8, episode reward: 112.700, mean reward:  2.817 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.516918, mean_q: 37.905867, mean_eps: 0.100000\n","     207967/2000000000: episode: 5636, duration: 4.845s, episode steps:  36, steps per second:   7, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 82.644357, mean_q: 38.130722, mean_eps: 0.100000\n","     208007/2000000000: episode: 5637, duration: 5.712s, episode steps:  40, steps per second:   7, episode reward:  2.300, mean reward:  0.057 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.612467, mean_q: 38.322398, mean_eps: 0.100000\n","     208043/2000000000: episode: 5638, duration: 4.762s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 71.301239, mean_q: 38.201666, mean_eps: 0.100000\n","     208080/2000000000: episode: 5639, duration: 4.736s, episode steps:  37, steps per second:   8, episode reward: 145.600, mean reward:  3.935 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 79.266396, mean_q: 38.734432, mean_eps: 0.100000\n","     208114/2000000000: episode: 5640, duration: 4.146s, episode steps:  34, steps per second:   8, episode reward: -31.000, mean reward: -0.912 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 76.773064, mean_q: 38.880231, mean_eps: 0.100000\n","     208148/2000000000: episode: 5641, duration: 4.396s, episode steps:  34, steps per second:   8, episode reward: 31.700, mean reward:  0.932 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 86.641632, mean_q: 37.894975, mean_eps: 0.100000\n","     208177/2000000000: episode: 5642, duration: 3.783s, episode steps:  29, steps per second:   8, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 86.405445, mean_q: 38.302690, mean_eps: 0.100000\n","     208217/2000000000: episode: 5643, duration: 5.406s, episode steps:  40, steps per second:   7, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.741830, mean_q: 38.887392, mean_eps: 0.100000\n","     208257/2000000000: episode: 5644, duration: 5.177s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.552389, mean_q: 38.280406, mean_eps: 0.100000\n","     208297/2000000000: episode: 5645, duration: 5.372s, episode steps:  40, steps per second:   7, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.900220, mean_q: 38.316014, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     208337/2000000000: episode: 5646, duration: 5.420s, episode steps:  40, steps per second:   7, episode reward: 138.000, mean reward:  3.450 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 79.787454, mean_q: 38.357304, mean_eps: 0.100000\n","     208375/2000000000: episode: 5647, duration: 4.793s, episode steps:  38, steps per second:   8, episode reward: 54.200, mean reward:  1.426 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 83.538256, mean_q: 38.212573, mean_eps: 0.100000\n","     208415/2000000000: episode: 5648, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.754619, mean_q: 38.819417, mean_eps: 0.100000\n","     208455/2000000000: episode: 5649, duration: 5.204s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.007042, mean_q: 37.958897, mean_eps: 0.100000\n","     208492/2000000000: episode: 5650, duration: 4.787s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 75.197288, mean_q: 38.720009, mean_eps: 0.100000\n","     208532/2000000000: episode: 5651, duration: 5.379s, episode steps:  40, steps per second:   7, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.947643, mean_q: 38.731216, mean_eps: 0.100000\n","     208563/2000000000: episode: 5652, duration: 4.208s, episode steps:  31, steps per second:   7, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 73.830051, mean_q: 39.068462, mean_eps: 0.100000\n","     208601/2000000000: episode: 5653, duration: 4.926s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 80.479758, mean_q: 37.924095, mean_eps: 0.100000\n","     208637/2000000000: episode: 5654, duration: 4.654s, episode steps:  36, steps per second:   8, episode reward: -44.900, mean reward: -1.247 [-20.000, 18.000], mean action: 0.944 [0.000, 2.000],  loss: 78.808473, mean_q: 38.692758, mean_eps: 0.100000\n","     208677/2000000000: episode: 5655, duration: 5.118s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.872199, mean_q: 38.102012, mean_eps: 0.100000\n","     208717/2000000000: episode: 5656, duration: 5.217s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.929537, mean_q: 38.667989, mean_eps: 0.100000\n","     208755/2000000000: episode: 5657, duration: 5.285s, episode steps:  38, steps per second:   7, episode reward: 18.400, mean reward:  0.484 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 78.996445, mean_q: 38.951440, mean_eps: 0.100000\n","     208792/2000000000: episode: 5658, duration: 5.001s, episode steps:  37, steps per second:   7, episode reward: -87.800, mean reward: -2.373 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 78.861688, mean_q: 38.509986, mean_eps: 0.100000\n","     208826/2000000000: episode: 5659, duration: 4.719s, episode steps:  34, steps per second:   7, episode reward: -10.700, mean reward: -0.315 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 74.531759, mean_q: 38.176814, mean_eps: 0.100000\n","     208864/2000000000: episode: 5660, duration: 5.059s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.527046, mean_q: 38.594333, mean_eps: 0.100000\n","     208902/2000000000: episode: 5661, duration: 4.735s, episode steps:  38, steps per second:   8, episode reward: -0.200, mean reward: -0.005 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 73.847378, mean_q: 38.317021, mean_eps: 0.100000\n","     208942/2000000000: episode: 5662, duration: 4.761s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.630293, mean_q: 38.486686, mean_eps: 0.100000\n","     208980/2000000000: episode: 5663, duration: 4.505s, episode steps:  38, steps per second:   8, episode reward: 64.800, mean reward:  1.705 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 81.465796, mean_q: 38.108236, mean_eps: 0.100000\n","     209017/2000000000: episode: 5664, duration: 4.508s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.234600, mean_q: 38.698832, mean_eps: 0.100000\n","     209057/2000000000: episode: 5665, duration: 5.126s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.567824, mean_q: 37.659637, mean_eps: 0.100000\n","     209092/2000000000: episode: 5666, duration: 4.409s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 79.880191, mean_q: 38.639228, mean_eps: 0.100000\n","     209128/2000000000: episode: 5667, duration: 4.532s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 72.906594, mean_q: 38.840685, mean_eps: 0.100000\n","     209165/2000000000: episode: 5668, duration: 4.853s, episode steps:  37, steps per second:   8, episode reward: 110.400, mean reward:  2.984 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 81.346206, mean_q: 38.430267, mean_eps: 0.100000\n","     209201/2000000000: episode: 5669, duration: 4.660s, episode steps:  36, steps per second:   8, episode reward: -4.300, mean reward: -0.119 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 76.330648, mean_q: 39.164010, mean_eps: 0.100000\n","     209233/2000000000: episode: 5670, duration: 4.174s, episode steps:  32, steps per second:   8, episode reward:  0.500, mean reward:  0.016 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 75.417670, mean_q: 39.536463, mean_eps: 0.100000\n","     209272/2000000000: episode: 5671, duration: 5.160s, episode steps:  39, steps per second:   8, episode reward: 61.700, mean reward:  1.582 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 80.141601, mean_q: 38.304197, mean_eps: 0.100000\n","     209303/2000000000: episode: 5672, duration: 4.129s, episode steps:  31, steps per second:   8, episode reward: 50.900, mean reward:  1.642 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 75.419778, mean_q: 39.155218, mean_eps: 0.100000\n","     209331/2000000000: episode: 5673, duration: 3.557s, episode steps:  28, steps per second:   8, episode reward: 101.700, mean reward:  3.632 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 73.979651, mean_q: 38.241843, mean_eps: 0.100000\n","     209371/2000000000: episode: 5674, duration: 4.987s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.273116, mean_q: 38.436542, mean_eps: 0.100000\n","     209401/2000000000: episode: 5675, duration: 4.003s, episode steps:  30, steps per second:   7, episode reward: 82.500, mean reward:  2.750 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 75.744105, mean_q: 38.771430, mean_eps: 0.100000\n","     209441/2000000000: episode: 5676, duration: 5.250s, episode steps:  40, steps per second:   8, episode reward: 112.300, mean reward:  2.807 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.924334, mean_q: 37.967897, mean_eps: 0.100000\n","     209478/2000000000: episode: 5677, duration: 4.546s, episode steps:  37, steps per second:   8, episode reward: 31.400, mean reward:  0.849 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 80.656546, mean_q: 38.111115, mean_eps: 0.100000\n","     209506/2000000000: episode: 5678, duration: 3.478s, episode steps:  28, steps per second:   8, episode reward: 140.400, mean reward:  5.014 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 78.265035, mean_q: 38.560724, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     209545/2000000000: episode: 5679, duration: 4.913s, episode steps:  39, steps per second:   8, episode reward: 115.200, mean reward:  2.954 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 78.289329, mean_q: 38.666579, mean_eps: 0.100000\n","     209570/2000000000: episode: 5680, duration: 3.246s, episode steps:  25, steps per second:   8, episode reward: -68.600, mean reward: -2.744 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 85.581499, mean_q: 38.845665, mean_eps: 0.100000\n","     209605/2000000000: episode: 5681, duration: 4.427s, episode steps:  35, steps per second:   8, episode reward: 60.400, mean reward:  1.726 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.865811, mean_q: 39.006178, mean_eps: 0.100000\n","     209645/2000000000: episode: 5682, duration: 4.831s, episode steps:  40, steps per second:   8, episode reward: 85.600, mean reward:  2.140 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.268608, mean_q: 38.453853, mean_eps: 0.100000\n","     209684/2000000000: episode: 5683, duration: 4.842s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 88.429538, mean_q: 38.183091, mean_eps: 0.100000\n","     209717/2000000000: episode: 5684, duration: 4.201s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 94.736824, mean_q: 39.120347, mean_eps: 0.100000\n","     209754/2000000000: episode: 5685, duration: 4.771s, episode steps:  37, steps per second:   8, episode reward: -1.400, mean reward: -0.038 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 75.176139, mean_q: 38.195132, mean_eps: 0.100000\n","     209794/2000000000: episode: 5686, duration: 4.913s, episode steps:  40, steps per second:   8, episode reward: 29.800, mean reward:  0.745 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.608908, mean_q: 38.420354, mean_eps: 0.100000\n","     209831/2000000000: episode: 5687, duration: 4.726s, episode steps:  37, steps per second:   8, episode reward: -90.400, mean reward: -2.443 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 77.167600, mean_q: 38.870094, mean_eps: 0.100000\n","     209871/2000000000: episode: 5688, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward: 106.000, mean reward:  2.650 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 82.584312, mean_q: 38.130307, mean_eps: 0.100000\n","     209905/2000000000: episode: 5689, duration: 4.233s, episode steps:  34, steps per second:   8, episode reward: 153.700, mean reward:  4.521 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 74.325503, mean_q: 38.483820, mean_eps: 0.100000\n","     209945/2000000000: episode: 5690, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: 109.200, mean reward:  2.730 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.985367, mean_q: 38.812156, mean_eps: 0.100000\n","     209983/2000000000: episode: 5691, duration: 4.820s, episode steps:  38, steps per second:   8, episode reward: 75.600, mean reward:  1.989 [-20.000, 18.000], mean action: 1.079 [0.000, 2.000],  loss: 78.824366, mean_q: 37.894837, mean_eps: 0.100000\n","     210021/2000000000: episode: 5692, duration: 4.678s, episode steps:  38, steps per second:   8, episode reward: -76.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 72.401022, mean_q: 38.415525, mean_eps: 0.100000\n","     210061/2000000000: episode: 5693, duration: 5.150s, episode steps:  40, steps per second:   8, episode reward: 44.200, mean reward:  1.105 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 88.815257, mean_q: 38.368772, mean_eps: 0.100000\n","     210096/2000000000: episode: 5694, duration: 4.498s, episode steps:  35, steps per second:   8, episode reward: -23.700, mean reward: -0.677 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 84.653344, mean_q: 38.383355, mean_eps: 0.100000\n","     210135/2000000000: episode: 5695, duration: 5.065s, episode steps:  39, steps per second:   8, episode reward: 177.100, mean reward:  4.541 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 80.747612, mean_q: 37.912486, mean_eps: 0.100000\n","     210162/2000000000: episode: 5696, duration: 3.628s, episode steps:  27, steps per second:   7, episode reward: 153.500, mean reward:  5.685 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.896887, mean_q: 38.644755, mean_eps: 0.100000\n","     210199/2000000000: episode: 5697, duration: 4.712s, episode steps:  37, steps per second:   8, episode reward: 128.900, mean reward:  3.484 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 86.263632, mean_q: 39.268774, mean_eps: 0.100000\n","     210229/2000000000: episode: 5698, duration: 3.894s, episode steps:  30, steps per second:   8, episode reward: 32.500, mean reward:  1.083 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 82.917241, mean_q: 38.458181, mean_eps: 0.100000\n","     210267/2000000000: episode: 5699, duration: 4.892s, episode steps:  38, steps per second:   8, episode reward: 95.900, mean reward:  2.524 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 76.558380, mean_q: 38.963975, mean_eps: 0.100000\n","     210301/2000000000: episode: 5700, duration: 4.374s, episode steps:  34, steps per second:   8, episode reward: -13.900, mean reward: -0.409 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 84.089051, mean_q: 37.503433, mean_eps: 0.100000\n","     210341/2000000000: episode: 5701, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.571360, mean_q: 38.375420, mean_eps: 0.100000\n","     210381/2000000000: episode: 5702, duration: 5.058s, episode steps:  40, steps per second:   8, episode reward: 124.500, mean reward:  3.113 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.781973, mean_q: 38.454527, mean_eps: 0.100000\n","     210419/2000000000: episode: 5703, duration: 4.525s, episode steps:  38, steps per second:   8, episode reward:  0.900, mean reward:  0.024 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 81.189410, mean_q: 38.075888, mean_eps: 0.100000\n","     210455/2000000000: episode: 5704, duration: 4.390s, episode steps:  36, steps per second:   8, episode reward: 10.100, mean reward:  0.281 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 79.629274, mean_q: 38.522678, mean_eps: 0.100000\n","     210495/2000000000: episode: 5705, duration: 4.785s, episode steps:  40, steps per second:   8, episode reward: 13.200, mean reward:  0.330 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.912129, mean_q: 38.501418, mean_eps: 0.100000\n","     210521/2000000000: episode: 5706, duration: 3.203s, episode steps:  26, steps per second:   8, episode reward: 36.500, mean reward:  1.404 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.466186, mean_q: 38.343491, mean_eps: 0.100000\n","     210554/2000000000: episode: 5707, duration: 4.087s, episode steps:  33, steps per second:   8, episode reward: 51.700, mean reward:  1.567 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 76.249227, mean_q: 38.044026, mean_eps: 0.100000\n","     210594/2000000000: episode: 5708, duration: 4.942s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.797333, mean_q: 39.168338, mean_eps: 0.100000\n","     210625/2000000000: episode: 5709, duration: 4.099s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.242554, mean_q: 39.794229, mean_eps: 0.100000\n","     210652/2000000000: episode: 5710, duration: 3.481s, episode steps:  27, steps per second:   8, episode reward: 52.900, mean reward:  1.959 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 79.073701, mean_q: 39.606431, mean_eps: 0.100000\n","     210688/2000000000: episode: 5711, duration: 4.622s, episode steps:  36, steps per second:   8, episode reward: -58.000, mean reward: -1.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 76.574111, mean_q: 38.518023, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     210717/2000000000: episode: 5712, duration: 3.886s, episode steps:  29, steps per second:   7, episode reward: 72.600, mean reward:  2.503 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 69.693280, mean_q: 38.818482, mean_eps: 0.100000\n","     210755/2000000000: episode: 5713, duration: 5.159s, episode steps:  38, steps per second:   7, episode reward: -69.700, mean reward: -1.834 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 83.094395, mean_q: 39.398676, mean_eps: 0.100000\n","     210795/2000000000: episode: 5714, duration: 5.461s, episode steps:  40, steps per second:   7, episode reward: -23.800, mean reward: -0.595 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 76.189061, mean_q: 38.784737, mean_eps: 0.100000\n","     210833/2000000000: episode: 5715, duration: 4.970s, episode steps:  38, steps per second:   8, episode reward: 98.900, mean reward:  2.603 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.756134, mean_q: 38.458888, mean_eps: 0.100000\n","     210869/2000000000: episode: 5716, duration: 4.629s, episode steps:  36, steps per second:   8, episode reward: -131.600, mean reward: -3.656 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 85.207555, mean_q: 39.030812, mean_eps: 0.100000\n","     210905/2000000000: episode: 5717, duration: 4.623s, episode steps:  36, steps per second:   8, episode reward: 37.600, mean reward:  1.044 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 83.527777, mean_q: 39.009505, mean_eps: 0.100000\n","     210939/2000000000: episode: 5718, duration: 4.301s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 78.399291, mean_q: 38.280386, mean_eps: 0.100000\n","     210979/2000000000: episode: 5719, duration: 4.964s, episode steps:  40, steps per second:   8, episode reward: 57.100, mean reward:  1.428 [-20.000, 19.100], mean action: 1.375 [0.000, 2.000],  loss: 77.984285, mean_q: 38.782251, mean_eps: 0.100000\n","     211010/2000000000: episode: 5720, duration: 3.859s, episode steps:  31, steps per second:   8, episode reward:  7.000, mean reward:  0.226 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 70.644524, mean_q: 38.471219, mean_eps: 0.100000\n","     211039/2000000000: episode: 5721, duration: 3.577s, episode steps:  29, steps per second:   8, episode reward: -80.700, mean reward: -2.783 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 83.078132, mean_q: 39.096563, mean_eps: 0.100000\n","     211074/2000000000: episode: 5722, duration: 4.229s, episode steps:  35, steps per second:   8, episode reward: -103.500, mean reward: -2.957 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 73.912879, mean_q: 39.437759, mean_eps: 0.100000\n","     211106/2000000000: episode: 5723, duration: 3.947s, episode steps:  32, steps per second:   8, episode reward: 146.400, mean reward:  4.575 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.448473, mean_q: 38.433276, mean_eps: 0.100000\n","     211144/2000000000: episode: 5724, duration: 4.845s, episode steps:  38, steps per second:   8, episode reward: -11.400, mean reward: -0.300 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 73.707659, mean_q: 38.468050, mean_eps: 0.100000\n","     211184/2000000000: episode: 5725, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: -46.700, mean reward: -1.167 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.716308, mean_q: 39.077181, mean_eps: 0.100000\n","     211210/2000000000: episode: 5726, duration: 3.172s, episode steps:  26, steps per second:   8, episode reward: 47.300, mean reward:  1.819 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 82.842582, mean_q: 38.988859, mean_eps: 0.100000\n","     211243/2000000000: episode: 5727, duration: 4.292s, episode steps:  33, steps per second:   8, episode reward: -12.500, mean reward: -0.379 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.096412, mean_q: 38.341765, mean_eps: 0.100000\n","     211272/2000000000: episode: 5728, duration: 3.722s, episode steps:  29, steps per second:   8, episode reward: 147.700, mean reward:  5.093 [-20.000, 18.000], mean action: 1.138 [0.000, 2.000],  loss: 74.782267, mean_q: 39.492866, mean_eps: 0.100000\n","     211306/2000000000: episode: 5729, duration: 4.485s, episode steps:  34, steps per second:   8, episode reward: 88.800, mean reward:  2.612 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 71.252057, mean_q: 38.306819, mean_eps: 0.100000\n","     211339/2000000000: episode: 5730, duration: 4.115s, episode steps:  33, steps per second:   8, episode reward: 166.500, mean reward:  5.045 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 75.216461, mean_q: 38.939081, mean_eps: 0.100000\n","     211376/2000000000: episode: 5731, duration: 4.773s, episode steps:  37, steps per second:   8, episode reward: 41.200, mean reward:  1.114 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.140678, mean_q: 39.427948, mean_eps: 0.100000\n","     211416/2000000000: episode: 5732, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: 64.500, mean reward:  1.612 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 76.822537, mean_q: 38.865160, mean_eps: 0.100000\n","     211451/2000000000: episode: 5733, duration: 4.593s, episode steps:  35, steps per second:   8, episode reward: -17.200, mean reward: -0.491 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 79.886296, mean_q: 38.931014, mean_eps: 0.100000\n","     211486/2000000000: episode: 5734, duration: 4.500s, episode steps:  35, steps per second:   8, episode reward: 179.500, mean reward:  5.129 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 73.930245, mean_q: 38.643260, mean_eps: 0.100000\n","     211516/2000000000: episode: 5735, duration: 3.797s, episode steps:  30, steps per second:   8, episode reward: -1.900, mean reward: -0.063 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 76.047329, mean_q: 38.647709, mean_eps: 0.100000\n","     211556/2000000000: episode: 5736, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: 125.200, mean reward:  3.130 [-17.200, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 78.863953, mean_q: 39.057793, mean_eps: 0.100000\n","     211586/2000000000: episode: 5737, duration: 4.219s, episode steps:  30, steps per second:   7, episode reward: 118.300, mean reward:  3.943 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.075201, mean_q: 38.576903, mean_eps: 0.100000\n","     211626/2000000000: episode: 5738, duration: 5.247s, episode steps:  40, steps per second:   8, episode reward: -22.100, mean reward: -0.553 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 90.206407, mean_q: 38.399960, mean_eps: 0.100000\n","     211666/2000000000: episode: 5739, duration: 5.036s, episode steps:  40, steps per second:   8, episode reward: -24.400, mean reward: -0.610 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.950490, mean_q: 37.563917, mean_eps: 0.100000\n","     211700/2000000000: episode: 5740, duration: 4.310s, episode steps:  34, steps per second:   8, episode reward: 110.900, mean reward:  3.262 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 73.785148, mean_q: 38.473640, mean_eps: 0.100000\n","     211728/2000000000: episode: 5741, duration: 3.657s, episode steps:  28, steps per second:   8, episode reward: 114.000, mean reward:  4.071 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 73.182165, mean_q: 38.233135, mean_eps: 0.100000\n","     211768/2000000000: episode: 5742, duration: 5.349s, episode steps:  40, steps per second:   7, episode reward: 143.000, mean reward:  3.575 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.010713, mean_q: 38.781239, mean_eps: 0.100000\n","     211808/2000000000: episode: 5743, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: 87.900, mean reward:  2.197 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.592525, mean_q: 38.179135, mean_eps: 0.100000\n","     211848/2000000000: episode: 5744, duration: 5.289s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.181974, mean_q: 39.541192, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     211878/2000000000: episode: 5745, duration: 4.106s, episode steps:  30, steps per second:   7, episode reward: -7.800, mean reward: -0.260 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 76.803253, mean_q: 38.150572, mean_eps: 0.100000\n","     211917/2000000000: episode: 5746, duration: 4.980s, episode steps:  39, steps per second:   8, episode reward:  4.500, mean reward:  0.115 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 85.672740, mean_q: 38.974850, mean_eps: 0.100000\n","     211957/2000000000: episode: 5747, duration: 5.291s, episode steps:  40, steps per second:   8, episode reward: 51.800, mean reward:  1.295 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 75.929084, mean_q: 38.696239, mean_eps: 0.100000\n","     211989/2000000000: episode: 5748, duration: 4.145s, episode steps:  32, steps per second:   8, episode reward: -38.700, mean reward: -1.209 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 74.295642, mean_q: 39.675270, mean_eps: 0.100000\n","     212029/2000000000: episode: 5749, duration: 5.005s, episode steps:  40, steps per second:   8, episode reward: 90.800, mean reward:  2.270 [-20.000, 18.100], mean action: 1.375 [0.000, 2.000],  loss: 77.548114, mean_q: 38.992084, mean_eps: 0.100000\n","     212069/2000000000: episode: 5750, duration: 5.090s, episode steps:  40, steps per second:   8, episode reward: 149.900, mean reward:  3.748 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.794808, mean_q: 38.742116, mean_eps: 0.100000\n","     212109/2000000000: episode: 5751, duration: 5.005s, episode steps:  40, steps per second:   8, episode reward: 72.600, mean reward:  1.815 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.475511, mean_q: 38.389059, mean_eps: 0.100000\n","     212143/2000000000: episode: 5752, duration: 4.378s, episode steps:  34, steps per second:   8, episode reward: 195.100, mean reward:  5.738 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 81.132175, mean_q: 38.347616, mean_eps: 0.100000\n","     212183/2000000000: episode: 5753, duration: 5.471s, episode steps:  40, steps per second:   7, episode reward: -34.000, mean reward: -0.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 76.237666, mean_q: 38.925936, mean_eps: 0.100000\n","     212221/2000000000: episode: 5754, duration: 5.064s, episode steps:  38, steps per second:   8, episode reward: 97.200, mean reward:  2.558 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 75.297913, mean_q: 38.127951, mean_eps: 0.100000\n","     212260/2000000000: episode: 5755, duration: 4.886s, episode steps:  39, steps per second:   8, episode reward: -90.800, mean reward: -2.328 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 77.612100, mean_q: 38.654704, mean_eps: 0.100000\n","     212300/2000000000: episode: 5756, duration: 5.106s, episode steps:  40, steps per second:   8, episode reward: 117.300, mean reward:  2.932 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 81.734865, mean_q: 38.344586, mean_eps: 0.100000\n","     212333/2000000000: episode: 5757, duration: 4.285s, episode steps:  33, steps per second:   8, episode reward: 80.800, mean reward:  2.448 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.549790, mean_q: 38.923023, mean_eps: 0.100000\n","     212366/2000000000: episode: 5758, duration: 4.164s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.288923, mean_q: 38.211942, mean_eps: 0.100000\n","     212401/2000000000: episode: 5759, duration: 4.533s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 79.048884, mean_q: 39.315486, mean_eps: 0.100000\n","     212434/2000000000: episode: 5760, duration: 4.216s, episode steps:  33, steps per second:   8, episode reward: -96.000, mean reward: -2.909 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 83.475524, mean_q: 39.133241, mean_eps: 0.100000\n","     212461/2000000000: episode: 5761, duration: 3.587s, episode steps:  27, steps per second:   8, episode reward: 60.400, mean reward:  2.237 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.191043, mean_q: 37.831596, mean_eps: 0.100000\n","     212501/2000000000: episode: 5762, duration: 5.087s, episode steps:  40, steps per second:   8, episode reward: 21.200, mean reward:  0.530 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 71.320998, mean_q: 38.783835, mean_eps: 0.100000\n","     212537/2000000000: episode: 5763, duration: 4.818s, episode steps:  36, steps per second:   7, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.804298, mean_q: 37.635253, mean_eps: 0.100000\n","     212568/2000000000: episode: 5764, duration: 4.193s, episode steps:  31, steps per second:   7, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.614428, mean_q: 39.560135, mean_eps: 0.100000\n","     212608/2000000000: episode: 5765, duration: 5.358s, episode steps:  40, steps per second:   7, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 71.592905, mean_q: 38.336505, mean_eps: 0.100000\n","     212648/2000000000: episode: 5766, duration: 5.280s, episode steps:  40, steps per second:   8, episode reward: 63.100, mean reward:  1.577 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.603245, mean_q: 39.130844, mean_eps: 0.100000\n","     212688/2000000000: episode: 5767, duration: 5.007s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 74.753200, mean_q: 38.217140, mean_eps: 0.100000\n","     212720/2000000000: episode: 5768, duration: 4.026s, episode steps:  32, steps per second:   8, episode reward: -96.000, mean reward: -3.000 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 76.684288, mean_q: 38.283337, mean_eps: 0.100000\n","     212760/2000000000: episode: 5769, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 89.771168, mean_q: 38.183661, mean_eps: 0.100000\n","     212800/2000000000: episode: 5770, duration: 5.185s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.845939, mean_q: 38.617252, mean_eps: 0.100000\n","     212830/2000000000: episode: 5771, duration: 3.785s, episode steps:  30, steps per second:   8, episode reward: 196.000, mean reward:  6.533 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.073853, mean_q: 38.150872, mean_eps: 0.100000\n","     212870/2000000000: episode: 5772, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: -4.900, mean reward: -0.122 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.872379, mean_q: 39.038447, mean_eps: 0.100000\n","     212910/2000000000: episode: 5773, duration: 5.011s, episode steps:  40, steps per second:   8, episode reward: 23.800, mean reward:  0.595 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.497364, mean_q: 37.998073, mean_eps: 0.100000\n","     212942/2000000000: episode: 5774, duration: 4.128s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 85.399428, mean_q: 39.269751, mean_eps: 0.100000\n","     212982/2000000000: episode: 5775, duration: 5.025s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.643751, mean_q: 38.133119, mean_eps: 0.100000\n","     213022/2000000000: episode: 5776, duration: 5.197s, episode steps:  40, steps per second:   8, episode reward: 17.600, mean reward:  0.440 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 73.020296, mean_q: 38.940211, mean_eps: 0.100000\n","     213052/2000000000: episode: 5777, duration: 3.883s, episode steps:  30, steps per second:   8, episode reward: 94.800, mean reward:  3.160 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 73.873489, mean_q: 38.681341, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     213092/2000000000: episode: 5778, duration: 4.978s, episode steps:  40, steps per second:   8, episode reward: -6.200, mean reward: -0.155 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.417866, mean_q: 38.553833, mean_eps: 0.100000\n","     213127/2000000000: episode: 5779, duration: 4.290s, episode steps:  35, steps per second:   8, episode reward: 166.400, mean reward:  4.754 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.455164, mean_q: 39.060178, mean_eps: 0.100000\n","     213167/2000000000: episode: 5780, duration: 4.835s, episode steps:  40, steps per second:   8, episode reward: -108.500, mean reward: -2.713 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.096810, mean_q: 38.584877, mean_eps: 0.100000\n","     213207/2000000000: episode: 5781, duration: 4.839s, episode steps:  40, steps per second:   8, episode reward: -131.600, mean reward: -3.290 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.985918, mean_q: 39.009291, mean_eps: 0.100000\n","     213234/2000000000: episode: 5782, duration: 3.452s, episode steps:  27, steps per second:   8, episode reward: 109.600, mean reward:  4.059 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.269255, mean_q: 39.582751, mean_eps: 0.100000\n","     213267/2000000000: episode: 5783, duration: 4.340s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 75.312822, mean_q: 38.323911, mean_eps: 0.100000\n","     213307/2000000000: episode: 5784, duration: 5.026s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.816112, mean_q: 38.836348, mean_eps: 0.100000\n","     213347/2000000000: episode: 5785, duration: 4.998s, episode steps:  40, steps per second:   8, episode reward: 74.000, mean reward:  1.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.859123, mean_q: 37.556524, mean_eps: 0.100000\n","     213383/2000000000: episode: 5786, duration: 4.447s, episode steps:  36, steps per second:   8, episode reward: 98.000, mean reward:  2.722 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.334870, mean_q: 38.947959, mean_eps: 0.100000\n","     213423/2000000000: episode: 5787, duration: 5.200s, episode steps:  40, steps per second:   8, episode reward: 72.500, mean reward:  1.812 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.650756, mean_q: 38.424023, mean_eps: 0.100000\n","     213450/2000000000: episode: 5788, duration: 3.366s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 66.779706, mean_q: 38.820188, mean_eps: 0.100000\n","     213481/2000000000: episode: 5789, duration: 3.953s, episode steps:  31, steps per second:   8, episode reward: 17.600, mean reward:  0.568 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 86.235882, mean_q: 37.722601, mean_eps: 0.100000\n","     213513/2000000000: episode: 5790, duration: 4.024s, episode steps:  32, steps per second:   8, episode reward: 157.400, mean reward:  4.919 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 80.790755, mean_q: 39.323977, mean_eps: 0.100000\n","     213553/2000000000: episode: 5791, duration: 5.027s, episode steps:  40, steps per second:   8, episode reward: 84.100, mean reward:  2.103 [-20.000, 18.000], mean action: 1.650 [0.000, 2.000],  loss: 75.296745, mean_q: 38.133955, mean_eps: 0.100000\n","     213593/2000000000: episode: 5792, duration: 5.051s, episode steps:  40, steps per second:   8, episode reward: 41.600, mean reward:  1.040 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.220438, mean_q: 37.719325, mean_eps: 0.100000\n","     213633/2000000000: episode: 5793, duration: 4.838s, episode steps:  40, steps per second:   8, episode reward: -9.000, mean reward: -0.225 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.428070, mean_q: 39.003522, mean_eps: 0.100000\n","     213673/2000000000: episode: 5794, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 115.900, mean reward:  2.897 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.359237, mean_q: 38.587965, mean_eps: 0.100000\n","     213710/2000000000: episode: 5795, duration: 4.935s, episode steps:  37, steps per second:   7, episode reward: -25.500, mean reward: -0.689 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 74.704489, mean_q: 38.493206, mean_eps: 0.100000\n","     213740/2000000000: episode: 5796, duration: 4.064s, episode steps:  30, steps per second:   7, episode reward: 148.000, mean reward:  4.933 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.260157, mean_q: 38.976241, mean_eps: 0.100000\n","     213778/2000000000: episode: 5797, duration: 5.188s, episode steps:  38, steps per second:   7, episode reward: -144.500, mean reward: -3.803 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.261989, mean_q: 37.741245, mean_eps: 0.100000\n","     213818/2000000000: episode: 5798, duration: 5.332s, episode steps:  40, steps per second:   8, episode reward: -8.400, mean reward: -0.210 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.245214, mean_q: 38.944742, mean_eps: 0.100000\n","     213844/2000000000: episode: 5799, duration: 3.706s, episode steps:  26, steps per second:   7, episode reward: 18.000, mean reward:  0.692 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 78.304775, mean_q: 38.789730, mean_eps: 0.100000\n","     213877/2000000000: episode: 5800, duration: 4.339s, episode steps:  33, steps per second:   8, episode reward: 40.600, mean reward:  1.230 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 84.913382, mean_q: 39.796003, mean_eps: 0.100000\n","     213917/2000000000: episode: 5801, duration: 5.038s, episode steps:  40, steps per second:   8, episode reward: 173.400, mean reward:  4.335 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.479078, mean_q: 38.494133, mean_eps: 0.100000\n","     213957/2000000000: episode: 5802, duration: 4.926s, episode steps:  40, steps per second:   8, episode reward: 115.100, mean reward:  2.878 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 81.799088, mean_q: 38.431031, mean_eps: 0.100000\n","     213997/2000000000: episode: 5803, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: -122.900, mean reward: -3.073 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 74.152929, mean_q: 39.341498, mean_eps: 0.100000\n","     214027/2000000000: episode: 5804, duration: 3.773s, episode steps:  30, steps per second:   8, episode reward: 32.200, mean reward:  1.073 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 77.100464, mean_q: 38.394636, mean_eps: 0.100000\n","     214058/2000000000: episode: 5805, duration: 4.016s, episode steps:  31, steps per second:   8, episode reward: 102.100, mean reward:  3.294 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.620226, mean_q: 37.915693, mean_eps: 0.100000\n","     214098/2000000000: episode: 5806, duration: 5.334s, episode steps:  40, steps per second:   7, episode reward: 174.600, mean reward:  4.365 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.084254, mean_q: 38.808480, mean_eps: 0.100000\n","     214135/2000000000: episode: 5807, duration: 4.791s, episode steps:  37, steps per second:   8, episode reward: 101.200, mean reward:  2.735 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 76.572282, mean_q: 38.411635, mean_eps: 0.100000\n","     214173/2000000000: episode: 5808, duration: 4.823s, episode steps:  38, steps per second:   8, episode reward: 134.500, mean reward:  3.539 [-20.000, 18.000], mean action: 1.421 [0.000, 2.000],  loss: 77.964679, mean_q: 39.173176, mean_eps: 0.100000\n","     214213/2000000000: episode: 5809, duration: 5.058s, episode steps:  40, steps per second:   8, episode reward:  0.300, mean reward:  0.007 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.935907, mean_q: 38.775900, mean_eps: 0.100000\n","     214253/2000000000: episode: 5810, duration: 5.271s, episode steps:  40, steps per second:   8, episode reward: 163.500, mean reward:  4.088 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.167093, mean_q: 38.595282, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     214282/2000000000: episode: 5811, duration: 4.003s, episode steps:  29, steps per second:   7, episode reward: -73.300, mean reward: -2.528 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 77.575790, mean_q: 39.260773, mean_eps: 0.100000\n","     214307/2000000000: episode: 5812, duration: 3.247s, episode steps:  25, steps per second:   8, episode reward: 99.800, mean reward:  3.992 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 69.072047, mean_q: 38.371362, mean_eps: 0.100000\n","     214347/2000000000: episode: 5813, duration: 5.109s, episode steps:  40, steps per second:   8, episode reward: -43.900, mean reward: -1.097 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 77.501601, mean_q: 38.277265, mean_eps: 0.100000\n","     214381/2000000000: episode: 5814, duration: 4.372s, episode steps:  34, steps per second:   8, episode reward: 142.300, mean reward:  4.185 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 75.411304, mean_q: 38.170965, mean_eps: 0.100000\n","     214416/2000000000: episode: 5815, duration: 4.558s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 72.481945, mean_q: 38.662290, mean_eps: 0.100000\n","     214454/2000000000: episode: 5816, duration: 4.821s, episode steps:  38, steps per second:   8, episode reward: 160.300, mean reward:  4.218 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 79.236876, mean_q: 39.030842, mean_eps: 0.100000\n","     214483/2000000000: episode: 5817, duration: 3.606s, episode steps:  29, steps per second:   8, episode reward: 193.700, mean reward:  6.679 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 75.496974, mean_q: 39.382437, mean_eps: 0.100000\n","     214517/2000000000: episode: 5818, duration: 4.397s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 79.461700, mean_q: 38.859388, mean_eps: 0.100000\n","     214557/2000000000: episode: 5819, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: 230.800, mean reward:  5.770 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 74.860465, mean_q: 38.988414, mean_eps: 0.100000\n","     214584/2000000000: episode: 5820, duration: 3.503s, episode steps:  27, steps per second:   8, episode reward: 52.800, mean reward:  1.956 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 82.100461, mean_q: 38.434130, mean_eps: 0.100000\n","     214618/2000000000: episode: 5821, duration: 4.403s, episode steps:  34, steps per second:   8, episode reward: 74.900, mean reward:  2.203 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 69.720838, mean_q: 37.897156, mean_eps: 0.100000\n","     214652/2000000000: episode: 5822, duration: 4.544s, episode steps:  34, steps per second:   7, episode reward: 41.800, mean reward:  1.229 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 78.798808, mean_q: 37.984206, mean_eps: 0.100000\n","     214692/2000000000: episode: 5823, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: 25.200, mean reward:  0.630 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.723525, mean_q: 38.205172, mean_eps: 0.100000\n","     214732/2000000000: episode: 5824, duration: 5.121s, episode steps:  40, steps per second:   8, episode reward: 10.800, mean reward:  0.270 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.838981, mean_q: 38.054231, mean_eps: 0.100000\n","     214772/2000000000: episode: 5825, duration: 4.898s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.173562, mean_q: 38.679395, mean_eps: 0.100000\n","     214807/2000000000: episode: 5826, duration: 4.446s, episode steps:  35, steps per second:   8, episode reward: 65.300, mean reward:  1.866 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.565572, mean_q: 38.613753, mean_eps: 0.100000\n","     214843/2000000000: episode: 5827, duration: 4.728s, episode steps:  36, steps per second:   8, episode reward: 41.400, mean reward:  1.150 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 83.625902, mean_q: 38.707037, mean_eps: 0.100000\n","     214883/2000000000: episode: 5828, duration: 5.211s, episode steps:  40, steps per second:   8, episode reward: 102.000, mean reward:  2.550 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.946733, mean_q: 38.254980, mean_eps: 0.100000\n","     214922/2000000000: episode: 5829, duration: 4.999s, episode steps:  39, steps per second:   8, episode reward: 35.900, mean reward:  0.921 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 73.093760, mean_q: 39.566894, mean_eps: 0.100000\n","     214950/2000000000: episode: 5830, duration: 3.730s, episode steps:  28, steps per second:   8, episode reward: 136.300, mean reward:  4.868 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 78.109702, mean_q: 39.075908, mean_eps: 0.100000\n","     214990/2000000000: episode: 5831, duration: 5.226s, episode steps:  40, steps per second:   8, episode reward: 20.300, mean reward:  0.507 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.322214, mean_q: 38.870694, mean_eps: 0.100000\n","     215014/2000000000: episode: 5832, duration: 3.082s, episode steps:  24, steps per second:   8, episode reward: 68.300, mean reward:  2.846 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 85.571934, mean_q: 38.166685, mean_eps: 0.100000\n","     215044/2000000000: episode: 5833, duration: 3.851s, episode steps:  30, steps per second:   8, episode reward: 74.300, mean reward:  2.477 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 77.533378, mean_q: 38.413093, mean_eps: 0.100000\n","     215077/2000000000: episode: 5834, duration: 4.203s, episode steps:  33, steps per second:   8, episode reward:  2.000, mean reward:  0.061 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.448292, mean_q: 38.644317, mean_eps: 0.100000\n","     215103/2000000000: episode: 5835, duration: 3.350s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 75.542799, mean_q: 38.774139, mean_eps: 0.100000\n","     215142/2000000000: episode: 5836, duration: 5.019s, episode steps:  39, steps per second:   8, episode reward: 49.400, mean reward:  1.267 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 75.608666, mean_q: 38.055841, mean_eps: 0.100000\n","     215173/2000000000: episode: 5837, duration: 3.955s, episode steps:  31, steps per second:   8, episode reward: 71.100, mean reward:  2.294 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 82.490895, mean_q: 38.428973, mean_eps: 0.100000\n","     215207/2000000000: episode: 5838, duration: 4.307s, episode steps:  34, steps per second:   8, episode reward: 82.600, mean reward:  2.429 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 81.161051, mean_q: 38.833033, mean_eps: 0.100000\n","     215243/2000000000: episode: 5839, duration: 4.534s, episode steps:  36, steps per second:   8, episode reward: 164.400, mean reward:  4.567 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.510738, mean_q: 38.277630, mean_eps: 0.100000\n","     215275/2000000000: episode: 5840, duration: 4.122s, episode steps:  32, steps per second:   8, episode reward: -60.300, mean reward: -1.884 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 82.704043, mean_q: 38.485097, mean_eps: 0.100000\n","     215315/2000000000: episode: 5841, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 36.200, mean reward:  0.905 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 84.363596, mean_q: 38.282659, mean_eps: 0.100000\n","     215355/2000000000: episode: 5842, duration: 5.532s, episode steps:  40, steps per second:   7, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.822538, mean_q: 38.820556, mean_eps: 0.100000\n","     215391/2000000000: episode: 5843, duration: 4.965s, episode steps:  36, steps per second:   7, episode reward: 17.600, mean reward:  0.489 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 74.476304, mean_q: 39.513894, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     215429/2000000000: episode: 5844, duration: 5.031s, episode steps:  38, steps per second:   8, episode reward: 35.100, mean reward:  0.924 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.711249, mean_q: 38.666159, mean_eps: 0.100000\n","     215461/2000000000: episode: 5845, duration: 4.452s, episode steps:  32, steps per second:   7, episode reward: 217.400, mean reward:  6.794 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 74.648488, mean_q: 38.003219, mean_eps: 0.100000\n","     215497/2000000000: episode: 5846, duration: 4.789s, episode steps:  36, steps per second:   8, episode reward: 147.000, mean reward:  4.083 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 79.590588, mean_q: 39.571503, mean_eps: 0.100000\n","     215537/2000000000: episode: 5847, duration: 5.242s, episode steps:  40, steps per second:   8, episode reward: -54.900, mean reward: -1.372 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.352892, mean_q: 38.495578, mean_eps: 0.100000\n","     215574/2000000000: episode: 5848, duration: 4.903s, episode steps:  37, steps per second:   8, episode reward: -86.300, mean reward: -2.332 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 70.688348, mean_q: 38.633303, mean_eps: 0.100000\n","     215612/2000000000: episode: 5849, duration: 5.059s, episode steps:  38, steps per second:   8, episode reward: 66.900, mean reward:  1.761 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 74.775862, mean_q: 39.012813, mean_eps: 0.100000\n","     215651/2000000000: episode: 5850, duration: 5.115s, episode steps:  39, steps per second:   8, episode reward: 50.600, mean reward:  1.297 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 72.024334, mean_q: 39.236964, mean_eps: 0.100000\n","     215691/2000000000: episode: 5851, duration: 5.076s, episode steps:  40, steps per second:   8, episode reward: 93.100, mean reward:  2.328 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 73.075110, mean_q: 38.912552, mean_eps: 0.100000\n","     215725/2000000000: episode: 5852, duration: 4.363s, episode steps:  34, steps per second:   8, episode reward: -29.700, mean reward: -0.874 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 73.337936, mean_q: 38.237633, mean_eps: 0.100000\n","     215762/2000000000: episode: 5853, duration: 4.739s, episode steps:  37, steps per second:   8, episode reward: 18.600, mean reward:  0.503 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.301694, mean_q: 38.238646, mean_eps: 0.100000\n","     215799/2000000000: episode: 5854, duration: 4.930s, episode steps:  37, steps per second:   8, episode reward: 221.400, mean reward:  5.984 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 75.765218, mean_q: 38.723881, mean_eps: 0.100000\n","     215836/2000000000: episode: 5855, duration: 5.552s, episode steps:  37, steps per second:   7, episode reward: 37.200, mean reward:  1.005 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 83.250968, mean_q: 38.455792, mean_eps: 0.100000\n","     215868/2000000000: episode: 5856, duration: 5.160s, episode steps:  32, steps per second:   6, episode reward: 112.700, mean reward:  3.522 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 80.729713, mean_q: 38.799205, mean_eps: 0.100000\n","     215908/2000000000: episode: 5857, duration: 5.649s, episode steps:  40, steps per second:   7, episode reward: 56.700, mean reward:  1.418 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.106780, mean_q: 38.636814, mean_eps: 0.100000\n","     215947/2000000000: episode: 5858, duration: 5.418s, episode steps:  39, steps per second:   7, episode reward: 261.800, mean reward:  6.713 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 75.539381, mean_q: 38.018411, mean_eps: 0.100000\n","     215981/2000000000: episode: 5859, duration: 4.613s, episode steps:  34, steps per second:   7, episode reward: -67.800, mean reward: -1.994 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 80.624835, mean_q: 39.123550, mean_eps: 0.100000\n","     216012/2000000000: episode: 5860, duration: 4.051s, episode steps:  31, steps per second:   8, episode reward: 147.600, mean reward:  4.761 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 75.790737, mean_q: 39.744344, mean_eps: 0.100000\n","     216041/2000000000: episode: 5861, duration: 3.726s, episode steps:  29, steps per second:   8, episode reward:  3.700, mean reward:  0.128 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 69.173768, mean_q: 38.682059, mean_eps: 0.100000\n","     216073/2000000000: episode: 5862, duration: 4.225s, episode steps:  32, steps per second:   8, episode reward: -63.300, mean reward: -1.978 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 74.186998, mean_q: 40.190436, mean_eps: 0.100000\n","     216108/2000000000: episode: 5863, duration: 4.631s, episode steps:  35, steps per second:   8, episode reward: 120.200, mean reward:  3.434 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 79.163584, mean_q: 38.503570, mean_eps: 0.100000\n","     216138/2000000000: episode: 5864, duration: 3.801s, episode steps:  30, steps per second:   8, episode reward: 32.100, mean reward:  1.070 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 85.900116, mean_q: 39.245021, mean_eps: 0.100000\n","     216173/2000000000: episode: 5865, duration: 4.534s, episode steps:  35, steps per second:   8, episode reward: 61.000, mean reward:  1.743 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 85.339106, mean_q: 38.626196, mean_eps: 0.100000\n","     216208/2000000000: episode: 5866, duration: 4.923s, episode steps:  35, steps per second:   7, episode reward: 24.000, mean reward:  0.686 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.431847, mean_q: 38.800631, mean_eps: 0.100000\n","     216243/2000000000: episode: 5867, duration: 4.525s, episode steps:  35, steps per second:   8, episode reward: 63.700, mean reward:  1.820 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 80.041233, mean_q: 38.335052, mean_eps: 0.100000\n","     216283/2000000000: episode: 5868, duration: 5.090s, episode steps:  40, steps per second:   8, episode reward: -4.300, mean reward: -0.107 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.625878, mean_q: 38.705945, mean_eps: 0.100000\n","     216318/2000000000: episode: 5869, duration: 4.732s, episode steps:  35, steps per second:   7, episode reward: 20.500, mean reward:  0.586 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 84.193020, mean_q: 38.057022, mean_eps: 0.100000\n","     216352/2000000000: episode: 5870, duration: 4.404s, episode steps:  34, steps per second:   8, episode reward: -186.700, mean reward: -5.491 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 81.592747, mean_q: 38.747570, mean_eps: 0.100000\n","     216384/2000000000: episode: 5871, duration: 4.148s, episode steps:  32, steps per second:   8, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 73.137095, mean_q: 39.877839, mean_eps: 0.100000\n","     216414/2000000000: episode: 5872, duration: 3.992s, episode steps:  30, steps per second:   8, episode reward: 164.600, mean reward:  5.487 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 72.421683, mean_q: 38.426782, mean_eps: 0.100000\n","     216454/2000000000: episode: 5873, duration: 5.100s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.385600, mean_q: 39.267982, mean_eps: 0.100000\n","     216488/2000000000: episode: 5874, duration: 4.233s, episode steps:  34, steps per second:   8, episode reward: 203.000, mean reward:  5.971 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.551043, mean_q: 39.141344, mean_eps: 0.100000\n","     216523/2000000000: episode: 5875, duration: 4.328s, episode steps:  35, steps per second:   8, episode reward: 38.000, mean reward:  1.086 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 79.473466, mean_q: 38.688891, mean_eps: 0.100000\n","     216552/2000000000: episode: 5876, duration: 3.699s, episode steps:  29, steps per second:   8, episode reward: -8.200, mean reward: -0.283 [-20.000, 18.000], mean action: 0.759 [0.000, 2.000],  loss: 83.848469, mean_q: 37.444943, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     216581/2000000000: episode: 5877, duration: 3.824s, episode steps:  29, steps per second:   8, episode reward: 115.300, mean reward:  3.976 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 79.836733, mean_q: 38.183994, mean_eps: 0.100000\n","     216616/2000000000: episode: 5878, duration: 4.448s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 75.422956, mean_q: 38.499843, mean_eps: 0.100000\n","     216653/2000000000: episode: 5879, duration: 4.802s, episode steps:  37, steps per second:   8, episode reward: 55.700, mean reward:  1.505 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 73.791075, mean_q: 39.175558, mean_eps: 0.100000\n","     216686/2000000000: episode: 5880, duration: 4.237s, episode steps:  33, steps per second:   8, episode reward: -31.600, mean reward: -0.958 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.969685, mean_q: 38.464258, mean_eps: 0.100000\n","     216722/2000000000: episode: 5881, duration: 4.614s, episode steps:  36, steps per second:   8, episode reward: 157.100, mean reward:  4.364 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 73.915889, mean_q: 38.998879, mean_eps: 0.100000\n","     216752/2000000000: episode: 5882, duration: 3.862s, episode steps:  30, steps per second:   8, episode reward: 18.900, mean reward:  0.630 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 74.516371, mean_q: 38.871709, mean_eps: 0.100000\n","     216785/2000000000: episode: 5883, duration: 4.414s, episode steps:  33, steps per second:   7, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 94.708649, mean_q: 37.728637, mean_eps: 0.100000\n","     216819/2000000000: episode: 5884, duration: 4.536s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 73.181523, mean_q: 39.361521, mean_eps: 0.100000\n","     216844/2000000000: episode: 5885, duration: 3.231s, episode steps:  25, steps per second:   8, episode reward: -58.000, mean reward: -2.320 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 76.675225, mean_q: 39.362210, mean_eps: 0.100000\n","     216877/2000000000: episode: 5886, duration: 4.222s, episode steps:  33, steps per second:   8, episode reward: -119.900, mean reward: -3.633 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 74.743725, mean_q: 39.436643, mean_eps: 0.100000\n","     216903/2000000000: episode: 5887, duration: 3.187s, episode steps:  26, steps per second:   8, episode reward: -20.000, mean reward: -0.769 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 82.927300, mean_q: 38.201826, mean_eps: 0.100000\n","     216943/2000000000: episode: 5888, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: 190.600, mean reward:  4.765 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 73.762010, mean_q: 39.076156, mean_eps: 0.100000\n","     216983/2000000000: episode: 5889, duration: 5.348s, episode steps:  40, steps per second:   7, episode reward: 106.900, mean reward:  2.672 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 73.445135, mean_q: 38.888004, mean_eps: 0.100000\n","     217020/2000000000: episode: 5890, duration: 4.768s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 78.649744, mean_q: 39.004681, mean_eps: 0.100000\n","     217055/2000000000: episode: 5891, duration: 4.386s, episode steps:  35, steps per second:   8, episode reward: 40.200, mean reward:  1.149 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 74.926014, mean_q: 39.600196, mean_eps: 0.100000\n","     217095/2000000000: episode: 5892, duration: 5.081s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.872302, mean_q: 39.075005, mean_eps: 0.100000\n","     217129/2000000000: episode: 5893, duration: 4.266s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 75.076498, mean_q: 38.528891, mean_eps: 0.100000\n","     217163/2000000000: episode: 5894, duration: 4.452s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 82.820372, mean_q: 39.298021, mean_eps: 0.100000\n","     217186/2000000000: episode: 5895, duration: 2.984s, episode steps:  23, steps per second:   8, episode reward: 18.000, mean reward:  0.783 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 81.918894, mean_q: 39.667514, mean_eps: 0.100000\n","     217226/2000000000: episode: 5896, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 161.900, mean reward:  4.047 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.393015, mean_q: 38.768480, mean_eps: 0.100000\n","     217262/2000000000: episode: 5897, duration: 4.582s, episode steps:  36, steps per second:   8, episode reward: -18.300, mean reward: -0.508 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 81.535590, mean_q: 38.146090, mean_eps: 0.100000\n","     217302/2000000000: episode: 5898, duration: 5.154s, episode steps:  40, steps per second:   8, episode reward:  6.000, mean reward:  0.150 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 79.511542, mean_q: 38.590659, mean_eps: 0.100000\n","     217342/2000000000: episode: 5899, duration: 5.153s, episode steps:  40, steps per second:   8, episode reward: -117.400, mean reward: -2.935 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 71.791988, mean_q: 38.682299, mean_eps: 0.100000\n","     217372/2000000000: episode: 5900, duration: 3.770s, episode steps:  30, steps per second:   8, episode reward: 19.300, mean reward:  0.643 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 81.115115, mean_q: 39.347994, mean_eps: 0.100000\n","     217412/2000000000: episode: 5901, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward: -90.500, mean reward: -2.263 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.034787, mean_q: 38.922911, mean_eps: 0.100000\n","     217452/2000000000: episode: 5902, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: 30.800, mean reward:  0.770 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 80.692086, mean_q: 38.109952, mean_eps: 0.100000\n","     217484/2000000000: episode: 5903, duration: 4.100s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.602001, mean_q: 38.245257, mean_eps: 0.100000\n","     217521/2000000000: episode: 5904, duration: 5.186s, episode steps:  37, steps per second:   7, episode reward: 24.200, mean reward:  0.654 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 74.032543, mean_q: 38.787013, mean_eps: 0.100000\n","     217551/2000000000: episode: 5905, duration: 4.163s, episode steps:  30, steps per second:   7, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 75.080430, mean_q: 38.965056, mean_eps: 0.100000\n","     217591/2000000000: episode: 5906, duration: 5.391s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.364891, mean_q: 37.806307, mean_eps: 0.100000\n","     217631/2000000000: episode: 5907, duration: 5.443s, episode steps:  40, steps per second:   7, episode reward: -29.400, mean reward: -0.735 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 74.529154, mean_q: 38.664117, mean_eps: 0.100000\n","     217665/2000000000: episode: 5908, duration: 4.221s, episode steps:  34, steps per second:   8, episode reward: 40.100, mean reward:  1.179 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 76.894158, mean_q: 39.262592, mean_eps: 0.100000\n","     217705/2000000000: episode: 5909, duration: 4.808s, episode steps:  40, steps per second:   8, episode reward: 34.000, mean reward:  0.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.417902, mean_q: 39.282296, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     217738/2000000000: episode: 5910, duration: 4.588s, episode steps:  33, steps per second:   7, episode reward: 125.300, mean reward:  3.797 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 75.606278, mean_q: 39.727553, mean_eps: 0.100000\n","     217778/2000000000: episode: 5911, duration: 5.323s, episode steps:  40, steps per second:   8, episode reward: 61.900, mean reward:  1.547 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 84.920828, mean_q: 38.809619, mean_eps: 0.100000\n","     217818/2000000000: episode: 5912, duration: 5.459s, episode steps:  40, steps per second:   7, episode reward: 44.900, mean reward:  1.122 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.635198, mean_q: 37.616345, mean_eps: 0.100000\n","     217858/2000000000: episode: 5913, duration: 5.031s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 79.724102, mean_q: 38.921752, mean_eps: 0.100000\n","     217894/2000000000: episode: 5914, duration: 4.567s, episode steps:  36, steps per second:   8, episode reward: 16.200, mean reward:  0.450 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 86.494635, mean_q: 39.131122, mean_eps: 0.100000\n","     217934/2000000000: episode: 5915, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: 29.400, mean reward:  0.735 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.754403, mean_q: 38.434627, mean_eps: 0.100000\n","     217974/2000000000: episode: 5916, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: -60.400, mean reward: -1.510 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.715764, mean_q: 38.590477, mean_eps: 0.100000\n","     218013/2000000000: episode: 5917, duration: 4.950s, episode steps:  39, steps per second:   8, episode reward: 11.400, mean reward:  0.292 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 83.213599, mean_q: 39.116349, mean_eps: 0.100000\n","     218052/2000000000: episode: 5918, duration: 5.036s, episode steps:  39, steps per second:   8, episode reward: -35.300, mean reward: -0.905 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 80.084656, mean_q: 39.322354, mean_eps: 0.100000\n","     218086/2000000000: episode: 5919, duration: 4.220s, episode steps:  34, steps per second:   8, episode reward: 176.400, mean reward:  5.188 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 72.777612, mean_q: 38.794166, mean_eps: 0.100000\n","     218126/2000000000: episode: 5920, duration: 5.013s, episode steps:  40, steps per second:   8, episode reward: -94.000, mean reward: -2.350 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.320428, mean_q: 38.809966, mean_eps: 0.100000\n","     218160/2000000000: episode: 5921, duration: 4.343s, episode steps:  34, steps per second:   8, episode reward: -134.000, mean reward: -3.941 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 76.932216, mean_q: 38.788193, mean_eps: 0.100000\n","     218194/2000000000: episode: 5922, duration: 4.637s, episode steps:  34, steps per second:   7, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 77.842485, mean_q: 38.697503, mean_eps: 0.100000\n","     218234/2000000000: episode: 5923, duration: 4.946s, episode steps:  40, steps per second:   8, episode reward: 89.900, mean reward:  2.248 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.220383, mean_q: 37.666153, mean_eps: 0.100000\n","     218272/2000000000: episode: 5924, duration: 4.742s, episode steps:  38, steps per second:   8, episode reward: 88.600, mean reward:  2.332 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 75.911404, mean_q: 38.767144, mean_eps: 0.100000\n","     218306/2000000000: episode: 5925, duration: 4.106s, episode steps:  34, steps per second:   8, episode reward: -17.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 83.478178, mean_q: 39.087237, mean_eps: 0.100000\n","     218339/2000000000: episode: 5926, duration: 4.119s, episode steps:  33, steps per second:   8, episode reward: 38.100, mean reward:  1.155 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 82.913515, mean_q: 39.508904, mean_eps: 0.100000\n","     218371/2000000000: episode: 5927, duration: 4.219s, episode steps:  32, steps per second:   8, episode reward: 70.600, mean reward:  2.206 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 79.349487, mean_q: 39.506060, mean_eps: 0.100000\n","     218402/2000000000: episode: 5928, duration: 4.115s, episode steps:  31, steps per second:   8, episode reward:  5.300, mean reward:  0.171 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 70.858845, mean_q: 38.795039, mean_eps: 0.100000\n","     218442/2000000000: episode: 5929, duration: 5.374s, episode steps:  40, steps per second:   7, episode reward:  5.700, mean reward:  0.143 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 70.158917, mean_q: 39.015034, mean_eps: 0.100000\n","     218482/2000000000: episode: 5930, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: 57.500, mean reward:  1.437 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 77.815414, mean_q: 39.273594, mean_eps: 0.100000\n","     218514/2000000000: episode: 5931, duration: 4.262s, episode steps:  32, steps per second:   8, episode reward: 54.700, mean reward:  1.709 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 72.796266, mean_q: 38.882667, mean_eps: 0.100000\n","     218546/2000000000: episode: 5932, duration: 4.360s, episode steps:  32, steps per second:   7, episode reward: 111.400, mean reward:  3.481 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 80.656927, mean_q: 39.649872, mean_eps: 0.100000\n","     218571/2000000000: episode: 5933, duration: 3.293s, episode steps:  25, steps per second:   8, episode reward: 31.600, mean reward:  1.264 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 77.805833, mean_q: 38.953327, mean_eps: 0.100000\n","     218601/2000000000: episode: 5934, duration: 3.950s, episode steps:  30, steps per second:   8, episode reward: 19.000, mean reward:  0.633 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 75.399364, mean_q: 39.173841, mean_eps: 0.100000\n","     218634/2000000000: episode: 5935, duration: 4.319s, episode steps:  33, steps per second:   8, episode reward: 103.900, mean reward:  3.148 [-20.000, 18.900], mean action: 1.091 [0.000, 2.000],  loss: 76.899936, mean_q: 39.127575, mean_eps: 0.100000\n","     218665/2000000000: episode: 5936, duration: 4.008s, episode steps:  31, steps per second:   8, episode reward: 87.300, mean reward:  2.816 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 86.143092, mean_q: 38.184850, mean_eps: 0.100000\n","     218696/2000000000: episode: 5937, duration: 3.839s, episode steps:  31, steps per second:   8, episode reward: 42.900, mean reward:  1.384 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 82.782089, mean_q: 38.976772, mean_eps: 0.100000\n","     218729/2000000000: episode: 5938, duration: 4.315s, episode steps:  33, steps per second:   8, episode reward: 37.000, mean reward:  1.121 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 76.318979, mean_q: 38.330257, mean_eps: 0.100000\n","     218761/2000000000: episode: 5939, duration: 4.159s, episode steps:  32, steps per second:   8, episode reward: 205.700, mean reward:  6.428 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 73.596976, mean_q: 38.949582, mean_eps: 0.100000\n","     218791/2000000000: episode: 5940, duration: 3.967s, episode steps:  30, steps per second:   8, episode reward: 108.700, mean reward:  3.623 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 74.681007, mean_q: 37.861369, mean_eps: 0.100000\n","     218831/2000000000: episode: 5941, duration: 5.185s, episode steps:  40, steps per second:   8, episode reward: -8.000, mean reward: -0.200 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.578461, mean_q: 39.256039, mean_eps: 0.100000\n","     218871/2000000000: episode: 5942, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: -7.600, mean reward: -0.190 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 91.075791, mean_q: 38.585330, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     218911/2000000000: episode: 5943, duration: 5.210s, episode steps:  40, steps per second:   8, episode reward: 175.900, mean reward:  4.398 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 74.897701, mean_q: 39.360893, mean_eps: 0.100000\n","     218951/2000000000: episode: 5944, duration: 5.188s, episode steps:  40, steps per second:   8, episode reward: 18.500, mean reward:  0.462 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.596827, mean_q: 37.987985, mean_eps: 0.100000\n","     218986/2000000000: episode: 5945, duration: 4.519s, episode steps:  35, steps per second:   8, episode reward: 114.800, mean reward:  3.280 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.400496, mean_q: 38.624335, mean_eps: 0.100000\n","     219019/2000000000: episode: 5946, duration: 4.459s, episode steps:  33, steps per second:   7, episode reward: 84.100, mean reward:  2.548 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 76.997326, mean_q: 38.104308, mean_eps: 0.100000\n","     219042/2000000000: episode: 5947, duration: 3.161s, episode steps:  23, steps per second:   7, episode reward: 56.000, mean reward:  2.435 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 78.504796, mean_q: 38.977935, mean_eps: 0.100000\n","     219076/2000000000: episode: 5948, duration: 4.492s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 81.904198, mean_q: 37.828328, mean_eps: 0.100000\n","     219107/2000000000: episode: 5949, duration: 4.151s, episode steps:  31, steps per second:   7, episode reward: 41.900, mean reward:  1.352 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 75.635566, mean_q: 39.215460, mean_eps: 0.100000\n","     219147/2000000000: episode: 5950, duration: 5.255s, episode steps:  40, steps per second:   8, episode reward: -99.900, mean reward: -2.498 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.695518, mean_q: 37.989357, mean_eps: 0.100000\n","     219187/2000000000: episode: 5951, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: 82.400, mean reward:  2.060 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.977965, mean_q: 37.554148, mean_eps: 0.100000\n","     219223/2000000000: episode: 5952, duration: 4.760s, episode steps:  36, steps per second:   8, episode reward: 105.100, mean reward:  2.919 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 76.166041, mean_q: 38.244648, mean_eps: 0.100000\n","     219263/2000000000: episode: 5953, duration: 5.311s, episode steps:  40, steps per second:   8, episode reward: 119.900, mean reward:  2.998 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 81.799951, mean_q: 38.774208, mean_eps: 0.100000\n","     219291/2000000000: episode: 5954, duration: 3.585s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 82.146545, mean_q: 38.011972, mean_eps: 0.100000\n","     219319/2000000000: episode: 5955, duration: 3.510s, episode steps:  28, steps per second:   8, episode reward: 150.900, mean reward:  5.389 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 77.363264, mean_q: 39.497212, mean_eps: 0.100000\n","     219348/2000000000: episode: 5956, duration: 3.940s, episode steps:  29, steps per second:   7, episode reward: -96.000, mean reward: -3.310 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 74.501178, mean_q: 38.839880, mean_eps: 0.100000\n","     219382/2000000000: episode: 5957, duration: 4.387s, episode steps:  34, steps per second:   8, episode reward: 238.200, mean reward:  7.006 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 81.137370, mean_q: 38.544819, mean_eps: 0.100000\n","     219422/2000000000: episode: 5958, duration: 5.171s, episode steps:  40, steps per second:   8, episode reward: 77.100, mean reward:  1.928 [-20.000, 19.100], mean action: 1.350 [0.000, 2.000],  loss: 83.217104, mean_q: 38.235926, mean_eps: 0.100000\n","     219462/2000000000: episode: 5959, duration: 5.302s, episode steps:  40, steps per second:   8, episode reward: 84.200, mean reward:  2.105 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.001132, mean_q: 38.384531, mean_eps: 0.100000\n","     219496/2000000000: episode: 5960, duration: 4.502s, episode steps:  34, steps per second:   8, episode reward: 156.600, mean reward:  4.606 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 77.607084, mean_q: 38.458401, mean_eps: 0.100000\n","     219536/2000000000: episode: 5961, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.911714, mean_q: 38.261092, mean_eps: 0.100000\n","     219575/2000000000: episode: 5962, duration: 4.864s, episode steps:  39, steps per second:   8, episode reward: 272.300, mean reward:  6.982 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 77.146590, mean_q: 38.361581, mean_eps: 0.100000\n","     219615/2000000000: episode: 5963, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: 60.400, mean reward:  1.510 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.003695, mean_q: 38.614812, mean_eps: 0.100000\n","     219655/2000000000: episode: 5964, duration: 5.136s, episode steps:  40, steps per second:   8, episode reward: 68.600, mean reward:  1.715 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 72.806514, mean_q: 39.362376, mean_eps: 0.100000\n","     219695/2000000000: episode: 5965, duration: 5.215s, episode steps:  40, steps per second:   8, episode reward: 196.000, mean reward:  4.900 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.608864, mean_q: 38.212194, mean_eps: 0.100000\n","     219734/2000000000: episode: 5966, duration: 5.249s, episode steps:  39, steps per second:   7, episode reward: 90.100, mean reward:  2.310 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 78.887187, mean_q: 39.122101, mean_eps: 0.100000\n","     219772/2000000000: episode: 5967, duration: 4.851s, episode steps:  38, steps per second:   8, episode reward: 50.300, mean reward:  1.324 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 76.032561, mean_q: 37.838998, mean_eps: 0.100000\n","     219809/2000000000: episode: 5968, duration: 4.547s, episode steps:  37, steps per second:   8, episode reward: 14.900, mean reward:  0.403 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 82.109875, mean_q: 38.631273, mean_eps: 0.100000\n","     219845/2000000000: episode: 5969, duration: 4.483s, episode steps:  36, steps per second:   8, episode reward: 107.600, mean reward:  2.989 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.335421, mean_q: 38.470133, mean_eps: 0.100000\n","     219885/2000000000: episode: 5970, duration: 4.949s, episode steps:  40, steps per second:   8, episode reward: 185.300, mean reward:  4.633 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.984474, mean_q: 39.818611, mean_eps: 0.100000\n","     219917/2000000000: episode: 5971, duration: 4.107s, episode steps:  32, steps per second:   8, episode reward: 16.500, mean reward:  0.516 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.195107, mean_q: 39.526983, mean_eps: 0.100000\n","     219943/2000000000: episode: 5972, duration: 3.405s, episode steps:  26, steps per second:   8, episode reward: 59.500, mean reward:  2.288 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 82.280879, mean_q: 38.853109, mean_eps: 0.100000\n","     219977/2000000000: episode: 5973, duration: 4.330s, episode steps:  34, steps per second:   8, episode reward: 105.600, mean reward:  3.106 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 78.782356, mean_q: 38.139409, mean_eps: 0.100000\n","     220005/2000000000: episode: 5974, duration: 3.724s, episode steps:  28, steps per second:   8, episode reward: -33.800, mean reward: -1.207 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 73.312777, mean_q: 39.206015, mean_eps: 0.100000\n","     220036/2000000000: episode: 5975, duration: 3.891s, episode steps:  31, steps per second:   8, episode reward: 74.100, mean reward:  2.390 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 86.624352, mean_q: 40.924558, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     220076/2000000000: episode: 5976, duration: 5.046s, episode steps:  40, steps per second:   8, episode reward: 84.500, mean reward:  2.112 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 91.636014, mean_q: 40.191163, mean_eps: 0.100000\n","     220106/2000000000: episode: 5977, duration: 3.881s, episode steps:  30, steps per second:   8, episode reward: 48.200, mean reward:  1.607 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 89.056506, mean_q: 39.801717, mean_eps: 0.100000\n","     220142/2000000000: episode: 5978, duration: 4.491s, episode steps:  36, steps per second:   8, episode reward: 173.000, mean reward:  4.806 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.575788, mean_q: 40.224158, mean_eps: 0.100000\n","     220168/2000000000: episode: 5979, duration: 3.380s, episode steps:  26, steps per second:   8, episode reward: 121.000, mean reward:  4.654 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 81.906922, mean_q: 40.238461, mean_eps: 0.100000\n","     220191/2000000000: episode: 5980, duration: 2.974s, episode steps:  23, steps per second:   8, episode reward: 67.700, mean reward:  2.943 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 76.063702, mean_q: 40.358573, mean_eps: 0.100000\n","     220227/2000000000: episode: 5981, duration: 4.553s, episode steps:  36, steps per second:   8, episode reward: 69.500, mean reward:  1.931 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 75.963929, mean_q: 39.714870, mean_eps: 0.100000\n","     220267/2000000000: episode: 5982, duration: 5.161s, episode steps:  40, steps per second:   8, episode reward: -148.700, mean reward: -3.718 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 84.527449, mean_q: 39.657093, mean_eps: 0.100000\n","     220304/2000000000: episode: 5983, duration: 4.917s, episode steps:  37, steps per second:   8, episode reward: 141.400, mean reward:  3.822 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 80.997742, mean_q: 40.428570, mean_eps: 0.100000\n","     220344/2000000000: episode: 5984, duration: 5.392s, episode steps:  40, steps per second:   7, episode reward: 115.700, mean reward:  2.892 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.517591, mean_q: 39.462067, mean_eps: 0.100000\n","     220384/2000000000: episode: 5985, duration: 5.376s, episode steps:  40, steps per second:   7, episode reward: 89.700, mean reward:  2.243 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 88.584119, mean_q: 39.474908, mean_eps: 0.100000\n","     220424/2000000000: episode: 5986, duration: 5.082s, episode steps:  40, steps per second:   8, episode reward: 111.400, mean reward:  2.785 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.468671, mean_q: 40.223414, mean_eps: 0.100000\n","     220459/2000000000: episode: 5987, duration: 4.660s, episode steps:  35, steps per second:   8, episode reward: 138.100, mean reward:  3.946 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.650374, mean_q: 39.034434, mean_eps: 0.100000\n","     220499/2000000000: episode: 5988, duration: 5.307s, episode steps:  40, steps per second:   8, episode reward: 61.900, mean reward:  1.548 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 88.795196, mean_q: 39.921772, mean_eps: 0.100000\n","     220539/2000000000: episode: 5989, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: 28.100, mean reward:  0.703 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 82.584512, mean_q: 40.439871, mean_eps: 0.100000\n","     220573/2000000000: episode: 5990, duration: 4.245s, episode steps:  34, steps per second:   8, episode reward: 65.800, mean reward:  1.935 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 85.026729, mean_q: 39.261180, mean_eps: 0.100000\n","     220607/2000000000: episode: 5991, duration: 4.235s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 87.403529, mean_q: 40.848736, mean_eps: 0.100000\n","     220640/2000000000: episode: 5992, duration: 4.195s, episode steps:  33, steps per second:   8, episode reward: -59.600, mean reward: -1.806 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 80.859535, mean_q: 39.245176, mean_eps: 0.100000\n","     220680/2000000000: episode: 5993, duration: 4.820s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 87.722975, mean_q: 40.545886, mean_eps: 0.100000\n","     220712/2000000000: episode: 5994, duration: 3.981s, episode steps:  32, steps per second:   8, episode reward: 161.900, mean reward:  5.059 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 76.154852, mean_q: 40.568896, mean_eps: 0.100000\n","     220750/2000000000: episode: 5995, duration: 4.847s, episode steps:  38, steps per second:   8, episode reward: 90.900, mean reward:  2.392 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 87.519199, mean_q: 39.529809, mean_eps: 0.100000\n","     220782/2000000000: episode: 5996, duration: 4.142s, episode steps:  32, steps per second:   8, episode reward: 24.100, mean reward:  0.753 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 80.878654, mean_q: 40.682738, mean_eps: 0.100000\n","     220822/2000000000: episode: 5997, duration: 5.435s, episode steps:  40, steps per second:   7, episode reward: 49.400, mean reward:  1.235 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 76.744507, mean_q: 40.000027, mean_eps: 0.100000\n","     220856/2000000000: episode: 5998, duration: 4.569s, episode steps:  34, steps per second:   7, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 78.983742, mean_q: 40.554456, mean_eps: 0.100000\n","     220896/2000000000: episode: 5999, duration: 5.172s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.185236, mean_q: 40.224192, mean_eps: 0.100000\n","     220927/2000000000: episode: 6000, duration: 4.051s, episode steps:  31, steps per second:   8, episode reward: 150.200, mean reward:  4.845 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.315494, mean_q: 39.751246, mean_eps: 0.100000\n","     220965/2000000000: episode: 6001, duration: 4.954s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 81.609503, mean_q: 40.344509, mean_eps: 0.100000\n","     221005/2000000000: episode: 6002, duration: 5.385s, episode steps:  40, steps per second:   7, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.103618, mean_q: 39.883740, mean_eps: 0.100000\n","     221030/2000000000: episode: 6003, duration: 3.306s, episode steps:  25, steps per second:   8, episode reward: -20.000, mean reward: -0.800 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 88.285753, mean_q: 40.132930, mean_eps: 0.100000\n","     221065/2000000000: episode: 6004, duration: 4.522s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 85.451994, mean_q: 40.033191, mean_eps: 0.100000\n","     221089/2000000000: episode: 6005, duration: 3.280s, episode steps:  24, steps per second:   7, episode reward: -39.300, mean reward: -1.637 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 84.246242, mean_q: 39.817848, mean_eps: 0.100000\n","     221129/2000000000: episode: 6006, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: 18.800, mean reward:  0.470 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 84.121388, mean_q: 39.504322, mean_eps: 0.100000\n","     221161/2000000000: episode: 6007, duration: 4.220s, episode steps:  32, steps per second:   8, episode reward: 68.900, mean reward:  2.153 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 84.681439, mean_q: 39.428786, mean_eps: 0.100000\n","     221198/2000000000: episode: 6008, duration: 4.788s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 86.353939, mean_q: 39.910214, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     221237/2000000000: episode: 6009, duration: 5.313s, episode steps:  39, steps per second:   7, episode reward: 170.000, mean reward:  4.359 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 75.098841, mean_q: 40.210373, mean_eps: 0.100000\n","     221277/2000000000: episode: 6010, duration: 5.476s, episode steps:  40, steps per second:   7, episode reward: 65.900, mean reward:  1.648 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.228708, mean_q: 39.867918, mean_eps: 0.100000\n","     221303/2000000000: episode: 6011, duration: 3.335s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 78.600722, mean_q: 39.361532, mean_eps: 0.100000\n","     221334/2000000000: episode: 6012, duration: 4.085s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 77.210671, mean_q: 39.752795, mean_eps: 0.100000\n","     221374/2000000000: episode: 6013, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward: 39.900, mean reward:  0.997 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 76.651458, mean_q: 40.383412, mean_eps: 0.100000\n","     221414/2000000000: episode: 6014, duration: 4.979s, episode steps:  40, steps per second:   8, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.494703, mean_q: 39.590180, mean_eps: 0.100000\n","     221446/2000000000: episode: 6015, duration: 4.228s, episode steps:  32, steps per second:   8, episode reward: 84.000, mean reward:  2.625 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 70.273424, mean_q: 39.800132, mean_eps: 0.100000\n","     221485/2000000000: episode: 6016, duration: 4.769s, episode steps:  39, steps per second:   8, episode reward: 100.400, mean reward:  2.574 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 80.113248, mean_q: 40.010040, mean_eps: 0.100000\n","     221519/2000000000: episode: 6017, duration: 4.285s, episode steps:  34, steps per second:   8, episode reward: -63.000, mean reward: -1.853 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 83.911719, mean_q: 39.783278, mean_eps: 0.100000\n","     221557/2000000000: episode: 6018, duration: 4.713s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 80.173724, mean_q: 39.920915, mean_eps: 0.100000\n","     221597/2000000000: episode: 6019, duration: 5.280s, episode steps:  40, steps per second:   8, episode reward: -72.100, mean reward: -1.802 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.936440, mean_q: 39.312475, mean_eps: 0.100000\n","     221636/2000000000: episode: 6020, duration: 4.979s, episode steps:  39, steps per second:   8, episode reward: -51.300, mean reward: -1.315 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 75.407776, mean_q: 39.346912, mean_eps: 0.100000\n","     221671/2000000000: episode: 6021, duration: 4.721s, episode steps:  35, steps per second:   7, episode reward: 156.000, mean reward:  4.457 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 82.691652, mean_q: 39.491770, mean_eps: 0.100000\n","     221711/2000000000: episode: 6022, duration: 5.415s, episode steps:  40, steps per second:   7, episode reward: 30.800, mean reward:  0.770 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 81.904751, mean_q: 40.518256, mean_eps: 0.100000\n","     221739/2000000000: episode: 6023, duration: 3.693s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 83.754119, mean_q: 40.763368, mean_eps: 0.100000\n","     221768/2000000000: episode: 6024, duration: 3.661s, episode steps:  29, steps per second:   8, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.726891, mean_q: 39.834498, mean_eps: 0.100000\n","     221806/2000000000: episode: 6025, duration: 4.911s, episode steps:  38, steps per second:   8, episode reward: 80.900, mean reward:  2.129 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 69.396942, mean_q: 40.003192, mean_eps: 0.100000\n","     221846/2000000000: episode: 6026, duration: 5.170s, episode steps:  40, steps per second:   8, episode reward: 68.300, mean reward:  1.707 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.089508, mean_q: 39.162226, mean_eps: 0.100000\n","     221886/2000000000: episode: 6027, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: -26.200, mean reward: -0.655 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 88.570281, mean_q: 38.954773, mean_eps: 0.100000\n","     221926/2000000000: episode: 6028, duration: 4.947s, episode steps:  40, steps per second:   8, episode reward: 77.800, mean reward:  1.945 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.606037, mean_q: 39.745125, mean_eps: 0.100000\n","     221961/2000000000: episode: 6029, duration: 4.475s, episode steps:  35, steps per second:   8, episode reward: 62.900, mean reward:  1.797 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 79.600297, mean_q: 40.677808, mean_eps: 0.100000\n","     221998/2000000000: episode: 6030, duration: 4.565s, episode steps:  37, steps per second:   8, episode reward: -2.200, mean reward: -0.059 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 83.748036, mean_q: 39.346643, mean_eps: 0.100000\n","     222027/2000000000: episode: 6031, duration: 3.864s, episode steps:  29, steps per second:   8, episode reward: 109.800, mean reward:  3.786 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 75.310657, mean_q: 39.369247, mean_eps: 0.100000\n","     222059/2000000000: episode: 6032, duration: 4.162s, episode steps:  32, steps per second:   8, episode reward: -96.000, mean reward: -3.000 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 82.187318, mean_q: 40.380086, mean_eps: 0.100000\n","     222099/2000000000: episode: 6033, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: 56.200, mean reward:  1.405 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 86.943945, mean_q: 39.503769, mean_eps: 0.100000\n","     222136/2000000000: episode: 6034, duration: 4.773s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 81.743113, mean_q: 40.090257, mean_eps: 0.100000\n","     222176/2000000000: episode: 6035, duration: 5.062s, episode steps:  40, steps per second:   8, episode reward: 71.900, mean reward:  1.797 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.633666, mean_q: 40.737816, mean_eps: 0.100000\n","     222203/2000000000: episode: 6036, duration: 3.636s, episode steps:  27, steps per second:   7, episode reward: 81.600, mean reward:  3.022 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 84.176606, mean_q: 40.355636, mean_eps: 0.100000\n","     222241/2000000000: episode: 6037, duration: 4.827s, episode steps:  38, steps per second:   8, episode reward: 98.100, mean reward:  2.582 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 78.844200, mean_q: 40.043552, mean_eps: 0.100000\n","     222271/2000000000: episode: 6038, duration: 3.983s, episode steps:  30, steps per second:   8, episode reward: 109.300, mean reward:  3.643 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 77.631025, mean_q: 40.139096, mean_eps: 0.100000\n","     222307/2000000000: episode: 6039, duration: 4.671s, episode steps:  36, steps per second:   8, episode reward: 32.700, mean reward:  0.908 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 77.708506, mean_q: 39.639395, mean_eps: 0.100000\n","     222338/2000000000: episode: 6040, duration: 4.006s, episode steps:  31, steps per second:   8, episode reward: 238.300, mean reward:  7.687 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 82.426552, mean_q: 40.117935, mean_eps: 0.100000\n","     222378/2000000000: episode: 6041, duration: 5.420s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.920113, mean_q: 40.167508, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     222418/2000000000: episode: 6042, duration: 5.455s, episode steps:  40, steps per second:   7, episode reward: 26.200, mean reward:  0.655 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 77.777600, mean_q: 40.070135, mean_eps: 0.100000\n","     222449/2000000000: episode: 6043, duration: 4.252s, episode steps:  31, steps per second:   7, episode reward: 30.800, mean reward:  0.994 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 82.077291, mean_q: 39.712269, mean_eps: 0.100000\n","     222478/2000000000: episode: 6044, duration: 3.973s, episode steps:  29, steps per second:   7, episode reward: -135.700, mean reward: -4.679 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 83.085723, mean_q: 40.399924, mean_eps: 0.100000\n","     222514/2000000000: episode: 6045, duration: 4.719s, episode steps:  36, steps per second:   8, episode reward: -3.100, mean reward: -0.086 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 86.730856, mean_q: 39.939835, mean_eps: 0.100000\n","     222553/2000000000: episode: 6046, duration: 4.940s, episode steps:  39, steps per second:   8, episode reward:  1.100, mean reward:  0.028 [-20.000, 19.800], mean action: 1.256 [0.000, 2.000],  loss: 79.446363, mean_q: 39.624432, mean_eps: 0.100000\n","     222584/2000000000: episode: 6047, duration: 4.148s, episode steps:  31, steps per second:   7, episode reward: 88.400, mean reward:  2.852 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.901906, mean_q: 40.497542, mean_eps: 0.100000\n","     222614/2000000000: episode: 6048, duration: 4.171s, episode steps:  30, steps per second:   7, episode reward: -1.900, mean reward: -0.063 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 78.157911, mean_q: 39.548404, mean_eps: 0.100000\n","     222654/2000000000: episode: 6049, duration: 5.316s, episode steps:  40, steps per second:   8, episode reward: 15.800, mean reward:  0.395 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 77.110943, mean_q: 39.948897, mean_eps: 0.100000\n","     222689/2000000000: episode: 6050, duration: 4.454s, episode steps:  35, steps per second:   8, episode reward: -58.700, mean reward: -1.677 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 87.702577, mean_q: 40.084896, mean_eps: 0.100000\n","     222729/2000000000: episode: 6051, duration: 5.542s, episode steps:  40, steps per second:   7, episode reward: -15.200, mean reward: -0.380 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.552290, mean_q: 39.529865, mean_eps: 0.100000\n","     222765/2000000000: episode: 6052, duration: 5.042s, episode steps:  36, steps per second:   7, episode reward: 100.100, mean reward:  2.781 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.260385, mean_q: 39.865309, mean_eps: 0.100000\n","     222797/2000000000: episode: 6053, duration: 4.510s, episode steps:  32, steps per second:   7, episode reward: 81.800, mean reward:  2.556 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 72.036471, mean_q: 39.331746, mean_eps: 0.100000\n","     222834/2000000000: episode: 6054, duration: 4.953s, episode steps:  37, steps per second:   7, episode reward: -6.100, mean reward: -0.165 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 83.263752, mean_q: 39.240253, mean_eps: 0.100000\n","     222864/2000000000: episode: 6055, duration: 3.776s, episode steps:  30, steps per second:   8, episode reward: -12.700, mean reward: -0.423 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 84.099294, mean_q: 39.538915, mean_eps: 0.100000\n","     222895/2000000000: episode: 6056, duration: 4.110s, episode steps:  31, steps per second:   8, episode reward: 56.100, mean reward:  1.810 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 84.748811, mean_q: 39.665709, mean_eps: 0.100000\n","     222924/2000000000: episode: 6057, duration: 3.770s, episode steps:  29, steps per second:   8, episode reward: 125.800, mean reward:  4.338 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 73.782286, mean_q: 40.253777, mean_eps: 0.100000\n","     222961/2000000000: episode: 6058, duration: 5.050s, episode steps:  37, steps per second:   7, episode reward: 72.300, mean reward:  1.954 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 80.806180, mean_q: 39.301157, mean_eps: 0.100000\n","     223001/2000000000: episode: 6059, duration: 5.195s, episode steps:  40, steps per second:   8, episode reward:  1.500, mean reward:  0.037 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.392169, mean_q: 39.029981, mean_eps: 0.100000\n","     223038/2000000000: episode: 6060, duration: 4.755s, episode steps:  37, steps per second:   8, episode reward:  0.900, mean reward:  0.024 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 77.196531, mean_q: 40.291146, mean_eps: 0.100000\n","     223078/2000000000: episode: 6061, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward: -45.200, mean reward: -1.130 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.388941, mean_q: 39.817956, mean_eps: 0.100000\n","     223118/2000000000: episode: 6062, duration: 4.998s, episode steps:  40, steps per second:   8, episode reward: 37.700, mean reward:  0.943 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.260975, mean_q: 39.961683, mean_eps: 0.100000\n","     223147/2000000000: episode: 6063, duration: 3.797s, episode steps:  29, steps per second:   8, episode reward: 43.700, mean reward:  1.507 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 78.998848, mean_q: 40.442518, mean_eps: 0.100000\n","     223187/2000000000: episode: 6064, duration: 5.175s, episode steps:  40, steps per second:   8, episode reward: -93.500, mean reward: -2.338 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 84.239664, mean_q: 40.108391, mean_eps: 0.100000\n","     223214/2000000000: episode: 6065, duration: 3.495s, episode steps:  27, steps per second:   8, episode reward: 126.000, mean reward:  4.667 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 77.691120, mean_q: 40.634554, mean_eps: 0.100000\n","     223254/2000000000: episode: 6066, duration: 5.067s, episode steps:  40, steps per second:   8, episode reward: 83.100, mean reward:  2.077 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.019300, mean_q: 39.481885, mean_eps: 0.100000\n","     223292/2000000000: episode: 6067, duration: 5.036s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 82.487223, mean_q: 39.578720, mean_eps: 0.100000\n","     223332/2000000000: episode: 6068, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 83.739103, mean_q: 39.836372, mean_eps: 0.100000\n","     223372/2000000000: episode: 6069, duration: 4.888s, episode steps:  40, steps per second:   8, episode reward: 14.700, mean reward:  0.367 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.621706, mean_q: 39.543331, mean_eps: 0.100000\n","     223403/2000000000: episode: 6070, duration: 3.984s, episode steps:  31, steps per second:   8, episode reward: 143.200, mean reward:  4.619 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.540732, mean_q: 39.855056, mean_eps: 0.100000\n","     223442/2000000000: episode: 6071, duration: 4.865s, episode steps:  39, steps per second:   8, episode reward: -19.800, mean reward: -0.508 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 76.819904, mean_q: 39.874149, mean_eps: 0.100000\n","     223480/2000000000: episode: 6072, duration: 4.856s, episode steps:  38, steps per second:   8, episode reward: 190.200, mean reward:  5.005 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 79.009403, mean_q: 39.855152, mean_eps: 0.100000\n","     223519/2000000000: episode: 6073, duration: 5.062s, episode steps:  39, steps per second:   8, episode reward: -230.300, mean reward: -5.905 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 76.868662, mean_q: 40.659596, mean_eps: 0.100000\n","     223545/2000000000: episode: 6074, duration: 3.469s, episode steps:  26, steps per second:   7, episode reward: 18.000, mean reward:  0.692 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 87.823295, mean_q: 39.890879, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     223584/2000000000: episode: 6075, duration: 5.184s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 76.802039, mean_q: 39.997771, mean_eps: 0.100000\n","     223622/2000000000: episode: 6076, duration: 4.931s, episode steps:  38, steps per second:   8, episode reward: 91.800, mean reward:  2.416 [-20.000, 18.000], mean action: 1.079 [0.000, 2.000],  loss: 80.948073, mean_q: 40.959239, mean_eps: 0.100000\n","     223659/2000000000: episode: 6077, duration: 4.836s, episode steps:  37, steps per second:   8, episode reward: 84.500, mean reward:  2.284 [-20.000, 19.200], mean action: 1.216 [0.000, 2.000],  loss: 78.413599, mean_q: 40.005171, mean_eps: 0.100000\n","     223690/2000000000: episode: 6078, duration: 4.070s, episode steps:  31, steps per second:   8, episode reward: 167.300, mean reward:  5.397 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 82.816093, mean_q: 39.885124, mean_eps: 0.100000\n","     223715/2000000000: episode: 6079, duration: 3.421s, episode steps:  25, steps per second:   7, episode reward: -58.200, mean reward: -2.328 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 77.281004, mean_q: 40.103092, mean_eps: 0.100000\n","     223747/2000000000: episode: 6080, duration: 4.464s, episode steps:  32, steps per second:   7, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 88.928414, mean_q: 39.470631, mean_eps: 0.100000\n","     223780/2000000000: episode: 6081, duration: 4.282s, episode steps:  33, steps per second:   8, episode reward: 45.200, mean reward:  1.370 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 72.667980, mean_q: 39.725030, mean_eps: 0.100000\n","     223812/2000000000: episode: 6082, duration: 4.042s, episode steps:  32, steps per second:   8, episode reward: 79.100, mean reward:  2.472 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 85.504171, mean_q: 39.742164, mean_eps: 0.100000\n","     223845/2000000000: episode: 6083, duration: 4.215s, episode steps:  33, steps per second:   8, episode reward: 68.500, mean reward:  2.076 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 75.022245, mean_q: 40.005783, mean_eps: 0.100000\n","     223877/2000000000: episode: 6084, duration: 4.437s, episode steps:  32, steps per second:   7, episode reward: 107.000, mean reward:  3.344 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.851748, mean_q: 40.424950, mean_eps: 0.100000\n","     223910/2000000000: episode: 6085, duration: 4.304s, episode steps:  33, steps per second:   8, episode reward:  1.000, mean reward:  0.030 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 80.400718, mean_q: 40.111666, mean_eps: 0.100000\n","     223940/2000000000: episode: 6086, duration: 3.957s, episode steps:  30, steps per second:   8, episode reward: 35.100, mean reward:  1.170 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 69.606906, mean_q: 39.920916, mean_eps: 0.100000\n","     223972/2000000000: episode: 6087, duration: 4.102s, episode steps:  32, steps per second:   8, episode reward: 42.900, mean reward:  1.341 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 79.621724, mean_q: 39.833058, mean_eps: 0.100000\n","     224009/2000000000: episode: 6088, duration: 4.807s, episode steps:  37, steps per second:   8, episode reward:  6.600, mean reward:  0.178 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 83.713422, mean_q: 39.624200, mean_eps: 0.100000\n","     224039/2000000000: episode: 6089, duration: 4.004s, episode steps:  30, steps per second:   7, episode reward: -2.300, mean reward: -0.077 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 78.009721, mean_q: 39.467249, mean_eps: 0.100000\n","     224078/2000000000: episode: 6090, duration: 4.994s, episode steps:  39, steps per second:   8, episode reward: 50.200, mean reward:  1.287 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 84.060337, mean_q: 39.613319, mean_eps: 0.100000\n","     224109/2000000000: episode: 6091, duration: 4.103s, episode steps:  31, steps per second:   8, episode reward: 90.400, mean reward:  2.916 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 82.065329, mean_q: 39.570475, mean_eps: 0.100000\n","     224149/2000000000: episode: 6092, duration: 4.991s, episode steps:  40, steps per second:   8, episode reward: 16.700, mean reward:  0.417 [-20.000, 19.000], mean action: 1.250 [0.000, 2.000],  loss: 78.195350, mean_q: 39.762516, mean_eps: 0.100000\n","     224182/2000000000: episode: 6093, duration: 4.369s, episode steps:  33, steps per second:   8, episode reward: 88.400, mean reward:  2.679 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 84.206357, mean_q: 39.357552, mean_eps: 0.100000\n","     224214/2000000000: episode: 6094, duration: 4.347s, episode steps:  32, steps per second:   7, episode reward: 90.900, mean reward:  2.841 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 80.761409, mean_q: 40.012425, mean_eps: 0.100000\n","     224243/2000000000: episode: 6095, duration: 4.041s, episode steps:  29, steps per second:   7, episode reward: 56.900, mean reward:  1.962 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.325872, mean_q: 40.315304, mean_eps: 0.100000\n","     224275/2000000000: episode: 6096, duration: 4.124s, episode steps:  32, steps per second:   8, episode reward: 122.600, mean reward:  3.831 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.081799, mean_q: 40.555977, mean_eps: 0.100000\n","     224315/2000000000: episode: 6097, duration: 5.157s, episode steps:  40, steps per second:   8, episode reward: 198.400, mean reward:  4.960 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.490496, mean_q: 40.039005, mean_eps: 0.100000\n","     224355/2000000000: episode: 6098, duration: 5.254s, episode steps:  40, steps per second:   8, episode reward: -45.900, mean reward: -1.147 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.456057, mean_q: 39.710500, mean_eps: 0.100000\n","     224386/2000000000: episode: 6099, duration: 4.008s, episode steps:  31, steps per second:   8, episode reward: 168.400, mean reward:  5.432 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 86.752803, mean_q: 40.535937, mean_eps: 0.100000\n","     224418/2000000000: episode: 6100, duration: 4.107s, episode steps:  32, steps per second:   8, episode reward: 36.100, mean reward:  1.128 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 87.455866, mean_q: 39.965522, mean_eps: 0.100000\n","     224447/2000000000: episode: 6101, duration: 3.754s, episode steps:  29, steps per second:   8, episode reward: 61.100, mean reward:  2.107 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 90.400210, mean_q: 39.651470, mean_eps: 0.100000\n","     224482/2000000000: episode: 6102, duration: 4.336s, episode steps:  35, steps per second:   8, episode reward: 77.300, mean reward:  2.209 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.289817, mean_q: 40.155092, mean_eps: 0.100000\n","     224508/2000000000: episode: 6103, duration: 3.293s, episode steps:  26, steps per second:   8, episode reward: 151.000, mean reward:  5.808 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 80.509606, mean_q: 39.858530, mean_eps: 0.100000\n","     224548/2000000000: episode: 6104, duration: 5.071s, episode steps:  40, steps per second:   8, episode reward: 41.900, mean reward:  1.047 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.181355, mean_q: 39.378018, mean_eps: 0.100000\n","     224588/2000000000: episode: 6105, duration: 5.235s, episode steps:  40, steps per second:   8, episode reward: 83.000, mean reward:  2.075 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 75.388522, mean_q: 39.658367, mean_eps: 0.100000\n","     224617/2000000000: episode: 6106, duration: 3.732s, episode steps:  29, steps per second:   8, episode reward:  3.200, mean reward:  0.110 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 77.101694, mean_q: 40.718240, mean_eps: 0.100000\n","     224645/2000000000: episode: 6107, duration: 3.832s, episode steps:  28, steps per second:   7, episode reward: 95.500, mean reward:  3.411 [-20.000, 19.900], mean action: 0.929 [0.000, 2.000],  loss: 89.274274, mean_q: 39.540570, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     224680/2000000000: episode: 6108, duration: 5.219s, episode steps:  35, steps per second:   7, episode reward: -64.800, mean reward: -1.851 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 83.121730, mean_q: 38.903933, mean_eps: 0.100000\n","     224720/2000000000: episode: 6109, duration: 5.733s, episode steps:  40, steps per second:   7, episode reward: 109.800, mean reward:  2.745 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.165762, mean_q: 39.771005, mean_eps: 0.100000\n","     224755/2000000000: episode: 6110, duration: 5.150s, episode steps:  35, steps per second:   7, episode reward: 94.400, mean reward:  2.697 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 72.341712, mean_q: 40.206233, mean_eps: 0.100000\n","     224791/2000000000: episode: 6111, duration: 5.154s, episode steps:  36, steps per second:   7, episode reward: 100.700, mean reward:  2.797 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.850912, mean_q: 40.746982, mean_eps: 0.100000\n","     224817/2000000000: episode: 6112, duration: 3.638s, episode steps:  26, steps per second:   7, episode reward: 179.300, mean reward:  6.896 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 77.986192, mean_q: 39.718253, mean_eps: 0.100000\n","     224854/2000000000: episode: 6113, duration: 5.022s, episode steps:  37, steps per second:   7, episode reward: 53.600, mean reward:  1.449 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 81.322375, mean_q: 40.046246, mean_eps: 0.100000\n","     224894/2000000000: episode: 6114, duration: 5.525s, episode steps:  40, steps per second:   7, episode reward: -116.200, mean reward: -2.905 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.276696, mean_q: 38.722507, mean_eps: 0.100000\n","     224924/2000000000: episode: 6115, duration: 4.259s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 88.385960, mean_q: 40.383026, mean_eps: 0.100000\n","     224955/2000000000: episode: 6116, duration: 4.240s, episode steps:  31, steps per second:   7, episode reward: 115.500, mean reward:  3.726 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 76.232577, mean_q: 39.425351, mean_eps: 0.100000\n","     224986/2000000000: episode: 6117, duration: 4.311s, episode steps:  31, steps per second:   7, episode reward: 127.100, mean reward:  4.100 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 71.480457, mean_q: 39.915744, mean_eps: 0.100000\n","     225017/2000000000: episode: 6118, duration: 4.410s, episode steps:  31, steps per second:   7, episode reward: 126.300, mean reward:  4.074 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 78.082096, mean_q: 40.344052, mean_eps: 0.100000\n","     225057/2000000000: episode: 6119, duration: 5.753s, episode steps:  40, steps per second:   7, episode reward: 45.600, mean reward:  1.140 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.692049, mean_q: 39.219266, mean_eps: 0.100000\n","     225097/2000000000: episode: 6120, duration: 5.450s, episode steps:  40, steps per second:   7, episode reward: 55.200, mean reward:  1.380 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.499183, mean_q: 40.609485, mean_eps: 0.100000\n","     225129/2000000000: episode: 6121, duration: 4.415s, episode steps:  32, steps per second:   7, episode reward: 40.800, mean reward:  1.275 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.594552, mean_q: 39.547188, mean_eps: 0.100000\n","     225167/2000000000: episode: 6122, duration: 5.550s, episode steps:  38, steps per second:   7, episode reward: -96.000, mean reward: -2.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 77.050753, mean_q: 39.494899, mean_eps: 0.100000\n","     225206/2000000000: episode: 6123, duration: 5.518s, episode steps:  39, steps per second:   7, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 81.092323, mean_q: 40.774332, mean_eps: 0.100000\n","     225246/2000000000: episode: 6124, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: -67.500, mean reward: -1.687 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.189772, mean_q: 39.664931, mean_eps: 0.100000\n","     225286/2000000000: episode: 6125, duration: 5.489s, episode steps:  40, steps per second:   7, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 79.471501, mean_q: 39.780297, mean_eps: 0.100000\n","     225322/2000000000: episode: 6126, duration: 5.609s, episode steps:  36, steps per second:   6, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.546859, mean_q: 39.996287, mean_eps: 0.100000\n","     225362/2000000000: episode: 6127, duration: 5.718s, episode steps:  40, steps per second:   7, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.131787, mean_q: 40.275181, mean_eps: 0.100000\n","     225394/2000000000: episode: 6128, duration: 4.643s, episode steps:  32, steps per second:   7, episode reward: 49.200, mean reward:  1.538 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.036628, mean_q: 39.050295, mean_eps: 0.100000\n","     225434/2000000000: episode: 6129, duration: 5.801s, episode steps:  40, steps per second:   7, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 91.344828, mean_q: 39.919915, mean_eps: 0.100000\n","     225466/2000000000: episode: 6130, duration: 4.530s, episode steps:  32, steps per second:   7, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 83.225327, mean_q: 40.601808, mean_eps: 0.100000\n","     225501/2000000000: episode: 6131, duration: 4.720s, episode steps:  35, steps per second:   7, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 77.159573, mean_q: 39.744959, mean_eps: 0.100000\n","     225541/2000000000: episode: 6132, duration: 5.300s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 82.046338, mean_q: 40.334492, mean_eps: 0.100000\n","     225574/2000000000: episode: 6133, duration: 4.461s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.938212, mean_q: 40.295821, mean_eps: 0.100000\n","     225607/2000000000: episode: 6134, duration: 4.261s, episode steps:  33, steps per second:   8, episode reward: -96.000, mean reward: -2.909 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 79.316029, mean_q: 40.749856, mean_eps: 0.100000\n","     225635/2000000000: episode: 6135, duration: 3.721s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.932826, mean_q: 39.314340, mean_eps: 0.100000\n","     225666/2000000000: episode: 6136, duration: 4.099s, episode steps:  31, steps per second:   8, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.289506, mean_q: 40.087649, mean_eps: 0.100000\n","     225706/2000000000: episode: 6137, duration: 5.340s, episode steps:  40, steps per second:   7, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.093328, mean_q: 40.298320, mean_eps: 0.100000\n","     225737/2000000000: episode: 6138, duration: 4.132s, episode steps:  31, steps per second:   8, episode reward: 110.800, mean reward:  3.574 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 79.783848, mean_q: 38.972919, mean_eps: 0.100000\n","     225777/2000000000: episode: 6139, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: 96.700, mean reward:  2.418 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.718875, mean_q: 39.738042, mean_eps: 0.100000\n","     225817/2000000000: episode: 6140, duration: 5.350s, episode steps:  40, steps per second:   7, episode reward: 12.600, mean reward:  0.315 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 79.239714, mean_q: 39.431342, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     225857/2000000000: episode: 6141, duration: 5.777s, episode steps:  40, steps per second:   7, episode reward: -97.600, mean reward: -2.440 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 83.311062, mean_q: 39.971879, mean_eps: 0.100000\n","     225897/2000000000: episode: 6142, duration: 5.778s, episode steps:  40, steps per second:   7, episode reward: 135.300, mean reward:  3.382 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.698230, mean_q: 39.570329, mean_eps: 0.100000\n","     225937/2000000000: episode: 6143, duration: 5.468s, episode steps:  40, steps per second:   7, episode reward: 125.600, mean reward:  3.140 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.877678, mean_q: 39.194775, mean_eps: 0.100000\n","     225971/2000000000: episode: 6144, duration: 4.672s, episode steps:  34, steps per second:   7, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 79.421390, mean_q: 40.031106, mean_eps: 0.100000\n","     225998/2000000000: episode: 6145, duration: 3.869s, episode steps:  27, steps per second:   7, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 82.511850, mean_q: 39.856815, mean_eps: 0.100000\n","     226036/2000000000: episode: 6146, duration: 5.308s, episode steps:  38, steps per second:   7, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 78.155540, mean_q: 40.155166, mean_eps: 0.100000\n","     226074/2000000000: episode: 6147, duration: 5.186s, episode steps:  38, steps per second:   7, episode reward: -43.100, mean reward: -1.134 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 74.404572, mean_q: 40.661257, mean_eps: 0.100000\n","     226109/2000000000: episode: 6148, duration: 5.125s, episode steps:  35, steps per second:   7, episode reward: -43.600, mean reward: -1.246 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 87.113085, mean_q: 39.678483, mean_eps: 0.100000\n","     226148/2000000000: episode: 6149, duration: 5.400s, episode steps:  39, steps per second:   7, episode reward: -13.300, mean reward: -0.341 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 77.275050, mean_q: 39.659956, mean_eps: 0.100000\n","     226188/2000000000: episode: 6150, duration: 5.554s, episode steps:  40, steps per second:   7, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.412389, mean_q: 39.483536, mean_eps: 0.100000\n","     226221/2000000000: episode: 6151, duration: 4.337s, episode steps:  33, steps per second:   8, episode reward: 132.900, mean reward:  4.027 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 74.376989, mean_q: 39.611026, mean_eps: 0.100000\n","     226258/2000000000: episode: 6152, duration: 4.924s, episode steps:  37, steps per second:   8, episode reward: 65.900, mean reward:  1.781 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 77.205315, mean_q: 39.916262, mean_eps: 0.100000\n","     226289/2000000000: episode: 6153, duration: 4.212s, episode steps:  31, steps per second:   7, episode reward: 141.000, mean reward:  4.548 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 72.926608, mean_q: 40.118868, mean_eps: 0.100000\n","     226329/2000000000: episode: 6154, duration: 5.607s, episode steps:  40, steps per second:   7, episode reward: 141.600, mean reward:  3.540 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.192487, mean_q: 40.252170, mean_eps: 0.100000\n","     226360/2000000000: episode: 6155, duration: 4.357s, episode steps:  31, steps per second:   7, episode reward: -58.000, mean reward: -1.871 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 70.452616, mean_q: 39.780428, mean_eps: 0.100000\n","     226389/2000000000: episode: 6156, duration: 3.960s, episode steps:  29, steps per second:   7, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.217486, mean_q: 40.263002, mean_eps: 0.100000\n","     226421/2000000000: episode: 6157, duration: 4.297s, episode steps:  32, steps per second:   7, episode reward: -74.000, mean reward: -2.313 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 78.023979, mean_q: 39.312881, mean_eps: 0.100000\n","     226454/2000000000: episode: 6158, duration: 4.346s, episode steps:  33, steps per second:   8, episode reward: 44.400, mean reward:  1.345 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 81.654593, mean_q: 39.804188, mean_eps: 0.100000\n","     226494/2000000000: episode: 6159, duration: 5.488s, episode steps:  40, steps per second:   7, episode reward: 93.300, mean reward:  2.332 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 77.903985, mean_q: 40.444836, mean_eps: 0.100000\n","     226529/2000000000: episode: 6160, duration: 4.774s, episode steps:  35, steps per second:   7, episode reward: 86.800, mean reward:  2.480 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.711334, mean_q: 39.506966, mean_eps: 0.100000\n","     226569/2000000000: episode: 6161, duration: 5.351s, episode steps:  40, steps per second:   7, episode reward: 120.500, mean reward:  3.012 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 81.587255, mean_q: 40.379358, mean_eps: 0.100000\n","     226608/2000000000: episode: 6162, duration: 5.056s, episode steps:  39, steps per second:   8, episode reward: 40.200, mean reward:  1.031 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 81.033916, mean_q: 40.765400, mean_eps: 0.100000\n","     226648/2000000000: episode: 6163, duration: 5.537s, episode steps:  40, steps per second:   7, episode reward: -41.100, mean reward: -1.028 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.363370, mean_q: 39.238490, mean_eps: 0.100000\n","     226676/2000000000: episode: 6164, duration: 4.005s, episode steps:  28, steps per second:   7, episode reward: 57.400, mean reward:  2.050 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 80.619194, mean_q: 40.716189, mean_eps: 0.100000\n","     226714/2000000000: episode: 6165, duration: 5.025s, episode steps:  38, steps per second:   8, episode reward: 113.900, mean reward:  2.997 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 84.845390, mean_q: 39.517649, mean_eps: 0.100000\n","     226751/2000000000: episode: 6166, duration: 4.962s, episode steps:  37, steps per second:   7, episode reward: -66.400, mean reward: -1.795 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 82.149153, mean_q: 40.298883, mean_eps: 0.100000\n","     226790/2000000000: episode: 6167, duration: 5.412s, episode steps:  39, steps per second:   7, episode reward: -10.200, mean reward: -0.262 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 75.164085, mean_q: 39.360557, mean_eps: 0.100000\n","     226830/2000000000: episode: 6168, duration: 5.560s, episode steps:  40, steps per second:   7, episode reward: 84.300, mean reward:  2.108 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.681254, mean_q: 39.975312, mean_eps: 0.100000\n","     226859/2000000000: episode: 6169, duration: 3.971s, episode steps:  29, steps per second:   7, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 89.814543, mean_q: 39.289563, mean_eps: 0.100000\n","     226889/2000000000: episode: 6170, duration: 4.178s, episode steps:  30, steps per second:   7, episode reward: 21.900, mean reward:  0.730 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 77.813307, mean_q: 40.036851, mean_eps: 0.100000\n","     226924/2000000000: episode: 6171, duration: 4.725s, episode steps:  35, steps per second:   7, episode reward: -79.400, mean reward: -2.269 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 69.693431, mean_q: 39.718168, mean_eps: 0.100000\n","     226954/2000000000: episode: 6172, duration: 4.351s, episode steps:  30, steps per second:   7, episode reward: 47.300, mean reward:  1.577 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.682810, mean_q: 39.215390, mean_eps: 0.100000\n","     226992/2000000000: episode: 6173, duration: 5.143s, episode steps:  38, steps per second:   7, episode reward: 135.400, mean reward:  3.563 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.518355, mean_q: 39.904371, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     227023/2000000000: episode: 6174, duration: 4.302s, episode steps:  31, steps per second:   7, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.135463, mean_q: 39.972882, mean_eps: 0.100000\n","     227063/2000000000: episode: 6175, duration: 6.464s, episode steps:  40, steps per second:   6, episode reward: 11.000, mean reward:  0.275 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 84.316552, mean_q: 40.455672, mean_eps: 0.100000\n","     227094/2000000000: episode: 6176, duration: 4.668s, episode steps:  31, steps per second:   7, episode reward: 105.300, mean reward:  3.397 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.333913, mean_q: 39.170866, mean_eps: 0.100000\n","     227124/2000000000: episode: 6177, duration: 4.578s, episode steps:  30, steps per second:   7, episode reward: 135.500, mean reward:  4.517 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 77.324548, mean_q: 40.133330, mean_eps: 0.100000\n","     227153/2000000000: episode: 6178, duration: 4.205s, episode steps:  29, steps per second:   7, episode reward: 110.800, mean reward:  3.821 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 76.656381, mean_q: 41.338427, mean_eps: 0.100000\n","     227193/2000000000: episode: 6179, duration: 5.521s, episode steps:  40, steps per second:   7, episode reward: 30.500, mean reward:  0.762 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.191440, mean_q: 40.840120, mean_eps: 0.100000\n","     227233/2000000000: episode: 6180, duration: 5.628s, episode steps:  40, steps per second:   7, episode reward: 52.700, mean reward:  1.317 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 77.389782, mean_q: 39.244219, mean_eps: 0.100000\n","     227273/2000000000: episode: 6181, duration: 5.403s, episode steps:  40, steps per second:   7, episode reward: 192.000, mean reward:  4.800 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 82.714414, mean_q: 39.519313, mean_eps: 0.100000\n","     227311/2000000000: episode: 6182, duration: 5.023s, episode steps:  38, steps per second:   8, episode reward: 146.700, mean reward:  3.861 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 78.112950, mean_q: 39.156418, mean_eps: 0.100000\n","     227348/2000000000: episode: 6183, duration: 5.004s, episode steps:  37, steps per second:   7, episode reward: -47.200, mean reward: -1.276 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 84.020608, mean_q: 40.127522, mean_eps: 0.100000\n","     227382/2000000000: episode: 6184, duration: 4.749s, episode steps:  34, steps per second:   7, episode reward: -48.500, mean reward: -1.426 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 80.013025, mean_q: 39.820384, mean_eps: 0.100000\n","     227422/2000000000: episode: 6185, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: 116.900, mean reward:  2.922 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 77.763524, mean_q: 40.586345, mean_eps: 0.100000\n","     227462/2000000000: episode: 6186, duration: 5.291s, episode steps:  40, steps per second:   8, episode reward: 142.000, mean reward:  3.550 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 75.619815, mean_q: 39.842440, mean_eps: 0.100000\n","     227501/2000000000: episode: 6187, duration: 5.359s, episode steps:  39, steps per second:   7, episode reward: 117.600, mean reward:  3.015 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 79.432251, mean_q: 40.108990, mean_eps: 0.100000\n","     227541/2000000000: episode: 6188, duration: 5.585s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.191145, mean_q: 40.256396, mean_eps: 0.100000\n","     227579/2000000000: episode: 6189, duration: 5.055s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 83.987612, mean_q: 40.267492, mean_eps: 0.100000\n","     227608/2000000000: episode: 6190, duration: 4.416s, episode steps:  29, steps per second:   7, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 77.748777, mean_q: 39.589013, mean_eps: 0.100000\n","     227641/2000000000: episode: 6191, duration: 4.462s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 74.321590, mean_q: 40.878885, mean_eps: 0.100000\n","     227667/2000000000: episode: 6192, duration: 3.510s, episode steps:  26, steps per second:   7, episode reward: -46.700, mean reward: -1.796 [-20.000, 19.600], mean action: 0.885 [0.000, 2.000],  loss: 78.984236, mean_q: 39.485811, mean_eps: 0.100000\n","     227707/2000000000: episode: 6193, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: 114.500, mean reward:  2.862 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.445307, mean_q: 40.204877, mean_eps: 0.100000\n","     227741/2000000000: episode: 6194, duration: 4.480s, episode steps:  34, steps per second:   8, episode reward: 32.800, mean reward:  0.965 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 79.359004, mean_q: 40.158812, mean_eps: 0.100000\n","     227781/2000000000: episode: 6195, duration: 5.184s, episode steps:  40, steps per second:   8, episode reward: 25.100, mean reward:  0.628 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.201999, mean_q: 40.287669, mean_eps: 0.100000\n","     227808/2000000000: episode: 6196, duration: 3.889s, episode steps:  27, steps per second:   7, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 84.988784, mean_q: 39.593875, mean_eps: 0.100000\n","     227848/2000000000: episode: 6197, duration: 5.373s, episode steps:  40, steps per second:   7, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.949734, mean_q: 40.477202, mean_eps: 0.100000\n","     227881/2000000000: episode: 6198, duration: 4.292s, episode steps:  33, steps per second:   8, episode reward: -103.000, mean reward: -3.121 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.063809, mean_q: 40.576638, mean_eps: 0.100000\n","     227921/2000000000: episode: 6199, duration: 5.370s, episode steps:  40, steps per second:   7, episode reward: 240.300, mean reward:  6.007 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 78.627575, mean_q: 39.852788, mean_eps: 0.100000\n","     227957/2000000000: episode: 6200, duration: 4.738s, episode steps:  36, steps per second:   8, episode reward: 122.400, mean reward:  3.400 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 83.310136, mean_q: 39.960691, mean_eps: 0.100000\n","     227997/2000000000: episode: 6201, duration: 5.248s, episode steps:  40, steps per second:   8, episode reward: 144.400, mean reward:  3.610 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.853811, mean_q: 39.600268, mean_eps: 0.100000\n","     228037/2000000000: episode: 6202, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: 144.100, mean reward:  3.602 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.811497, mean_q: 40.282992, mean_eps: 0.100000\n","     228065/2000000000: episode: 6203, duration: 3.610s, episode steps:  28, steps per second:   8, episode reward: 60.800, mean reward:  2.171 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 80.825661, mean_q: 40.874862, mean_eps: 0.100000\n","     228095/2000000000: episode: 6204, duration: 3.777s, episode steps:  30, steps per second:   8, episode reward: 50.500, mean reward:  1.683 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 79.289529, mean_q: 40.632044, mean_eps: 0.100000\n","     228135/2000000000: episode: 6205, duration: 5.269s, episode steps:  40, steps per second:   8, episode reward: -38.900, mean reward: -0.972 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 85.750586, mean_q: 39.651567, mean_eps: 0.100000\n","     228162/2000000000: episode: 6206, duration: 3.478s, episode steps:  27, steps per second:   8, episode reward:  9.600, mean reward:  0.356 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 73.355741, mean_q: 40.039464, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     228202/2000000000: episode: 6207, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: 157.100, mean reward:  3.928 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.265569, mean_q: 38.875311, mean_eps: 0.100000\n","     228235/2000000000: episode: 6208, duration: 4.448s, episode steps:  33, steps per second:   7, episode reward: 80.300, mean reward:  2.433 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 73.680037, mean_q: 40.445999, mean_eps: 0.100000\n","     228266/2000000000: episode: 6209, duration: 4.029s, episode steps:  31, steps per second:   8, episode reward: -78.900, mean reward: -2.545 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.199556, mean_q: 40.434165, mean_eps: 0.100000\n","     228299/2000000000: episode: 6210, duration: 4.183s, episode steps:  33, steps per second:   8, episode reward: 149.300, mean reward:  4.524 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.275835, mean_q: 39.598865, mean_eps: 0.100000\n","     228339/2000000000: episode: 6211, duration: 5.187s, episode steps:  40, steps per second:   8, episode reward: 13.400, mean reward:  0.335 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.178953, mean_q: 39.907809, mean_eps: 0.100000\n","     228376/2000000000: episode: 6212, duration: 4.673s, episode steps:  37, steps per second:   8, episode reward: 206.700, mean reward:  5.586 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 82.745742, mean_q: 40.286530, mean_eps: 0.100000\n","     228416/2000000000: episode: 6213, duration: 5.155s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 84.533081, mean_q: 40.369510, mean_eps: 0.100000\n","     228454/2000000000: episode: 6214, duration: 5.351s, episode steps:  38, steps per second:   7, episode reward: 221.500, mean reward:  5.829 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 86.330933, mean_q: 40.461439, mean_eps: 0.100000\n","     228488/2000000000: episode: 6215, duration: 4.595s, episode steps:  34, steps per second:   7, episode reward: 118.800, mean reward:  3.494 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 85.579095, mean_q: 39.474184, mean_eps: 0.100000\n","     228522/2000000000: episode: 6216, duration: 4.871s, episode steps:  34, steps per second:   7, episode reward: 176.000, mean reward:  5.176 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 82.165320, mean_q: 40.794873, mean_eps: 0.100000\n","     228557/2000000000: episode: 6217, duration: 4.485s, episode steps:  35, steps per second:   8, episode reward: 46.300, mean reward:  1.323 [-20.000, 19.600], mean action: 1.114 [0.000, 2.000],  loss: 75.820615, mean_q: 40.455724, mean_eps: 0.100000\n","     228597/2000000000: episode: 6218, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: -10.900, mean reward: -0.272 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 70.737597, mean_q: 39.480813, mean_eps: 0.100000\n","     228628/2000000000: episode: 6219, duration: 4.278s, episode steps:  31, steps per second:   7, episode reward: 202.500, mean reward:  6.532 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 73.779399, mean_q: 40.415328, mean_eps: 0.100000\n","     228657/2000000000: episode: 6220, duration: 4.108s, episode steps:  29, steps per second:   7, episode reward: 103.100, mean reward:  3.555 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.048981, mean_q: 39.686683, mean_eps: 0.100000\n","     228683/2000000000: episode: 6221, duration: 3.572s, episode steps:  26, steps per second:   7, episode reward: 107.200, mean reward:  4.123 [-20.000, 18.900], mean action: 0.808 [0.000, 2.000],  loss: 76.476690, mean_q: 39.457432, mean_eps: 0.100000\n","     228723/2000000000: episode: 6222, duration: 5.506s, episode steps:  40, steps per second:   7, episode reward: 199.800, mean reward:  4.995 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.649392, mean_q: 40.319016, mean_eps: 0.100000\n","     228754/2000000000: episode: 6223, duration: 4.151s, episode steps:  31, steps per second:   7, episode reward: 113.300, mean reward:  3.655 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 81.649785, mean_q: 39.943176, mean_eps: 0.100000\n","     228794/2000000000: episode: 6224, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward: 100.400, mean reward:  2.510 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 87.227569, mean_q: 40.799653, mean_eps: 0.100000\n","     228834/2000000000: episode: 6225, duration: 5.305s, episode steps:  40, steps per second:   8, episode reward: 153.100, mean reward:  3.827 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.061229, mean_q: 39.654003, mean_eps: 0.100000\n","     228872/2000000000: episode: 6226, duration: 5.072s, episode steps:  38, steps per second:   7, episode reward: 92.500, mean reward:  2.434 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 85.425799, mean_q: 40.114850, mean_eps: 0.100000\n","     228909/2000000000: episode: 6227, duration: 4.912s, episode steps:  37, steps per second:   8, episode reward: -71.800, mean reward: -1.941 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 75.779074, mean_q: 40.954881, mean_eps: 0.100000\n","     228940/2000000000: episode: 6228, duration: 4.186s, episode steps:  31, steps per second:   7, episode reward: 95.200, mean reward:  3.071 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 80.983892, mean_q: 40.232148, mean_eps: 0.100000\n","     228969/2000000000: episode: 6229, duration: 3.809s, episode steps:  29, steps per second:   8, episode reward: 140.200, mean reward:  4.834 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 82.899647, mean_q: 40.184429, mean_eps: 0.100000\n","     228995/2000000000: episode: 6230, duration: 3.587s, episode steps:  26, steps per second:   7, episode reward: 22.300, mean reward:  0.858 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 88.825701, mean_q: 40.868158, mean_eps: 0.100000\n","     229034/2000000000: episode: 6231, duration: 5.431s, episode steps:  39, steps per second:   7, episode reward: 64.100, mean reward:  1.644 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 83.473537, mean_q: 39.875777, mean_eps: 0.100000\n","     229073/2000000000: episode: 6232, duration: 5.193s, episode steps:  39, steps per second:   8, episode reward: 198.800, mean reward:  5.097 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 88.052371, mean_q: 40.021740, mean_eps: 0.100000\n","     229111/2000000000: episode: 6233, duration: 5.260s, episode steps:  38, steps per second:   7, episode reward: 145.000, mean reward:  3.816 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 85.571229, mean_q: 38.961047, mean_eps: 0.100000\n","     229147/2000000000: episode: 6234, duration: 4.641s, episode steps:  36, steps per second:   8, episode reward: -96.000, mean reward: -2.667 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 84.011203, mean_q: 39.771012, mean_eps: 0.100000\n","     229177/2000000000: episode: 6235, duration: 3.835s, episode steps:  30, steps per second:   8, episode reward: -60.800, mean reward: -2.027 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.908605, mean_q: 41.119516, mean_eps: 0.100000\n","     229217/2000000000: episode: 6236, duration: 5.226s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.470812, mean_q: 40.417041, mean_eps: 0.100000\n","     229253/2000000000: episode: 6237, duration: 4.646s, episode steps:  36, steps per second:   8, episode reward: 156.800, mean reward:  4.356 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 72.874452, mean_q: 39.517475, mean_eps: 0.100000\n","     229285/2000000000: episode: 6238, duration: 4.332s, episode steps:  32, steps per second:   7, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 75.986390, mean_q: 40.023275, mean_eps: 0.100000\n","     229325/2000000000: episode: 6239, duration: 5.264s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 83.440856, mean_q: 39.174602, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     229355/2000000000: episode: 6240, duration: 3.907s, episode steps:  30, steps per second:   8, episode reward: -22.600, mean reward: -0.753 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 76.830659, mean_q: 39.936139, mean_eps: 0.100000\n","     229388/2000000000: episode: 6241, duration: 4.333s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 84.364152, mean_q: 39.724742, mean_eps: 0.100000\n","     229428/2000000000: episode: 6242, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: -117.400, mean reward: -2.935 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.501236, mean_q: 39.462992, mean_eps: 0.100000\n","     229468/2000000000: episode: 6243, duration: 5.137s, episode steps:  40, steps per second:   8, episode reward: 82.600, mean reward:  2.065 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 76.074706, mean_q: 40.564753, mean_eps: 0.100000\n","     229500/2000000000: episode: 6244, duration: 4.282s, episode steps:  32, steps per second:   7, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.672511, mean_q: 39.458850, mean_eps: 0.100000\n","     229539/2000000000: episode: 6245, duration: 5.189s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 82.205003, mean_q: 39.883505, mean_eps: 0.100000\n","     229574/2000000000: episode: 6246, duration: 4.662s, episode steps:  35, steps per second:   8, episode reward: 208.000, mean reward:  5.943 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 75.741634, mean_q: 41.023923, mean_eps: 0.100000\n","     229608/2000000000: episode: 6247, duration: 4.595s, episode steps:  34, steps per second:   7, episode reward:  1.300, mean reward:  0.038 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 78.878613, mean_q: 39.865791, mean_eps: 0.100000\n","     229636/2000000000: episode: 6248, duration: 4.009s, episode steps:  28, steps per second:   7, episode reward: 70.400, mean reward:  2.514 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 70.462429, mean_q: 39.672773, mean_eps: 0.100000\n","     229667/2000000000: episode: 6249, duration: 4.544s, episode steps:  31, steps per second:   7, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 80.269737, mean_q: 40.075204, mean_eps: 0.100000\n","     229702/2000000000: episode: 6250, duration: 5.417s, episode steps:  35, steps per second:   6, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 78.373485, mean_q: 39.338349, mean_eps: 0.100000\n","     229741/2000000000: episode: 6251, duration: 5.405s, episode steps:  39, steps per second:   7, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 90.048536, mean_q: 39.532829, mean_eps: 0.100000\n","     229781/2000000000: episode: 6252, duration: 5.383s, episode steps:  40, steps per second:   7, episode reward: 59.000, mean reward:  1.475 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 74.917673, mean_q: 39.400010, mean_eps: 0.100000\n","     229812/2000000000: episode: 6253, duration: 4.100s, episode steps:  31, steps per second:   8, episode reward: 34.700, mean reward:  1.119 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 74.582979, mean_q: 40.485017, mean_eps: 0.100000\n","     229843/2000000000: episode: 6254, duration: 4.064s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 76.834111, mean_q: 40.955410, mean_eps: 0.100000\n","     229883/2000000000: episode: 6255, duration: 5.263s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.174766, mean_q: 38.922177, mean_eps: 0.100000\n","     229921/2000000000: episode: 6256, duration: 4.976s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 82.797010, mean_q: 40.607161, mean_eps: 0.100000\n","     229961/2000000000: episode: 6257, duration: 5.152s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.981428, mean_q: 40.130629, mean_eps: 0.100000\n","     230001/2000000000: episode: 6258, duration: 5.345s, episode steps:  40, steps per second:   7, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.898279, mean_q: 39.978147, mean_eps: 0.100000\n","     230038/2000000000: episode: 6259, duration: 4.901s, episode steps:  37, steps per second:   8, episode reward: 235.700, mean reward:  6.370 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 86.453434, mean_q: 40.655714, mean_eps: 0.100000\n","     230072/2000000000: episode: 6260, duration: 4.619s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 77.451727, mean_q: 40.629704, mean_eps: 0.100000\n","     230112/2000000000: episode: 6261, duration: 5.369s, episode steps:  40, steps per second:   7, episode reward: 32.200, mean reward:  0.805 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 88.982958, mean_q: 42.018217, mean_eps: 0.100000\n","     230149/2000000000: episode: 6262, duration: 4.867s, episode steps:  37, steps per second:   8, episode reward: 16.600, mean reward:  0.449 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.880685, mean_q: 40.819422, mean_eps: 0.100000\n","     230189/2000000000: episode: 6263, duration: 5.372s, episode steps:  40, steps per second:   7, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 85.603930, mean_q: 41.288153, mean_eps: 0.100000\n","     230216/2000000000: episode: 6264, duration: 3.972s, episode steps:  27, steps per second:   7, episode reward: 47.300, mean reward:  1.752 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 81.176572, mean_q: 40.698291, mean_eps: 0.100000\n","     230255/2000000000: episode: 6265, duration: 5.389s, episode steps:  39, steps per second:   7, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 95.947283, mean_q: 41.442210, mean_eps: 0.100000\n","     230295/2000000000: episode: 6266, duration: 5.239s, episode steps:  40, steps per second:   8, episode reward: -104.800, mean reward: -2.620 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.722794, mean_q: 41.519790, mean_eps: 0.100000\n","     230324/2000000000: episode: 6267, duration: 3.643s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 81.627849, mean_q: 40.974875, mean_eps: 0.100000\n","     230364/2000000000: episode: 6268, duration: 4.955s, episode steps:  40, steps per second:   8, episode reward: -2.800, mean reward: -0.070 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.648719, mean_q: 40.288935, mean_eps: 0.100000\n","     230403/2000000000: episode: 6269, duration: 4.978s, episode steps:  39, steps per second:   8, episode reward: -59.000, mean reward: -1.513 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 84.408585, mean_q: 41.111392, mean_eps: 0.100000\n","     230437/2000000000: episode: 6270, duration: 4.227s, episode steps:  34, steps per second:   8, episode reward: 108.100, mean reward:  3.179 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 86.051463, mean_q: 41.143396, mean_eps: 0.100000\n","     230467/2000000000: episode: 6271, duration: 3.853s, episode steps:  30, steps per second:   8, episode reward: 18.200, mean reward:  0.607 [-20.000, 18.000], mean action: 0.767 [0.000, 2.000],  loss: 82.422246, mean_q: 41.183168, mean_eps: 0.100000\n","     230507/2000000000: episode: 6272, duration: 5.242s, episode steps:  40, steps per second:   8, episode reward: 26.700, mean reward:  0.667 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.754129, mean_q: 41.489493, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     230547/2000000000: episode: 6273, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward: 39.800, mean reward:  0.995 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 84.359595, mean_q: 40.539314, mean_eps: 0.100000\n","     230579/2000000000: episode: 6274, duration: 4.297s, episode steps:  32, steps per second:   7, episode reward: 23.600, mean reward:  0.737 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 81.839243, mean_q: 41.688641, mean_eps: 0.100000\n","     230619/2000000000: episode: 6275, duration: 5.313s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.776364, mean_q: 41.354622, mean_eps: 0.100000\n","     230656/2000000000: episode: 6276, duration: 4.822s, episode steps:  37, steps per second:   8, episode reward: 143.500, mean reward:  3.878 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 79.851327, mean_q: 40.989664, mean_eps: 0.100000\n","     230696/2000000000: episode: 6277, duration: 5.318s, episode steps:  40, steps per second:   8, episode reward: 28.200, mean reward:  0.705 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 85.395675, mean_q: 40.759899, mean_eps: 0.100000\n","     230736/2000000000: episode: 6278, duration: 5.269s, episode steps:  40, steps per second:   8, episode reward: 127.100, mean reward:  3.177 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.318381, mean_q: 41.110437, mean_eps: 0.100000\n","     230775/2000000000: episode: 6279, duration: 5.090s, episode steps:  39, steps per second:   8, episode reward: 179.300, mean reward:  4.597 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 84.875661, mean_q: 41.553497, mean_eps: 0.100000\n","     230804/2000000000: episode: 6280, duration: 3.697s, episode steps:  29, steps per second:   8, episode reward: 93.000, mean reward:  3.207 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 74.662698, mean_q: 41.301138, mean_eps: 0.100000\n","     230844/2000000000: episode: 6281, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: -14.000, mean reward: -0.350 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 83.433573, mean_q: 41.386467, mean_eps: 0.100000\n","     230883/2000000000: episode: 6282, duration: 5.241s, episode steps:  39, steps per second:   7, episode reward: 29.800, mean reward:  0.764 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 86.203603, mean_q: 40.860932, mean_eps: 0.100000\n","     230912/2000000000: episode: 6283, duration: 3.772s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 82.565626, mean_q: 42.547631, mean_eps: 0.100000\n","     230952/2000000000: episode: 6284, duration: 5.092s, episode steps:  40, steps per second:   8, episode reward: 57.300, mean reward:  1.433 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.234572, mean_q: 41.226295, mean_eps: 0.100000\n","     230979/2000000000: episode: 6285, duration: 3.589s, episode steps:  27, steps per second:   8, episode reward: 140.100, mean reward:  5.189 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 79.736943, mean_q: 41.880620, mean_eps: 0.100000\n","     231012/2000000000: episode: 6286, duration: 4.143s, episode steps:  33, steps per second:   8, episode reward: -7.900, mean reward: -0.239 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 76.036520, mean_q: 40.877598, mean_eps: 0.100000\n","     231048/2000000000: episode: 6287, duration: 4.688s, episode steps:  36, steps per second:   8, episode reward: 70.400, mean reward:  1.956 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 71.886186, mean_q: 41.293177, mean_eps: 0.100000\n","     231077/2000000000: episode: 6288, duration: 3.903s, episode steps:  29, steps per second:   7, episode reward: 128.700, mean reward:  4.438 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 81.192427, mean_q: 40.801127, mean_eps: 0.100000\n","     231117/2000000000: episode: 6289, duration: 5.202s, episode steps:  40, steps per second:   8, episode reward: 104.400, mean reward:  2.610 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.085319, mean_q: 41.410460, mean_eps: 0.100000\n","     231151/2000000000: episode: 6290, duration: 4.480s, episode steps:  34, steps per second:   8, episode reward: 102.300, mean reward:  3.009 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 69.648510, mean_q: 41.973388, mean_eps: 0.100000\n","     231188/2000000000: episode: 6291, duration: 4.851s, episode steps:  37, steps per second:   8, episode reward: 300.000, mean reward:  8.108 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.044048, mean_q: 42.146384, mean_eps: 0.100000\n","     231220/2000000000: episode: 6292, duration: 4.260s, episode steps:  32, steps per second:   8, episode reward: 73.600, mean reward:  2.300 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.148802, mean_q: 40.977517, mean_eps: 0.100000\n","     231260/2000000000: episode: 6293, duration: 5.186s, episode steps:  40, steps per second:   8, episode reward: 118.400, mean reward:  2.960 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.580127, mean_q: 41.250285, mean_eps: 0.100000\n","     231296/2000000000: episode: 6294, duration: 4.542s, episode steps:  36, steps per second:   8, episode reward: 20.900, mean reward:  0.581 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 86.268305, mean_q: 41.730009, mean_eps: 0.100000\n","     231326/2000000000: episode: 6295, duration: 4.017s, episode steps:  30, steps per second:   7, episode reward: 58.800, mean reward:  1.960 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 78.751689, mean_q: 41.260854, mean_eps: 0.100000\n","     231362/2000000000: episode: 6296, duration: 4.556s, episode steps:  36, steps per second:   8, episode reward: 40.200, mean reward:  1.117 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 80.682852, mean_q: 41.583339, mean_eps: 0.100000\n","     231392/2000000000: episode: 6297, duration: 3.946s, episode steps:  30, steps per second:   8, episode reward: 67.100, mean reward:  2.237 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.946996, mean_q: 41.479160, mean_eps: 0.100000\n","     231420/2000000000: episode: 6298, duration: 3.759s, episode steps:  28, steps per second:   7, episode reward: 79.200, mean reward:  2.829 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 71.638161, mean_q: 41.828100, mean_eps: 0.100000\n","     231449/2000000000: episode: 6299, duration: 3.887s, episode steps:  29, steps per second:   7, episode reward: -3.500, mean reward: -0.121 [-20.000, 18.000], mean action: 1.207 [0.000, 2.000],  loss: 90.973982, mean_q: 41.480757, mean_eps: 0.100000\n","     231484/2000000000: episode: 6300, duration: 4.695s, episode steps:  35, steps per second:   7, episode reward: 151.000, mean reward:  4.314 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 73.075858, mean_q: 42.405835, mean_eps: 0.100000\n","     231510/2000000000: episode: 6301, duration: 3.574s, episode steps:  26, steps per second:   7, episode reward: -21.200, mean reward: -0.815 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 82.922866, mean_q: 40.999045, mean_eps: 0.100000\n","     231546/2000000000: episode: 6302, duration: 4.677s, episode steps:  36, steps per second:   8, episode reward: 30.500, mean reward:  0.847 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 85.944115, mean_q: 41.172001, mean_eps: 0.100000\n","     231576/2000000000: episode: 6303, duration: 3.966s, episode steps:  30, steps per second:   8, episode reward: 39.000, mean reward:  1.300 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.247186, mean_q: 41.122061, mean_eps: 0.100000\n","     231610/2000000000: episode: 6304, duration: 4.294s, episode steps:  34, steps per second:   8, episode reward: 61.500, mean reward:  1.809 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 79.691571, mean_q: 41.926621, mean_eps: 0.100000\n","     231648/2000000000: episode: 6305, duration: 4.870s, episode steps:  38, steps per second:   8, episode reward:  7.400, mean reward:  0.195 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 82.045599, mean_q: 41.396617, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     231688/2000000000: episode: 6306, duration: 5.267s, episode steps:  40, steps per second:   8, episode reward: 93.000, mean reward:  2.325 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 86.108249, mean_q: 40.855404, mean_eps: 0.100000\n","     231723/2000000000: episode: 6307, duration: 4.586s, episode steps:  35, steps per second:   8, episode reward: -85.800, mean reward: -2.451 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.470706, mean_q: 40.876261, mean_eps: 0.100000\n","     231753/2000000000: episode: 6308, duration: 3.781s, episode steps:  30, steps per second:   8, episode reward: -9.700, mean reward: -0.323 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 79.467755, mean_q: 41.358147, mean_eps: 0.100000\n","     231781/2000000000: episode: 6309, duration: 3.653s, episode steps:  28, steps per second:   8, episode reward: 102.800, mean reward:  3.671 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 78.142822, mean_q: 40.535834, mean_eps: 0.100000\n","     231809/2000000000: episode: 6310, duration: 3.762s, episode steps:  28, steps per second:   7, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 76.691427, mean_q: 42.008717, mean_eps: 0.100000\n","     231837/2000000000: episode: 6311, duration: 3.594s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 79.143681, mean_q: 41.640940, mean_eps: 0.100000\n","     231877/2000000000: episode: 6312, duration: 5.075s, episode steps:  40, steps per second:   8, episode reward: -61.800, mean reward: -1.545 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 77.326312, mean_q: 42.009105, mean_eps: 0.100000\n","     231909/2000000000: episode: 6313, duration: 4.054s, episode steps:  32, steps per second:   8, episode reward: 85.300, mean reward:  2.666 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.251998, mean_q: 41.424174, mean_eps: 0.100000\n","     231944/2000000000: episode: 6314, duration: 4.478s, episode steps:  35, steps per second:   8, episode reward: 39.100, mean reward:  1.117 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 94.199744, mean_q: 40.027730, mean_eps: 0.100000\n","     231984/2000000000: episode: 6315, duration: 5.119s, episode steps:  40, steps per second:   8, episode reward: 21.100, mean reward:  0.528 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 84.705230, mean_q: 40.553837, mean_eps: 0.100000\n","     232023/2000000000: episode: 6316, duration: 5.250s, episode steps:  39, steps per second:   7, episode reward: 114.900, mean reward:  2.946 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 87.543861, mean_q: 41.292153, mean_eps: 0.100000\n","     232056/2000000000: episode: 6317, duration: 4.328s, episode steps:  33, steps per second:   8, episode reward: 132.800, mean reward:  4.024 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 77.272164, mean_q: 41.754825, mean_eps: 0.100000\n","     232085/2000000000: episode: 6318, duration: 3.841s, episode steps:  29, steps per second:   8, episode reward: -34.600, mean reward: -1.193 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 85.359766, mean_q: 41.266455, mean_eps: 0.100000\n","     232122/2000000000: episode: 6319, duration: 4.910s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.156753, mean_q: 41.572835, mean_eps: 0.100000\n","     232157/2000000000: episode: 6320, duration: 4.596s, episode steps:  35, steps per second:   8, episode reward: 149.100, mean reward:  4.260 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.819213, mean_q: 41.198162, mean_eps: 0.100000\n","     232196/2000000000: episode: 6321, duration: 4.872s, episode steps:  39, steps per second:   8, episode reward: 170.000, mean reward:  4.359 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 79.599356, mean_q: 41.284981, mean_eps: 0.100000\n","     232236/2000000000: episode: 6322, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 112.400, mean reward:  2.810 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 81.865969, mean_q: 41.168352, mean_eps: 0.100000\n","     232269/2000000000: episode: 6323, duration: 4.313s, episode steps:  33, steps per second:   8, episode reward: 202.400, mean reward:  6.133 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 86.845109, mean_q: 41.227723, mean_eps: 0.100000\n","     232301/2000000000: episode: 6324, duration: 4.371s, episode steps:  32, steps per second:   7, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 82.970241, mean_q: 40.774200, mean_eps: 0.100000\n","     232339/2000000000: episode: 6325, duration: 5.008s, episode steps:  38, steps per second:   8, episode reward: 64.900, mean reward:  1.708 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 84.601987, mean_q: 41.003576, mean_eps: 0.100000\n","     232379/2000000000: episode: 6326, duration: 5.261s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.996525, mean_q: 41.235192, mean_eps: 0.100000\n","     232410/2000000000: episode: 6327, duration: 3.967s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.179029, mean_q: 41.591665, mean_eps: 0.100000\n","     232445/2000000000: episode: 6328, duration: 4.556s, episode steps:  35, steps per second:   8, episode reward: 49.600, mean reward:  1.417 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 89.024134, mean_q: 41.123556, mean_eps: 0.100000\n","     232485/2000000000: episode: 6329, duration: 5.350s, episode steps:  40, steps per second:   7, episode reward: 111.800, mean reward:  2.795 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.915168, mean_q: 41.226063, mean_eps: 0.100000\n","     232518/2000000000: episode: 6330, duration: 4.411s, episode steps:  33, steps per second:   7, episode reward: 78.200, mean reward:  2.370 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 79.260202, mean_q: 41.170306, mean_eps: 0.100000\n","     232549/2000000000: episode: 6331, duration: 4.148s, episode steps:  31, steps per second:   7, episode reward: 113.000, mean reward:  3.645 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 85.962278, mean_q: 40.704710, mean_eps: 0.100000\n","     232582/2000000000: episode: 6332, duration: 4.610s, episode steps:  33, steps per second:   7, episode reward: 72.700, mean reward:  2.203 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 79.676953, mean_q: 41.304548, mean_eps: 0.100000\n","     232622/2000000000: episode: 6333, duration: 5.444s, episode steps:  40, steps per second:   7, episode reward: 54.900, mean reward:  1.372 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.299583, mean_q: 40.747039, mean_eps: 0.100000\n","     232662/2000000000: episode: 6334, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: 88.400, mean reward:  2.210 [-20.000, 18.400], mean action: 1.375 [0.000, 2.000],  loss: 80.529363, mean_q: 41.205912, mean_eps: 0.100000\n","     232697/2000000000: episode: 6335, duration: 4.673s, episode steps:  35, steps per second:   7, episode reward:  1.500, mean reward:  0.043 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 80.088502, mean_q: 41.249652, mean_eps: 0.100000\n","     232737/2000000000: episode: 6336, duration: 5.547s, episode steps:  40, steps per second:   7, episode reward: -55.400, mean reward: -1.385 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 70.346403, mean_q: 41.026565, mean_eps: 0.100000\n","     232777/2000000000: episode: 6337, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: -7.800, mean reward: -0.195 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.882555, mean_q: 41.063107, mean_eps: 0.100000\n","     232817/2000000000: episode: 6338, duration: 5.174s, episode steps:  40, steps per second:   8, episode reward: 84.900, mean reward:  2.122 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.936988, mean_q: 40.552756, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     232849/2000000000: episode: 6339, duration: 4.117s, episode steps:  32, steps per second:   8, episode reward: 219.400, mean reward:  6.856 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 75.658862, mean_q: 40.861594, mean_eps: 0.100000\n","     232889/2000000000: episode: 6340, duration: 4.985s, episode steps:  40, steps per second:   8, episode reward: 47.800, mean reward:  1.195 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.417356, mean_q: 40.982343, mean_eps: 0.100000\n","     232917/2000000000: episode: 6341, duration: 3.318s, episode steps:  28, steps per second:   8, episode reward: 109.500, mean reward:  3.911 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.711580, mean_q: 41.425616, mean_eps: 0.100000\n","     232955/2000000000: episode: 6342, duration: 4.683s, episode steps:  38, steps per second:   8, episode reward: -50.900, mean reward: -1.339 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 76.174012, mean_q: 41.229014, mean_eps: 0.100000\n","     232990/2000000000: episode: 6343, duration: 4.325s, episode steps:  35, steps per second:   8, episode reward: 124.800, mean reward:  3.566 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 83.213757, mean_q: 42.210082, mean_eps: 0.100000\n","     233017/2000000000: episode: 6344, duration: 3.352s, episode steps:  27, steps per second:   8, episode reward: 268.800, mean reward:  9.956 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.558336, mean_q: 41.311765, mean_eps: 0.100000\n","     233057/2000000000: episode: 6345, duration: 5.169s, episode steps:  40, steps per second:   8, episode reward: 44.600, mean reward:  1.115 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.522197, mean_q: 40.444881, mean_eps: 0.100000\n","     233087/2000000000: episode: 6346, duration: 3.934s, episode steps:  30, steps per second:   8, episode reward: 91.200, mean reward:  3.040 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 87.251630, mean_q: 41.197911, mean_eps: 0.100000\n","     233126/2000000000: episode: 6347, duration: 5.184s, episode steps:  39, steps per second:   8, episode reward: 101.400, mean reward:  2.600 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 85.440035, mean_q: 40.950096, mean_eps: 0.100000\n","     233166/2000000000: episode: 6348, duration: 5.122s, episode steps:  40, steps per second:   8, episode reward: -16.500, mean reward: -0.413 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.001972, mean_q: 41.155085, mean_eps: 0.100000\n","     233203/2000000000: episode: 6349, duration: 4.601s, episode steps:  37, steps per second:   8, episode reward: 64.400, mean reward:  1.741 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 79.110526, mean_q: 40.532545, mean_eps: 0.100000\n","     233236/2000000000: episode: 6350, duration: 4.159s, episode steps:  33, steps per second:   8, episode reward: 55.400, mean reward:  1.679 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 83.148009, mean_q: 41.087448, mean_eps: 0.100000\n","     233272/2000000000: episode: 6351, duration: 4.441s, episode steps:  36, steps per second:   8, episode reward: 145.800, mean reward:  4.050 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 73.448954, mean_q: 41.792218, mean_eps: 0.100000\n","     233305/2000000000: episode: 6352, duration: 3.923s, episode steps:  33, steps per second:   8, episode reward: 81.100, mean reward:  2.458 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 86.217468, mean_q: 40.466189, mean_eps: 0.100000\n","     233340/2000000000: episode: 6353, duration: 4.483s, episode steps:  35, steps per second:   8, episode reward: 97.800, mean reward:  2.794 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 87.327248, mean_q: 41.565359, mean_eps: 0.100000\n","     233368/2000000000: episode: 6354, duration: 3.546s, episode steps:  28, steps per second:   8, episode reward: 64.900, mean reward:  2.318 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 86.216777, mean_q: 41.510019, mean_eps: 0.100000\n","     233400/2000000000: episode: 6355, duration: 3.863s, episode steps:  32, steps per second:   8, episode reward: 171.700, mean reward:  5.366 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 79.072058, mean_q: 42.132432, mean_eps: 0.100000\n","     233440/2000000000: episode: 6356, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.658215, mean_q: 41.534697, mean_eps: 0.100000\n","     233474/2000000000: episode: 6357, duration: 4.485s, episode steps:  34, steps per second:   8, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.878944, mean_q: 42.234467, mean_eps: 0.100000\n","     233512/2000000000: episode: 6358, duration: 4.861s, episode steps:  38, steps per second:   8, episode reward: -112.900, mean reward: -2.971 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 70.825479, mean_q: 40.828499, mean_eps: 0.100000\n","     233544/2000000000: episode: 6359, duration: 4.265s, episode steps:  32, steps per second:   8, episode reward: 191.300, mean reward:  5.978 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 81.832364, mean_q: 40.669629, mean_eps: 0.100000\n","     233582/2000000000: episode: 6360, duration: 4.921s, episode steps:  38, steps per second:   8, episode reward: 39.100, mean reward:  1.029 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 85.923111, mean_q: 41.450560, mean_eps: 0.100000\n","     233610/2000000000: episode: 6361, duration: 3.810s, episode steps:  28, steps per second:   7, episode reward: 124.100, mean reward:  4.432 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 84.236009, mean_q: 41.249689, mean_eps: 0.100000\n","     233648/2000000000: episode: 6362, duration: 5.119s, episode steps:  38, steps per second:   7, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 83.658840, mean_q: 40.536145, mean_eps: 0.100000\n","     233680/2000000000: episode: 6363, duration: 4.308s, episode steps:  32, steps per second:   7, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.103791, mean_q: 42.667900, mean_eps: 0.100000\n","     233710/2000000000: episode: 6364, duration: 3.832s, episode steps:  30, steps per second:   8, episode reward: -35.600, mean reward: -1.187 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 81.046429, mean_q: 41.203689, mean_eps: 0.100000\n","     233750/2000000000: episode: 6365, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: 83.500, mean reward:  2.088 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.952047, mean_q: 40.764953, mean_eps: 0.100000\n","     233788/2000000000: episode: 6366, duration: 4.998s, episode steps:  38, steps per second:   8, episode reward:  1.400, mean reward:  0.037 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 86.740908, mean_q: 41.756213, mean_eps: 0.100000\n","     233815/2000000000: episode: 6367, duration: 3.638s, episode steps:  27, steps per second:   7, episode reward: -58.000, mean reward: -2.148 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 89.576067, mean_q: 41.005994, mean_eps: 0.100000\n","     233849/2000000000: episode: 6368, duration: 4.975s, episode steps:  34, steps per second:   7, episode reward: 108.000, mean reward:  3.176 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.713239, mean_q: 40.994114, mean_eps: 0.100000\n","     233883/2000000000: episode: 6369, duration: 4.673s, episode steps:  34, steps per second:   7, episode reward: 156.600, mean reward:  4.606 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 81.963926, mean_q: 41.338991, mean_eps: 0.100000\n","     233913/2000000000: episode: 6370, duration: 4.209s, episode steps:  30, steps per second:   7, episode reward: -13.400, mean reward: -0.447 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 74.764322, mean_q: 41.955293, mean_eps: 0.100000\n","     233946/2000000000: episode: 6371, duration: 4.367s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 83.974877, mean_q: 41.470854, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     233986/2000000000: episode: 6372, duration: 5.456s, episode steps:  40, steps per second:   7, episode reward: 208.000, mean reward:  5.200 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 73.353829, mean_q: 41.402018, mean_eps: 0.100000\n","     234018/2000000000: episode: 6373, duration: 4.289s, episode steps:  32, steps per second:   7, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.827989, mean_q: 41.621622, mean_eps: 0.100000\n","     234044/2000000000: episode: 6374, duration: 3.466s, episode steps:  26, steps per second:   8, episode reward: 18.000, mean reward:  0.692 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 75.932251, mean_q: 40.989273, mean_eps: 0.100000\n","     234078/2000000000: episode: 6375, duration: 4.810s, episode steps:  34, steps per second:   7, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 83.107493, mean_q: 40.919961, mean_eps: 0.100000\n","     234115/2000000000: episode: 6376, duration: 4.972s, episode steps:  37, steps per second:   7, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 81.032245, mean_q: 40.538579, mean_eps: 0.100000\n","     234147/2000000000: episode: 6377, duration: 4.111s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.892901, mean_q: 41.248445, mean_eps: 0.100000\n","     234187/2000000000: episode: 6378, duration: 5.440s, episode steps:  40, steps per second:   7, episode reward: 24.000, mean reward:  0.600 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 83.479761, mean_q: 40.611317, mean_eps: 0.100000\n","     234224/2000000000: episode: 6379, duration: 4.793s, episode steps:  37, steps per second:   8, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 77.045859, mean_q: 41.341257, mean_eps: 0.100000\n","     234264/2000000000: episode: 6380, duration: 5.280s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.628835, mean_q: 42.009323, mean_eps: 0.100000\n","     234301/2000000000: episode: 6381, duration: 5.043s, episode steps:  37, steps per second:   7, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 76.171762, mean_q: 41.398316, mean_eps: 0.100000\n","     234336/2000000000: episode: 6382, duration: 4.836s, episode steps:  35, steps per second:   7, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 78.197110, mean_q: 41.219811, mean_eps: 0.100000\n","     234376/2000000000: episode: 6383, duration: 5.378s, episode steps:  40, steps per second:   7, episode reward: 199.700, mean reward:  4.992 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.103900, mean_q: 41.104823, mean_eps: 0.100000\n","     234412/2000000000: episode: 6384, duration: 4.986s, episode steps:  36, steps per second:   7, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 75.103726, mean_q: 41.612542, mean_eps: 0.100000\n","     234446/2000000000: episode: 6385, duration: 4.703s, episode steps:  34, steps per second:   7, episode reward: 73.300, mean reward:  2.156 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 78.857637, mean_q: 41.988256, mean_eps: 0.100000\n","     234480/2000000000: episode: 6386, duration: 4.553s, episode steps:  34, steps per second:   7, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 81.187569, mean_q: 40.998087, mean_eps: 0.100000\n","     234510/2000000000: episode: 6387, duration: 3.978s, episode steps:  30, steps per second:   8, episode reward: 80.900, mean reward:  2.697 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 85.525859, mean_q: 41.366770, mean_eps: 0.100000\n","     234546/2000000000: episode: 6388, duration: 4.851s, episode steps:  36, steps per second:   7, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 83.810866, mean_q: 41.961998, mean_eps: 0.100000\n","     234584/2000000000: episode: 6389, duration: 4.999s, episode steps:  38, steps per second:   8, episode reward: 77.900, mean reward:  2.050 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 80.665118, mean_q: 40.233069, mean_eps: 0.100000\n","     234624/2000000000: episode: 6390, duration: 5.062s, episode steps:  40, steps per second:   8, episode reward: 210.000, mean reward:  5.250 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 84.864814, mean_q: 41.194649, mean_eps: 0.100000\n","     234649/2000000000: episode: 6391, duration: 3.338s, episode steps:  25, steps per second:   7, episode reward: -104.100, mean reward: -4.164 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 79.814230, mean_q: 41.565343, mean_eps: 0.100000\n","     234682/2000000000: episode: 6392, duration: 4.341s, episode steps:  33, steps per second:   8, episode reward: -39.000, mean reward: -1.182 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 96.052835, mean_q: 40.690896, mean_eps: 0.100000\n","     234713/2000000000: episode: 6393, duration: 4.162s, episode steps:  31, steps per second:   7, episode reward: 63.000, mean reward:  2.032 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 82.490508, mean_q: 41.260369, mean_eps: 0.100000\n","     234747/2000000000: episode: 6394, duration: 4.796s, episode steps:  34, steps per second:   7, episode reward:  9.200, mean reward:  0.271 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 80.052554, mean_q: 41.202303, mean_eps: 0.100000\n","     234780/2000000000: episode: 6395, duration: 4.201s, episode steps:  33, steps per second:   8, episode reward: 60.800, mean reward:  1.842 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 80.238745, mean_q: 40.703489, mean_eps: 0.100000\n","     234820/2000000000: episode: 6396, duration: 5.267s, episode steps:  40, steps per second:   8, episode reward: 109.400, mean reward:  2.735 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.907388, mean_q: 40.527709, mean_eps: 0.100000\n","     234855/2000000000: episode: 6397, duration: 4.533s, episode steps:  35, steps per second:   8, episode reward: 61.200, mean reward:  1.749 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 86.658023, mean_q: 40.862563, mean_eps: 0.100000\n","     234895/2000000000: episode: 6398, duration: 4.812s, episode steps:  40, steps per second:   8, episode reward: -11.600, mean reward: -0.290 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 80.464717, mean_q: 40.651441, mean_eps: 0.100000\n","     234926/2000000000: episode: 6399, duration: 3.820s, episode steps:  31, steps per second:   8, episode reward: 37.200, mean reward:  1.200 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 84.013476, mean_q: 42.431547, mean_eps: 0.100000\n","     234957/2000000000: episode: 6400, duration: 3.869s, episode steps:  31, steps per second:   8, episode reward: 110.000, mean reward:  3.548 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 88.434514, mean_q: 40.690898, mean_eps: 0.100000\n","     234984/2000000000: episode: 6401, duration: 3.386s, episode steps:  27, steps per second:   8, episode reward: 134.900, mean reward:  4.996 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 84.930120, mean_q: 41.661934, mean_eps: 0.100000\n","     235020/2000000000: episode: 6402, duration: 4.627s, episode steps:  36, steps per second:   8, episode reward: 167.500, mean reward:  4.653 [-20.000, 18.000], mean action: 1.389 [0.000, 2.000],  loss: 88.061553, mean_q: 41.297318, mean_eps: 0.100000\n","     235051/2000000000: episode: 6403, duration: 4.110s, episode steps:  31, steps per second:   8, episode reward: 114.800, mean reward:  3.703 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 78.768671, mean_q: 40.998807, mean_eps: 0.100000\n","     235091/2000000000: episode: 6404, duration: 5.172s, episode steps:  40, steps per second:   8, episode reward:  4.500, mean reward:  0.112 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.831214, mean_q: 41.433523, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     235123/2000000000: episode: 6405, duration: 4.047s, episode steps:  32, steps per second:   8, episode reward: 82.200, mean reward:  2.569 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 81.138257, mean_q: 42.187248, mean_eps: 0.100000\n","     235161/2000000000: episode: 6406, duration: 4.832s, episode steps:  38, steps per second:   8, episode reward: 162.000, mean reward:  4.263 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 77.369535, mean_q: 40.984749, mean_eps: 0.100000\n","     235198/2000000000: episode: 6407, duration: 4.818s, episode steps:  37, steps per second:   8, episode reward: 270.200, mean reward:  7.303 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 76.684026, mean_q: 40.513970, mean_eps: 0.100000\n","     235238/2000000000: episode: 6408, duration: 5.126s, episode steps:  40, steps per second:   8, episode reward: 79.800, mean reward:  1.995 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 71.033827, mean_q: 41.749898, mean_eps: 0.100000\n","     235267/2000000000: episode: 6409, duration: 3.869s, episode steps:  29, steps per second:   7, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 1.172 [0.000, 2.000],  loss: 80.635737, mean_q: 41.450920, mean_eps: 0.100000\n","     235301/2000000000: episode: 6410, duration: 4.271s, episode steps:  34, steps per second:   8, episode reward: 136.800, mean reward:  4.024 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 84.758994, mean_q: 41.557330, mean_eps: 0.100000\n","     235341/2000000000: episode: 6411, duration: 5.137s, episode steps:  40, steps per second:   8, episode reward: 126.500, mean reward:  3.163 [-20.000, 18.000], mean action: 1.600 [0.000, 2.000],  loss: 85.951881, mean_q: 40.438463, mean_eps: 0.100000\n","     235371/2000000000: episode: 6412, duration: 4.008s, episode steps:  30, steps per second:   7, episode reward: -13.200, mean reward: -0.440 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 87.850613, mean_q: 41.424946, mean_eps: 0.100000\n","     235411/2000000000: episode: 6413, duration: 5.099s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 86.574292, mean_q: 41.615111, mean_eps: 0.100000\n","     235442/2000000000: episode: 6414, duration: 3.876s, episode steps:  31, steps per second:   8, episode reward: 109.300, mean reward:  3.526 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.847014, mean_q: 40.939394, mean_eps: 0.100000\n","     235476/2000000000: episode: 6415, duration: 4.380s, episode steps:  34, steps per second:   8, episode reward: -68.600, mean reward: -2.018 [-20.000, 19.100], mean action: 1.088 [0.000, 2.000],  loss: 67.826599, mean_q: 40.600433, mean_eps: 0.100000\n","     235509/2000000000: episode: 6416, duration: 4.346s, episode steps:  33, steps per second:   8, episode reward:  0.200, mean reward:  0.006 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.399615, mean_q: 41.914104, mean_eps: 0.100000\n","     235548/2000000000: episode: 6417, duration: 4.510s, episode steps:  39, steps per second:   9, episode reward: 61.900, mean reward:  1.587 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 78.053179, mean_q: 40.535954, mean_eps: 0.100000\n","     235582/2000000000: episode: 6418, duration: 4.562s, episode steps:  34, steps per second:   7, episode reward: 175.200, mean reward:  5.153 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 81.836862, mean_q: 41.361156, mean_eps: 0.100000\n","     235612/2000000000: episode: 6419, duration: 3.892s, episode steps:  30, steps per second:   8, episode reward: -17.200, mean reward: -0.573 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.504594, mean_q: 41.221997, mean_eps: 0.100000\n","     235648/2000000000: episode: 6420, duration: 4.766s, episode steps:  36, steps per second:   8, episode reward: 69.500, mean reward:  1.931 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 81.142913, mean_q: 40.597774, mean_eps: 0.100000\n","     235676/2000000000: episode: 6421, duration: 3.796s, episode steps:  28, steps per second:   7, episode reward: 78.300, mean reward:  2.796 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 78.210719, mean_q: 40.451342, mean_eps: 0.100000\n","     235715/2000000000: episode: 6422, duration: 4.793s, episode steps:  39, steps per second:   8, episode reward: 35.300, mean reward:  0.905 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 80.746976, mean_q: 41.347673, mean_eps: 0.100000\n","     235748/2000000000: episode: 6423, duration: 4.238s, episode steps:  33, steps per second:   8, episode reward: 31.600, mean reward:  0.958 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 87.145353, mean_q: 40.712658, mean_eps: 0.100000\n","     235778/2000000000: episode: 6424, duration: 4.316s, episode steps:  30, steps per second:   7, episode reward: 119.300, mean reward:  3.977 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 79.978585, mean_q: 40.569166, mean_eps: 0.100000\n","     235811/2000000000: episode: 6425, duration: 4.562s, episode steps:  33, steps per second:   7, episode reward: 109.200, mean reward:  3.309 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 76.841347, mean_q: 41.868782, mean_eps: 0.100000\n","     235851/2000000000: episode: 6426, duration: 5.706s, episode steps:  40, steps per second:   7, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 81.944785, mean_q: 41.196179, mean_eps: 0.100000\n","     235890/2000000000: episode: 6427, duration: 4.978s, episode steps:  39, steps per second:   8, episode reward:  7.800, mean reward:  0.200 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 87.826109, mean_q: 41.367051, mean_eps: 0.100000\n","     235925/2000000000: episode: 6428, duration: 4.456s, episode steps:  35, steps per second:   8, episode reward: 192.500, mean reward:  5.500 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 77.154864, mean_q: 41.223289, mean_eps: 0.100000\n","     235959/2000000000: episode: 6429, duration: 4.829s, episode steps:  34, steps per second:   7, episode reward: -52.200, mean reward: -1.535 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 84.367649, mean_q: 40.946942, mean_eps: 0.100000\n","     235986/2000000000: episode: 6430, duration: 3.550s, episode steps:  27, steps per second:   8, episode reward: 37.700, mean reward:  1.396 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.000032, mean_q: 42.059817, mean_eps: 0.100000\n","     236014/2000000000: episode: 6431, duration: 3.723s, episode steps:  28, steps per second:   8, episode reward: 113.300, mean reward:  4.046 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 80.438700, mean_q: 41.116037, mean_eps: 0.100000\n","     236054/2000000000: episode: 6432, duration: 5.559s, episode steps:  40, steps per second:   7, episode reward: 71.200, mean reward:  1.780 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.034368, mean_q: 40.605776, mean_eps: 0.100000\n","     236086/2000000000: episode: 6433, duration: 3.980s, episode steps:  32, steps per second:   8, episode reward: 76.300, mean reward:  2.384 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 78.567148, mean_q: 41.748596, mean_eps: 0.100000\n","     236123/2000000000: episode: 6434, duration: 4.754s, episode steps:  37, steps per second:   8, episode reward: 124.800, mean reward:  3.373 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 75.919783, mean_q: 42.488295, mean_eps: 0.100000\n","     236152/2000000000: episode: 6435, duration: 3.607s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 77.504817, mean_q: 40.737244, mean_eps: 0.100000\n","     236192/2000000000: episode: 6436, duration: 5.296s, episode steps:  40, steps per second:   8, episode reward: 130.800, mean reward:  3.270 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 87.108780, mean_q: 41.532294, mean_eps: 0.100000\n","     236232/2000000000: episode: 6437, duration: 5.132s, episode steps:  40, steps per second:   8, episode reward: 151.700, mean reward:  3.793 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.860701, mean_q: 40.840401, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     236272/2000000000: episode: 6438, duration: 5.327s, episode steps:  40, steps per second:   8, episode reward: 144.700, mean reward:  3.617 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 81.343622, mean_q: 40.943907, mean_eps: 0.100000\n","     236311/2000000000: episode: 6439, duration: 5.222s, episode steps:  39, steps per second:   7, episode reward: 68.400, mean reward:  1.754 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 82.705272, mean_q: 40.885239, mean_eps: 0.100000\n","     236344/2000000000: episode: 6440, duration: 4.446s, episode steps:  33, steps per second:   7, episode reward: 25.600, mean reward:  0.776 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 79.059887, mean_q: 41.478185, mean_eps: 0.100000\n","     236384/2000000000: episode: 6441, duration: 5.261s, episode steps:  40, steps per second:   8, episode reward: -35.300, mean reward: -0.882 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.518367, mean_q: 40.643180, mean_eps: 0.100000\n","     236422/2000000000: episode: 6442, duration: 4.999s, episode steps:  38, steps per second:   8, episode reward: -11.500, mean reward: -0.303 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 76.810648, mean_q: 41.282276, mean_eps: 0.100000\n","     236462/2000000000: episode: 6443, duration: 5.338s, episode steps:  40, steps per second:   7, episode reward: 63.100, mean reward:  1.578 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 77.046044, mean_q: 40.905667, mean_eps: 0.100000\n","     236502/2000000000: episode: 6444, duration: 5.273s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 80.974774, mean_q: 41.136488, mean_eps: 0.100000\n","     236536/2000000000: episode: 6445, duration: 4.655s, episode steps:  34, steps per second:   7, episode reward: -3.400, mean reward: -0.100 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 81.501618, mean_q: 41.760727, mean_eps: 0.100000\n","     236574/2000000000: episode: 6446, duration: 4.913s, episode steps:  38, steps per second:   8, episode reward: -6.700, mean reward: -0.176 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 77.098149, mean_q: 41.422255, mean_eps: 0.100000\n","     236607/2000000000: episode: 6447, duration: 4.234s, episode steps:  33, steps per second:   8, episode reward:  0.900, mean reward:  0.027 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.680662, mean_q: 41.348764, mean_eps: 0.100000\n","     236633/2000000000: episode: 6448, duration: 3.530s, episode steps:  26, steps per second:   7, episode reward: 80.800, mean reward:  3.108 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 88.134461, mean_q: 40.380970, mean_eps: 0.100000\n","     236664/2000000000: episode: 6449, duration: 3.889s, episode steps:  31, steps per second:   8, episode reward: 157.700, mean reward:  5.087 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 81.947457, mean_q: 40.707491, mean_eps: 0.100000\n","     236695/2000000000: episode: 6450, duration: 4.077s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 80.852546, mean_q: 42.136372, mean_eps: 0.100000\n","     236723/2000000000: episode: 6451, duration: 3.704s, episode steps:  28, steps per second:   8, episode reward: 133.200, mean reward:  4.757 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 78.085006, mean_q: 41.785633, mean_eps: 0.100000\n","     236754/2000000000: episode: 6452, duration: 4.172s, episode steps:  31, steps per second:   7, episode reward: 60.400, mean reward:  1.948 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.763746, mean_q: 41.119975, mean_eps: 0.100000\n","     236782/2000000000: episode: 6453, duration: 3.803s, episode steps:  28, steps per second:   7, episode reward: 46.400, mean reward:  1.657 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 81.490926, mean_q: 40.962162, mean_eps: 0.100000\n","     236815/2000000000: episode: 6454, duration: 4.298s, episode steps:  33, steps per second:   8, episode reward: -35.600, mean reward: -1.079 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 75.274251, mean_q: 41.335904, mean_eps: 0.100000\n","     236848/2000000000: episode: 6455, duration: 4.159s, episode steps:  33, steps per second:   8, episode reward: 84.200, mean reward:  2.552 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 78.398698, mean_q: 42.157433, mean_eps: 0.100000\n","     236885/2000000000: episode: 6456, duration: 4.845s, episode steps:  37, steps per second:   8, episode reward: 14.200, mean reward:  0.384 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 75.797854, mean_q: 41.222548, mean_eps: 0.100000\n","     236923/2000000000: episode: 6457, duration: 4.976s, episode steps:  38, steps per second:   8, episode reward: -4.100, mean reward: -0.108 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 84.675267, mean_q: 40.900980, mean_eps: 0.100000\n","     236957/2000000000: episode: 6458, duration: 4.413s, episode steps:  34, steps per second:   8, episode reward: 134.800, mean reward:  3.965 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.838970, mean_q: 41.470348, mean_eps: 0.100000\n","     236989/2000000000: episode: 6459, duration: 4.104s, episode steps:  32, steps per second:   8, episode reward: 53.500, mean reward:  1.672 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 73.460701, mean_q: 41.549186, mean_eps: 0.100000\n","     237011/2000000000: episode: 6460, duration: 2.835s, episode steps:  22, steps per second:   8, episode reward: 152.000, mean reward:  6.909 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 83.024419, mean_q: 41.470866, mean_eps: 0.100000\n","     237040/2000000000: episode: 6461, duration: 3.853s, episode steps:  29, steps per second:   8, episode reward: 178.600, mean reward:  6.159 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 78.010032, mean_q: 40.753349, mean_eps: 0.100000\n","     237064/2000000000: episode: 6462, duration: 3.109s, episode steps:  24, steps per second:   8, episode reward: -1.100, mean reward: -0.046 [-20.000, 18.000], mean action: 0.583 [0.000, 2.000],  loss: 78.797184, mean_q: 41.008689, mean_eps: 0.100000\n","     237097/2000000000: episode: 6463, duration: 4.592s, episode steps:  33, steps per second:   7, episode reward: 134.400, mean reward:  4.073 [-13.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 87.712551, mean_q: 41.370004, mean_eps: 0.100000\n","     237123/2000000000: episode: 6464, duration: 3.434s, episode steps:  26, steps per second:   8, episode reward: 97.200, mean reward:  3.738 [-20.000, 18.000], mean action: 1.077 [0.000, 2.000],  loss: 74.453641, mean_q: 42.078617, mean_eps: 0.100000\n","     237154/2000000000: episode: 6465, duration: 4.113s, episode steps:  31, steps per second:   8, episode reward: 129.700, mean reward:  4.184 [-20.000, 19.600], mean action: 1.194 [0.000, 2.000],  loss: 73.315430, mean_q: 42.054205, mean_eps: 0.100000\n","     237181/2000000000: episode: 6466, duration: 3.530s, episode steps:  27, steps per second:   8, episode reward: 120.600, mean reward:  4.467 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 78.463552, mean_q: 41.034658, mean_eps: 0.100000\n","     237217/2000000000: episode: 6467, duration: 5.037s, episode steps:  36, steps per second:   7, episode reward: 63.100, mean reward:  1.753 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 85.798637, mean_q: 41.376431, mean_eps: 0.100000\n","     237245/2000000000: episode: 6468, duration: 3.641s, episode steps:  28, steps per second:   8, episode reward: 54.200, mean reward:  1.936 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 74.109720, mean_q: 41.367376, mean_eps: 0.100000\n","     237284/2000000000: episode: 6469, duration: 5.325s, episode steps:  39, steps per second:   7, episode reward: -63.800, mean reward: -1.636 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 81.504281, mean_q: 41.121396, mean_eps: 0.100000\n","     237322/2000000000: episode: 6470, duration: 4.754s, episode steps:  38, steps per second:   8, episode reward: 143.200, mean reward:  3.768 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 78.712854, mean_q: 41.317621, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     237348/2000000000: episode: 6471, duration: 3.317s, episode steps:  26, steps per second:   8, episode reward: 137.000, mean reward:  5.269 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 71.321655, mean_q: 42.520993, mean_eps: 0.100000\n","     237385/2000000000: episode: 6472, duration: 4.596s, episode steps:  37, steps per second:   8, episode reward: 128.500, mean reward:  3.473 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 80.017595, mean_q: 40.824006, mean_eps: 0.100000\n","     237420/2000000000: episode: 6473, duration: 4.386s, episode steps:  35, steps per second:   8, episode reward: 103.300, mean reward:  2.951 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.581759, mean_q: 41.168448, mean_eps: 0.100000\n","     237449/2000000000: episode: 6474, duration: 3.597s, episode steps:  29, steps per second:   8, episode reward: 131.000, mean reward:  4.517 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 80.104471, mean_q: 40.847739, mean_eps: 0.100000\n","     237482/2000000000: episode: 6475, duration: 4.042s, episode steps:  33, steps per second:   8, episode reward: 63.100, mean reward:  1.912 [-20.000, 18.300], mean action: 1.061 [0.000, 2.000],  loss: 77.265293, mean_q: 41.664839, mean_eps: 0.100000\n","     237522/2000000000: episode: 6476, duration: 5.136s, episode steps:  40, steps per second:   8, episode reward: 48.100, mean reward:  1.203 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.810096, mean_q: 40.429559, mean_eps: 0.100000\n","     237562/2000000000: episode: 6477, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: 63.200, mean reward:  1.580 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 85.723745, mean_q: 41.540089, mean_eps: 0.100000\n","     237602/2000000000: episode: 6478, duration: 5.242s, episode steps:  40, steps per second:   8, episode reward: 163.900, mean reward:  4.097 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 68.385078, mean_q: 41.118655, mean_eps: 0.100000\n","     237628/2000000000: episode: 6479, duration: 3.502s, episode steps:  26, steps per second:   7, episode reward: 63.900, mean reward:  2.458 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 74.392632, mean_q: 40.560314, mean_eps: 0.100000\n","     237664/2000000000: episode: 6480, duration: 4.618s, episode steps:  36, steps per second:   8, episode reward: 153.100, mean reward:  4.253 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 82.824782, mean_q: 41.138661, mean_eps: 0.100000\n","     237704/2000000000: episode: 6481, duration: 5.576s, episode steps:  40, steps per second:   7, episode reward: 57.400, mean reward:  1.435 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.948712, mean_q: 41.253808, mean_eps: 0.100000\n","     237736/2000000000: episode: 6482, duration: 4.654s, episode steps:  32, steps per second:   7, episode reward: -131.100, mean reward: -4.097 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.399882, mean_q: 41.766283, mean_eps: 0.100000\n","     237771/2000000000: episode: 6483, duration: 4.788s, episode steps:  35, steps per second:   7, episode reward: -58.000, mean reward: -1.657 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 79.180432, mean_q: 41.668217, mean_eps: 0.100000\n","     237805/2000000000: episode: 6484, duration: 4.389s, episode steps:  34, steps per second:   8, episode reward: 33.600, mean reward:  0.988 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.250373, mean_q: 41.719593, mean_eps: 0.100000\n","     237845/2000000000: episode: 6485, duration: 5.360s, episode steps:  40, steps per second:   7, episode reward: 42.200, mean reward:  1.055 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.045273, mean_q: 41.254759, mean_eps: 0.100000\n","     237879/2000000000: episode: 6486, duration: 4.639s, episode steps:  34, steps per second:   7, episode reward: 241.700, mean reward:  7.109 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 82.481774, mean_q: 41.232032, mean_eps: 0.100000\n","     237916/2000000000: episode: 6487, duration: 5.106s, episode steps:  37, steps per second:   7, episode reward: -99.100, mean reward: -2.678 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 88.694081, mean_q: 41.725707, mean_eps: 0.100000\n","     237946/2000000000: episode: 6488, duration: 4.079s, episode steps:  30, steps per second:   7, episode reward: 21.400, mean reward:  0.713 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 83.879581, mean_q: 40.870317, mean_eps: 0.100000\n","     237986/2000000000: episode: 6489, duration: 5.402s, episode steps:  40, steps per second:   7, episode reward: 27.700, mean reward:  0.692 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.116032, mean_q: 40.971762, mean_eps: 0.100000\n","     238026/2000000000: episode: 6490, duration: 5.287s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.079541, mean_q: 41.174474, mean_eps: 0.100000\n","     238055/2000000000: episode: 6491, duration: 3.835s, episode steps:  29, steps per second:   8, episode reward: -31.600, mean reward: -1.090 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 79.709320, mean_q: 40.108896, mean_eps: 0.100000\n","     238089/2000000000: episode: 6492, duration: 4.465s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 80.668388, mean_q: 41.497615, mean_eps: 0.100000\n","     238129/2000000000: episode: 6493, duration: 5.340s, episode steps:  40, steps per second:   7, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.002121, mean_q: 41.050947, mean_eps: 0.100000\n","     238167/2000000000: episode: 6494, duration: 4.985s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 84.056453, mean_q: 40.867336, mean_eps: 0.100000\n","     238207/2000000000: episode: 6495, duration: 5.286s, episode steps:  40, steps per second:   8, episode reward: 19.600, mean reward:  0.490 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.369605, mean_q: 41.760682, mean_eps: 0.100000\n","     238247/2000000000: episode: 6496, duration: 5.228s, episode steps:  40, steps per second:   8, episode reward: 18.200, mean reward:  0.455 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.157670, mean_q: 41.560023, mean_eps: 0.100000\n","     238285/2000000000: episode: 6497, duration: 5.033s, episode steps:  38, steps per second:   8, episode reward: 30.800, mean reward:  0.811 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 88.756063, mean_q: 41.028967, mean_eps: 0.100000\n","     238316/2000000000: episode: 6498, duration: 4.255s, episode steps:  31, steps per second:   7, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 73.090810, mean_q: 41.132344, mean_eps: 0.100000\n","     238356/2000000000: episode: 6499, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.781311, mean_q: 41.627322, mean_eps: 0.100000\n","     238380/2000000000: episode: 6500, duration: 3.400s, episode steps:  24, steps per second:   7, episode reward: 94.000, mean reward:  3.917 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 79.035512, mean_q: 40.987377, mean_eps: 0.100000\n","     238420/2000000000: episode: 6501, duration: 5.713s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.061348, mean_q: 41.769614, mean_eps: 0.100000\n","     238459/2000000000: episode: 6502, duration: 5.486s, episode steps:  39, steps per second:   7, episode reward: -96.000, mean reward: -2.462 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 76.267708, mean_q: 40.876158, mean_eps: 0.100000\n","     238486/2000000000: episode: 6503, duration: 3.693s, episode steps:  27, steps per second:   7, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 75.350387, mean_q: 41.061966, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     238524/2000000000: episode: 6504, duration: 4.809s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 87.054464, mean_q: 40.543617, mean_eps: 0.100000\n","     238564/2000000000: episode: 6505, duration: 4.931s, episode steps:  40, steps per second:   8, episode reward: 38.800, mean reward:  0.970 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.508163, mean_q: 40.637192, mean_eps: 0.100000\n","     238594/2000000000: episode: 6506, duration: 3.826s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 82.448866, mean_q: 40.947134, mean_eps: 0.100000\n","     238626/2000000000: episode: 6507, duration: 4.211s, episode steps:  32, steps per second:   8, episode reward: 77.500, mean reward:  2.422 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 79.226334, mean_q: 40.973807, mean_eps: 0.100000\n","     238662/2000000000: episode: 6508, duration: 4.718s, episode steps:  36, steps per second:   8, episode reward: 133.500, mean reward:  3.708 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 79.079795, mean_q: 41.680418, mean_eps: 0.100000\n","     238689/2000000000: episode: 6509, duration: 3.587s, episode steps:  27, steps per second:   8, episode reward: 62.200, mean reward:  2.304 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 86.062307, mean_q: 41.695580, mean_eps: 0.100000\n","     238724/2000000000: episode: 6510, duration: 4.636s, episode steps:  35, steps per second:   8, episode reward: 10.000, mean reward:  0.286 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.340821, mean_q: 40.656693, mean_eps: 0.100000\n","     238764/2000000000: episode: 6511, duration: 5.355s, episode steps:  40, steps per second:   7, episode reward:  4.700, mean reward:  0.117 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 86.065344, mean_q: 41.305514, mean_eps: 0.100000\n","     238804/2000000000: episode: 6512, duration: 5.257s, episode steps:  40, steps per second:   8, episode reward: 117.900, mean reward:  2.947 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.668608, mean_q: 41.760152, mean_eps: 0.100000\n","     238832/2000000000: episode: 6513, duration: 3.729s, episode steps:  28, steps per second:   8, episode reward: -96.000, mean reward: -3.429 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 82.403735, mean_q: 41.667731, mean_eps: 0.100000\n","     238867/2000000000: episode: 6514, duration: 4.600s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 89.188804, mean_q: 40.166541, mean_eps: 0.100000\n","     238899/2000000000: episode: 6515, duration: 4.244s, episode steps:  32, steps per second:   8, episode reward: 91.800, mean reward:  2.869 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 84.020704, mean_q: 41.337026, mean_eps: 0.100000\n","     238936/2000000000: episode: 6516, duration: 4.676s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 83.529006, mean_q: 41.776946, mean_eps: 0.100000\n","     238966/2000000000: episode: 6517, duration: 3.904s, episode steps:  30, steps per second:   8, episode reward: -77.200, mean reward: -2.573 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 75.451140, mean_q: 41.430308, mean_eps: 0.100000\n","     239003/2000000000: episode: 6518, duration: 4.642s, episode steps:  37, steps per second:   8, episode reward: 24.000, mean reward:  0.649 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 82.427829, mean_q: 40.608620, mean_eps: 0.100000\n","     239040/2000000000: episode: 6519, duration: 4.776s, episode steps:  37, steps per second:   8, episode reward: -29.800, mean reward: -0.805 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 82.080622, mean_q: 41.328623, mean_eps: 0.100000\n","     239068/2000000000: episode: 6520, duration: 3.639s, episode steps:  28, steps per second:   8, episode reward: 187.200, mean reward:  6.686 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.762667, mean_q: 40.787557, mean_eps: 0.100000\n","     239101/2000000000: episode: 6521, duration: 4.201s, episode steps:  33, steps per second:   8, episode reward: 145.000, mean reward:  4.394 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 85.181873, mean_q: 40.865368, mean_eps: 0.100000\n","     239129/2000000000: episode: 6522, duration: 3.498s, episode steps:  28, steps per second:   8, episode reward: 45.100, mean reward:  1.611 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 78.919399, mean_q: 40.702098, mean_eps: 0.100000\n","     239169/2000000000: episode: 6523, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: -47.600, mean reward: -1.190 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.752646, mean_q: 41.048506, mean_eps: 0.100000\n","     239209/2000000000: episode: 6524, duration: 5.388s, episode steps:  40, steps per second:   7, episode reward: 163.200, mean reward:  4.080 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 81.527781, mean_q: 41.136510, mean_eps: 0.100000\n","     239249/2000000000: episode: 6525, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.190765, mean_q: 41.098273, mean_eps: 0.100000\n","     239277/2000000000: episode: 6526, duration: 3.497s, episode steps:  28, steps per second:   8, episode reward: 24.100, mean reward:  0.861 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 83.695281, mean_q: 41.084671, mean_eps: 0.100000\n","     239310/2000000000: episode: 6527, duration: 4.546s, episode steps:  33, steps per second:   7, episode reward: 82.000, mean reward:  2.485 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 81.403380, mean_q: 40.991106, mean_eps: 0.100000\n","     239346/2000000000: episode: 6528, duration: 4.668s, episode steps:  36, steps per second:   8, episode reward: -19.100, mean reward: -0.531 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 76.114855, mean_q: 41.226242, mean_eps: 0.100000\n","     239383/2000000000: episode: 6529, duration: 5.144s, episode steps:  37, steps per second:   7, episode reward: 132.300, mean reward:  3.576 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 80.855432, mean_q: 40.890836, mean_eps: 0.100000\n","     239421/2000000000: episode: 6530, duration: 4.926s, episode steps:  38, steps per second:   8, episode reward: 60.000, mean reward:  1.579 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 76.628172, mean_q: 41.248298, mean_eps: 0.100000\n","     239454/2000000000: episode: 6531, duration: 4.425s, episode steps:  33, steps per second:   7, episode reward: 54.000, mean reward:  1.636 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 76.610745, mean_q: 41.723894, mean_eps: 0.100000\n","     239490/2000000000: episode: 6532, duration: 4.760s, episode steps:  36, steps per second:   8, episode reward: 81.600, mean reward:  2.267 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.508257, mean_q: 40.730775, mean_eps: 0.100000\n","     239530/2000000000: episode: 6533, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 81.506158, mean_q: 40.830798, mean_eps: 0.100000\n","     239570/2000000000: episode: 6534, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 246.000, mean reward:  6.150 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.777882, mean_q: 40.990921, mean_eps: 0.100000\n","     239610/2000000000: episode: 6535, duration: 5.349s, episode steps:  40, steps per second:   7, episode reward: 82.200, mean reward:  2.055 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.911545, mean_q: 42.177761, mean_eps: 0.100000\n","     239643/2000000000: episode: 6536, duration: 4.206s, episode steps:  33, steps per second:   8, episode reward: 49.300, mean reward:  1.494 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 79.145352, mean_q: 41.457290, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     239681/2000000000: episode: 6537, duration: 5.073s, episode steps:  38, steps per second:   7, episode reward: 151.200, mean reward:  3.979 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 75.734640, mean_q: 41.623976, mean_eps: 0.100000\n","     239721/2000000000: episode: 6538, duration: 5.032s, episode steps:  40, steps per second:   8, episode reward: 133.500, mean reward:  3.337 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.515020, mean_q: 40.324249, mean_eps: 0.100000\n","     239756/2000000000: episode: 6539, duration: 4.616s, episode steps:  35, steps per second:   8, episode reward: 96.600, mean reward:  2.760 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 81.335462, mean_q: 42.219972, mean_eps: 0.100000\n","     239796/2000000000: episode: 6540, duration: 6.272s, episode steps:  40, steps per second:   6, episode reward: 46.800, mean reward:  1.170 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.087419, mean_q: 40.658677, mean_eps: 0.100000\n","     239827/2000000000: episode: 6541, duration: 4.181s, episode steps:  31, steps per second:   7, episode reward: 58.300, mean reward:  1.881 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.881741, mean_q: 41.617421, mean_eps: 0.100000\n","     239867/2000000000: episode: 6542, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward: 128.300, mean reward:  3.207 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.117333, mean_q: 40.732017, mean_eps: 0.100000\n","     239903/2000000000: episode: 6543, duration: 4.925s, episode steps:  36, steps per second:   7, episode reward: 20.100, mean reward:  0.558 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.697455, mean_q: 40.795466, mean_eps: 0.100000\n","     239943/2000000000: episode: 6544, duration: 5.638s, episode steps:  40, steps per second:   7, episode reward: -8.400, mean reward: -0.210 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.580999, mean_q: 40.676554, mean_eps: 0.100000\n","     239978/2000000000: episode: 6545, duration: 4.823s, episode steps:  35, steps per second:   7, episode reward: 27.400, mean reward:  0.783 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 76.436565, mean_q: 42.078126, mean_eps: 0.100000\n","     240013/2000000000: episode: 6546, duration: 4.461s, episode steps:  35, steps per second:   8, episode reward: 140.300, mean reward:  4.009 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 91.577256, mean_q: 40.885443, mean_eps: 0.100000\n","     240050/2000000000: episode: 6547, duration: 4.675s, episode steps:  37, steps per second:   8, episode reward: 196.900, mean reward:  5.322 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 91.097548, mean_q: 42.558422, mean_eps: 0.100000\n","     240080/2000000000: episode: 6548, duration: 4.140s, episode steps:  30, steps per second:   7, episode reward: 63.900, mean reward:  2.130 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 87.629271, mean_q: 42.890987, mean_eps: 0.100000\n","     240120/2000000000: episode: 6549, duration: 5.341s, episode steps:  40, steps per second:   7, episode reward: 96.100, mean reward:  2.402 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.871784, mean_q: 42.124014, mean_eps: 0.100000\n","     240158/2000000000: episode: 6550, duration: 5.387s, episode steps:  38, steps per second:   7, episode reward: -29.800, mean reward: -0.784 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 84.436669, mean_q: 43.076881, mean_eps: 0.100000\n","     240188/2000000000: episode: 6551, duration: 4.213s, episode steps:  30, steps per second:   7, episode reward: 141.600, mean reward:  4.720 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 84.363636, mean_q: 42.280362, mean_eps: 0.100000\n","     240217/2000000000: episode: 6552, duration: 4.224s, episode steps:  29, steps per second:   7, episode reward: 28.000, mean reward:  0.966 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 77.575906, mean_q: 42.180621, mean_eps: 0.100000\n","     240250/2000000000: episode: 6553, duration: 4.494s, episode steps:  33, steps per second:   7, episode reward: 154.200, mean reward:  4.673 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 78.646609, mean_q: 43.070939, mean_eps: 0.100000\n","     240290/2000000000: episode: 6554, duration: 5.414s, episode steps:  40, steps per second:   7, episode reward: 53.300, mean reward:  1.333 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 90.664882, mean_q: 42.393575, mean_eps: 0.100000\n","     240330/2000000000: episode: 6555, duration: 5.286s, episode steps:  40, steps per second:   8, episode reward: -80.200, mean reward: -2.005 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.686582, mean_q: 41.279624, mean_eps: 0.100000\n","     240365/2000000000: episode: 6556, duration: 4.443s, episode steps:  35, steps per second:   8, episode reward: -88.600, mean reward: -2.531 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 77.477309, mean_q: 42.121151, mean_eps: 0.100000\n","     240398/2000000000: episode: 6557, duration: 4.193s, episode steps:  33, steps per second:   8, episode reward: 38.200, mean reward:  1.158 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 72.209519, mean_q: 42.735931, mean_eps: 0.100000\n","     240438/2000000000: episode: 6558, duration: 5.513s, episode steps:  40, steps per second:   7, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 90.207522, mean_q: 42.762341, mean_eps: 0.100000\n","     240478/2000000000: episode: 6559, duration: 5.393s, episode steps:  40, steps per second:   7, episode reward: 141.100, mean reward:  3.528 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 75.242822, mean_q: 42.067047, mean_eps: 0.100000\n","     240510/2000000000: episode: 6560, duration: 4.036s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 86.316458, mean_q: 42.397017, mean_eps: 0.100000\n","     240549/2000000000: episode: 6561, duration: 4.984s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.359 [0.000, 2.000],  loss: 81.206628, mean_q: 42.089629, mean_eps: 0.100000\n","     240577/2000000000: episode: 6562, duration: 3.740s, episode steps:  28, steps per second:   7, episode reward: -77.900, mean reward: -2.782 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 76.507909, mean_q: 42.236864, mean_eps: 0.100000\n","     240610/2000000000: episode: 6563, duration: 4.304s, episode steps:  33, steps per second:   8, episode reward: -27.000, mean reward: -0.818 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 82.718794, mean_q: 42.280320, mean_eps: 0.100000\n","     240644/2000000000: episode: 6564, duration: 4.337s, episode steps:  34, steps per second:   8, episode reward: 113.300, mean reward:  3.332 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 90.355927, mean_q: 42.723251, mean_eps: 0.100000\n","     240678/2000000000: episode: 6565, duration: 4.444s, episode steps:  34, steps per second:   8, episode reward: 221.400, mean reward:  6.512 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 83.967007, mean_q: 42.254907, mean_eps: 0.100000\n","     240718/2000000000: episode: 6566, duration: 5.188s, episode steps:  40, steps per second:   8, episode reward: 38.700, mean reward:  0.968 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 87.917330, mean_q: 42.041526, mean_eps: 0.100000\n","     240758/2000000000: episode: 6567, duration: 5.396s, episode steps:  40, steps per second:   7, episode reward: 24.500, mean reward:  0.613 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 81.581174, mean_q: 42.436292, mean_eps: 0.100000\n","     240798/2000000000: episode: 6568, duration: 5.689s, episode steps:  40, steps per second:   7, episode reward: 108.700, mean reward:  2.718 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 76.685018, mean_q: 42.416322, mean_eps: 0.100000\n","     240837/2000000000: episode: 6569, duration: 5.158s, episode steps:  39, steps per second:   8, episode reward: 93.400, mean reward:  2.395 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 80.998285, mean_q: 41.731644, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     240877/2000000000: episode: 6570, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: 100.200, mean reward:  2.505 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.631276, mean_q: 41.895734, mean_eps: 0.100000\n","     240907/2000000000: episode: 6571, duration: 4.287s, episode steps:  30, steps per second:   7, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.618161, mean_q: 43.027997, mean_eps: 0.100000\n","     240939/2000000000: episode: 6572, duration: 4.275s, episode steps:  32, steps per second:   7, episode reward: 103.000, mean reward:  3.219 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 87.430506, mean_q: 42.693636, mean_eps: 0.100000\n","     240976/2000000000: episode: 6573, duration: 4.886s, episode steps:  37, steps per second:   8, episode reward: 73.900, mean reward:  1.997 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 81.888580, mean_q: 42.520619, mean_eps: 0.100000\n","     241012/2000000000: episode: 6574, duration: 4.827s, episode steps:  36, steps per second:   7, episode reward: -6.000, mean reward: -0.167 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 82.898375, mean_q: 42.773099, mean_eps: 0.100000\n","     241052/2000000000: episode: 6575, duration: 5.321s, episode steps:  40, steps per second:   8, episode reward: 35.000, mean reward:  0.875 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 76.475315, mean_q: 42.585187, mean_eps: 0.100000\n","     241092/2000000000: episode: 6576, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 77.200, mean reward:  1.930 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.026824, mean_q: 43.140828, mean_eps: 0.100000\n","     241130/2000000000: episode: 6577, duration: 4.804s, episode steps:  38, steps per second:   8, episode reward: 127.800, mean reward:  3.363 [-20.000, 18.000], mean action: 1.368 [0.000, 2.000],  loss: 80.679353, mean_q: 42.705305, mean_eps: 0.100000\n","     241160/2000000000: episode: 6578, duration: 4.073s, episode steps:  30, steps per second:   7, episode reward: 46.600, mean reward:  1.553 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.886820, mean_q: 43.226879, mean_eps: 0.100000\n","     241200/2000000000: episode: 6579, duration: 5.314s, episode steps:  40, steps per second:   8, episode reward: 113.900, mean reward:  2.847 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.044695, mean_q: 42.224195, mean_eps: 0.100000\n","     241235/2000000000: episode: 6580, duration: 4.818s, episode steps:  35, steps per second:   7, episode reward: 24.300, mean reward:  0.694 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 93.941411, mean_q: 41.234720, mean_eps: 0.100000\n","     241270/2000000000: episode: 6581, duration: 4.670s, episode steps:  35, steps per second:   7, episode reward: 66.500, mean reward:  1.900 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 81.179284, mean_q: 43.228374, mean_eps: 0.100000\n","     241310/2000000000: episode: 6582, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: 59.300, mean reward:  1.482 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 81.109415, mean_q: 41.819432, mean_eps: 0.100000\n","     241338/2000000000: episode: 6583, duration: 3.898s, episode steps:  28, steps per second:   7, episode reward: 66.200, mean reward:  2.364 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 79.344863, mean_q: 42.953621, mean_eps: 0.100000\n","     241367/2000000000: episode: 6584, duration: 3.718s, episode steps:  29, steps per second:   8, episode reward: 102.200, mean reward:  3.524 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 85.699415, mean_q: 43.030004, mean_eps: 0.100000\n","     241393/2000000000: episode: 6585, duration: 3.509s, episode steps:  26, steps per second:   7, episode reward: 43.600, mean reward:  1.677 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 79.256503, mean_q: 43.608841, mean_eps: 0.100000\n","     241427/2000000000: episode: 6586, duration: 4.419s, episode steps:  34, steps per second:   8, episode reward: 108.100, mean reward:  3.179 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 77.506819, mean_q: 42.126719, mean_eps: 0.100000\n","     241452/2000000000: episode: 6587, duration: 3.515s, episode steps:  25, steps per second:   7, episode reward: 149.400, mean reward:  5.976 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 79.416842, mean_q: 42.371790, mean_eps: 0.100000\n","     241487/2000000000: episode: 6588, duration: 5.282s, episode steps:  35, steps per second:   7, episode reward: 53.400, mean reward:  1.526 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 82.613275, mean_q: 42.182913, mean_eps: 0.100000\n","     241525/2000000000: episode: 6589, duration: 5.677s, episode steps:  38, steps per second:   7, episode reward: 65.100, mean reward:  1.713 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 79.456069, mean_q: 42.394112, mean_eps: 0.100000\n","     241549/2000000000: episode: 6590, duration: 3.168s, episode steps:  24, steps per second:   8, episode reward: 39.300, mean reward:  1.637 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 85.803558, mean_q: 43.170283, mean_eps: 0.100000\n","     241582/2000000000: episode: 6591, duration: 4.184s, episode steps:  33, steps per second:   8, episode reward: 131.100, mean reward:  3.973 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.407355, mean_q: 42.098143, mean_eps: 0.100000\n","     241609/2000000000: episode: 6592, duration: 3.590s, episode steps:  27, steps per second:   8, episode reward: 59.100, mean reward:  2.189 [-20.000, 18.000], mean action: 1.074 [0.000, 2.000],  loss: 84.802029, mean_q: 42.956410, mean_eps: 0.100000\n","     241641/2000000000: episode: 6593, duration: 4.170s, episode steps:  32, steps per second:   8, episode reward: 158.300, mean reward:  4.947 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 80.643762, mean_q: 41.832815, mean_eps: 0.100000\n","     241666/2000000000: episode: 6594, duration: 3.424s, episode steps:  25, steps per second:   7, episode reward: 61.700, mean reward:  2.468 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 93.406420, mean_q: 41.804720, mean_eps: 0.100000\n","     241693/2000000000: episode: 6595, duration: 3.711s, episode steps:  27, steps per second:   7, episode reward: 137.500, mean reward:  5.093 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 89.210331, mean_q: 41.321593, mean_eps: 0.100000\n","     241728/2000000000: episode: 6596, duration: 4.457s, episode steps:  35, steps per second:   8, episode reward: 177.100, mean reward:  5.060 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.130909, mean_q: 42.708117, mean_eps: 0.100000\n","     241762/2000000000: episode: 6597, duration: 4.544s, episode steps:  34, steps per second:   7, episode reward: -5.400, mean reward: -0.159 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 83.563492, mean_q: 42.801264, mean_eps: 0.100000\n","     241790/2000000000: episode: 6598, duration: 3.826s, episode steps:  28, steps per second:   7, episode reward: 164.800, mean reward:  5.886 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 83.045144, mean_q: 43.059690, mean_eps: 0.100000\n","     241830/2000000000: episode: 6599, duration: 5.300s, episode steps:  40, steps per second:   8, episode reward: 163.100, mean reward:  4.077 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.298277, mean_q: 41.960022, mean_eps: 0.100000\n","     241858/2000000000: episode: 6600, duration: 3.878s, episode steps:  28, steps per second:   7, episode reward: 116.900, mean reward:  4.175 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 83.752563, mean_q: 43.289210, mean_eps: 0.100000\n","     241898/2000000000: episode: 6601, duration: 5.392s, episode steps:  40, steps per second:   7, episode reward: 79.700, mean reward:  1.992 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.675875, mean_q: 42.270133, mean_eps: 0.100000\n","     241926/2000000000: episode: 6602, duration: 3.733s, episode steps:  28, steps per second:   8, episode reward: 81.100, mean reward:  2.896 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 90.330359, mean_q: 42.899835, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     241966/2000000000: episode: 6603, duration: 5.039s, episode steps:  40, steps per second:   8, episode reward: 124.300, mean reward:  3.107 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.105478, mean_q: 43.248686, mean_eps: 0.100000\n","     242006/2000000000: episode: 6604, duration: 5.237s, episode steps:  40, steps per second:   8, episode reward: -2.800, mean reward: -0.070 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.213947, mean_q: 42.168297, mean_eps: 0.100000\n","     242046/2000000000: episode: 6605, duration: 5.196s, episode steps:  40, steps per second:   8, episode reward: 106.000, mean reward:  2.650 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.035847, mean_q: 42.385122, mean_eps: 0.100000\n","     242076/2000000000: episode: 6606, duration: 3.828s, episode steps:  30, steps per second:   8, episode reward: 21.900, mean reward:  0.730 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.909919, mean_q: 43.344379, mean_eps: 0.100000\n","     242107/2000000000: episode: 6607, duration: 3.982s, episode steps:  31, steps per second:   8, episode reward: 148.600, mean reward:  4.794 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 88.728248, mean_q: 41.004373, mean_eps: 0.100000\n","     242136/2000000000: episode: 6608, duration: 3.795s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 76.631685, mean_q: 42.751206, mean_eps: 0.100000\n","     242173/2000000000: episode: 6609, duration: 4.648s, episode steps:  37, steps per second:   8, episode reward: 72.000, mean reward:  1.946 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.090555, mean_q: 42.471142, mean_eps: 0.100000\n","     242213/2000000000: episode: 6610, duration: 5.273s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.626481, mean_q: 42.763058, mean_eps: 0.100000\n","     242239/2000000000: episode: 6611, duration: 3.651s, episode steps:  26, steps per second:   7, episode reward: 99.500, mean reward:  3.827 [-20.000, 18.400], mean action: 0.731 [0.000, 2.000],  loss: 86.267437, mean_q: 43.440864, mean_eps: 0.100000\n","     242276/2000000000: episode: 6612, duration: 5.025s, episode steps:  37, steps per second:   7, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 78.078916, mean_q: 41.937982, mean_eps: 0.100000\n","     242305/2000000000: episode: 6613, duration: 4.084s, episode steps:  29, steps per second:   7, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 84.669321, mean_q: 42.358435, mean_eps: 0.100000\n","     242339/2000000000: episode: 6614, duration: 4.486s, episode steps:  34, steps per second:   8, episode reward: -119.400, mean reward: -3.512 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 74.165495, mean_q: 42.472806, mean_eps: 0.100000\n","     242369/2000000000: episode: 6615, duration: 4.192s, episode steps:  30, steps per second:   7, episode reward: 13.100, mean reward:  0.437 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 89.332276, mean_q: 41.954314, mean_eps: 0.100000\n","     242409/2000000000: episode: 6616, duration: 5.188s, episode steps:  40, steps per second:   8, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 81.572514, mean_q: 42.607300, mean_eps: 0.100000\n","     242449/2000000000: episode: 6617, duration: 5.255s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.301422, mean_q: 41.958282, mean_eps: 0.100000\n","     242487/2000000000: episode: 6618, duration: 4.981s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 84.268046, mean_q: 42.799585, mean_eps: 0.100000\n","     242527/2000000000: episode: 6619, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward: -9.700, mean reward: -0.242 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.125322, mean_q: 41.593294, mean_eps: 0.100000\n","     242567/2000000000: episode: 6620, duration: 5.093s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 84.107255, mean_q: 41.284940, mean_eps: 0.100000\n","     242605/2000000000: episode: 6621, duration: 5.029s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 86.597935, mean_q: 42.267088, mean_eps: 0.100000\n","     242643/2000000000: episode: 6622, duration: 4.852s, episode steps:  38, steps per second:   8, episode reward: 158.100, mean reward:  4.161 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 82.006159, mean_q: 42.896944, mean_eps: 0.100000\n","     242675/2000000000: episode: 6623, duration: 4.298s, episode steps:  32, steps per second:   7, episode reward:  3.500, mean reward:  0.109 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.522400, mean_q: 41.844091, mean_eps: 0.100000\n","     242705/2000000000: episode: 6624, duration: 3.987s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 74.510701, mean_q: 43.784653, mean_eps: 0.100000\n","     242734/2000000000: episode: 6625, duration: 3.764s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 78.733805, mean_q: 42.857537, mean_eps: 0.100000\n","     242762/2000000000: episode: 6626, duration: 3.799s, episode steps:  28, steps per second:   7, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 77.541371, mean_q: 42.388290, mean_eps: 0.100000\n","     242792/2000000000: episode: 6627, duration: 3.892s, episode steps:  30, steps per second:   8, episode reward: -18.200, mean reward: -0.607 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 81.176975, mean_q: 42.567689, mean_eps: 0.100000\n","     242828/2000000000: episode: 6628, duration: 4.556s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 75.744390, mean_q: 42.374861, mean_eps: 0.100000\n","     242859/2000000000: episode: 6629, duration: 3.972s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 82.906633, mean_q: 42.926299, mean_eps: 0.100000\n","     242899/2000000000: episode: 6630, duration: 5.043s, episode steps:  40, steps per second:   8, episode reward: 130.100, mean reward:  3.253 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.935249, mean_q: 42.622180, mean_eps: 0.100000\n","     242923/2000000000: episode: 6631, duration: 3.111s, episode steps:  24, steps per second:   8, episode reward: -46.300, mean reward: -1.929 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 80.636676, mean_q: 43.361897, mean_eps: 0.100000\n","     242947/2000000000: episode: 6632, duration: 3.106s, episode steps:  24, steps per second:   8, episode reward: 170.000, mean reward:  7.083 [-20.000, 18.000], mean action: 0.917 [0.000, 2.000],  loss: 79.942880, mean_q: 42.484051, mean_eps: 0.100000\n","     242979/2000000000: episode: 6633, duration: 4.052s, episode steps:  32, steps per second:   8, episode reward: 123.000, mean reward:  3.844 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 78.435560, mean_q: 42.297512, mean_eps: 0.100000\n","     243012/2000000000: episode: 6634, duration: 4.274s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.049274, mean_q: 42.011564, mean_eps: 0.100000\n","     243044/2000000000: episode: 6635, duration: 4.481s, episode steps:  32, steps per second:   7, episode reward: -73.800, mean reward: -2.306 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 89.100274, mean_q: 42.559239, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     243084/2000000000: episode: 6636, duration: 5.416s, episode steps:  40, steps per second:   7, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 76.569440, mean_q: 42.402795, mean_eps: 0.100000\n","     243120/2000000000: episode: 6637, duration: 4.735s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 81.246153, mean_q: 42.548763, mean_eps: 0.100000\n","     243160/2000000000: episode: 6638, duration: 5.380s, episode steps:  40, steps per second:   7, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.774710, mean_q: 42.574802, mean_eps: 0.100000\n","     243191/2000000000: episode: 6639, duration: 4.076s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 87.830766, mean_q: 41.982788, mean_eps: 0.100000\n","     243231/2000000000: episode: 6640, duration: 5.389s, episode steps:  40, steps per second:   7, episode reward: 190.000, mean reward:  4.750 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.692759, mean_q: 42.563662, mean_eps: 0.100000\n","     243260/2000000000: episode: 6641, duration: 3.785s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.981458, mean_q: 41.882313, mean_eps: 0.100000\n","     243300/2000000000: episode: 6642, duration: 5.148s, episode steps:  40, steps per second:   8, episode reward: -47.400, mean reward: -1.185 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 82.614611, mean_q: 42.652833, mean_eps: 0.100000\n","     243340/2000000000: episode: 6643, duration: 5.254s, episode steps:  40, steps per second:   8, episode reward:  0.500, mean reward:  0.012 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 75.183896, mean_q: 42.250683, mean_eps: 0.100000\n","     243377/2000000000: episode: 6644, duration: 4.739s, episode steps:  37, steps per second:   8, episode reward: 39.300, mean reward:  1.062 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 79.125072, mean_q: 43.111522, mean_eps: 0.100000\n","     243416/2000000000: episode: 6645, duration: 4.881s, episode steps:  39, steps per second:   8, episode reward: -81.100, mean reward: -2.079 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 82.210695, mean_q: 43.084567, mean_eps: 0.100000\n","     243450/2000000000: episode: 6646, duration: 4.303s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 86.095427, mean_q: 43.221276, mean_eps: 0.100000\n","     243473/2000000000: episode: 6647, duration: 3.018s, episode steps:  23, steps per second:   8, episode reward: 15.500, mean reward:  0.674 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 81.958429, mean_q: 42.595709, mean_eps: 0.100000\n","     243509/2000000000: episode: 6648, duration: 4.747s, episode steps:  36, steps per second:   8, episode reward: 38.100, mean reward:  1.058 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.413409, mean_q: 42.914564, mean_eps: 0.100000\n","     243538/2000000000: episode: 6649, duration: 3.837s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 92.355951, mean_q: 41.876891, mean_eps: 0.100000\n","     243568/2000000000: episode: 6650, duration: 3.812s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 83.250932, mean_q: 42.309573, mean_eps: 0.100000\n","     243608/2000000000: episode: 6651, duration: 5.454s, episode steps:  40, steps per second:   7, episode reward: 122.200, mean reward:  3.055 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 82.558510, mean_q: 42.425210, mean_eps: 0.100000\n","     243640/2000000000: episode: 6652, duration: 4.177s, episode steps:  32, steps per second:   8, episode reward: -45.600, mean reward: -1.425 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 79.430765, mean_q: 42.477719, mean_eps: 0.100000\n","     243673/2000000000: episode: 6653, duration: 4.473s, episode steps:  33, steps per second:   7, episode reward: 48.900, mean reward:  1.482 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.995049, mean_q: 42.894605, mean_eps: 0.100000\n","     243699/2000000000: episode: 6654, duration: 3.500s, episode steps:  26, steps per second:   7, episode reward: 85.300, mean reward:  3.281 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 82.317890, mean_q: 41.360270, mean_eps: 0.100000\n","     243726/2000000000: episode: 6655, duration: 3.749s, episode steps:  27, steps per second:   7, episode reward: 121.400, mean reward:  4.496 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 80.174700, mean_q: 42.846004, mean_eps: 0.100000\n","     243766/2000000000: episode: 6656, duration: 5.091s, episode steps:  40, steps per second:   8, episode reward: 33.200, mean reward:  0.830 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 83.878973, mean_q: 42.739719, mean_eps: 0.100000\n","     243804/2000000000: episode: 6657, duration: 5.002s, episode steps:  38, steps per second:   8, episode reward: 147.300, mean reward:  3.876 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 78.841515, mean_q: 42.195655, mean_eps: 0.100000\n","     243835/2000000000: episode: 6658, duration: 4.290s, episode steps:  31, steps per second:   7, episode reward: 114.000, mean reward:  3.677 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 72.558887, mean_q: 41.761423, mean_eps: 0.100000\n","     243868/2000000000: episode: 6659, duration: 4.582s, episode steps:  33, steps per second:   7, episode reward: 147.000, mean reward:  4.455 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.643570, mean_q: 43.153572, mean_eps: 0.100000\n","     243903/2000000000: episode: 6660, duration: 4.689s, episode steps:  35, steps per second:   7, episode reward: 102.300, mean reward:  2.923 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.719313, mean_q: 43.041721, mean_eps: 0.100000\n","     243928/2000000000: episode: 6661, duration: 3.189s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.840173, mean_q: 43.086978, mean_eps: 0.100000\n","     243968/2000000000: episode: 6662, duration: 5.199s, episode steps:  40, steps per second:   8, episode reward: 144.000, mean reward:  3.600 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 83.225567, mean_q: 42.476724, mean_eps: 0.100000\n","     244002/2000000000: episode: 6663, duration: 4.439s, episode steps:  34, steps per second:   8, episode reward: -62.000, mean reward: -1.824 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 85.444632, mean_q: 42.295797, mean_eps: 0.100000\n","     244032/2000000000: episode: 6664, duration: 3.738s, episode steps:  30, steps per second:   8, episode reward: 28.200, mean reward:  0.940 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 79.147195, mean_q: 43.451063, mean_eps: 0.100000\n","     244071/2000000000: episode: 6665, duration: 5.314s, episode steps:  39, steps per second:   7, episode reward: 148.000, mean reward:  3.795 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.923102, mean_q: 43.182656, mean_eps: 0.100000\n","     244099/2000000000: episode: 6666, duration: 3.699s, episode steps:  28, steps per second:   8, episode reward: 80.300, mean reward:  2.868 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 91.188822, mean_q: 43.104103, mean_eps: 0.100000\n","     244131/2000000000: episode: 6667, duration: 4.291s, episode steps:  32, steps per second:   7, episode reward: 29.900, mean reward:  0.934 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 82.717419, mean_q: 41.990871, mean_eps: 0.100000\n","     244163/2000000000: episode: 6668, duration: 4.118s, episode steps:  32, steps per second:   8, episode reward: 33.700, mean reward:  1.053 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 90.219947, mean_q: 42.482625, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     244195/2000000000: episode: 6669, duration: 4.368s, episode steps:  32, steps per second:   7, episode reward: 145.700, mean reward:  4.553 [-20.000, 19.300], mean action: 0.938 [0.000, 2.000],  loss: 78.291182, mean_q: 41.756192, mean_eps: 0.100000\n","     244224/2000000000: episode: 6670, duration: 3.907s, episode steps:  29, steps per second:   7, episode reward: 82.800, mean reward:  2.855 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 83.369228, mean_q: 43.700658, mean_eps: 0.100000\n","     244249/2000000000: episode: 6671, duration: 3.241s, episode steps:  25, steps per second:   8, episode reward: 89.100, mean reward:  3.564 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 83.388887, mean_q: 42.269967, mean_eps: 0.100000\n","     244289/2000000000: episode: 6672, duration: 5.555s, episode steps:  40, steps per second:   7, episode reward: 177.900, mean reward:  4.448 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.706016, mean_q: 42.017540, mean_eps: 0.100000\n","     244324/2000000000: episode: 6673, duration: 4.465s, episode steps:  35, steps per second:   8, episode reward: 215.000, mean reward:  6.143 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 81.543283, mean_q: 43.215714, mean_eps: 0.100000\n","     244363/2000000000: episode: 6674, duration: 5.166s, episode steps:  39, steps per second:   8, episode reward: 99.000, mean reward:  2.538 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 80.379163, mean_q: 43.104249, mean_eps: 0.100000\n","     244399/2000000000: episode: 6675, duration: 4.785s, episode steps:  36, steps per second:   8, episode reward: 81.300, mean reward:  2.258 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 84.266014, mean_q: 42.227003, mean_eps: 0.100000\n","     244439/2000000000: episode: 6676, duration: 5.253s, episode steps:  40, steps per second:   8, episode reward: 73.400, mean reward:  1.835 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 74.941435, mean_q: 42.742635, mean_eps: 0.100000\n","     244471/2000000000: episode: 6677, duration: 4.327s, episode steps:  32, steps per second:   7, episode reward: 119.800, mean reward:  3.744 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 86.360345, mean_q: 42.205555, mean_eps: 0.100000\n","     244496/2000000000: episode: 6678, duration: 3.592s, episode steps:  25, steps per second:   7, episode reward:  7.400, mean reward:  0.296 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 79.053405, mean_q: 42.637744, mean_eps: 0.100000\n","     244524/2000000000: episode: 6679, duration: 3.975s, episode steps:  28, steps per second:   7, episode reward: 61.800, mean reward:  2.207 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 74.367871, mean_q: 42.374791, mean_eps: 0.100000\n","     244558/2000000000: episode: 6680, duration: 4.514s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 78.296788, mean_q: 42.629326, mean_eps: 0.100000\n","     244595/2000000000: episode: 6681, duration: 4.700s, episode steps:  37, steps per second:   8, episode reward: -2.300, mean reward: -0.062 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 78.466833, mean_q: 43.787892, mean_eps: 0.100000\n","     244625/2000000000: episode: 6682, duration: 4.034s, episode steps:  30, steps per second:   7, episode reward: 30.000, mean reward:  1.000 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 93.043752, mean_q: 41.363791, mean_eps: 0.100000\n","     244655/2000000000: episode: 6683, duration: 4.258s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.930471, mean_q: 41.749619, mean_eps: 0.100000\n","     244694/2000000000: episode: 6684, duration: 5.178s, episode steps:  39, steps per second:   8, episode reward: 168.800, mean reward:  4.328 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 75.034222, mean_q: 42.648173, mean_eps: 0.100000\n","     244732/2000000000: episode: 6685, duration: 5.238s, episode steps:  38, steps per second:   7, episode reward: 124.800, mean reward:  3.284 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 80.408963, mean_q: 42.539088, mean_eps: 0.100000\n","     244762/2000000000: episode: 6686, duration: 3.935s, episode steps:  30, steps per second:   8, episode reward: 138.900, mean reward:  4.630 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 81.192298, mean_q: 43.412607, mean_eps: 0.100000\n","     244802/2000000000: episode: 6687, duration: 5.660s, episode steps:  40, steps per second:   7, episode reward: 108.000, mean reward:  2.700 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.886006, mean_q: 43.031887, mean_eps: 0.100000\n","     244834/2000000000: episode: 6688, duration: 4.304s, episode steps:  32, steps per second:   7, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 89.229130, mean_q: 42.903357, mean_eps: 0.100000\n","     244874/2000000000: episode: 6689, duration: 5.318s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 88.119118, mean_q: 42.442870, mean_eps: 0.100000\n","     244914/2000000000: episode: 6690, duration: 5.200s, episode steps:  40, steps per second:   8, episode reward: -70.000, mean reward: -1.750 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 82.700395, mean_q: 41.418410, mean_eps: 0.100000\n","     244948/2000000000: episode: 6691, duration: 4.563s, episode steps:  34, steps per second:   7, episode reward: -20.300, mean reward: -0.597 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 79.822472, mean_q: 42.828995, mean_eps: 0.100000\n","     244984/2000000000: episode: 6692, duration: 4.728s, episode steps:  36, steps per second:   8, episode reward: 51.200, mean reward:  1.422 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 82.850970, mean_q: 42.418960, mean_eps: 0.100000\n","     245017/2000000000: episode: 6693, duration: 4.302s, episode steps:  33, steps per second:   8, episode reward: 73.800, mean reward:  2.236 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 86.758780, mean_q: 42.252681, mean_eps: 0.100000\n","     245050/2000000000: episode: 6694, duration: 4.189s, episode steps:  33, steps per second:   8, episode reward: 147.300, mean reward:  4.464 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 85.561817, mean_q: 42.435173, mean_eps: 0.100000\n","     245090/2000000000: episode: 6695, duration: 5.034s, episode steps:  40, steps per second:   8, episode reward:  4.200, mean reward:  0.105 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 76.114945, mean_q: 42.450061, mean_eps: 0.100000\n","     245125/2000000000: episode: 6696, duration: 4.398s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 77.225271, mean_q: 42.671375, mean_eps: 0.100000\n","     245165/2000000000: episode: 6697, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: 172.000, mean reward:  4.300 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 86.106893, mean_q: 42.629368, mean_eps: 0.100000\n","     245198/2000000000: episode: 6698, duration: 4.174s, episode steps:  33, steps per second:   8, episode reward: 145.500, mean reward:  4.409 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.383629, mean_q: 41.916104, mean_eps: 0.100000\n","     245238/2000000000: episode: 6699, duration: 5.056s, episode steps:  40, steps per second:   8, episode reward: 119.700, mean reward:  2.992 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.334698, mean_q: 42.337138, mean_eps: 0.100000\n","     245268/2000000000: episode: 6700, duration: 3.852s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 79.648432, mean_q: 42.521592, mean_eps: 0.100000\n","     245298/2000000000: episode: 6701, duration: 3.891s, episode steps:  30, steps per second:   8, episode reward: 219.000, mean reward:  7.300 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 83.179277, mean_q: 43.246855, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     245338/2000000000: episode: 6702, duration: 5.314s, episode steps:  40, steps per second:   8, episode reward: 54.500, mean reward:  1.362 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.832565, mean_q: 42.860336, mean_eps: 0.100000\n","     245374/2000000000: episode: 6703, duration: 4.875s, episode steps:  36, steps per second:   7, episode reward: -9.700, mean reward: -0.269 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 80.699209, mean_q: 42.670500, mean_eps: 0.100000\n","     245404/2000000000: episode: 6704, duration: 4.165s, episode steps:  30, steps per second:   7, episode reward: -38.000, mean reward: -1.267 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.995133, mean_q: 43.405387, mean_eps: 0.100000\n","     245436/2000000000: episode: 6705, duration: 4.449s, episode steps:  32, steps per second:   7, episode reward: 95.700, mean reward:  2.991 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.619561, mean_q: 41.899733, mean_eps: 0.100000\n","     245473/2000000000: episode: 6706, duration: 4.835s, episode steps:  37, steps per second:   8, episode reward: 144.600, mean reward:  3.908 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 83.796456, mean_q: 41.775106, mean_eps: 0.100000\n","     245513/2000000000: episode: 6707, duration: 5.493s, episode steps:  40, steps per second:   7, episode reward: 85.900, mean reward:  2.148 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.237319, mean_q: 42.688263, mean_eps: 0.100000\n","     245548/2000000000: episode: 6708, duration: 4.534s, episode steps:  35, steps per second:   8, episode reward: -35.400, mean reward: -1.011 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 76.607629, mean_q: 42.084433, mean_eps: 0.100000\n","     245588/2000000000: episode: 6709, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward:  2.400, mean reward:  0.060 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.066513, mean_q: 42.703456, mean_eps: 0.100000\n","     245621/2000000000: episode: 6710, duration: 4.455s, episode steps:  33, steps per second:   7, episode reward: -32.100, mean reward: -0.973 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 75.587425, mean_q: 43.310012, mean_eps: 0.100000\n","     245653/2000000000: episode: 6711, duration: 4.224s, episode steps:  32, steps per second:   8, episode reward: 50.600, mean reward:  1.581 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 73.997126, mean_q: 42.843190, mean_eps: 0.100000\n","     245690/2000000000: episode: 6712, duration: 4.883s, episode steps:  37, steps per second:   8, episode reward: 132.500, mean reward:  3.581 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 79.537900, mean_q: 42.560741, mean_eps: 0.100000\n","     245730/2000000000: episode: 6713, duration: 5.155s, episode steps:  40, steps per second:   8, episode reward: 102.200, mean reward:  2.555 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.764029, mean_q: 43.008226, mean_eps: 0.100000\n","     245760/2000000000: episode: 6714, duration: 3.948s, episode steps:  30, steps per second:   8, episode reward: 152.800, mean reward:  5.093 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 82.543627, mean_q: 42.535628, mean_eps: 0.100000\n","     245789/2000000000: episode: 6715, duration: 3.817s, episode steps:  29, steps per second:   8, episode reward: 131.200, mean reward:  4.524 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 76.010732, mean_q: 42.931983, mean_eps: 0.100000\n","     245829/2000000000: episode: 6716, duration: 5.376s, episode steps:  40, steps per second:   7, episode reward: 98.800, mean reward:  2.470 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.039315, mean_q: 42.984345, mean_eps: 0.100000\n","     245861/2000000000: episode: 6717, duration: 4.586s, episode steps:  32, steps per second:   7, episode reward: 260.200, mean reward:  8.131 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 80.681978, mean_q: 43.356154, mean_eps: 0.100000\n","     245890/2000000000: episode: 6718, duration: 3.942s, episode steps:  29, steps per second:   7, episode reward: 174.000, mean reward:  6.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 74.900731, mean_q: 42.642785, mean_eps: 0.100000\n","     245923/2000000000: episode: 6719, duration: 4.534s, episode steps:  33, steps per second:   7, episode reward: 58.100, mean reward:  1.761 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 84.635853, mean_q: 41.506921, mean_eps: 0.100000\n","     245959/2000000000: episode: 6720, duration: 4.701s, episode steps:  36, steps per second:   8, episode reward: 16.700, mean reward:  0.464 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 76.617102, mean_q: 42.840618, mean_eps: 0.100000\n","     245996/2000000000: episode: 6721, duration: 4.925s, episode steps:  37, steps per second:   8, episode reward: 126.600, mean reward:  3.422 [-20.000, 18.000], mean action: 1.027 [0.000, 2.000],  loss: 78.957145, mean_q: 42.783387, mean_eps: 0.100000\n","     246030/2000000000: episode: 6722, duration: 4.305s, episode steps:  34, steps per second:   8, episode reward: 289.100, mean reward:  8.503 [-16.800, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 81.902820, mean_q: 42.372580, mean_eps: 0.100000\n","     246057/2000000000: episode: 6723, duration: 3.436s, episode steps:  27, steps per second:   8, episode reward: 149.700, mean reward:  5.544 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 78.808077, mean_q: 43.026822, mean_eps: 0.100000\n","     246086/2000000000: episode: 6724, duration: 3.851s, episode steps:  29, steps per second:   8, episode reward: 206.400, mean reward:  7.117 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 75.149101, mean_q: 43.134472, mean_eps: 0.100000\n","     246118/2000000000: episode: 6725, duration: 4.080s, episode steps:  32, steps per second:   8, episode reward: 96.600, mean reward:  3.019 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.750552, mean_q: 41.579229, mean_eps: 0.100000\n","     246150/2000000000: episode: 6726, duration: 4.189s, episode steps:  32, steps per second:   8, episode reward: 100.200, mean reward:  3.131 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 86.396010, mean_q: 41.996374, mean_eps: 0.100000\n","     246188/2000000000: episode: 6727, duration: 4.948s, episode steps:  38, steps per second:   8, episode reward: -19.700, mean reward: -0.518 [-20.000, 18.000], mean action: 1.342 [0.000, 2.000],  loss: 83.823425, mean_q: 42.301011, mean_eps: 0.100000\n","     246228/2000000000: episode: 6728, duration: 5.201s, episode steps:  40, steps per second:   8, episode reward: -44.700, mean reward: -1.117 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 87.133247, mean_q: 41.923483, mean_eps: 0.100000\n","     246262/2000000000: episode: 6729, duration: 4.415s, episode steps:  34, steps per second:   8, episode reward: 72.700, mean reward:  2.138 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 77.456420, mean_q: 43.383626, mean_eps: 0.100000\n","     246298/2000000000: episode: 6730, duration: 4.529s, episode steps:  36, steps per second:   8, episode reward: 42.200, mean reward:  1.172 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.118279, mean_q: 42.887593, mean_eps: 0.100000\n","     246338/2000000000: episode: 6731, duration: 5.156s, episode steps:  40, steps per second:   8, episode reward: 47.800, mean reward:  1.195 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 84.559719, mean_q: 42.802719, mean_eps: 0.100000\n","     246375/2000000000: episode: 6732, duration: 4.913s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 82.483724, mean_q: 41.952979, mean_eps: 0.100000\n","     246410/2000000000: episode: 6733, duration: 4.654s, episode steps:  35, steps per second:   8, episode reward: -60.900, mean reward: -1.740 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 75.958068, mean_q: 43.495196, mean_eps: 0.100000\n","     246441/2000000000: episode: 6734, duration: 4.272s, episode steps:  31, steps per second:   7, episode reward: -17.000, mean reward: -0.548 [-20.000, 18.000], mean action: 0.839 [0.000, 2.000],  loss: 85.015165, mean_q: 43.218503, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     246470/2000000000: episode: 6735, duration: 3.911s, episode steps:  29, steps per second:   7, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 84.800554, mean_q: 42.650644, mean_eps: 0.100000\n","     246496/2000000000: episode: 6736, duration: 3.483s, episode steps:  26, steps per second:   7, episode reward: 89.300, mean reward:  3.435 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 76.315518, mean_q: 42.807975, mean_eps: 0.100000\n","     246526/2000000000: episode: 6737, duration: 4.134s, episode steps:  30, steps per second:   7, episode reward: 221.300, mean reward:  7.377 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 84.827049, mean_q: 42.666402, mean_eps: 0.100000\n","     246566/2000000000: episode: 6738, duration: 6.310s, episode steps:  40, steps per second:   6, episode reward: 171.000, mean reward:  4.275 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.920516, mean_q: 42.499524, mean_eps: 0.100000\n","     246601/2000000000: episode: 6739, duration: 5.025s, episode steps:  35, steps per second:   7, episode reward: 149.000, mean reward:  4.257 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 84.368612, mean_q: 42.318881, mean_eps: 0.100000\n","     246632/2000000000: episode: 6740, duration: 3.997s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.347358, mean_q: 42.215745, mean_eps: 0.100000\n","     246672/2000000000: episode: 6741, duration: 5.420s, episode steps:  40, steps per second:   7, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 79.717846, mean_q: 42.739595, mean_eps: 0.100000\n","     246712/2000000000: episode: 6742, duration: 5.189s, episode steps:  40, steps per second:   8, episode reward: 125.000, mean reward:  3.125 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.232166, mean_q: 43.103224, mean_eps: 0.100000\n","     246737/2000000000: episode: 6743, duration: 3.341s, episode steps:  25, steps per second:   7, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.600 [0.000, 2.000],  loss: 80.398925, mean_q: 42.665995, mean_eps: 0.100000\n","     246767/2000000000: episode: 6744, duration: 3.987s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 81.599679, mean_q: 42.299619, mean_eps: 0.100000\n","     246795/2000000000: episode: 6745, duration: 3.580s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 80.606809, mean_q: 42.960191, mean_eps: 0.100000\n","     246831/2000000000: episode: 6746, duration: 4.627s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.166005, mean_q: 42.479158, mean_eps: 0.100000\n","     246862/2000000000: episode: 6747, duration: 4.083s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.632494, mean_q: 42.620427, mean_eps: 0.100000\n","     246894/2000000000: episode: 6748, duration: 4.334s, episode steps:  32, steps per second:   7, episode reward: 158.400, mean reward:  4.950 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 77.289415, mean_q: 42.527058, mean_eps: 0.100000\n","     246930/2000000000: episode: 6749, duration: 4.606s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 83.524750, mean_q: 42.686376, mean_eps: 0.100000\n","     246970/2000000000: episode: 6750, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.851490, mean_q: 42.531616, mean_eps: 0.100000\n","     247002/2000000000: episode: 6751, duration: 4.205s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.144720, mean_q: 43.384222, mean_eps: 0.100000\n","     247042/2000000000: episode: 6752, duration: 5.355s, episode steps:  40, steps per second:   7, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.169971, mean_q: 43.248489, mean_eps: 0.100000\n","     247082/2000000000: episode: 6753, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward: -3.800, mean reward: -0.095 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 79.092990, mean_q: 43.104678, mean_eps: 0.100000\n","     247108/2000000000: episode: 6754, duration: 3.356s, episode steps:  26, steps per second:   8, episode reward: 208.000, mean reward:  8.000 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 76.264514, mean_q: 41.750309, mean_eps: 0.100000\n","     247136/2000000000: episode: 6755, duration: 3.595s, episode steps:  28, steps per second:   8, episode reward: -134.000, mean reward: -4.786 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 82.041404, mean_q: 41.710182, mean_eps: 0.100000\n","     247168/2000000000: episode: 6756, duration: 4.107s, episode steps:  32, steps per second:   8, episode reward: 41.900, mean reward:  1.309 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.848420, mean_q: 42.337976, mean_eps: 0.100000\n","     247208/2000000000: episode: 6757, duration: 5.101s, episode steps:  40, steps per second:   8, episode reward:  4.500, mean reward:  0.113 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.117786, mean_q: 42.655291, mean_eps: 0.100000\n","     247248/2000000000: episode: 6758, duration: 5.146s, episode steps:  40, steps per second:   8, episode reward: 101.600, mean reward:  2.540 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.664454, mean_q: 42.827155, mean_eps: 0.100000\n","     247279/2000000000: episode: 6759, duration: 4.105s, episode steps:  31, steps per second:   8, episode reward: 231.900, mean reward:  7.481 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 77.153350, mean_q: 43.902410, mean_eps: 0.100000\n","     247312/2000000000: episode: 6760, duration: 4.302s, episode steps:  33, steps per second:   8, episode reward: 58.500, mean reward:  1.773 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.609649, mean_q: 41.764928, mean_eps: 0.100000\n","     247352/2000000000: episode: 6761, duration: 5.224s, episode steps:  40, steps per second:   8, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.644312, mean_q: 42.234578, mean_eps: 0.100000\n","     247392/2000000000: episode: 6762, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.474417, mean_q: 42.315157, mean_eps: 0.100000\n","     247423/2000000000: episode: 6763, duration: 4.017s, episode steps:  31, steps per second:   8, episode reward: 12.400, mean reward:  0.400 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.927513, mean_q: 43.209572, mean_eps: 0.100000\n","     247456/2000000000: episode: 6764, duration: 4.331s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 79.644136, mean_q: 42.693626, mean_eps: 0.100000\n","     247495/2000000000: episode: 6765, duration: 5.064s, episode steps:  39, steps per second:   8, episode reward: 35.200, mean reward:  0.903 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 93.812740, mean_q: 42.672789, mean_eps: 0.100000\n","     247525/2000000000: episode: 6766, duration: 4.088s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 84.665108, mean_q: 42.243589, mean_eps: 0.100000\n","     247563/2000000000: episode: 6767, duration: 5.020s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 84.713628, mean_q: 41.821146, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     247599/2000000000: episode: 6768, duration: 4.642s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 77.571590, mean_q: 42.545432, mean_eps: 0.100000\n","     247630/2000000000: episode: 6769, duration: 4.038s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 81.936989, mean_q: 41.810508, mean_eps: 0.100000\n","     247668/2000000000: episode: 6770, duration: 5.062s, episode steps:  38, steps per second:   8, episode reward: 87.200, mean reward:  2.295 [-20.000, 18.900], mean action: 1.289 [0.000, 2.000],  loss: 79.183116, mean_q: 42.289727, mean_eps: 0.100000\n","     247702/2000000000: episode: 6771, duration: 4.478s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 85.258596, mean_q: 42.804223, mean_eps: 0.100000\n","     247737/2000000000: episode: 6772, duration: 4.637s, episode steps:  35, steps per second:   8, episode reward: 193.900, mean reward:  5.540 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 76.154434, mean_q: 43.449327, mean_eps: 0.100000\n","     247762/2000000000: episode: 6773, duration: 3.227s, episode steps:  25, steps per second:   8, episode reward: 284.000, mean reward: 11.360 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 83.847347, mean_q: 41.078826, mean_eps: 0.100000\n","     247802/2000000000: episode: 6774, duration: 5.133s, episode steps:  40, steps per second:   8, episode reward: 124.800, mean reward:  3.120 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.694862, mean_q: 42.543022, mean_eps: 0.100000\n","     247828/2000000000: episode: 6775, duration: 3.380s, episode steps:  26, steps per second:   8, episode reward: 13.800, mean reward:  0.531 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 80.015124, mean_q: 42.738845, mean_eps: 0.100000\n","     247853/2000000000: episode: 6776, duration: 3.239s, episode steps:  25, steps per second:   8, episode reward: 43.000, mean reward:  1.720 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 82.034775, mean_q: 42.976393, mean_eps: 0.100000\n","     247885/2000000000: episode: 6777, duration: 4.030s, episode steps:  32, steps per second:   8, episode reward: 24.600, mean reward:  0.769 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.974555, mean_q: 42.739669, mean_eps: 0.100000\n","     247912/2000000000: episode: 6778, duration: 3.516s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 82.303012, mean_q: 42.444225, mean_eps: 0.100000\n","     247946/2000000000: episode: 6779, duration: 4.378s, episode steps:  34, steps per second:   8, episode reward: 246.000, mean reward:  7.235 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 81.416111, mean_q: 42.334396, mean_eps: 0.100000\n","     247978/2000000000: episode: 6780, duration: 4.308s, episode steps:  32, steps per second:   7, episode reward: 171.600, mean reward:  5.362 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 86.173444, mean_q: 42.695101, mean_eps: 0.100000\n","     248012/2000000000: episode: 6781, duration: 4.491s, episode steps:  34, steps per second:   8, episode reward: 159.800, mean reward:  4.700 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 85.235948, mean_q: 43.443021, mean_eps: 0.100000\n","     248051/2000000000: episode: 6782, duration: 5.291s, episode steps:  39, steps per second:   7, episode reward: 185.400, mean reward:  4.754 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 89.535862, mean_q: 41.778687, mean_eps: 0.100000\n","     248082/2000000000: episode: 6783, duration: 4.076s, episode steps:  31, steps per second:   8, episode reward: 90.600, mean reward:  2.923 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.138656, mean_q: 42.709155, mean_eps: 0.100000\n","     248121/2000000000: episode: 6784, duration: 5.063s, episode steps:  39, steps per second:   8, episode reward: 145.200, mean reward:  3.723 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 78.820404, mean_q: 41.986038, mean_eps: 0.100000\n","     248149/2000000000: episode: 6785, duration: 3.919s, episode steps:  28, steps per second:   7, episode reward: 117.000, mean reward:  4.179 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 88.889743, mean_q: 42.190974, mean_eps: 0.100000\n","     248184/2000000000: episode: 6786, duration: 4.627s, episode steps:  35, steps per second:   8, episode reward: 153.800, mean reward:  4.394 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.658744, mean_q: 42.750997, mean_eps: 0.100000\n","     248217/2000000000: episode: 6787, duration: 4.273s, episode steps:  33, steps per second:   8, episode reward: 71.700, mean reward:  2.173 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 77.592823, mean_q: 43.453382, mean_eps: 0.100000\n","     248250/2000000000: episode: 6788, duration: 4.535s, episode steps:  33, steps per second:   7, episode reward:  0.100, mean reward:  0.003 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 77.543106, mean_q: 42.340783, mean_eps: 0.100000\n","     248283/2000000000: episode: 6789, duration: 4.636s, episode steps:  33, steps per second:   7, episode reward: -42.000, mean reward: -1.273 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 86.911157, mean_q: 42.084257, mean_eps: 0.100000\n","     248316/2000000000: episode: 6790, duration: 4.556s, episode steps:  33, steps per second:   7, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 76.358527, mean_q: 42.666214, mean_eps: 0.100000\n","     248347/2000000000: episode: 6791, duration: 4.203s, episode steps:  31, steps per second:   7, episode reward: 87.300, mean reward:  2.816 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 82.968558, mean_q: 42.865226, mean_eps: 0.100000\n","     248385/2000000000: episode: 6792, duration: 5.487s, episode steps:  38, steps per second:   7, episode reward: 162.000, mean reward:  4.263 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 75.989848, mean_q: 42.031924, mean_eps: 0.100000\n","     248418/2000000000: episode: 6793, duration: 4.558s, episode steps:  33, steps per second:   7, episode reward: 121.200, mean reward:  3.673 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.413859, mean_q: 42.128058, mean_eps: 0.100000\n","     248454/2000000000: episode: 6794, duration: 4.659s, episode steps:  36, steps per second:   8, episode reward: -12.200, mean reward: -0.339 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 91.683012, mean_q: 42.515448, mean_eps: 0.100000\n","     248490/2000000000: episode: 6795, duration: 4.667s, episode steps:  36, steps per second:   8, episode reward: 132.700, mean reward:  3.686 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 77.760332, mean_q: 42.773231, mean_eps: 0.100000\n","     248519/2000000000: episode: 6796, duration: 3.713s, episode steps:  29, steps per second:   8, episode reward: 186.400, mean reward:  6.428 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 81.091676, mean_q: 43.004856, mean_eps: 0.100000\n","     248548/2000000000: episode: 6797, duration: 3.849s, episode steps:  29, steps per second:   8, episode reward: 153.500, mean reward:  5.293 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 90.533724, mean_q: 41.516650, mean_eps: 0.100000\n","     248578/2000000000: episode: 6798, duration: 3.803s, episode steps:  30, steps per second:   8, episode reward: 80.200, mean reward:  2.673 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 84.920090, mean_q: 42.881524, mean_eps: 0.100000\n","     248615/2000000000: episode: 6799, duration: 4.560s, episode steps:  37, steps per second:   8, episode reward: 109.400, mean reward:  2.957 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 76.650107, mean_q: 42.502914, mean_eps: 0.100000\n","     248644/2000000000: episode: 6800, duration: 3.732s, episode steps:  29, steps per second:   8, episode reward: 47.200, mean reward:  1.628 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 87.989486, mean_q: 42.329516, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     248683/2000000000: episode: 6801, duration: 5.008s, episode steps:  39, steps per second:   8, episode reward: 34.600, mean reward:  0.887 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 76.973107, mean_q: 42.157992, mean_eps: 0.100000\n","     248723/2000000000: episode: 6802, duration: 5.191s, episode steps:  40, steps per second:   8, episode reward: 99.700, mean reward:  2.493 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.804715, mean_q: 41.812525, mean_eps: 0.100000\n","     248763/2000000000: episode: 6803, duration: 5.252s, episode steps:  40, steps per second:   8, episode reward: -51.700, mean reward: -1.292 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 81.972058, mean_q: 42.303492, mean_eps: 0.100000\n","     248801/2000000000: episode: 6804, duration: 5.223s, episode steps:  38, steps per second:   7, episode reward: 55.800, mean reward:  1.468 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 81.036585, mean_q: 43.304814, mean_eps: 0.100000\n","     248841/2000000000: episode: 6805, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward: -62.200, mean reward: -1.555 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 79.668785, mean_q: 41.711798, mean_eps: 0.100000\n","     248875/2000000000: episode: 6806, duration: 4.448s, episode steps:  34, steps per second:   8, episode reward: -40.600, mean reward: -1.194 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 73.253322, mean_q: 43.245844, mean_eps: 0.100000\n","     248910/2000000000: episode: 6807, duration: 4.500s, episode steps:  35, steps per second:   8, episode reward: 114.000, mean reward:  3.257 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 83.396050, mean_q: 41.590816, mean_eps: 0.100000\n","     248943/2000000000: episode: 6808, duration: 4.431s, episode steps:  33, steps per second:   7, episode reward: 130.900, mean reward:  3.967 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.530114, mean_q: 42.889885, mean_eps: 0.100000\n","     248976/2000000000: episode: 6809, duration: 4.117s, episode steps:  33, steps per second:   8, episode reward: 44.800, mean reward:  1.358 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 82.041827, mean_q: 42.450176, mean_eps: 0.100000\n","     249006/2000000000: episode: 6810, duration: 3.762s, episode steps:  30, steps per second:   8, episode reward: 28.200, mean reward:  0.940 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 72.465532, mean_q: 43.080263, mean_eps: 0.100000\n","     249041/2000000000: episode: 6811, duration: 4.433s, episode steps:  35, steps per second:   8, episode reward: -44.800, mean reward: -1.280 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 80.422216, mean_q: 43.355359, mean_eps: 0.100000\n","     249073/2000000000: episode: 6812, duration: 3.963s, episode steps:  32, steps per second:   8, episode reward: 47.000, mean reward:  1.469 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.536664, mean_q: 42.293478, mean_eps: 0.100000\n","     249105/2000000000: episode: 6813, duration: 4.043s, episode steps:  32, steps per second:   8, episode reward: 44.000, mean reward:  1.375 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 78.280344, mean_q: 42.699411, mean_eps: 0.100000\n","     249145/2000000000: episode: 6814, duration: 5.208s, episode steps:  40, steps per second:   8, episode reward: 51.200, mean reward:  1.280 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 79.263433, mean_q: 42.427625, mean_eps: 0.100000\n","     249172/2000000000: episode: 6815, duration: 3.476s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 83.504971, mean_q: 42.950872, mean_eps: 0.100000\n","     249200/2000000000: episode: 6816, duration: 3.621s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 73.631949, mean_q: 41.991644, mean_eps: 0.100000\n","     249230/2000000000: episode: 6817, duration: 3.850s, episode steps:  30, steps per second:   8, episode reward: 33.900, mean reward:  1.130 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.848327, mean_q: 43.591636, mean_eps: 0.100000\n","     249270/2000000000: episode: 6818, duration: 5.388s, episode steps:  40, steps per second:   7, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 75.276938, mean_q: 42.250086, mean_eps: 0.100000\n","     249300/2000000000: episode: 6819, duration: 3.965s, episode steps:  30, steps per second:   8, episode reward: 23.200, mean reward:  0.773 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 82.124239, mean_q: 42.883127, mean_eps: 0.100000\n","     249332/2000000000: episode: 6820, duration: 4.335s, episode steps:  32, steps per second:   7, episode reward: 112.300, mean reward:  3.509 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 78.143156, mean_q: 42.297984, mean_eps: 0.100000\n","     249367/2000000000: episode: 6821, duration: 4.696s, episode steps:  35, steps per second:   7, episode reward: 113.500, mean reward:  3.243 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 78.811297, mean_q: 42.195567, mean_eps: 0.100000\n","     249394/2000000000: episode: 6822, duration: 3.825s, episode steps:  27, steps per second:   7, episode reward: 59.200, mean reward:  2.193 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 86.519123, mean_q: 42.806377, mean_eps: 0.100000\n","     249425/2000000000: episode: 6823, duration: 4.418s, episode steps:  31, steps per second:   7, episode reward: 49.300, mean reward:  1.590 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.409108, mean_q: 42.289593, mean_eps: 0.100000\n","     249451/2000000000: episode: 6824, duration: 3.690s, episode steps:  26, steps per second:   7, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 84.640637, mean_q: 43.613525, mean_eps: 0.100000\n","     249485/2000000000: episode: 6825, duration: 4.568s, episode steps:  34, steps per second:   7, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 79.298669, mean_q: 42.740871, mean_eps: 0.100000\n","     249515/2000000000: episode: 6826, duration: 4.216s, episode steps:  30, steps per second:   7, episode reward: 64.700, mean reward:  2.157 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 84.375727, mean_q: 43.203548, mean_eps: 0.100000\n","     249543/2000000000: episode: 6827, duration: 4.159s, episode steps:  28, steps per second:   7, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 79.358435, mean_q: 40.818569, mean_eps: 0.100000\n","     249577/2000000000: episode: 6828, duration: 4.786s, episode steps:  34, steps per second:   7, episode reward: 23.400, mean reward:  0.688 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 88.901959, mean_q: 43.789369, mean_eps: 0.100000\n","     249608/2000000000: episode: 6829, duration: 4.356s, episode steps:  31, steps per second:   7, episode reward: 135.600, mean reward:  4.374 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 81.792484, mean_q: 43.207279, mean_eps: 0.100000\n","     249648/2000000000: episode: 6830, duration: 5.198s, episode steps:  40, steps per second:   8, episode reward: 262.700, mean reward:  6.567 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.993581, mean_q: 42.405799, mean_eps: 0.100000\n","     249688/2000000000: episode: 6831, duration: 5.167s, episode steps:  40, steps per second:   8, episode reward: 105.300, mean reward:  2.632 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.330514, mean_q: 43.005252, mean_eps: 0.100000\n","     249727/2000000000: episode: 6832, duration: 4.981s, episode steps:  39, steps per second:   8, episode reward: 19.300, mean reward:  0.495 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 86.326778, mean_q: 42.081117, mean_eps: 0.100000\n","     249764/2000000000: episode: 6833, duration: 4.886s, episode steps:  37, steps per second:   8, episode reward: 150.400, mean reward:  4.065 [-20.000, 18.000], mean action: 1.027 [0.000, 2.000],  loss: 70.645436, mean_q: 43.203868, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     249795/2000000000: episode: 6834, duration: 4.553s, episode steps:  31, steps per second:   7, episode reward: 138.600, mean reward:  4.471 [-20.000, 18.200], mean action: 1.000 [0.000, 2.000],  loss: 82.721577, mean_q: 43.616750, mean_eps: 0.100000\n","     249825/2000000000: episode: 6835, duration: 4.347s, episode steps:  30, steps per second:   7, episode reward: 51.000, mean reward:  1.700 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 80.550603, mean_q: 42.342859, mean_eps: 0.100000\n","     249864/2000000000: episode: 6836, duration: 5.210s, episode steps:  39, steps per second:   7, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 80.696760, mean_q: 43.175962, mean_eps: 0.100000\n","     249904/2000000000: episode: 6837, duration: 5.424s, episode steps:  40, steps per second:   7, episode reward: 122.200, mean reward:  3.055 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 91.057059, mean_q: 42.145607, mean_eps: 0.100000\n","     249937/2000000000: episode: 6838, duration: 4.491s, episode steps:  33, steps per second:   7, episode reward: -33.200, mean reward: -1.006 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 81.775814, mean_q: 42.201709, mean_eps: 0.100000\n","     249965/2000000000: episode: 6839, duration: 3.690s, episode steps:  28, steps per second:   8, episode reward: 64.500, mean reward:  2.304 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 90.168188, mean_q: 42.831099, mean_eps: 0.100000\n","     249996/2000000000: episode: 6840, duration: 4.200s, episode steps:  31, steps per second:   7, episode reward:  9.800, mean reward:  0.316 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 80.088455, mean_q: 43.242232, mean_eps: 0.100000\n","     250033/2000000000: episode: 6841, duration: 5.136s, episode steps:  37, steps per second:   7, episode reward: 80.700, mean reward:  2.181 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 85.238128, mean_q: 43.110872, mean_eps: 0.100000\n","     250068/2000000000: episode: 6842, duration: 4.538s, episode steps:  35, steps per second:   8, episode reward: 107.400, mean reward:  3.069 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 97.146300, mean_q: 43.962921, mean_eps: 0.100000\n","     250108/2000000000: episode: 6843, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward: 63.200, mean reward:  1.580 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 90.552296, mean_q: 43.079000, mean_eps: 0.100000\n","     250136/2000000000: episode: 6844, duration: 3.782s, episode steps:  28, steps per second:   7, episode reward: -10.500, mean reward: -0.375 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 85.422913, mean_q: 43.132339, mean_eps: 0.100000\n","     250172/2000000000: episode: 6845, duration: 5.230s, episode steps:  36, steps per second:   7, episode reward: 67.900, mean reward:  1.886 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 81.607161, mean_q: 43.858729, mean_eps: 0.100000\n","     250208/2000000000: episode: 6846, duration: 4.683s, episode steps:  36, steps per second:   8, episode reward: 111.000, mean reward:  3.083 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 85.343412, mean_q: 43.348740, mean_eps: 0.100000\n","     250240/2000000000: episode: 6847, duration: 4.256s, episode steps:  32, steps per second:   8, episode reward: 179.000, mean reward:  5.594 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 80.332422, mean_q: 43.729917, mean_eps: 0.100000\n","     250272/2000000000: episode: 6848, duration: 4.231s, episode steps:  32, steps per second:   8, episode reward: 94.400, mean reward:  2.950 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 87.036198, mean_q: 43.364406, mean_eps: 0.100000\n","     250298/2000000000: episode: 6849, duration: 3.392s, episode steps:  26, steps per second:   8, episode reward: 137.400, mean reward:  5.285 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 79.740159, mean_q: 44.547740, mean_eps: 0.100000\n","     250322/2000000000: episode: 6850, duration: 3.113s, episode steps:  24, steps per second:   8, episode reward: 112.200, mean reward:  4.675 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 83.864689, mean_q: 44.591492, mean_eps: 0.100000\n","     250350/2000000000: episode: 6851, duration: 3.722s, episode steps:  28, steps per second:   8, episode reward: 87.900, mean reward:  3.139 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 93.416764, mean_q: 44.021674, mean_eps: 0.100000\n","     250385/2000000000: episode: 6852, duration: 4.868s, episode steps:  35, steps per second:   7, episode reward: 81.100, mean reward:  2.317 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 80.665385, mean_q: 44.650066, mean_eps: 0.100000\n","     250412/2000000000: episode: 6853, duration: 3.736s, episode steps:  27, steps per second:   7, episode reward: 258.200, mean reward:  9.563 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 82.865837, mean_q: 45.070858, mean_eps: 0.100000\n","     250452/2000000000: episode: 6854, duration: 5.526s, episode steps:  40, steps per second:   7, episode reward: 50.700, mean reward:  1.267 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 90.714156, mean_q: 43.923989, mean_eps: 0.100000\n","     250480/2000000000: episode: 6855, duration: 3.646s, episode steps:  28, steps per second:   8, episode reward: 269.000, mean reward:  9.607 [-14.300, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 84.094354, mean_q: 43.686530, mean_eps: 0.100000\n","     250512/2000000000: episode: 6856, duration: 4.352s, episode steps:  32, steps per second:   7, episode reward: 37.000, mean reward:  1.156 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 80.529850, mean_q: 45.529243, mean_eps: 0.100000\n","     250545/2000000000: episode: 6857, duration: 4.498s, episode steps:  33, steps per second:   7, episode reward: 93.700, mean reward:  2.839 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.157202, mean_q: 42.967600, mean_eps: 0.100000\n","     250579/2000000000: episode: 6858, duration: 4.645s, episode steps:  34, steps per second:   7, episode reward: 118.500, mean reward:  3.485 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 88.761009, mean_q: 43.855184, mean_eps: 0.100000\n","     250605/2000000000: episode: 6859, duration: 3.508s, episode steps:  26, steps per second:   7, episode reward: 106.800, mean reward:  4.108 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 88.011220, mean_q: 43.846914, mean_eps: 0.100000\n","     250638/2000000000: episode: 6860, duration: 4.320s, episode steps:  33, steps per second:   8, episode reward: -16.400, mean reward: -0.497 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.632396, mean_q: 44.635536, mean_eps: 0.100000\n","     250669/2000000000: episode: 6861, duration: 4.081s, episode steps:  31, steps per second:   8, episode reward: 168.400, mean reward:  5.432 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.052612, mean_q: 44.116147, mean_eps: 0.100000\n","     250701/2000000000: episode: 6862, duration: 4.307s, episode steps:  32, steps per second:   7, episode reward: 101.800, mean reward:  3.181 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 87.622078, mean_q: 44.758803, mean_eps: 0.100000\n","     250741/2000000000: episode: 6863, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward: 112.600, mean reward:  2.815 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 89.753104, mean_q: 43.724500, mean_eps: 0.100000\n","     250765/2000000000: episode: 6864, duration: 3.318s, episode steps:  24, steps per second:   7, episode reward:  8.100, mean reward:  0.337 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 86.641226, mean_q: 44.405687, mean_eps: 0.100000\n","     250805/2000000000: episode: 6865, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: 30.500, mean reward:  0.762 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 91.561244, mean_q: 43.129166, mean_eps: 0.100000\n","     250832/2000000000: episode: 6866, duration: 3.763s, episode steps:  27, steps per second:   7, episode reward: -20.000, mean reward: -0.741 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.203775, mean_q: 42.709342, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     250855/2000000000: episode: 6867, duration: 3.094s, episode steps:  23, steps per second:   7, episode reward: 94.000, mean reward:  4.087 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 89.743104, mean_q: 43.115307, mean_eps: 0.100000\n","     250881/2000000000: episode: 6868, duration: 3.182s, episode steps:  26, steps per second:   8, episode reward: -50.800, mean reward: -1.954 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 75.124509, mean_q: 43.167004, mean_eps: 0.100000\n","     250913/2000000000: episode: 6869, duration: 4.199s, episode steps:  32, steps per second:   8, episode reward: 135.000, mean reward:  4.219 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 80.857219, mean_q: 44.002637, mean_eps: 0.100000\n","     250944/2000000000: episode: 6870, duration: 4.271s, episode steps:  31, steps per second:   7, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 92.165775, mean_q: 43.486396, mean_eps: 0.100000\n","     250984/2000000000: episode: 6871, duration: 5.268s, episode steps:  40, steps per second:   8, episode reward: 78.900, mean reward:  1.972 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.707860, mean_q: 42.619618, mean_eps: 0.100000\n","     251016/2000000000: episode: 6872, duration: 4.134s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 85.255912, mean_q: 43.498686, mean_eps: 0.100000\n","     251053/2000000000: episode: 6873, duration: 4.809s, episode steps:  37, steps per second:   8, episode reward: -12.400, mean reward: -0.335 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 87.104734, mean_q: 44.695801, mean_eps: 0.100000\n","     251086/2000000000: episode: 6874, duration: 4.535s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 96.629521, mean_q: 44.224921, mean_eps: 0.100000\n","     251116/2000000000: episode: 6875, duration: 4.152s, episode steps:  30, steps per second:   7, episode reward: -172.000, mean reward: -5.733 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 73.807489, mean_q: 45.002865, mean_eps: 0.100000\n","     251142/2000000000: episode: 6876, duration: 3.643s, episode steps:  26, steps per second:   7, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 84.440871, mean_q: 43.932923, mean_eps: 0.100000\n","     251171/2000000000: episode: 6877, duration: 4.053s, episode steps:  29, steps per second:   7, episode reward: 62.900, mean reward:  2.169 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 84.314688, mean_q: 42.731316, mean_eps: 0.100000\n","     251203/2000000000: episode: 6878, duration: 4.247s, episode steps:  32, steps per second:   8, episode reward: 257.500, mean reward:  8.047 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 82.999937, mean_q: 43.423565, mean_eps: 0.100000\n","     251230/2000000000: episode: 6879, duration: 3.571s, episode steps:  27, steps per second:   8, episode reward: 170.000, mean reward:  6.296 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 85.067355, mean_q: 44.982848, mean_eps: 0.100000\n","     251268/2000000000: episode: 6880, duration: 5.432s, episode steps:  38, steps per second:   7, episode reward: 155.900, mean reward:  4.103 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 81.665635, mean_q: 44.215584, mean_eps: 0.100000\n","     251296/2000000000: episode: 6881, duration: 3.721s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 95.556558, mean_q: 44.395180, mean_eps: 0.100000\n","     251333/2000000000: episode: 6882, duration: 5.113s, episode steps:  37, steps per second:   7, episode reward: 144.100, mean reward:  3.895 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 82.306077, mean_q: 43.788127, mean_eps: 0.100000\n","     251368/2000000000: episode: 6883, duration: 4.531s, episode steps:  35, steps per second:   8, episode reward: 246.000, mean reward:  7.029 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 95.310607, mean_q: 45.160778, mean_eps: 0.100000\n","     251396/2000000000: episode: 6884, duration: 3.635s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 78.083457, mean_q: 43.196039, mean_eps: 0.100000\n","     251429/2000000000: episode: 6885, duration: 4.181s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.453017, mean_q: 45.135392, mean_eps: 0.100000\n","     251465/2000000000: episode: 6886, duration: 4.603s, episode steps:  36, steps per second:   8, episode reward: 78.800, mean reward:  2.189 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 81.757264, mean_q: 44.679759, mean_eps: 0.100000\n","     251498/2000000000: episode: 6887, duration: 4.299s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.359102, mean_q: 43.592593, mean_eps: 0.100000\n","     251538/2000000000: episode: 6888, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: 64.300, mean reward:  1.608 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.278918, mean_q: 44.571499, mean_eps: 0.100000\n","     251574/2000000000: episode: 6889, duration: 4.661s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 87.156930, mean_q: 43.285535, mean_eps: 0.100000\n","     251614/2000000000: episode: 6890, duration: 4.935s, episode steps:  40, steps per second:   8, episode reward: 56.900, mean reward:  1.422 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.176308, mean_q: 43.843418, mean_eps: 0.100000\n","     251641/2000000000: episode: 6891, duration: 3.468s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.858258, mean_q: 42.784211, mean_eps: 0.100000\n","     251681/2000000000: episode: 6892, duration: 5.120s, episode steps:  40, steps per second:   8, episode reward: 149.300, mean reward:  3.733 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.278098, mean_q: 43.407838, mean_eps: 0.100000\n","     251710/2000000000: episode: 6893, duration: 3.764s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 82.612507, mean_q: 44.248411, mean_eps: 0.100000\n","     251740/2000000000: episode: 6894, duration: 4.068s, episode steps:  30, steps per second:   7, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 84.049071, mean_q: 44.365615, mean_eps: 0.100000\n","     251765/2000000000: episode: 6895, duration: 3.225s, episode steps:  25, steps per second:   8, episode reward: -75.300, mean reward: -3.012 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 78.348361, mean_q: 43.196725, mean_eps: 0.100000\n","     251790/2000000000: episode: 6896, duration: 3.260s, episode steps:  25, steps per second:   8, episode reward: 25.300, mean reward:  1.012 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 79.194518, mean_q: 44.552758, mean_eps: 0.100000\n","     251820/2000000000: episode: 6897, duration: 3.838s, episode steps:  30, steps per second:   8, episode reward: 142.800, mean reward:  4.760 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 84.251258, mean_q: 43.868357, mean_eps: 0.100000\n","     251860/2000000000: episode: 6898, duration: 5.636s, episode steps:  40, steps per second:   7, episode reward: 19.100, mean reward:  0.478 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.523687, mean_q: 43.144337, mean_eps: 0.100000\n","     251895/2000000000: episode: 6899, duration: 4.650s, episode steps:  35, steps per second:   8, episode reward: -110.600, mean reward: -3.160 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 72.171123, mean_q: 43.920216, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     251928/2000000000: episode: 6900, duration: 4.585s, episode steps:  33, steps per second:   7, episode reward: 53.500, mean reward:  1.621 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 81.554184, mean_q: 43.855093, mean_eps: 0.100000\n","     251965/2000000000: episode: 6901, duration: 4.985s, episode steps:  37, steps per second:   7, episode reward: -19.600, mean reward: -0.530 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 88.844204, mean_q: 44.435383, mean_eps: 0.100000\n","     252001/2000000000: episode: 6902, duration: 4.930s, episode steps:  36, steps per second:   7, episode reward: 32.600, mean reward:  0.906 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 82.372066, mean_q: 43.099535, mean_eps: 0.100000\n","     252026/2000000000: episode: 6903, duration: 3.590s, episode steps:  25, steps per second:   7, episode reward: 109.500, mean reward:  4.380 [-20.000, 18.000], mean action: 0.960 [0.000, 2.000],  loss: 79.604156, mean_q: 45.022537, mean_eps: 0.100000\n","     252052/2000000000: episode: 6904, duration: 3.497s, episode steps:  26, steps per second:   7, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 87.608810, mean_q: 43.995261, mean_eps: 0.100000\n","     252081/2000000000: episode: 6905, duration: 4.009s, episode steps:  29, steps per second:   7, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 85.620207, mean_q: 43.864778, mean_eps: 0.100000\n","     252109/2000000000: episode: 6906, duration: 3.859s, episode steps:  28, steps per second:   7, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 80.920972, mean_q: 44.159132, mean_eps: 0.100000\n","     252137/2000000000: episode: 6907, duration: 3.765s, episode steps:  28, steps per second:   7, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 85.724145, mean_q: 43.999723, mean_eps: 0.100000\n","     252170/2000000000: episode: 6908, duration: 4.184s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 88.640260, mean_q: 43.469866, mean_eps: 0.100000\n","     252200/2000000000: episode: 6909, duration: 3.917s, episode steps:  30, steps per second:   8, episode reward: 124.200, mean reward:  4.140 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.029667, mean_q: 44.294153, mean_eps: 0.100000\n","     252240/2000000000: episode: 6910, duration: 5.206s, episode steps:  40, steps per second:   8, episode reward: -18.100, mean reward: -0.453 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 78.281262, mean_q: 43.554552, mean_eps: 0.100000\n","     252267/2000000000: episode: 6911, duration: 3.507s, episode steps:  27, steps per second:   8, episode reward: 71.100, mean reward:  2.633 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 86.661837, mean_q: 44.158805, mean_eps: 0.100000\n","     252305/2000000000: episode: 6912, duration: 5.328s, episode steps:  38, steps per second:   7, episode reward: 25.700, mean reward:  0.676 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 86.859641, mean_q: 43.680649, mean_eps: 0.100000\n","     252337/2000000000: episode: 6913, duration: 4.550s, episode steps:  32, steps per second:   7, episode reward: -22.300, mean reward: -0.697 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 86.339843, mean_q: 43.896973, mean_eps: 0.100000\n","     252369/2000000000: episode: 6914, duration: 4.476s, episode steps:  32, steps per second:   7, episode reward: 124.000, mean reward:  3.875 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 81.446123, mean_q: 44.667960, mean_eps: 0.100000\n","     252409/2000000000: episode: 6915, duration: 5.550s, episode steps:  40, steps per second:   7, episode reward: 120.400, mean reward:  3.010 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 88.042837, mean_q: 43.366858, mean_eps: 0.100000\n","     252449/2000000000: episode: 6916, duration: 5.356s, episode steps:  40, steps per second:   7, episode reward: -34.500, mean reward: -0.862 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 82.036884, mean_q: 43.378920, mean_eps: 0.100000\n","     252476/2000000000: episode: 6917, duration: 3.559s, episode steps:  27, steps per second:   8, episode reward: 36.900, mean reward:  1.367 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 74.209087, mean_q: 43.941395, mean_eps: 0.100000\n","     252510/2000000000: episode: 6918, duration: 4.677s, episode steps:  34, steps per second:   7, episode reward: 58.800, mean reward:  1.729 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 86.391034, mean_q: 43.737270, mean_eps: 0.100000\n","     252545/2000000000: episode: 6919, duration: 4.619s, episode steps:  35, steps per second:   8, episode reward: 137.100, mean reward:  3.917 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 74.986761, mean_q: 44.827154, mean_eps: 0.100000\n","     252576/2000000000: episode: 6920, duration: 3.984s, episode steps:  31, steps per second:   8, episode reward: 101.700, mean reward:  3.281 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 77.032986, mean_q: 44.517017, mean_eps: 0.100000\n","     252608/2000000000: episode: 6921, duration: 4.263s, episode steps:  32, steps per second:   8, episode reward: 74.900, mean reward:  2.341 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.865880, mean_q: 43.844257, mean_eps: 0.100000\n","     252647/2000000000: episode: 6922, duration: 5.286s, episode steps:  39, steps per second:   7, episode reward: 52.300, mean reward:  1.341 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 88.406986, mean_q: 43.257745, mean_eps: 0.100000\n","     252675/2000000000: episode: 6923, duration: 3.775s, episode steps:  28, steps per second:   7, episode reward: 180.800, mean reward:  6.457 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.630169, mean_q: 43.449500, mean_eps: 0.100000\n","     252710/2000000000: episode: 6924, duration: 4.589s, episode steps:  35, steps per second:   8, episode reward: 69.000, mean reward:  1.971 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 83.806435, mean_q: 43.750286, mean_eps: 0.100000\n","     252736/2000000000: episode: 6925, duration: 3.302s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 81.056572, mean_q: 42.877199, mean_eps: 0.100000\n","     252772/2000000000: episode: 6926, duration: 4.546s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 86.069751, mean_q: 44.213722, mean_eps: 0.100000\n","     252812/2000000000: episode: 6927, duration: 5.011s, episode steps:  40, steps per second:   8, episode reward: 139.800, mean reward:  3.495 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.785358, mean_q: 44.262775, mean_eps: 0.100000\n","     252842/2000000000: episode: 6928, duration: 3.856s, episode steps:  30, steps per second:   8, episode reward: 122.600, mean reward:  4.087 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 88.147459, mean_q: 44.025635, mean_eps: 0.100000\n","     252882/2000000000: episode: 6929, duration: 5.125s, episode steps:  40, steps per second:   8, episode reward: 12.900, mean reward:  0.323 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.741279, mean_q: 43.990413, mean_eps: 0.100000\n","     252911/2000000000: episode: 6930, duration: 3.741s, episode steps:  29, steps per second:   8, episode reward: 138.200, mean reward:  4.766 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.393157, mean_q: 44.033841, mean_eps: 0.100000\n","     252939/2000000000: episode: 6931, duration: 3.638s, episode steps:  28, steps per second:   8, episode reward: -35.000, mean reward: -1.250 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 81.416061, mean_q: 45.120113, mean_eps: 0.100000\n","     252977/2000000000: episode: 6932, duration: 4.892s, episode steps:  38, steps per second:   8, episode reward:  5.400, mean reward:  0.142 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 78.142809, mean_q: 43.786969, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     253005/2000000000: episode: 6933, duration: 3.607s, episode steps:  28, steps per second:   8, episode reward: 165.300, mean reward:  5.904 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 87.020189, mean_q: 43.692755, mean_eps: 0.100000\n","     253042/2000000000: episode: 6934, duration: 4.565s, episode steps:  37, steps per second:   8, episode reward: 204.900, mean reward:  5.538 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 85.300503, mean_q: 43.080617, mean_eps: 0.100000\n","     253064/2000000000: episode: 6935, duration: 2.750s, episode steps:  22, steps per second:   8, episode reward: 111.300, mean reward:  5.059 [-20.000, 18.000], mean action: 0.818 [0.000, 2.000],  loss: 93.287605, mean_q: 45.193189, mean_eps: 0.100000\n","     253100/2000000000: episode: 6936, duration: 4.535s, episode steps:  36, steps per second:   8, episode reward: 98.800, mean reward:  2.744 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 86.995912, mean_q: 44.705589, mean_eps: 0.100000\n","     253139/2000000000: episode: 6937, duration: 5.014s, episode steps:  39, steps per second:   8, episode reward: 197.700, mean reward:  5.069 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 83.570646, mean_q: 43.565468, mean_eps: 0.100000\n","     253176/2000000000: episode: 6938, duration: 5.035s, episode steps:  37, steps per second:   7, episode reward: -10.900, mean reward: -0.295 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 85.044211, mean_q: 43.293133, mean_eps: 0.100000\n","     253209/2000000000: episode: 6939, duration: 4.316s, episode steps:  33, steps per second:   8, episode reward: 39.600, mean reward:  1.200 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.297148, mean_q: 43.672079, mean_eps: 0.100000\n","     253238/2000000000: episode: 6940, duration: 3.782s, episode steps:  29, steps per second:   8, episode reward: -12.900, mean reward: -0.445 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 84.646662, mean_q: 43.975151, mean_eps: 0.100000\n","     253278/2000000000: episode: 6941, duration: 5.183s, episode steps:  40, steps per second:   8, episode reward:  3.700, mean reward:  0.093 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 77.744337, mean_q: 44.979996, mean_eps: 0.100000\n","     253306/2000000000: episode: 6942, duration: 3.655s, episode steps:  28, steps per second:   8, episode reward: -59.400, mean reward: -2.121 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 75.906750, mean_q: 44.454121, mean_eps: 0.100000\n","     253334/2000000000: episode: 6943, duration: 3.588s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 86.421119, mean_q: 44.817685, mean_eps: 0.100000\n","     253360/2000000000: episode: 6944, duration: 3.301s, episode steps:  26, steps per second:   8, episode reward: -43.700, mean reward: -1.681 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 73.805306, mean_q: 45.167989, mean_eps: 0.100000\n","     253400/2000000000: episode: 6945, duration: 5.153s, episode steps:  40, steps per second:   8, episode reward: -20.000, mean reward: -0.500 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 86.024908, mean_q: 43.583736, mean_eps: 0.100000\n","     253436/2000000000: episode: 6946, duration: 4.725s, episode steps:  36, steps per second:   8, episode reward: -134.000, mean reward: -3.722 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 80.677235, mean_q: 43.992102, mean_eps: 0.100000\n","     253460/2000000000: episode: 6947, duration: 3.146s, episode steps:  24, steps per second:   8, episode reward: 68.200, mean reward:  2.842 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 98.178010, mean_q: 44.906607, mean_eps: 0.100000\n","     253497/2000000000: episode: 6948, duration: 4.874s, episode steps:  37, steps per second:   8, episode reward: 51.200, mean reward:  1.384 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 80.808623, mean_q: 43.850329, mean_eps: 0.100000\n","     253537/2000000000: episode: 6949, duration: 5.244s, episode steps:  40, steps per second:   8, episode reward:  6.600, mean reward:  0.165 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 72.281816, mean_q: 43.642504, mean_eps: 0.100000\n","     253572/2000000000: episode: 6950, duration: 4.557s, episode steps:  35, steps per second:   8, episode reward: 58.900, mean reward:  1.683 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 86.218766, mean_q: 43.016921, mean_eps: 0.100000\n","     253606/2000000000: episode: 6951, duration: 4.380s, episode steps:  34, steps per second:   8, episode reward: 47.900, mean reward:  1.409 [-20.000, 18.000], mean action: 0.941 [0.000, 2.000],  loss: 83.455034, mean_q: 43.503438, mean_eps: 0.100000\n","     253646/2000000000: episode: 6952, duration: 5.170s, episode steps:  40, steps per second:   8, episode reward: 77.900, mean reward:  1.947 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 75.901771, mean_q: 44.036522, mean_eps: 0.100000\n","     253672/2000000000: episode: 6953, duration: 3.430s, episode steps:  26, steps per second:   8, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.112604, mean_q: 44.146034, mean_eps: 0.100000\n","     253705/2000000000: episode: 6954, duration: 4.436s, episode steps:  33, steps per second:   7, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.892405, mean_q: 44.057014, mean_eps: 0.100000\n","     253732/2000000000: episode: 6955, duration: 3.705s, episode steps:  27, steps per second:   7, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 88.643340, mean_q: 43.462658, mean_eps: 0.100000\n","     253768/2000000000: episode: 6956, duration: 5.111s, episode steps:  36, steps per second:   7, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 79.106369, mean_q: 43.424861, mean_eps: 0.100000\n","     253808/2000000000: episode: 6957, duration: 5.754s, episode steps:  40, steps per second:   7, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 88.432443, mean_q: 42.982959, mean_eps: 0.100000\n","     253844/2000000000: episode: 6958, duration: 5.110s, episode steps:  36, steps per second:   7, episode reward: 75.300, mean reward:  2.092 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 83.208243, mean_q: 44.591354, mean_eps: 0.100000\n","     253884/2000000000: episode: 6959, duration: 5.220s, episode steps:  40, steps per second:   8, episode reward: 94.100, mean reward:  2.353 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 85.643955, mean_q: 43.990626, mean_eps: 0.100000\n","     253922/2000000000: episode: 6960, duration: 5.216s, episode steps:  38, steps per second:   7, episode reward: 59.700, mean reward:  1.571 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 78.657551, mean_q: 43.892649, mean_eps: 0.100000\n","     253962/2000000000: episode: 6961, duration: 5.526s, episode steps:  40, steps per second:   7, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.985202, mean_q: 44.046447, mean_eps: 0.100000\n","     254002/2000000000: episode: 6962, duration: 5.385s, episode steps:  40, steps per second:   7, episode reward: 162.800, mean reward:  4.070 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.754186, mean_q: 44.162689, mean_eps: 0.100000\n","     254039/2000000000: episode: 6963, duration: 4.796s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 82.862740, mean_q: 44.488622, mean_eps: 0.100000\n","     254068/2000000000: episode: 6964, duration: 3.753s, episode steps:  29, steps per second:   8, episode reward: 90.800, mean reward:  3.131 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 79.625372, mean_q: 43.992644, mean_eps: 0.100000\n","     254106/2000000000: episode: 6965, duration: 4.998s, episode steps:  38, steps per second:   8, episode reward: 69.400, mean reward:  1.826 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 87.973718, mean_q: 43.249059, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     254133/2000000000: episode: 6966, duration: 3.514s, episode steps:  27, steps per second:   8, episode reward: 178.900, mean reward:  6.626 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 75.328116, mean_q: 43.458615, mean_eps: 0.100000\n","     254164/2000000000: episode: 6967, duration: 3.959s, episode steps:  31, steps per second:   8, episode reward: 37.900, mean reward:  1.223 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.932580, mean_q: 43.391716, mean_eps: 0.100000\n","     254204/2000000000: episode: 6968, duration: 5.035s, episode steps:  40, steps per second:   8, episode reward: 68.100, mean reward:  1.702 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 84.588797, mean_q: 43.290897, mean_eps: 0.100000\n","     254234/2000000000: episode: 6969, duration: 3.874s, episode steps:  30, steps per second:   8, episode reward: -99.100, mean reward: -3.303 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 77.758322, mean_q: 44.766975, mean_eps: 0.100000\n","     254266/2000000000: episode: 6970, duration: 4.127s, episode steps:  32, steps per second:   8, episode reward: 50.700, mean reward:  1.584 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 78.680510, mean_q: 44.157372, mean_eps: 0.100000\n","     254301/2000000000: episode: 6971, duration: 4.562s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 85.475441, mean_q: 42.845215, mean_eps: 0.100000\n","     254334/2000000000: episode: 6972, duration: 4.208s, episode steps:  33, steps per second:   8, episode reward: 32.700, mean reward:  0.991 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 81.211724, mean_q: 43.685724, mean_eps: 0.100000\n","     254370/2000000000: episode: 6973, duration: 4.978s, episode steps:  36, steps per second:   7, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 94.781453, mean_q: 43.969992, mean_eps: 0.100000\n","     254401/2000000000: episode: 6974, duration: 4.156s, episode steps:  31, steps per second:   7, episode reward: 39.000, mean reward:  1.258 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 79.776717, mean_q: 44.429566, mean_eps: 0.100000\n","     254431/2000000000: episode: 6975, duration: 4.098s, episode steps:  30, steps per second:   7, episode reward: 82.700, mean reward:  2.757 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 96.200540, mean_q: 43.915989, mean_eps: 0.100000\n","     254454/2000000000: episode: 6976, duration: 3.229s, episode steps:  23, steps per second:   7, episode reward: 18.000, mean reward:  0.783 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 87.362575, mean_q: 44.122880, mean_eps: 0.100000\n","     254490/2000000000: episode: 6977, duration: 4.667s, episode steps:  36, steps per second:   8, episode reward: 116.900, mean reward:  3.247 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 84.754410, mean_q: 44.479071, mean_eps: 0.100000\n","     254519/2000000000: episode: 6978, duration: 3.678s, episode steps:  29, steps per second:   8, episode reward: 88.100, mean reward:  3.038 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 87.366067, mean_q: 44.965897, mean_eps: 0.100000\n","     254551/2000000000: episode: 6979, duration: 4.158s, episode steps:  32, steps per second:   8, episode reward: 103.100, mean reward:  3.222 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 90.008821, mean_q: 43.550409, mean_eps: 0.100000\n","     254591/2000000000: episode: 6980, duration: 5.283s, episode steps:  40, steps per second:   8, episode reward: 198.300, mean reward:  4.957 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.543328, mean_q: 44.624724, mean_eps: 0.100000\n","     254628/2000000000: episode: 6981, duration: 5.078s, episode steps:  37, steps per second:   7, episode reward: 19.500, mean reward:  0.527 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 86.626305, mean_q: 44.149941, mean_eps: 0.100000\n","     254651/2000000000: episode: 6982, duration: 2.964s, episode steps:  23, steps per second:   8, episode reward: 20.700, mean reward:  0.900 [-20.000, 18.000], mean action: 0.870 [0.000, 2.000],  loss: 85.317897, mean_q: 43.259566, mean_eps: 0.100000\n","     254677/2000000000: episode: 6983, duration: 3.380s, episode steps:  26, steps per second:   8, episode reward: 87.500, mean reward:  3.365 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 86.236802, mean_q: 43.837325, mean_eps: 0.100000\n","     254707/2000000000: episode: 6984, duration: 3.943s, episode steps:  30, steps per second:   8, episode reward: 174.900, mean reward:  5.830 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.532239, mean_q: 44.599298, mean_eps: 0.100000\n","     254738/2000000000: episode: 6985, duration: 3.915s, episode steps:  31, steps per second:   8, episode reward: 92.400, mean reward:  2.981 [-20.000, 18.400], mean action: 0.935 [0.000, 2.000],  loss: 90.497855, mean_q: 44.334281, mean_eps: 0.100000\n","     254773/2000000000: episode: 6986, duration: 4.399s, episode steps:  35, steps per second:   8, episode reward: -18.000, mean reward: -0.514 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 86.662159, mean_q: 44.070763, mean_eps: 0.100000\n","     254811/2000000000: episode: 6987, duration: 5.232s, episode steps:  38, steps per second:   7, episode reward: 130.900, mean reward:  3.445 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 81.160171, mean_q: 43.443858, mean_eps: 0.100000\n","     254851/2000000000: episode: 6988, duration: 5.466s, episode steps:  40, steps per second:   7, episode reward: 71.600, mean reward:  1.790 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.297154, mean_q: 43.789741, mean_eps: 0.100000\n","     254888/2000000000: episode: 6989, duration: 5.083s, episode steps:  37, steps per second:   7, episode reward: 162.400, mean reward:  4.389 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 89.256393, mean_q: 43.595723, mean_eps: 0.100000\n","     254921/2000000000: episode: 6990, duration: 4.546s, episode steps:  33, steps per second:   7, episode reward: 200.100, mean reward:  6.064 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 84.112246, mean_q: 43.377448, mean_eps: 0.100000\n","     254950/2000000000: episode: 6991, duration: 4.009s, episode steps:  29, steps per second:   7, episode reward: 185.400, mean reward:  6.393 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 82.538829, mean_q: 43.838747, mean_eps: 0.100000\n","     254990/2000000000: episode: 6992, duration: 5.258s, episode steps:  40, steps per second:   8, episode reward: 104.800, mean reward:  2.620 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.590553, mean_q: 43.757655, mean_eps: 0.100000\n","     255021/2000000000: episode: 6993, duration: 3.838s, episode steps:  31, steps per second:   8, episode reward: 46.500, mean reward:  1.500 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 80.686794, mean_q: 44.653479, mean_eps: 0.100000\n","     255050/2000000000: episode: 6994, duration: 3.575s, episode steps:  29, steps per second:   8, episode reward: 28.500, mean reward:  0.983 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 87.518569, mean_q: 43.805821, mean_eps: 0.100000\n","     255090/2000000000: episode: 6995, duration: 5.103s, episode steps:  40, steps per second:   8, episode reward: 25.900, mean reward:  0.647 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 82.256679, mean_q: 43.858892, mean_eps: 0.100000\n","     255119/2000000000: episode: 6996, duration: 3.691s, episode steps:  29, steps per second:   8, episode reward: 46.800, mean reward:  1.614 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 79.905440, mean_q: 43.765627, mean_eps: 0.100000\n","     255151/2000000000: episode: 6997, duration: 4.051s, episode steps:  32, steps per second:   8, episode reward: 98.300, mean reward:  3.072 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 75.383817, mean_q: 44.207060, mean_eps: 0.100000\n","     255177/2000000000: episode: 6998, duration: 3.365s, episode steps:  26, steps per second:   8, episode reward: 18.700, mean reward:  0.719 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 80.138445, mean_q: 45.010065, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     255202/2000000000: episode: 6999, duration: 3.358s, episode steps:  25, steps per second:   7, episode reward: 104.400, mean reward:  4.176 [-20.000, 18.000], mean action: 0.600 [0.000, 2.000],  loss: 74.471309, mean_q: 45.049205, mean_eps: 0.100000\n","     255232/2000000000: episode: 7000, duration: 3.844s, episode steps:  30, steps per second:   8, episode reward: 141.300, mean reward:  4.710 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 80.889129, mean_q: 43.726780, mean_eps: 0.100000\n","     255272/2000000000: episode: 7001, duration: 5.022s, episode steps:  40, steps per second:   8, episode reward: 29.800, mean reward:  0.745 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.022288, mean_q: 43.552420, mean_eps: 0.100000\n","     255308/2000000000: episode: 7002, duration: 4.526s, episode steps:  36, steps per second:   8, episode reward: 43.900, mean reward:  1.219 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.963357, mean_q: 43.735349, mean_eps: 0.100000\n","     255348/2000000000: episode: 7003, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: -42.100, mean reward: -1.053 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.727624, mean_q: 44.145448, mean_eps: 0.100000\n","     255386/2000000000: episode: 7004, duration: 5.009s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 78.246185, mean_q: 43.825732, mean_eps: 0.100000\n","     255414/2000000000: episode: 7005, duration: 3.692s, episode steps:  28, steps per second:   8, episode reward: 104.200, mean reward:  3.721 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 90.666382, mean_q: 43.678844, mean_eps: 0.100000\n","     255446/2000000000: episode: 7006, duration: 3.972s, episode steps:  32, steps per second:   8, episode reward: 50.700, mean reward:  1.584 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 87.141966, mean_q: 44.202702, mean_eps: 0.100000\n","     255486/2000000000: episode: 7007, duration: 5.277s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.525441, mean_q: 43.309389, mean_eps: 0.100000\n","     255514/2000000000: episode: 7008, duration: 4.206s, episode steps:  28, steps per second:   7, episode reward: -21.800, mean reward: -0.779 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 76.914045, mean_q: 45.193749, mean_eps: 0.100000\n","     255540/2000000000: episode: 7009, duration: 3.453s, episode steps:  26, steps per second:   8, episode reward: 18.000, mean reward:  0.692 [-20.000, 18.000], mean action: 0.615 [0.000, 2.000],  loss: 70.607716, mean_q: 43.973112, mean_eps: 0.100000\n","     255574/2000000000: episode: 7010, duration: 4.577s, episode steps:  34, steps per second:   7, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 79.189173, mean_q: 43.460342, mean_eps: 0.100000\n","     255602/2000000000: episode: 7011, duration: 4.176s, episode steps:  28, steps per second:   7, episode reward: 155.000, mean reward:  5.536 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 83.553185, mean_q: 43.989055, mean_eps: 0.100000\n","     255630/2000000000: episode: 7012, duration: 3.746s, episode steps:  28, steps per second:   7, episode reward: -5.900, mean reward: -0.211 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 89.771202, mean_q: 44.521121, mean_eps: 0.100000\n","     255661/2000000000: episode: 7013, duration: 4.108s, episode steps:  31, steps per second:   8, episode reward: 89.800, mean reward:  2.897 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 79.011769, mean_q: 44.336411, mean_eps: 0.100000\n","     255698/2000000000: episode: 7014, duration: 4.765s, episode steps:  37, steps per second:   8, episode reward: 67.300, mean reward:  1.819 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 87.760303, mean_q: 43.627892, mean_eps: 0.100000\n","     255738/2000000000: episode: 7015, duration: 5.276s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 78.091641, mean_q: 44.100304, mean_eps: 0.100000\n","     255772/2000000000: episode: 7016, duration: 4.615s, episode steps:  34, steps per second:   7, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 80.336325, mean_q: 43.112178, mean_eps: 0.100000\n","     255801/2000000000: episode: 7017, duration: 3.746s, episode steps:  29, steps per second:   8, episode reward: 82.000, mean reward:  2.828 [-20.000, 18.000], mean action: 1.138 [0.000, 2.000],  loss: 80.083246, mean_q: 43.839092, mean_eps: 0.100000\n","     255841/2000000000: episode: 7018, duration: 5.067s, episode steps:  40, steps per second:   8, episode reward: 84.600, mean reward:  2.115 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 91.175451, mean_q: 43.823192, mean_eps: 0.100000\n","     255872/2000000000: episode: 7019, duration: 3.857s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 79.812984, mean_q: 43.283090, mean_eps: 0.100000\n","     255904/2000000000: episode: 7020, duration: 4.002s, episode steps:  32, steps per second:   8, episode reward: -69.800, mean reward: -2.181 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 76.873776, mean_q: 44.859499, mean_eps: 0.100000\n","     255939/2000000000: episode: 7021, duration: 4.436s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 89.271340, mean_q: 43.830654, mean_eps: 0.100000\n","     255976/2000000000: episode: 7022, duration: 5.081s, episode steps:  37, steps per second:   7, episode reward: 117.800, mean reward:  3.184 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 80.093440, mean_q: 44.690555, mean_eps: 0.100000\n","     256009/2000000000: episode: 7023, duration: 4.295s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 82.552474, mean_q: 44.288961, mean_eps: 0.100000\n","     256049/2000000000: episode: 7024, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 95.913241, mean_q: 44.480710, mean_eps: 0.100000\n","     256079/2000000000: episode: 7025, duration: 3.783s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 85.820018, mean_q: 44.221877, mean_eps: 0.100000\n","     256111/2000000000: episode: 7026, duration: 4.189s, episode steps:  32, steps per second:   8, episode reward: 112.000, mean reward:  3.500 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 82.651116, mean_q: 43.904367, mean_eps: 0.100000\n","     256145/2000000000: episode: 7027, duration: 4.532s, episode steps:  34, steps per second:   8, episode reward: -58.000, mean reward: -1.706 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 83.009089, mean_q: 43.983043, mean_eps: 0.100000\n","     256180/2000000000: episode: 7028, duration: 4.645s, episode steps:  35, steps per second:   8, episode reward: 102.200, mean reward:  2.920 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 74.966484, mean_q: 44.712735, mean_eps: 0.100000\n","     256208/2000000000: episode: 7029, duration: 3.636s, episode steps:  28, steps per second:   8, episode reward: 75.700, mean reward:  2.704 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 80.064689, mean_q: 45.278406, mean_eps: 0.100000\n","     256237/2000000000: episode: 7030, duration: 3.849s, episode steps:  29, steps per second:   8, episode reward: 157.300, mean reward:  5.424 [-20.000, 18.000], mean action: 1.138 [0.000, 2.000],  loss: 82.765955, mean_q: 44.175593, mean_eps: 0.100000\n","     256272/2000000000: episode: 7031, duration: 4.550s, episode steps:  35, steps per second:   8, episode reward: -61.400, mean reward: -1.754 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 79.515479, mean_q: 43.821549, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     256310/2000000000: episode: 7032, duration: 4.995s, episode steps:  38, steps per second:   8, episode reward: 141.800, mean reward:  3.732 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 80.680030, mean_q: 44.208942, mean_eps: 0.100000\n","     256345/2000000000: episode: 7033, duration: 4.412s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 82.949874, mean_q: 43.329804, mean_eps: 0.100000\n","     256385/2000000000: episode: 7034, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: 66.100, mean reward:  1.652 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.476284, mean_q: 44.147621, mean_eps: 0.100000\n","     256418/2000000000: episode: 7035, duration: 4.481s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 88.084524, mean_q: 43.490446, mean_eps: 0.100000\n","     256452/2000000000: episode: 7036, duration: 4.390s, episode steps:  34, steps per second:   8, episode reward: 196.400, mean reward:  5.776 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.119466, mean_q: 45.338271, mean_eps: 0.100000\n","     256490/2000000000: episode: 7037, duration: 4.835s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 82.611129, mean_q: 44.460190, mean_eps: 0.100000\n","     256521/2000000000: episode: 7038, duration: 4.160s, episode steps:  31, steps per second:   7, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 91.659642, mean_q: 44.087059, mean_eps: 0.100000\n","     256551/2000000000: episode: 7039, duration: 3.839s, episode steps:  30, steps per second:   8, episode reward: -96.000, mean reward: -3.200 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 92.288505, mean_q: 42.409458, mean_eps: 0.100000\n","     256585/2000000000: episode: 7040, duration: 4.406s, episode steps:  34, steps per second:   8, episode reward: 74.700, mean reward:  2.197 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.086686, mean_q: 45.068265, mean_eps: 0.100000\n","     256617/2000000000: episode: 7041, duration: 4.202s, episode steps:  32, steps per second:   8, episode reward: 33.900, mean reward:  1.059 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 77.561582, mean_q: 44.932309, mean_eps: 0.100000\n","     256647/2000000000: episode: 7042, duration: 4.017s, episode steps:  30, steps per second:   7, episode reward: -97.500, mean reward: -3.250 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 78.581153, mean_q: 44.725830, mean_eps: 0.100000\n","     256687/2000000000: episode: 7043, duration: 5.225s, episode steps:  40, steps per second:   8, episode reward: 62.400, mean reward:  1.560 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 90.203230, mean_q: 44.150390, mean_eps: 0.100000\n","     256727/2000000000: episode: 7044, duration: 5.103s, episode steps:  40, steps per second:   8, episode reward: -18.000, mean reward: -0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.132223, mean_q: 43.987151, mean_eps: 0.100000\n","     256765/2000000000: episode: 7045, duration: 5.009s, episode steps:  38, steps per second:   8, episode reward: 132.400, mean reward:  3.484 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 87.132450, mean_q: 43.762047, mean_eps: 0.100000\n","     256796/2000000000: episode: 7046, duration: 3.921s, episode steps:  31, steps per second:   8, episode reward: 225.800, mean reward:  7.284 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 86.578742, mean_q: 43.452092, mean_eps: 0.100000\n","     256836/2000000000: episode: 7047, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.266712, mean_q: 44.019907, mean_eps: 0.100000\n","     256874/2000000000: episode: 7048, duration: 4.792s, episode steps:  38, steps per second:   8, episode reward: 73.000, mean reward:  1.921 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 78.591747, mean_q: 43.944899, mean_eps: 0.100000\n","     256902/2000000000: episode: 7049, duration: 3.667s, episode steps:  28, steps per second:   8, episode reward: 175.300, mean reward:  6.261 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 88.325581, mean_q: 43.972531, mean_eps: 0.100000\n","     256939/2000000000: episode: 7050, duration: 4.881s, episode steps:  37, steps per second:   8, episode reward: 112.500, mean reward:  3.041 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 82.792631, mean_q: 43.270085, mean_eps: 0.100000\n","     256979/2000000000: episode: 7051, duration: 5.246s, episode steps:  40, steps per second:   8, episode reward: 173.200, mean reward:  4.330 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.775956, mean_q: 44.279179, mean_eps: 0.100000\n","     257015/2000000000: episode: 7052, duration: 4.452s, episode steps:  36, steps per second:   8, episode reward: -59.300, mean reward: -1.647 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 86.247426, mean_q: 43.263256, mean_eps: 0.100000\n","     257049/2000000000: episode: 7053, duration: 4.380s, episode steps:  34, steps per second:   8, episode reward: 76.800, mean reward:  2.259 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 88.652192, mean_q: 43.291897, mean_eps: 0.100000\n","     257084/2000000000: episode: 7054, duration: 4.446s, episode steps:  35, steps per second:   8, episode reward: -5.400, mean reward: -0.154 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 85.014210, mean_q: 43.883174, mean_eps: 0.100000\n","     257120/2000000000: episode: 7055, duration: 4.633s, episode steps:  36, steps per second:   8, episode reward: 133.600, mean reward:  3.711 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 87.242651, mean_q: 44.347864, mean_eps: 0.100000\n","     257157/2000000000: episode: 7056, duration: 4.642s, episode steps:  37, steps per second:   8, episode reward: -59.500, mean reward: -1.608 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 81.915146, mean_q: 43.486611, mean_eps: 0.100000\n","     257194/2000000000: episode: 7057, duration: 4.615s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 85.229220, mean_q: 43.823248, mean_eps: 0.100000\n","     257225/2000000000: episode: 7058, duration: 4.134s, episode steps:  31, steps per second:   7, episode reward: 96.600, mean reward:  3.116 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.648548, mean_q: 43.784645, mean_eps: 0.100000\n","     257254/2000000000: episode: 7059, duration: 3.858s, episode steps:  29, steps per second:   8, episode reward: 157.900, mean reward:  5.445 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.066484, mean_q: 44.686989, mean_eps: 0.100000\n","     257293/2000000000: episode: 7060, duration: 5.028s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 82.397539, mean_q: 44.260916, mean_eps: 0.100000\n","     257332/2000000000: episode: 7061, duration: 5.112s, episode steps:  39, steps per second:   8, episode reward: 114.700, mean reward:  2.941 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 75.374047, mean_q: 44.470672, mean_eps: 0.100000\n","     257357/2000000000: episode: 7062, duration: 3.303s, episode steps:  25, steps per second:   8, episode reward: 15.600, mean reward:  0.624 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 90.771588, mean_q: 44.317218, mean_eps: 0.100000\n","     257389/2000000000: episode: 7063, duration: 4.247s, episode steps:  32, steps per second:   8, episode reward: 167.100, mean reward:  5.222 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 81.942258, mean_q: 43.811132, mean_eps: 0.100000\n","     257420/2000000000: episode: 7064, duration: 4.045s, episode steps:  31, steps per second:   8, episode reward: 50.500, mean reward:  1.629 [-20.000, 18.100], mean action: 1.032 [0.000, 2.000],  loss: 77.907461, mean_q: 43.981307, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     257456/2000000000: episode: 7065, duration: 4.670s, episode steps:  36, steps per second:   8, episode reward: 50.000, mean reward:  1.389 [-20.000, 19.200], mean action: 1.167 [0.000, 2.000],  loss: 88.859997, mean_q: 44.537775, mean_eps: 0.100000\n","     257487/2000000000: episode: 7066, duration: 4.076s, episode steps:  31, steps per second:   8, episode reward: -59.900, mean reward: -1.932 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 79.988729, mean_q: 43.953511, mean_eps: 0.100000\n","     257527/2000000000: episode: 7067, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: 130.800, mean reward:  3.270 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.354566, mean_q: 43.326431, mean_eps: 0.100000\n","     257550/2000000000: episode: 7068, duration: 3.097s, episode steps:  23, steps per second:   7, episode reward: 226.400, mean reward:  9.843 [-20.000, 18.000], mean action: 0.913 [0.000, 2.000],  loss: 75.419054, mean_q: 44.565224, mean_eps: 0.100000\n","     257581/2000000000: episode: 7069, duration: 4.299s, episode steps:  31, steps per second:   7, episode reward: 127.500, mean reward:  4.113 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 78.200375, mean_q: 44.566439, mean_eps: 0.100000\n","     257619/2000000000: episode: 7070, duration: 5.047s, episode steps:  38, steps per second:   8, episode reward: -48.800, mean reward: -1.284 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 78.218697, mean_q: 43.916805, mean_eps: 0.100000\n","     257651/2000000000: episode: 7071, duration: 4.096s, episode steps:  32, steps per second:   8, episode reward: 93.400, mean reward:  2.919 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.677031, mean_q: 43.882689, mean_eps: 0.100000\n","     257680/2000000000: episode: 7072, duration: 4.118s, episode steps:  29, steps per second:   7, episode reward: 147.000, mean reward:  5.069 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 85.816345, mean_q: 43.587032, mean_eps: 0.100000\n","     257713/2000000000: episode: 7073, duration: 4.293s, episode steps:  33, steps per second:   8, episode reward: 170.500, mean reward:  5.167 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 81.334421, mean_q: 44.266968, mean_eps: 0.100000\n","     257744/2000000000: episode: 7074, duration: 4.088s, episode steps:  31, steps per second:   8, episode reward: 138.700, mean reward:  4.474 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 81.585072, mean_q: 44.207891, mean_eps: 0.100000\n","     257772/2000000000: episode: 7075, duration: 3.725s, episode steps:  28, steps per second:   8, episode reward: 35.000, mean reward:  1.250 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 84.681924, mean_q: 44.330456, mean_eps: 0.100000\n","     257801/2000000000: episode: 7076, duration: 3.863s, episode steps:  29, steps per second:   8, episode reward: 46.900, mean reward:  1.617 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 78.473590, mean_q: 43.574884, mean_eps: 0.100000\n","     257841/2000000000: episode: 7077, duration: 5.321s, episode steps:  40, steps per second:   8, episode reward: -10.400, mean reward: -0.260 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.240292, mean_q: 44.397124, mean_eps: 0.100000\n","     257880/2000000000: episode: 7078, duration: 5.097s, episode steps:  39, steps per second:   8, episode reward: 53.600, mean reward:  1.374 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 76.607105, mean_q: 43.764748, mean_eps: 0.100000\n","     257906/2000000000: episode: 7079, duration: 3.497s, episode steps:  26, steps per second:   7, episode reward: 28.700, mean reward:  1.104 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 83.386211, mean_q: 44.457799, mean_eps: 0.100000\n","     257945/2000000000: episode: 7080, duration: 5.185s, episode steps:  39, steps per second:   8, episode reward: 212.600, mean reward:  5.451 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 83.713143, mean_q: 42.874162, mean_eps: 0.100000\n","     257969/2000000000: episode: 7081, duration: 3.378s, episode steps:  24, steps per second:   7, episode reward: 104.200, mean reward:  4.342 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 76.396056, mean_q: 43.858538, mean_eps: 0.100000\n","     258000/2000000000: episode: 7082, duration: 4.219s, episode steps:  31, steps per second:   7, episode reward: 31.100, mean reward:  1.003 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 80.291094, mean_q: 43.609277, mean_eps: 0.100000\n","     258028/2000000000: episode: 7083, duration: 3.684s, episode steps:  28, steps per second:   8, episode reward: 29.600, mean reward:  1.057 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 82.439886, mean_q: 45.076273, mean_eps: 0.100000\n","     258057/2000000000: episode: 7084, duration: 3.654s, episode steps:  29, steps per second:   8, episode reward: 113.400, mean reward:  3.910 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 83.875415, mean_q: 44.384351, mean_eps: 0.100000\n","     258093/2000000000: episode: 7085, duration: 5.031s, episode steps:  36, steps per second:   7, episode reward: 148.500, mean reward:  4.125 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 83.938033, mean_q: 45.405514, mean_eps: 0.100000\n","     258127/2000000000: episode: 7086, duration: 4.653s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 84.150096, mean_q: 43.962509, mean_eps: 0.100000\n","     258167/2000000000: episode: 7087, duration: 5.383s, episode steps:  40, steps per second:   7, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.920439, mean_q: 43.705870, mean_eps: 0.100000\n","     258207/2000000000: episode: 7088, duration: 5.050s, episode steps:  40, steps per second:   8, episode reward: 19.300, mean reward:  0.483 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.773781, mean_q: 43.092176, mean_eps: 0.100000\n","     258240/2000000000: episode: 7089, duration: 4.454s, episode steps:  33, steps per second:   7, episode reward: 108.200, mean reward:  3.279 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.756948, mean_q: 44.120031, mean_eps: 0.100000\n","     258279/2000000000: episode: 7090, duration: 5.077s, episode steps:  39, steps per second:   8, episode reward: 161.000, mean reward:  4.128 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 82.596428, mean_q: 43.937084, mean_eps: 0.100000\n","     258303/2000000000: episode: 7091, duration: 3.182s, episode steps:  24, steps per second:   8, episode reward: 20.400, mean reward:  0.850 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 87.831139, mean_q: 44.162389, mean_eps: 0.100000\n","     258341/2000000000: episode: 7092, duration: 4.696s, episode steps:  38, steps per second:   8, episode reward: -64.300, mean reward: -1.692 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 87.468086, mean_q: 44.775053, mean_eps: 0.100000\n","     258381/2000000000: episode: 7093, duration: 4.853s, episode steps:  40, steps per second:   8, episode reward: -74.000, mean reward: -1.850 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.759043, mean_q: 44.941969, mean_eps: 0.100000\n","     258419/2000000000: episode: 7094, duration: 4.818s, episode steps:  38, steps per second:   8, episode reward: 67.300, mean reward:  1.771 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 80.043726, mean_q: 44.431613, mean_eps: 0.100000\n","     258459/2000000000: episode: 7095, duration: 5.047s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.595539, mean_q: 43.396182, mean_eps: 0.100000\n","     258496/2000000000: episode: 7096, duration: 4.592s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 87.478347, mean_q: 44.092446, mean_eps: 0.100000\n","     258535/2000000000: episode: 7097, duration: 5.220s, episode steps:  39, steps per second:   7, episode reward: 45.700, mean reward:  1.172 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 86.426121, mean_q: 44.138839, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     258559/2000000000: episode: 7098, duration: 3.327s, episode steps:  24, steps per second:   7, episode reward: 217.300, mean reward:  9.054 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 80.840729, mean_q: 44.415686, mean_eps: 0.100000\n","     258599/2000000000: episode: 7099, duration: 5.535s, episode steps:  40, steps per second:   7, episode reward: 136.000, mean reward:  3.400 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 86.085102, mean_q: 44.265024, mean_eps: 0.100000\n","     258639/2000000000: episode: 7100, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward: 159.900, mean reward:  3.997 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.007611, mean_q: 43.726155, mean_eps: 0.100000\n","     258679/2000000000: episode: 7101, duration: 5.164s, episode steps:  40, steps per second:   8, episode reward: -22.900, mean reward: -0.572 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 95.819255, mean_q: 44.661195, mean_eps: 0.100000\n","     258719/2000000000: episode: 7102, duration: 5.489s, episode steps:  40, steps per second:   7, episode reward: -4.100, mean reward: -0.102 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.480052, mean_q: 43.673678, mean_eps: 0.100000\n","     258749/2000000000: episode: 7103, duration: 4.122s, episode steps:  30, steps per second:   7, episode reward: -48.100, mean reward: -1.603 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 80.329119, mean_q: 43.214352, mean_eps: 0.100000\n","     258780/2000000000: episode: 7104, duration: 4.090s, episode steps:  31, steps per second:   8, episode reward: 119.900, mean reward:  3.868 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.507069, mean_q: 44.306966, mean_eps: 0.100000\n","     258811/2000000000: episode: 7105, duration: 4.460s, episode steps:  31, steps per second:   7, episode reward: 21.600, mean reward:  0.697 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.580260, mean_q: 44.468460, mean_eps: 0.100000\n","     258840/2000000000: episode: 7106, duration: 3.980s, episode steps:  29, steps per second:   7, episode reward: 148.800, mean reward:  5.131 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 92.023775, mean_q: 44.148822, mean_eps: 0.100000\n","     258873/2000000000: episode: 7107, duration: 4.690s, episode steps:  33, steps per second:   7, episode reward: -75.400, mean reward: -2.285 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 90.884161, mean_q: 43.455691, mean_eps: 0.100000\n","     258908/2000000000: episode: 7108, duration: 4.661s, episode steps:  35, steps per second:   8, episode reward: -48.900, mean reward: -1.397 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 78.321784, mean_q: 44.622108, mean_eps: 0.100000\n","     258945/2000000000: episode: 7109, duration: 5.028s, episode steps:  37, steps per second:   7, episode reward: 110.900, mean reward:  2.997 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 83.033005, mean_q: 43.917911, mean_eps: 0.100000\n","     258976/2000000000: episode: 7110, duration: 4.261s, episode steps:  31, steps per second:   7, episode reward: 78.800, mean reward:  2.542 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 82.787140, mean_q: 44.171846, mean_eps: 0.100000\n","     259006/2000000000: episode: 7111, duration: 3.848s, episode steps:  30, steps per second:   8, episode reward: 94.400, mean reward:  3.147 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 86.524957, mean_q: 44.263697, mean_eps: 0.100000\n","     259035/2000000000: episode: 7112, duration: 3.768s, episode steps:  29, steps per second:   8, episode reward: 95.800, mean reward:  3.303 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 82.340463, mean_q: 43.954992, mean_eps: 0.100000\n","     259075/2000000000: episode: 7113, duration: 5.340s, episode steps:  40, steps per second:   7, episode reward: 40.500, mean reward:  1.012 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 83.163755, mean_q: 44.435355, mean_eps: 0.100000\n","     259104/2000000000: episode: 7114, duration: 3.787s, episode steps:  29, steps per second:   8, episode reward: 173.900, mean reward:  5.997 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 84.667198, mean_q: 44.486494, mean_eps: 0.100000\n","     259138/2000000000: episode: 7115, duration: 4.496s, episode steps:  34, steps per second:   8, episode reward: 167.100, mean reward:  4.915 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.501157, mean_q: 44.233144, mean_eps: 0.100000\n","     259162/2000000000: episode: 7116, duration: 3.254s, episode steps:  24, steps per second:   7, episode reward: 126.100, mean reward:  5.254 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 87.390852, mean_q: 44.475181, mean_eps: 0.100000\n","     259196/2000000000: episode: 7117, duration: 4.435s, episode steps:  34, steps per second:   8, episode reward: 154.500, mean reward:  4.544 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 73.723856, mean_q: 44.735370, mean_eps: 0.100000\n","     259230/2000000000: episode: 7118, duration: 4.427s, episode steps:  34, steps per second:   8, episode reward: 52.500, mean reward:  1.544 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 82.289624, mean_q: 44.250979, mean_eps: 0.100000\n","     259266/2000000000: episode: 7119, duration: 4.875s, episode steps:  36, steps per second:   7, episode reward: 85.500, mean reward:  2.375 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 69.186736, mean_q: 43.746300, mean_eps: 0.100000\n","     259306/2000000000: episode: 7120, duration: 5.360s, episode steps:  40, steps per second:   7, episode reward: 138.400, mean reward:  3.460 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.520760, mean_q: 44.336887, mean_eps: 0.100000\n","     259337/2000000000: episode: 7121, duration: 4.061s, episode steps:  31, steps per second:   8, episode reward: 136.200, mean reward:  4.394 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 85.108038, mean_q: 43.823041, mean_eps: 0.100000\n","     259367/2000000000: episode: 7122, duration: 4.061s, episode steps:  30, steps per second:   7, episode reward: 95.700, mean reward:  3.190 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 76.478348, mean_q: 42.993047, mean_eps: 0.100000\n","     259400/2000000000: episode: 7123, duration: 4.350s, episode steps:  33, steps per second:   8, episode reward: 50.200, mean reward:  1.521 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.545878, mean_q: 44.610010, mean_eps: 0.100000\n","     259429/2000000000: episode: 7124, duration: 3.748s, episode steps:  29, steps per second:   8, episode reward: 96.600, mean reward:  3.331 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 87.433449, mean_q: 45.311772, mean_eps: 0.100000\n","     259461/2000000000: episode: 7125, duration: 4.074s, episode steps:  32, steps per second:   8, episode reward: 175.500, mean reward:  5.484 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 82.748275, mean_q: 44.288344, mean_eps: 0.100000\n","     259492/2000000000: episode: 7126, duration: 4.262s, episode steps:  31, steps per second:   7, episode reward: 48.800, mean reward:  1.574 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 88.326664, mean_q: 43.607164, mean_eps: 0.100000\n","     259529/2000000000: episode: 7127, duration: 5.038s, episode steps:  37, steps per second:   7, episode reward: 140.200, mean reward:  3.789 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 82.407412, mean_q: 44.164527, mean_eps: 0.100000\n","     259569/2000000000: episode: 7128, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: 68.600, mean reward:  1.715 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 85.851177, mean_q: 43.702329, mean_eps: 0.100000\n","     259605/2000000000: episode: 7129, duration: 4.721s, episode steps:  36, steps per second:   8, episode reward: 28.700, mean reward:  0.797 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.515575, mean_q: 43.756092, mean_eps: 0.100000\n","     259634/2000000000: episode: 7130, duration: 4.011s, episode steps:  29, steps per second:   7, episode reward: 27.100, mean reward:  0.934 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 88.520199, mean_q: 44.853703, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     259667/2000000000: episode: 7131, duration: 4.381s, episode steps:  33, steps per second:   8, episode reward: 63.700, mean reward:  1.930 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 82.348423, mean_q: 43.534323, mean_eps: 0.100000\n","     259694/2000000000: episode: 7132, duration: 3.605s, episode steps:  27, steps per second:   7, episode reward: -86.100, mean reward: -3.189 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 83.435296, mean_q: 43.426882, mean_eps: 0.100000\n","     259730/2000000000: episode: 7133, duration: 4.615s, episode steps:  36, steps per second:   8, episode reward: 37.300, mean reward:  1.036 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 85.703701, mean_q: 44.335904, mean_eps: 0.100000\n","     259758/2000000000: episode: 7134, duration: 3.680s, episode steps:  28, steps per second:   8, episode reward: -172.000, mean reward: -6.143 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 81.058976, mean_q: 44.013899, mean_eps: 0.100000\n","     259792/2000000000: episode: 7135, duration: 4.465s, episode steps:  34, steps per second:   8, episode reward: -22.400, mean reward: -0.659 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.656340, mean_q: 43.429193, mean_eps: 0.100000\n","     259828/2000000000: episode: 7136, duration: 4.713s, episode steps:  36, steps per second:   8, episode reward: 152.200, mean reward:  4.228 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 89.183582, mean_q: 44.383444, mean_eps: 0.100000\n","     259859/2000000000: episode: 7137, duration: 4.391s, episode steps:  31, steps per second:   7, episode reward: 85.300, mean reward:  2.752 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 81.673043, mean_q: 44.615241, mean_eps: 0.100000\n","     259890/2000000000: episode: 7138, duration: 4.047s, episode steps:  31, steps per second:   8, episode reward: 152.600, mean reward:  4.923 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 81.549675, mean_q: 43.208474, mean_eps: 0.100000\n","     259930/2000000000: episode: 7139, duration: 5.517s, episode steps:  40, steps per second:   7, episode reward: 172.100, mean reward:  4.303 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 85.591794, mean_q: 43.567572, mean_eps: 0.100000\n","     259970/2000000000: episode: 7140, duration: 5.471s, episode steps:  40, steps per second:   7, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.695327, mean_q: 44.547106, mean_eps: 0.100000\n","     259998/2000000000: episode: 7141, duration: 3.872s, episode steps:  28, steps per second:   7, episode reward: 118.100, mean reward:  4.218 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 76.988594, mean_q: 43.891804, mean_eps: 0.100000\n","     260038/2000000000: episode: 7142, duration: 5.158s, episode steps:  40, steps per second:   8, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 83.620217, mean_q: 43.292617, mean_eps: 0.100000\n","     260077/2000000000: episode: 7143, duration: 5.293s, episode steps:  39, steps per second:   7, episode reward: 157.200, mean reward:  4.031 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 88.399844, mean_q: 43.428864, mean_eps: 0.100000\n","     260107/2000000000: episode: 7144, duration: 4.172s, episode steps:  30, steps per second:   7, episode reward: 53.900, mean reward:  1.797 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 78.611417, mean_q: 45.725325, mean_eps: 0.100000\n","     260143/2000000000: episode: 7145, duration: 4.650s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 91.508926, mean_q: 43.831345, mean_eps: 0.100000\n","     260175/2000000000: episode: 7146, duration: 4.227s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.279682, mean_q: 44.384519, mean_eps: 0.100000\n","     260210/2000000000: episode: 7147, duration: 4.720s, episode steps:  35, steps per second:   7, episode reward: 31.600, mean reward:  0.903 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 84.510388, mean_q: 43.300882, mean_eps: 0.100000\n","     260250/2000000000: episode: 7148, duration: 5.342s, episode steps:  40, steps per second:   7, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 96.262120, mean_q: 43.198596, mean_eps: 0.100000\n","     260290/2000000000: episode: 7149, duration: 5.374s, episode steps:  40, steps per second:   7, episode reward: 61.600, mean reward:  1.540 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.845342, mean_q: 43.901481, mean_eps: 0.100000\n","     260329/2000000000: episode: 7150, duration: 4.908s, episode steps:  39, steps per second:   8, episode reward: -23.700, mean reward: -0.608 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 82.490615, mean_q: 43.233130, mean_eps: 0.100000\n","     260369/2000000000: episode: 7151, duration: 5.367s, episode steps:  40, steps per second:   7, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.848504, mean_q: 43.392229, mean_eps: 0.100000\n","     260402/2000000000: episode: 7152, duration: 4.130s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.675364, mean_q: 44.123707, mean_eps: 0.100000\n","     260431/2000000000: episode: 7153, duration: 3.691s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.649430, mean_q: 43.061051, mean_eps: 0.100000\n","     260467/2000000000: episode: 7154, duration: 4.555s, episode steps:  36, steps per second:   8, episode reward: 29.200, mean reward:  0.811 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 90.208793, mean_q: 43.634484, mean_eps: 0.100000\n","     260503/2000000000: episode: 7155, duration: 4.674s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 81.563022, mean_q: 43.021685, mean_eps: 0.100000\n","     260538/2000000000: episode: 7156, duration: 4.447s, episode steps:  35, steps per second:   8, episode reward: 169.800, mean reward:  4.851 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 85.042588, mean_q: 43.786373, mean_eps: 0.100000\n","     260576/2000000000: episode: 7157, duration: 4.887s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 82.156905, mean_q: 43.917979, mean_eps: 0.100000\n","     260616/2000000000: episode: 7158, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: 81.600, mean reward:  2.040 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.744896, mean_q: 43.543892, mean_eps: 0.100000\n","     260656/2000000000: episode: 7159, duration: 5.315s, episode steps:  40, steps per second:   8, episode reward: 87.200, mean reward:  2.180 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 84.638767, mean_q: 44.079273, mean_eps: 0.100000\n","     260691/2000000000: episode: 7160, duration: 4.497s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 87.484226, mean_q: 43.824262, mean_eps: 0.100000\n","     260731/2000000000: episode: 7161, duration: 5.142s, episode steps:  40, steps per second:   8, episode reward: 85.400, mean reward:  2.135 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.460653, mean_q: 43.773544, mean_eps: 0.100000\n","     260762/2000000000: episode: 7162, duration: 4.214s, episode steps:  31, steps per second:   7, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 81.999639, mean_q: 42.466935, mean_eps: 0.100000\n","     260793/2000000000: episode: 7163, duration: 3.895s, episode steps:  31, steps per second:   8, episode reward: 19.700, mean reward:  0.635 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 90.899236, mean_q: 43.892628, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     260830/2000000000: episode: 7164, duration: 4.684s, episode steps:  37, steps per second:   8, episode reward: -96.000, mean reward: -2.595 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 84.756948, mean_q: 43.092845, mean_eps: 0.100000\n","     260863/2000000000: episode: 7165, duration: 4.172s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 88.201374, mean_q: 43.281468, mean_eps: 0.100000\n","     260898/2000000000: episode: 7166, duration: 4.328s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 83.047608, mean_q: 44.313946, mean_eps: 0.100000\n","     260927/2000000000: episode: 7167, duration: 3.660s, episode steps:  29, steps per second:   8, episode reward: -15.400, mean reward: -0.531 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 77.559003, mean_q: 43.784341, mean_eps: 0.100000\n","     260962/2000000000: episode: 7168, duration: 4.659s, episode steps:  35, steps per second:   8, episode reward: 126.400, mean reward:  3.611 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 78.539107, mean_q: 44.763088, mean_eps: 0.100000\n","     260991/2000000000: episode: 7169, duration: 3.636s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 80.823857, mean_q: 42.741173, mean_eps: 0.100000\n","     261031/2000000000: episode: 7170, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: -11.800, mean reward: -0.295 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.535133, mean_q: 43.920274, mean_eps: 0.100000\n","     261064/2000000000: episode: 7171, duration: 4.422s, episode steps:  33, steps per second:   7, episode reward: 92.500, mean reward:  2.803 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 79.525124, mean_q: 43.607869, mean_eps: 0.100000\n","     261104/2000000000: episode: 7172, duration: 5.152s, episode steps:  40, steps per second:   8, episode reward: 156.800, mean reward:  3.920 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.608590, mean_q: 43.439868, mean_eps: 0.100000\n","     261139/2000000000: episode: 7173, duration: 4.757s, episode steps:  35, steps per second:   7, episode reward: 134.800, mean reward:  3.851 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 81.861200, mean_q: 43.845840, mean_eps: 0.100000\n","     261173/2000000000: episode: 7174, duration: 4.404s, episode steps:  34, steps per second:   8, episode reward: 74.100, mean reward:  2.179 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 83.599077, mean_q: 43.972693, mean_eps: 0.100000\n","     261203/2000000000: episode: 7175, duration: 3.842s, episode steps:  30, steps per second:   8, episode reward: 47.500, mean reward:  1.583 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 78.075376, mean_q: 43.864599, mean_eps: 0.100000\n","     261243/2000000000: episode: 7176, duration: 5.339s, episode steps:  40, steps per second:   7, episode reward: 221.800, mean reward:  5.545 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.140048, mean_q: 43.500447, mean_eps: 0.100000\n","     261265/2000000000: episode: 7177, duration: 2.862s, episode steps:  22, steps per second:   8, episode reward: 147.800, mean reward:  6.718 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 77.525591, mean_q: 44.146494, mean_eps: 0.100000\n","     261305/2000000000: episode: 7178, duration: 5.349s, episode steps:  40, steps per second:   7, episode reward: -34.200, mean reward: -0.855 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 83.042007, mean_q: 44.111733, mean_eps: 0.100000\n","     261337/2000000000: episode: 7179, duration: 4.335s, episode steps:  32, steps per second:   7, episode reward: -5.600, mean reward: -0.175 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 86.895488, mean_q: 44.038878, mean_eps: 0.100000\n","     261372/2000000000: episode: 7180, duration: 4.572s, episode steps:  35, steps per second:   8, episode reward: -54.000, mean reward: -1.543 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 81.552035, mean_q: 44.353469, mean_eps: 0.100000\n","     261398/2000000000: episode: 7181, duration: 3.415s, episode steps:  26, steps per second:   8, episode reward: 10.800, mean reward:  0.415 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 85.566441, mean_q: 44.472841, mean_eps: 0.100000\n","     261434/2000000000: episode: 7182, duration: 4.708s, episode steps:  36, steps per second:   8, episode reward: 168.300, mean reward:  4.675 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 81.133545, mean_q: 44.388491, mean_eps: 0.100000\n","     261467/2000000000: episode: 7183, duration: 4.138s, episode steps:  33, steps per second:   8, episode reward: 54.400, mean reward:  1.648 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 80.319986, mean_q: 44.149552, mean_eps: 0.100000\n","     261490/2000000000: episode: 7184, duration: 2.884s, episode steps:  23, steps per second:   8, episode reward: 284.000, mean reward: 12.348 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 92.261137, mean_q: 44.523725, mean_eps: 0.100000\n","     261529/2000000000: episode: 7185, duration: 5.125s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 86.283102, mean_q: 44.399150, mean_eps: 0.100000\n","     261557/2000000000: episode: 7186, duration: 3.807s, episode steps:  28, steps per second:   7, episode reward: 51.500, mean reward:  1.839 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 79.628324, mean_q: 44.658114, mean_eps: 0.100000\n","     261597/2000000000: episode: 7187, duration: 5.214s, episode steps:  40, steps per second:   8, episode reward: 139.900, mean reward:  3.498 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.028854, mean_q: 44.145041, mean_eps: 0.100000\n","     261630/2000000000: episode: 7188, duration: 4.332s, episode steps:  33, steps per second:   8, episode reward: -20.900, mean reward: -0.633 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 74.452561, mean_q: 44.385340, mean_eps: 0.100000\n","     261658/2000000000: episode: 7189, duration: 3.695s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 79.094231, mean_q: 44.658268, mean_eps: 0.100000\n","     261693/2000000000: episode: 7190, duration: 4.529s, episode steps:  35, steps per second:   8, episode reward: 178.100, mean reward:  5.089 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 88.712522, mean_q: 43.687124, mean_eps: 0.100000\n","     261733/2000000000: episode: 7191, duration: 5.012s, episode steps:  40, steps per second:   8, episode reward: 14.500, mean reward:  0.363 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 89.672465, mean_q: 43.688653, mean_eps: 0.100000\n","     261761/2000000000: episode: 7192, duration: 3.746s, episode steps:  28, steps per second:   7, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 87.103403, mean_q: 42.684807, mean_eps: 0.100000\n","     261801/2000000000: episode: 7193, duration: 5.533s, episode steps:  40, steps per second:   7, episode reward: 113.600, mean reward:  2.840 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 84.440496, mean_q: 44.411794, mean_eps: 0.100000\n","     261841/2000000000: episode: 7194, duration: 5.451s, episode steps:  40, steps per second:   7, episode reward: 129.300, mean reward:  3.232 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.727856, mean_q: 43.921289, mean_eps: 0.100000\n","     261880/2000000000: episode: 7195, duration: 5.362s, episode steps:  39, steps per second:   7, episode reward: 62.100, mean reward:  1.592 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 82.775936, mean_q: 43.655273, mean_eps: 0.100000\n","     261907/2000000000: episode: 7196, duration: 3.496s, episode steps:  27, steps per second:   8, episode reward: 48.200, mean reward:  1.785 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 90.947712, mean_q: 43.333920, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     261947/2000000000: episode: 7197, duration: 5.176s, episode steps:  40, steps per second:   8, episode reward: 60.100, mean reward:  1.503 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 84.887585, mean_q: 43.427451, mean_eps: 0.100000\n","     261977/2000000000: episode: 7198, duration: 3.913s, episode steps:  30, steps per second:   8, episode reward: -19.600, mean reward: -0.653 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 84.520506, mean_q: 44.108994, mean_eps: 0.100000\n","     262003/2000000000: episode: 7199, duration: 3.750s, episode steps:  26, steps per second:   7, episode reward: 42.600, mean reward:  1.638 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 74.468016, mean_q: 43.831670, mean_eps: 0.100000\n","     262038/2000000000: episode: 7200, duration: 4.825s, episode steps:  35, steps per second:   7, episode reward: 111.600, mean reward:  3.189 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.682384, mean_q: 44.140071, mean_eps: 0.100000\n","     262078/2000000000: episode: 7201, duration: 5.439s, episode steps:  40, steps per second:   7, episode reward: 62.500, mean reward:  1.563 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.723212, mean_q: 43.273470, mean_eps: 0.100000\n","     262107/2000000000: episode: 7202, duration: 4.229s, episode steps:  29, steps per second:   7, episode reward: 122.800, mean reward:  4.234 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.602778, mean_q: 44.818514, mean_eps: 0.100000\n","     262141/2000000000: episode: 7203, duration: 4.695s, episode steps:  34, steps per second:   7, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 78.253476, mean_q: 44.288137, mean_eps: 0.100000\n","     262174/2000000000: episode: 7204, duration: 4.444s, episode steps:  33, steps per second:   7, episode reward: 117.200, mean reward:  3.552 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 79.247254, mean_q: 44.010562, mean_eps: 0.100000\n","     262209/2000000000: episode: 7205, duration: 4.748s, episode steps:  35, steps per second:   7, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.419596, mean_q: 43.323733, mean_eps: 0.100000\n","     262239/2000000000: episode: 7206, duration: 3.909s, episode steps:  30, steps per second:   8, episode reward: 84.000, mean reward:  2.800 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 74.977462, mean_q: 44.452981, mean_eps: 0.100000\n","     262279/2000000000: episode: 7207, duration: 5.535s, episode steps:  40, steps per second:   7, episode reward: -6.000, mean reward: -0.150 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.292133, mean_q: 43.816031, mean_eps: 0.100000\n","     262319/2000000000: episode: 7208, duration: 5.463s, episode steps:  40, steps per second:   7, episode reward: 68.400, mean reward:  1.710 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.266776, mean_q: 43.890611, mean_eps: 0.100000\n","     262353/2000000000: episode: 7209, duration: 5.003s, episode steps:  34, steps per second:   7, episode reward: -14.500, mean reward: -0.426 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 93.453744, mean_q: 44.368291, mean_eps: 0.100000\n","     262385/2000000000: episode: 7210, duration: 4.227s, episode steps:  32, steps per second:   8, episode reward: 93.900, mean reward:  2.934 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.338046, mean_q: 42.954207, mean_eps: 0.100000\n","     262419/2000000000: episode: 7211, duration: 4.287s, episode steps:  34, steps per second:   8, episode reward: -18.500, mean reward: -0.544 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 83.077387, mean_q: 44.756194, mean_eps: 0.100000\n","     262459/2000000000: episode: 7212, duration: 5.346s, episode steps:  40, steps per second:   7, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 90.741179, mean_q: 43.668385, mean_eps: 0.100000\n","     262496/2000000000: episode: 7213, duration: 4.833s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 89.467835, mean_q: 42.701764, mean_eps: 0.100000\n","     262533/2000000000: episode: 7214, duration: 4.888s, episode steps:  37, steps per second:   8, episode reward: 170.000, mean reward:  4.595 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 81.317346, mean_q: 43.351682, mean_eps: 0.100000\n","     262559/2000000000: episode: 7215, duration: 3.847s, episode steps:  26, steps per second:   7, episode reward: 128.000, mean reward:  4.923 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 79.637560, mean_q: 44.444682, mean_eps: 0.100000\n","     262595/2000000000: episode: 7216, duration: 5.211s, episode steps:  36, steps per second:   7, episode reward: 136.600, mean reward:  3.794 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.093445, mean_q: 44.625630, mean_eps: 0.100000\n","     262630/2000000000: episode: 7217, duration: 4.881s, episode steps:  35, steps per second:   7, episode reward: 64.900, mean reward:  1.854 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 76.031722, mean_q: 43.150767, mean_eps: 0.100000\n","     262656/2000000000: episode: 7218, duration: 3.521s, episode steps:  26, steps per second:   7, episode reward: 68.000, mean reward:  2.615 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 73.622104, mean_q: 44.070284, mean_eps: 0.100000\n","     262696/2000000000: episode: 7219, duration: 5.216s, episode steps:  40, steps per second:   8, episode reward: 21.400, mean reward:  0.535 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.081882, mean_q: 43.682586, mean_eps: 0.100000\n","     262735/2000000000: episode: 7220, duration: 4.963s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 85.230879, mean_q: 43.464630, mean_eps: 0.100000\n","     262767/2000000000: episode: 7221, duration: 4.208s, episode steps:  32, steps per second:   8, episode reward: -2.700, mean reward: -0.084 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.809651, mean_q: 44.083898, mean_eps: 0.100000\n","     262796/2000000000: episode: 7222, duration: 4.069s, episode steps:  29, steps per second:   7, episode reward: 203.300, mean reward:  7.010 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 77.575124, mean_q: 45.073900, mean_eps: 0.100000\n","     262828/2000000000: episode: 7223, duration: 4.363s, episode steps:  32, steps per second:   7, episode reward: 59.400, mean reward:  1.856 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 77.967339, mean_q: 43.712070, mean_eps: 0.100000\n","     262868/2000000000: episode: 7224, duration: 5.212s, episode steps:  40, steps per second:   8, episode reward: 48.200, mean reward:  1.205 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 85.607616, mean_q: 43.874351, mean_eps: 0.100000\n","     262895/2000000000: episode: 7225, duration: 3.630s, episode steps:  27, steps per second:   7, episode reward: 65.200, mean reward:  2.415 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 79.530037, mean_q: 44.840437, mean_eps: 0.100000\n","     262932/2000000000: episode: 7226, duration: 5.196s, episode steps:  37, steps per second:   7, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 83.189698, mean_q: 43.731042, mean_eps: 0.100000\n","     262969/2000000000: episode: 7227, duration: 5.015s, episode steps:  37, steps per second:   7, episode reward: 57.700, mean reward:  1.559 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 78.085942, mean_q: 43.826990, mean_eps: 0.100000\n","     262997/2000000000: episode: 7228, duration: 3.931s, episode steps:  28, steps per second:   7, episode reward: 33.200, mean reward:  1.186 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 81.187852, mean_q: 45.104428, mean_eps: 0.100000\n","     263030/2000000000: episode: 7229, duration: 4.550s, episode steps:  33, steps per second:   7, episode reward: -46.100, mean reward: -1.397 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 82.914140, mean_q: 43.723330, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     263070/2000000000: episode: 7230, duration: 4.886s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.898957, mean_q: 44.281088, mean_eps: 0.100000\n","     263103/2000000000: episode: 7231, duration: 4.315s, episode steps:  33, steps per second:   8, episode reward: -49.000, mean reward: -1.485 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 71.502725, mean_q: 44.369944, mean_eps: 0.100000\n","     263136/2000000000: episode: 7232, duration: 4.229s, episode steps:  33, steps per second:   8, episode reward: 111.400, mean reward:  3.376 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.806696, mean_q: 42.941558, mean_eps: 0.100000\n","     263168/2000000000: episode: 7233, duration: 4.138s, episode steps:  32, steps per second:   8, episode reward: 160.200, mean reward:  5.006 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 80.376558, mean_q: 44.703863, mean_eps: 0.100000\n","     263201/2000000000: episode: 7234, duration: 4.253s, episode steps:  33, steps per second:   8, episode reward: 145.000, mean reward:  4.394 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 80.723103, mean_q: 43.765921, mean_eps: 0.100000\n","     263236/2000000000: episode: 7235, duration: 4.435s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 84.999033, mean_q: 43.053695, mean_eps: 0.100000\n","     263276/2000000000: episode: 7236, duration: 4.727s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.368211, mean_q: 43.917932, mean_eps: 0.100000\n","     263306/2000000000: episode: 7237, duration: 3.600s, episode steps:  30, steps per second:   8, episode reward: 166.900, mean reward:  5.563 [-18.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 82.841690, mean_q: 44.457659, mean_eps: 0.100000\n","     263342/2000000000: episode: 7238, duration: 4.266s, episode steps:  36, steps per second:   8, episode reward: 141.000, mean reward:  3.917 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 75.496691, mean_q: 44.043900, mean_eps: 0.100000\n","     263371/2000000000: episode: 7239, duration: 3.521s, episode steps:  29, steps per second:   8, episode reward: 59.600, mean reward:  2.055 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 81.010989, mean_q: 43.079895, mean_eps: 0.100000\n","     263404/2000000000: episode: 7240, duration: 4.554s, episode steps:  33, steps per second:   7, episode reward: 74.200, mean reward:  2.248 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.978540, mean_q: 44.045360, mean_eps: 0.100000\n","     263440/2000000000: episode: 7241, duration: 5.304s, episode steps:  36, steps per second:   7, episode reward: 170.100, mean reward:  4.725 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 77.677883, mean_q: 44.506551, mean_eps: 0.100000\n","     263473/2000000000: episode: 7242, duration: 4.838s, episode steps:  33, steps per second:   7, episode reward: 147.000, mean reward:  4.455 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.822117, mean_q: 44.447582, mean_eps: 0.100000\n","     263510/2000000000: episode: 7243, duration: 4.957s, episode steps:  37, steps per second:   7, episode reward: 148.700, mean reward:  4.019 [-20.000, 18.100], mean action: 1.243 [0.000, 2.000],  loss: 89.133758, mean_q: 43.947795, mean_eps: 0.100000\n","     263538/2000000000: episode: 7244, duration: 3.739s, episode steps:  28, steps per second:   7, episode reward: 76.500, mean reward:  2.732 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 76.907728, mean_q: 44.709964, mean_eps: 0.100000\n","     263569/2000000000: episode: 7245, duration: 4.241s, episode steps:  31, steps per second:   7, episode reward: 31.400, mean reward:  1.013 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 87.526807, mean_q: 43.168359, mean_eps: 0.100000\n","     263608/2000000000: episode: 7246, duration: 4.819s, episode steps:  39, steps per second:   8, episode reward: 199.700, mean reward:  5.121 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 82.984741, mean_q: 44.753612, mean_eps: 0.100000\n","     263639/2000000000: episode: 7247, duration: 3.995s, episode steps:  31, steps per second:   8, episode reward: 53.800, mean reward:  1.735 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.220806, mean_q: 43.366598, mean_eps: 0.100000\n","     263675/2000000000: episode: 7248, duration: 4.977s, episode steps:  36, steps per second:   7, episode reward: 124.400, mean reward:  3.456 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 81.223657, mean_q: 43.488609, mean_eps: 0.100000\n","     263715/2000000000: episode: 7249, duration: 5.131s, episode steps:  40, steps per second:   8, episode reward: 117.700, mean reward:  2.942 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 84.709024, mean_q: 43.670874, mean_eps: 0.100000\n","     263754/2000000000: episode: 7250, duration: 4.957s, episode steps:  39, steps per second:   8, episode reward: 132.800, mean reward:  3.405 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 81.457087, mean_q: 44.685999, mean_eps: 0.100000\n","     263785/2000000000: episode: 7251, duration: 3.986s, episode steps:  31, steps per second:   8, episode reward: 35.500, mean reward:  1.145 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.849269, mean_q: 44.402390, mean_eps: 0.100000\n","     263821/2000000000: episode: 7252, duration: 4.671s, episode steps:  36, steps per second:   8, episode reward: -51.800, mean reward: -1.439 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 87.270900, mean_q: 43.417766, mean_eps: 0.100000\n","     263848/2000000000: episode: 7253, duration: 3.528s, episode steps:  27, steps per second:   8, episode reward: 54.900, mean reward:  2.033 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 86.890891, mean_q: 43.235624, mean_eps: 0.100000\n","     263880/2000000000: episode: 7254, duration: 4.211s, episode steps:  32, steps per second:   8, episode reward: 145.000, mean reward:  4.531 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 80.852093, mean_q: 44.309479, mean_eps: 0.100000\n","     263909/2000000000: episode: 7255, duration: 3.789s, episode steps:  29, steps per second:   8, episode reward: 82.300, mean reward:  2.838 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 79.654193, mean_q: 43.925320, mean_eps: 0.100000\n","     263931/2000000000: episode: 7256, duration: 2.942s, episode steps:  22, steps per second:   7, episode reward: 75.000, mean reward:  3.409 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 86.276306, mean_q: 46.064656, mean_eps: 0.100000\n","     263971/2000000000: episode: 7257, duration: 5.181s, episode steps:  40, steps per second:   8, episode reward: 141.300, mean reward:  3.532 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 82.566604, mean_q: 43.156140, mean_eps: 0.100000\n","     264003/2000000000: episode: 7258, duration: 4.235s, episode steps:  32, steps per second:   8, episode reward: 144.300, mean reward:  4.509 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 80.820231, mean_q: 45.010626, mean_eps: 0.100000\n","     264042/2000000000: episode: 7259, duration: 5.076s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 85.869520, mean_q: 43.315369, mean_eps: 0.100000\n","     264078/2000000000: episode: 7260, duration: 4.383s, episode steps:  36, steps per second:   8, episode reward: 208.000, mean reward:  5.778 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 78.255682, mean_q: 43.359645, mean_eps: 0.100000\n","     264118/2000000000: episode: 7261, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: 18.700, mean reward:  0.468 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 86.432957, mean_q: 43.496135, mean_eps: 0.100000\n","     264154/2000000000: episode: 7262, duration: 4.796s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 89.701707, mean_q: 43.529416, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     264180/2000000000: episode: 7263, duration: 3.188s, episode steps:  26, steps per second:   8, episode reward: 75.900, mean reward:  2.919 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 84.176846, mean_q: 43.857452, mean_eps: 0.100000\n","     264220/2000000000: episode: 7264, duration: 4.733s, episode steps:  40, steps per second:   8, episode reward: 93.800, mean reward:  2.345 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 83.106774, mean_q: 42.736869, mean_eps: 0.100000\n","     264255/2000000000: episode: 7265, duration: 4.222s, episode steps:  35, steps per second:   8, episode reward: 95.300, mean reward:  2.723 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 91.422187, mean_q: 43.858478, mean_eps: 0.100000\n","     264286/2000000000: episode: 7266, duration: 3.871s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 74.000541, mean_q: 44.309567, mean_eps: 0.100000\n","     264317/2000000000: episode: 7267, duration: 4.348s, episode steps:  31, steps per second:   7, episode reward: 246.000, mean reward:  7.935 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 90.882624, mean_q: 43.843510, mean_eps: 0.100000\n","     264346/2000000000: episode: 7268, duration: 3.814s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.340318, mean_q: 44.312426, mean_eps: 0.100000\n","     264381/2000000000: episode: 7269, duration: 4.629s, episode steps:  35, steps per second:   8, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 83.327901, mean_q: 43.388257, mean_eps: 0.100000\n","     264415/2000000000: episode: 7270, duration: 4.196s, episode steps:  34, steps per second:   8, episode reward: 156.200, mean reward:  4.594 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 77.810105, mean_q: 44.286845, mean_eps: 0.100000\n","     264449/2000000000: episode: 7271, duration: 4.317s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 80.098539, mean_q: 43.592856, mean_eps: 0.100000\n","     264480/2000000000: episode: 7272, duration: 4.156s, episode steps:  31, steps per second:   7, episode reward: 77.800, mean reward:  2.510 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 80.196342, mean_q: 44.577777, mean_eps: 0.100000\n","     264505/2000000000: episode: 7273, duration: 3.226s, episode steps:  25, steps per second:   8, episode reward: 208.000, mean reward:  8.320 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 91.380458, mean_q: 43.195986, mean_eps: 0.100000\n","     264544/2000000000: episode: 7274, duration: 5.400s, episode steps:  39, steps per second:   7, episode reward: 246.000, mean reward:  6.308 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 77.988565, mean_q: 43.459717, mean_eps: 0.100000\n","     264580/2000000000: episode: 7275, duration: 4.872s, episode steps:  36, steps per second:   7, episode reward: 100.600, mean reward:  2.794 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 83.000378, mean_q: 42.839277, mean_eps: 0.100000\n","     264615/2000000000: episode: 7276, duration: 4.734s, episode steps:  35, steps per second:   7, episode reward: -96.700, mean reward: -2.763 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 78.456332, mean_q: 43.511711, mean_eps: 0.100000\n","     264645/2000000000: episode: 7277, duration: 3.884s, episode steps:  30, steps per second:   8, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 77.982002, mean_q: 44.780305, mean_eps: 0.100000\n","     264685/2000000000: episode: 7278, duration: 5.328s, episode steps:  40, steps per second:   8, episode reward: 20.600, mean reward:  0.515 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.723688, mean_q: 43.885207, mean_eps: 0.100000\n","     264718/2000000000: episode: 7279, duration: 4.227s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 81.318504, mean_q: 43.823975, mean_eps: 0.100000\n","     264758/2000000000: episode: 7280, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward:  2.000, mean reward:  0.050 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.977819, mean_q: 44.017195, mean_eps: 0.100000\n","     264786/2000000000: episode: 7281, duration: 3.694s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 74.986978, mean_q: 44.378233, mean_eps: 0.100000\n","     264818/2000000000: episode: 7282, duration: 4.097s, episode steps:  32, steps per second:   8, episode reward: 145.900, mean reward:  4.559 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 77.085376, mean_q: 44.195769, mean_eps: 0.100000\n","     264849/2000000000: episode: 7283, duration: 4.223s, episode steps:  31, steps per second:   7, episode reward: 88.200, mean reward:  2.845 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 82.986720, mean_q: 44.454028, mean_eps: 0.100000\n","     264888/2000000000: episode: 7284, duration: 5.050s, episode steps:  39, steps per second:   8, episode reward: -134.000, mean reward: -3.436 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 81.902241, mean_q: 43.216312, mean_eps: 0.100000\n","     264916/2000000000: episode: 7285, duration: 3.558s, episode steps:  28, steps per second:   8, episode reward: -96.000, mean reward: -3.429 [-20.000, 18.000], mean action: 0.714 [0.000, 2.000],  loss: 77.103557, mean_q: 44.177728, mean_eps: 0.100000\n","     264948/2000000000: episode: 7286, duration: 3.995s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.874314, mean_q: 44.470798, mean_eps: 0.100000\n","     264980/2000000000: episode: 7287, duration: 3.891s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 80.157750, mean_q: 43.886018, mean_eps: 0.100000\n","     265014/2000000000: episode: 7288, duration: 4.013s, episode steps:  34, steps per second:   8, episode reward: 141.500, mean reward:  4.162 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.711141, mean_q: 43.763141, mean_eps: 0.100000\n","     265043/2000000000: episode: 7289, duration: 3.296s, episode steps:  29, steps per second:   9, episode reward: 86.100, mean reward:  2.969 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 78.247402, mean_q: 43.605413, mean_eps: 0.100000\n","     265083/2000000000: episode: 7290, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: 199.800, mean reward:  4.995 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.922939, mean_q: 44.137019, mean_eps: 0.100000\n","     265123/2000000000: episode: 7291, duration: 4.742s, episode steps:  40, steps per second:   8, episode reward: 60.900, mean reward:  1.522 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.374093, mean_q: 43.894456, mean_eps: 0.100000\n","     265152/2000000000: episode: 7292, duration: 3.571s, episode steps:  29, steps per second:   8, episode reward: 101.400, mean reward:  3.497 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.044323, mean_q: 43.004674, mean_eps: 0.100000\n","     265185/2000000000: episode: 7293, duration: 3.855s, episode steps:  33, steps per second:   9, episode reward: 40.900, mean reward:  1.239 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 80.447962, mean_q: 44.659913, mean_eps: 0.100000\n","     265214/2000000000: episode: 7294, duration: 3.390s, episode steps:  29, steps per second:   9, episode reward: -42.700, mean reward: -1.472 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 81.783537, mean_q: 43.803716, mean_eps: 0.100000\n","     265253/2000000000: episode: 7295, duration: 4.489s, episode steps:  39, steps per second:   9, episode reward: -25.000, mean reward: -0.641 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 82.402796, mean_q: 42.850725, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     265291/2000000000: episode: 7296, duration: 4.492s, episode steps:  38, steps per second:   8, episode reward: 175.600, mean reward:  4.621 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 81.030507, mean_q: 43.674858, mean_eps: 0.100000\n","     265318/2000000000: episode: 7297, duration: 3.101s, episode steps:  27, steps per second:   9, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 87.479279, mean_q: 44.463232, mean_eps: 0.100000\n","     265358/2000000000: episode: 7298, duration: 4.805s, episode steps:  40, steps per second:   8, episode reward: 123.900, mean reward:  3.098 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 79.927612, mean_q: 43.523590, mean_eps: 0.100000\n","     265397/2000000000: episode: 7299, duration: 4.423s, episode steps:  39, steps per second:   9, episode reward: 32.400, mean reward:  0.831 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 92.251844, mean_q: 43.439761, mean_eps: 0.100000\n","     265433/2000000000: episode: 7300, duration: 4.263s, episode steps:  36, steps per second:   8, episode reward: 119.400, mean reward:  3.317 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 79.090559, mean_q: 44.757331, mean_eps: 0.100000\n","     265473/2000000000: episode: 7301, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 101.000, mean reward:  2.525 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.933955, mean_q: 43.760313, mean_eps: 0.100000\n","     265512/2000000000: episode: 7302, duration: 4.828s, episode steps:  39, steps per second:   8, episode reward: -26.200, mean reward: -0.672 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 80.510889, mean_q: 44.039984, mean_eps: 0.100000\n","     265540/2000000000: episode: 7303, duration: 3.576s, episode steps:  28, steps per second:   8, episode reward: 223.400, mean reward:  7.979 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 88.374719, mean_q: 43.497078, mean_eps: 0.100000\n","     265575/2000000000: episode: 7304, duration: 4.443s, episode steps:  35, steps per second:   8, episode reward: 154.600, mean reward:  4.417 [-20.000, 18.000], mean action: 0.914 [0.000, 2.000],  loss: 89.162676, mean_q: 42.580573, mean_eps: 0.100000\n","     265615/2000000000: episode: 7305, duration: 5.149s, episode steps:  40, steps per second:   8, episode reward: -22.100, mean reward: -0.553 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.284730, mean_q: 42.937391, mean_eps: 0.100000\n","     265649/2000000000: episode: 7306, duration: 4.408s, episode steps:  34, steps per second:   8, episode reward: 116.100, mean reward:  3.415 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 82.503397, mean_q: 43.973809, mean_eps: 0.100000\n","     265689/2000000000: episode: 7307, duration: 5.750s, episode steps:  40, steps per second:   7, episode reward: 231.600, mean reward:  5.790 [-2.400, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 86.804596, mean_q: 43.224477, mean_eps: 0.100000\n","     265724/2000000000: episode: 7308, duration: 4.588s, episode steps:  35, steps per second:   8, episode reward: 206.500, mean reward:  5.900 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 80.033252, mean_q: 44.634672, mean_eps: 0.100000\n","     265755/2000000000: episode: 7309, duration: 4.140s, episode steps:  31, steps per second:   7, episode reward: -18.200, mean reward: -0.587 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 83.516201, mean_q: 43.903861, mean_eps: 0.100000\n","     265783/2000000000: episode: 7310, duration: 3.736s, episode steps:  28, steps per second:   7, episode reward: 149.300, mean reward:  5.332 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 77.353088, mean_q: 43.687541, mean_eps: 0.100000\n","     265823/2000000000: episode: 7311, duration: 5.675s, episode steps:  40, steps per second:   7, episode reward: 104.600, mean reward:  2.615 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.021522, mean_q: 44.445130, mean_eps: 0.100000\n","     265850/2000000000: episode: 7312, duration: 3.764s, episode steps:  27, steps per second:   7, episode reward: -29.500, mean reward: -1.093 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 83.396932, mean_q: 43.794424, mean_eps: 0.100000\n","     265883/2000000000: episode: 7313, duration: 4.868s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.359015, mean_q: 43.849992, mean_eps: 0.100000\n","     265923/2000000000: episode: 7314, duration: 5.454s, episode steps:  40, steps per second:   7, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 78.313138, mean_q: 44.166590, mean_eps: 0.100000\n","     265959/2000000000: episode: 7315, duration: 4.860s, episode steps:  36, steps per second:   7, episode reward: 38.700, mean reward:  1.075 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 89.624686, mean_q: 43.841338, mean_eps: 0.100000\n","     265998/2000000000: episode: 7316, duration: 4.889s, episode steps:  39, steps per second:   8, episode reward: 99.100, mean reward:  2.541 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 81.069176, mean_q: 44.870037, mean_eps: 0.100000\n","     266029/2000000000: episode: 7317, duration: 4.203s, episode steps:  31, steps per second:   7, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 76.401405, mean_q: 44.305322, mean_eps: 0.100000\n","     266054/2000000000: episode: 7318, duration: 3.244s, episode steps:  25, steps per second:   8, episode reward: 88.600, mean reward:  3.544 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 86.321215, mean_q: 44.065051, mean_eps: 0.100000\n","     266091/2000000000: episode: 7319, duration: 4.883s, episode steps:  37, steps per second:   8, episode reward: 148.900, mean reward:  4.024 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 79.387689, mean_q: 44.376118, mean_eps: 0.100000\n","     266121/2000000000: episode: 7320, duration: 4.008s, episode steps:  30, steps per second:   7, episode reward: 173.500, mean reward:  5.783 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 92.695239, mean_q: 44.156667, mean_eps: 0.100000\n","     266145/2000000000: episode: 7321, duration: 3.369s, episode steps:  24, steps per second:   7, episode reward: 190.400, mean reward:  7.933 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 82.677320, mean_q: 43.137286, mean_eps: 0.100000\n","     266176/2000000000: episode: 7322, duration: 4.483s, episode steps:  31, steps per second:   7, episode reward: -72.800, mean reward: -2.348 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 92.088828, mean_q: 44.116759, mean_eps: 0.100000\n","     266212/2000000000: episode: 7323, duration: 4.781s, episode steps:  36, steps per second:   8, episode reward: 131.500, mean reward:  3.653 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 93.626212, mean_q: 43.842010, mean_eps: 0.100000\n","     266248/2000000000: episode: 7324, duration: 4.738s, episode steps:  36, steps per second:   8, episode reward:  2.400, mean reward:  0.067 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 76.198265, mean_q: 44.310282, mean_eps: 0.100000\n","     266286/2000000000: episode: 7325, duration: 5.050s, episode steps:  38, steps per second:   8, episode reward: 130.000, mean reward:  3.421 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 81.755581, mean_q: 44.034223, mean_eps: 0.100000\n","     266316/2000000000: episode: 7326, duration: 3.865s, episode steps:  30, steps per second:   8, episode reward: 152.800, mean reward:  5.093 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.306957, mean_q: 44.146936, mean_eps: 0.100000\n","     266356/2000000000: episode: 7327, duration: 5.256s, episode steps:  40, steps per second:   8, episode reward: 53.500, mean reward:  1.338 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.875555, mean_q: 43.480016, mean_eps: 0.100000\n","     266385/2000000000: episode: 7328, duration: 3.700s, episode steps:  29, steps per second:   8, episode reward: 233.100, mean reward:  8.038 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 85.316409, mean_q: 43.619214, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     266416/2000000000: episode: 7329, duration: 4.006s, episode steps:  31, steps per second:   8, episode reward: 32.300, mean reward:  1.042 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 84.529899, mean_q: 43.234480, mean_eps: 0.100000\n","     266455/2000000000: episode: 7330, duration: 4.946s, episode steps:  39, steps per second:   8, episode reward: 110.400, mean reward:  2.831 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 77.536872, mean_q: 44.164151, mean_eps: 0.100000\n","     266483/2000000000: episode: 7331, duration: 3.914s, episode steps:  28, steps per second:   7, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 84.199221, mean_q: 44.380042, mean_eps: 0.100000\n","     266523/2000000000: episode: 7332, duration: 5.314s, episode steps:  40, steps per second:   8, episode reward: 79.700, mean reward:  1.993 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 80.845974, mean_q: 44.209824, mean_eps: 0.100000\n","     266554/2000000000: episode: 7333, duration: 4.200s, episode steps:  31, steps per second:   7, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 79.628198, mean_q: 43.044451, mean_eps: 0.100000\n","     266589/2000000000: episode: 7334, duration: 4.639s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 82.599419, mean_q: 43.753744, mean_eps: 0.100000\n","     266629/2000000000: episode: 7335, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 21.300, mean reward:  0.533 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.934050, mean_q: 43.861263, mean_eps: 0.100000\n","     266659/2000000000: episode: 7336, duration: 3.901s, episode steps:  30, steps per second:   8, episode reward: 89.200, mean reward:  2.973 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 80.439327, mean_q: 42.902756, mean_eps: 0.100000\n","     266689/2000000000: episode: 7337, duration: 3.868s, episode steps:  30, steps per second:   8, episode reward:  7.400, mean reward:  0.247 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 78.277425, mean_q: 43.878606, mean_eps: 0.100000\n","     266718/2000000000: episode: 7338, duration: 3.878s, episode steps:  29, steps per second:   7, episode reward: 58.300, mean reward:  2.010 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 75.594269, mean_q: 44.980908, mean_eps: 0.100000\n","     266752/2000000000: episode: 7339, duration: 4.726s, episode steps:  34, steps per second:   7, episode reward:  6.100, mean reward:  0.179 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 80.063154, mean_q: 44.964978, mean_eps: 0.100000\n","     266784/2000000000: episode: 7340, duration: 4.135s, episode steps:  32, steps per second:   8, episode reward: 64.400, mean reward:  2.012 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 83.616703, mean_q: 44.691702, mean_eps: 0.100000\n","     266822/2000000000: episode: 7341, duration: 5.220s, episode steps:  38, steps per second:   7, episode reward: 57.800, mean reward:  1.521 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 78.203528, mean_q: 43.251154, mean_eps: 0.100000\n","     266857/2000000000: episode: 7342, duration: 4.730s, episode steps:  35, steps per second:   7, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 80.385034, mean_q: 43.288504, mean_eps: 0.100000\n","     266896/2000000000: episode: 7343, duration: 4.936s, episode steps:  39, steps per second:   8, episode reward: 44.800, mean reward:  1.149 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 76.648004, mean_q: 43.008272, mean_eps: 0.100000\n","     266931/2000000000: episode: 7344, duration: 4.463s, episode steps:  35, steps per second:   8, episode reward: 18.000, mean reward:  0.514 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 91.568170, mean_q: 43.872676, mean_eps: 0.100000\n","     266962/2000000000: episode: 7345, duration: 4.300s, episode steps:  31, steps per second:   7, episode reward: 83.600, mean reward:  2.697 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 79.515288, mean_q: 43.228165, mean_eps: 0.100000\n","     266996/2000000000: episode: 7346, duration: 4.738s, episode steps:  34, steps per second:   7, episode reward: 167.800, mean reward:  4.935 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 73.915713, mean_q: 42.966484, mean_eps: 0.100000\n","     267029/2000000000: episode: 7347, duration: 4.403s, episode steps:  33, steps per second:   7, episode reward: 18.500, mean reward:  0.561 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 80.854853, mean_q: 43.887046, mean_eps: 0.100000\n","     267069/2000000000: episode: 7348, duration: 5.179s, episode steps:  40, steps per second:   8, episode reward: 155.900, mean reward:  3.897 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 89.647549, mean_q: 43.911487, mean_eps: 0.100000\n","     267101/2000000000: episode: 7349, duration: 4.223s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 83.250927, mean_q: 44.449874, mean_eps: 0.100000\n","     267138/2000000000: episode: 7350, duration: 4.770s, episode steps:  37, steps per second:   8, episode reward: 153.700, mean reward:  4.154 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 75.140005, mean_q: 44.360436, mean_eps: 0.100000\n","     267165/2000000000: episode: 7351, duration: 3.772s, episode steps:  27, steps per second:   7, episode reward: 135.600, mean reward:  5.022 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 77.190201, mean_q: 43.524083, mean_eps: 0.100000\n","     267190/2000000000: episode: 7352, duration: 3.357s, episode steps:  25, steps per second:   7, episode reward: 88.100, mean reward:  3.524 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 85.547916, mean_q: 43.732637, mean_eps: 0.100000\n","     267224/2000000000: episode: 7353, duration: 4.504s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 77.487751, mean_q: 43.505000, mean_eps: 0.100000\n","     267257/2000000000: episode: 7354, duration: 4.293s, episode steps:  33, steps per second:   8, episode reward: 103.200, mean reward:  3.127 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 71.222102, mean_q: 44.709868, mean_eps: 0.100000\n","     267293/2000000000: episode: 7355, duration: 4.660s, episode steps:  36, steps per second:   8, episode reward:  9.100, mean reward:  0.253 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 84.028434, mean_q: 43.747379, mean_eps: 0.100000\n","     267317/2000000000: episode: 7356, duration: 3.231s, episode steps:  24, steps per second:   7, episode reward: 158.600, mean reward:  6.608 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 88.567001, mean_q: 44.259244, mean_eps: 0.100000\n","     267352/2000000000: episode: 7357, duration: 4.337s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.263980, mean_q: 43.370781, mean_eps: 0.100000\n","     267377/2000000000: episode: 7358, duration: 3.182s, episode steps:  25, steps per second:   8, episode reward: 178.600, mean reward:  7.144 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 83.223244, mean_q: 44.012818, mean_eps: 0.100000\n","     267413/2000000000: episode: 7359, duration: 4.585s, episode steps:  36, steps per second:   8, episode reward: 88.000, mean reward:  2.444 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 80.482752, mean_q: 43.918233, mean_eps: 0.100000\n","     267441/2000000000: episode: 7360, duration: 3.587s, episode steps:  28, steps per second:   8, episode reward: 44.200, mean reward:  1.579 [-20.000, 18.000], mean action: 0.679 [0.000, 2.000],  loss: 85.266635, mean_q: 43.814319, mean_eps: 0.100000\n","     267473/2000000000: episode: 7361, duration: 4.145s, episode steps:  32, steps per second:   8, episode reward: 50.200, mean reward:  1.569 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 81.601043, mean_q: 44.770142, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     267499/2000000000: episode: 7362, duration: 3.310s, episode steps:  26, steps per second:   8, episode reward: 15.500, mean reward:  0.596 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 81.508057, mean_q: 44.681798, mean_eps: 0.100000\n","     267533/2000000000: episode: 7363, duration: 4.327s, episode steps:  34, steps per second:   8, episode reward: 59.600, mean reward:  1.753 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 83.654677, mean_q: 43.371268, mean_eps: 0.100000\n","     267563/2000000000: episode: 7364, duration: 3.864s, episode steps:  30, steps per second:   8, episode reward: 82.400, mean reward:  2.747 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 75.868666, mean_q: 43.882508, mean_eps: 0.100000\n","     267595/2000000000: episode: 7365, duration: 4.269s, episode steps:  32, steps per second:   7, episode reward: 69.700, mean reward:  2.178 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 85.841455, mean_q: 42.714306, mean_eps: 0.100000\n","     267632/2000000000: episode: 7366, duration: 4.973s, episode steps:  37, steps per second:   7, episode reward: 181.700, mean reward:  4.911 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 79.174747, mean_q: 43.987984, mean_eps: 0.100000\n","     267662/2000000000: episode: 7367, duration: 4.021s, episode steps:  30, steps per second:   7, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 85.104820, mean_q: 45.114744, mean_eps: 0.100000\n","     267690/2000000000: episode: 7368, duration: 3.730s, episode steps:  28, steps per second:   8, episode reward: 86.500, mean reward:  3.089 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 83.348053, mean_q: 43.970245, mean_eps: 0.100000\n","     267718/2000000000: episode: 7369, duration: 4.014s, episode steps:  28, steps per second:   7, episode reward: -20.300, mean reward: -0.725 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 80.313654, mean_q: 45.032753, mean_eps: 0.100000\n","     267757/2000000000: episode: 7370, duration: 5.073s, episode steps:  39, steps per second:   8, episode reward:  1.400, mean reward:  0.036 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 85.122209, mean_q: 43.721422, mean_eps: 0.100000\n","     267791/2000000000: episode: 7371, duration: 4.310s, episode steps:  34, steps per second:   8, episode reward: 105.900, mean reward:  3.115 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 76.979849, mean_q: 44.215295, mean_eps: 0.100000\n","     267820/2000000000: episode: 7372, duration: 3.593s, episode steps:  29, steps per second:   8, episode reward: 19.900, mean reward:  0.686 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 85.441128, mean_q: 44.461250, mean_eps: 0.100000\n","     267860/2000000000: episode: 7373, duration: 5.413s, episode steps:  40, steps per second:   7, episode reward: 119.000, mean reward:  2.975 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.069761, mean_q: 44.489007, mean_eps: 0.100000\n","     267900/2000000000: episode: 7374, duration: 5.633s, episode steps:  40, steps per second:   7, episode reward: -10.000, mean reward: -0.250 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.773713, mean_q: 44.423944, mean_eps: 0.100000\n","     267929/2000000000: episode: 7375, duration: 3.966s, episode steps:  29, steps per second:   7, episode reward: 180.600, mean reward:  6.228 [-20.000, 18.000], mean action: 0.759 [0.000, 2.000],  loss: 75.207458, mean_q: 44.202918, mean_eps: 0.100000\n","     267964/2000000000: episode: 7376, duration: 4.583s, episode steps:  35, steps per second:   8, episode reward: 37.700, mean reward:  1.077 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 76.570183, mean_q: 43.760974, mean_eps: 0.100000\n","     267994/2000000000: episode: 7377, duration: 3.996s, episode steps:  30, steps per second:   8, episode reward: -44.200, mean reward: -1.473 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 79.115767, mean_q: 44.287630, mean_eps: 0.100000\n","     268018/2000000000: episode: 7378, duration: 3.140s, episode steps:  24, steps per second:   8, episode reward: 28.700, mean reward:  1.196 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 85.232189, mean_q: 44.812902, mean_eps: 0.100000\n","     268049/2000000000: episode: 7379, duration: 4.240s, episode steps:  31, steps per second:   7, episode reward: 217.500, mean reward:  7.016 [-20.000, 18.100], mean action: 1.097 [0.000, 2.000],  loss: 88.572773, mean_q: 43.608949, mean_eps: 0.100000\n","     268087/2000000000: episode: 7380, duration: 5.128s, episode steps:  38, steps per second:   7, episode reward: 169.000, mean reward:  4.447 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 81.887502, mean_q: 44.120043, mean_eps: 0.100000\n","     268114/2000000000: episode: 7381, duration: 4.094s, episode steps:  27, steps per second:   7, episode reward: 205.400, mean reward:  7.607 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 86.821042, mean_q: 44.812472, mean_eps: 0.100000\n","     268139/2000000000: episode: 7382, duration: 3.712s, episode steps:  25, steps per second:   7, episode reward: -51.800, mean reward: -2.072 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 98.624142, mean_q: 43.591794, mean_eps: 0.100000\n","     268169/2000000000: episode: 7383, duration: 4.374s, episode steps:  30, steps per second:   7, episode reward: 209.600, mean reward:  6.987 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 91.669121, mean_q: 44.546429, mean_eps: 0.100000\n","     268198/2000000000: episode: 7384, duration: 3.957s, episode steps:  29, steps per second:   7, episode reward: 97.700, mean reward:  3.369 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 87.740456, mean_q: 44.069791, mean_eps: 0.100000\n","     268235/2000000000: episode: 7385, duration: 4.748s, episode steps:  37, steps per second:   8, episode reward: -69.900, mean reward: -1.889 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.641210, mean_q: 44.337182, mean_eps: 0.100000\n","     268266/2000000000: episode: 7386, duration: 3.753s, episode steps:  31, steps per second:   8, episode reward: 216.600, mean reward:  6.987 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 85.897565, mean_q: 43.257849, mean_eps: 0.100000\n","     268301/2000000000: episode: 7387, duration: 4.478s, episode steps:  35, steps per second:   8, episode reward: 42.100, mean reward:  1.203 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 86.682283, mean_q: 44.393151, mean_eps: 0.100000\n","     268339/2000000000: episode: 7388, duration: 4.753s, episode steps:  38, steps per second:   8, episode reward: 95.300, mean reward:  2.508 [-20.000, 19.000], mean action: 1.158 [0.000, 2.000],  loss: 77.434272, mean_q: 43.295733, mean_eps: 0.100000\n","     268371/2000000000: episode: 7389, duration: 4.121s, episode steps:  32, steps per second:   8, episode reward: 149.100, mean reward:  4.659 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 79.403423, mean_q: 43.204606, mean_eps: 0.100000\n","     268411/2000000000: episode: 7390, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: 106.000, mean reward:  2.650 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 73.491771, mean_q: 43.415110, mean_eps: 0.100000\n","     268451/2000000000: episode: 7391, duration: 5.184s, episode steps:  40, steps per second:   8, episode reward: -90.600, mean reward: -2.265 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.865017, mean_q: 44.029597, mean_eps: 0.100000\n","     268491/2000000000: episode: 7392, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: 121.000, mean reward:  3.025 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 82.312720, mean_q: 44.688705, mean_eps: 0.100000\n","     268530/2000000000: episode: 7393, duration: 4.972s, episode steps:  39, steps per second:   8, episode reward: 94.000, mean reward:  2.410 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 81.361703, mean_q: 44.149512, mean_eps: 0.100000\n","     268561/2000000000: episode: 7394, duration: 3.892s, episode steps:  31, steps per second:   8, episode reward: 93.100, mean reward:  3.003 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 79.315637, mean_q: 42.750093, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     268597/2000000000: episode: 7395, duration: 4.582s, episode steps:  36, steps per second:   8, episode reward: 236.800, mean reward:  6.578 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 78.656895, mean_q: 44.708053, mean_eps: 0.100000\n","     268623/2000000000: episode: 7396, duration: 3.215s, episode steps:  26, steps per second:   8, episode reward: 81.100, mean reward:  3.119 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 68.914609, mean_q: 43.985800, mean_eps: 0.100000\n","     268658/2000000000: episode: 7397, duration: 4.177s, episode steps:  35, steps per second:   8, episode reward: 32.100, mean reward:  0.917 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 82.026165, mean_q: 44.144984, mean_eps: 0.100000\n","     268687/2000000000: episode: 7398, duration: 3.740s, episode steps:  29, steps per second:   8, episode reward: -42.400, mean reward: -1.462 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.267656, mean_q: 44.499424, mean_eps: 0.100000\n","     268725/2000000000: episode: 7399, duration: 4.914s, episode steps:  38, steps per second:   8, episode reward: 230.400, mean reward:  6.063 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 83.018172, mean_q: 43.137133, mean_eps: 0.100000\n","     268759/2000000000: episode: 7400, duration: 4.443s, episode steps:  34, steps per second:   8, episode reward: 144.000, mean reward:  4.235 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 77.761177, mean_q: 44.063562, mean_eps: 0.100000\n","     268796/2000000000: episode: 7401, duration: 5.107s, episode steps:  37, steps per second:   7, episode reward: -15.000, mean reward: -0.405 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 86.193733, mean_q: 43.942694, mean_eps: 0.100000\n","     268820/2000000000: episode: 7402, duration: 3.162s, episode steps:  24, steps per second:   8, episode reward: 136.000, mean reward:  5.667 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 80.634818, mean_q: 44.187782, mean_eps: 0.100000\n","     268846/2000000000: episode: 7403, duration: 3.403s, episode steps:  26, steps per second:   8, episode reward: 75.600, mean reward:  2.908 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 78.895232, mean_q: 44.205895, mean_eps: 0.100000\n","     268874/2000000000: episode: 7404, duration: 3.655s, episode steps:  28, steps per second:   8, episode reward: 92.000, mean reward:  3.286 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 89.863945, mean_q: 43.566301, mean_eps: 0.100000\n","     268914/2000000000: episode: 7405, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 79.646590, mean_q: 44.297123, mean_eps: 0.100000\n","     268945/2000000000: episode: 7406, duration: 3.957s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 81.736016, mean_q: 43.248504, mean_eps: 0.100000\n","     268985/2000000000: episode: 7407, duration: 5.185s, episode steps:  40, steps per second:   8, episode reward:  8.600, mean reward:  0.215 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.137221, mean_q: 44.778989, mean_eps: 0.100000\n","     269015/2000000000: episode: 7408, duration: 3.851s, episode steps:  30, steps per second:   8, episode reward: -39.300, mean reward: -1.310 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 78.761057, mean_q: 44.270307, mean_eps: 0.100000\n","     269046/2000000000: episode: 7409, duration: 4.391s, episode steps:  31, steps per second:   7, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 73.628898, mean_q: 43.479419, mean_eps: 0.100000\n","     269084/2000000000: episode: 7410, duration: 5.338s, episode steps:  38, steps per second:   7, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 83.290057, mean_q: 43.740714, mean_eps: 0.100000\n","     269120/2000000000: episode: 7411, duration: 4.769s, episode steps:  36, steps per second:   8, episode reward: -10.200, mean reward: -0.283 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 82.453428, mean_q: 44.292245, mean_eps: 0.100000\n","     269154/2000000000: episode: 7412, duration: 4.501s, episode steps:  34, steps per second:   8, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 84.020624, mean_q: 43.590058, mean_eps: 0.100000\n","     269186/2000000000: episode: 7413, duration: 4.293s, episode steps:  32, steps per second:   7, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 78.890093, mean_q: 43.402990, mean_eps: 0.100000\n","     269221/2000000000: episode: 7414, duration: 4.604s, episode steps:  35, steps per second:   8, episode reward: -9.200, mean reward: -0.263 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 80.626790, mean_q: 44.014296, mean_eps: 0.100000\n","     269254/2000000000: episode: 7415, duration: 4.286s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 87.029778, mean_q: 44.047062, mean_eps: 0.100000\n","     269290/2000000000: episode: 7416, duration: 4.556s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.716183, mean_q: 44.715841, mean_eps: 0.100000\n","     269317/2000000000: episode: 7417, duration: 3.597s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 79.306150, mean_q: 43.934618, mean_eps: 0.100000\n","     269347/2000000000: episode: 7418, duration: 4.118s, episode steps:  30, steps per second:   7, episode reward: -58.000, mean reward: -1.933 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 79.801801, mean_q: 44.470377, mean_eps: 0.100000\n","     269380/2000000000: episode: 7419, duration: 4.505s, episode steps:  33, steps per second:   7, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 73.273910, mean_q: 44.939752, mean_eps: 0.100000\n","     269409/2000000000: episode: 7420, duration: 3.748s, episode steps:  29, steps per second:   8, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.952354, mean_q: 43.424253, mean_eps: 0.100000\n","     269449/2000000000: episode: 7421, duration: 5.472s, episode steps:  40, steps per second:   7, episode reward: 82.000, mean reward:  2.050 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.598269, mean_q: 43.402764, mean_eps: 0.100000\n","     269472/2000000000: episode: 7422, duration: 3.162s, episode steps:  23, steps per second:   7, episode reward: 24.700, mean reward:  1.074 [-20.000, 18.000], mean action: 0.522 [0.000, 2.000],  loss: 77.846903, mean_q: 43.811447, mean_eps: 0.100000\n","     269511/2000000000: episode: 7423, duration: 5.061s, episode steps:  39, steps per second:   8, episode reward: 36.400, mean reward:  0.933 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.493573, mean_q: 44.295287, mean_eps: 0.100000\n","     269551/2000000000: episode: 7424, duration: 5.266s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.715090, mean_q: 42.951191, mean_eps: 0.100000\n","     269591/2000000000: episode: 7425, duration: 5.247s, episode steps:  40, steps per second:   8, episode reward: 49.000, mean reward:  1.225 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 74.102970, mean_q: 43.543742, mean_eps: 0.100000\n","     269631/2000000000: episode: 7426, duration: 5.304s, episode steps:  40, steps per second:   8, episode reward: 104.900, mean reward:  2.622 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.952198, mean_q: 43.966193, mean_eps: 0.100000\n","     269664/2000000000: episode: 7427, duration: 4.605s, episode steps:  33, steps per second:   7, episode reward: 47.500, mean reward:  1.439 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.989550, mean_q: 45.029106, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     269698/2000000000: episode: 7428, duration: 4.954s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 74.639575, mean_q: 44.216708, mean_eps: 0.100000\n","     269735/2000000000: episode: 7429, duration: 5.268s, episode steps:  37, steps per second:   7, episode reward: 102.400, mean reward:  2.768 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 79.830370, mean_q: 44.057311, mean_eps: 0.100000\n","     269775/2000000000: episode: 7430, duration: 5.351s, episode steps:  40, steps per second:   7, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 76.023362, mean_q: 43.787427, mean_eps: 0.100000\n","     269805/2000000000: episode: 7431, duration: 3.837s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 73.409113, mean_q: 43.596603, mean_eps: 0.100000\n","     269845/2000000000: episode: 7432, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: 32.100, mean reward:  0.802 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 77.730326, mean_q: 44.200719, mean_eps: 0.100000\n","     269878/2000000000: episode: 7433, duration: 4.384s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 79.657906, mean_q: 44.429526, mean_eps: 0.100000\n","     269911/2000000000: episode: 7434, duration: 4.554s, episode steps:  33, steps per second:   7, episode reward: 60.200, mean reward:  1.824 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 79.198672, mean_q: 43.594867, mean_eps: 0.100000\n","     269941/2000000000: episode: 7435, duration: 3.880s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 82.370639, mean_q: 43.441297, mean_eps: 0.100000\n","     269968/2000000000: episode: 7436, duration: 3.539s, episode steps:  27, steps per second:   8, episode reward: 158.800, mean reward:  5.881 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 76.690982, mean_q: 43.355054, mean_eps: 0.100000\n","     269998/2000000000: episode: 7437, duration: 3.862s, episode steps:  30, steps per second:   8, episode reward: 28.700, mean reward:  0.957 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 74.932212, mean_q: 44.041657, mean_eps: 0.100000\n","     270038/2000000000: episode: 7438, duration: 5.054s, episode steps:  40, steps per second:   8, episode reward: 47.000, mean reward:  1.175 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.619300, mean_q: 45.343535, mean_eps: 0.100000\n","     270078/2000000000: episode: 7439, duration: 5.184s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 92.736082, mean_q: 45.834933, mean_eps: 0.100000\n","     270108/2000000000: episode: 7440, duration: 3.930s, episode steps:  30, steps per second:   8, episode reward: 43.300, mean reward:  1.443 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 95.433562, mean_q: 44.777438, mean_eps: 0.100000\n","     270148/2000000000: episode: 7441, duration: 5.295s, episode steps:  40, steps per second:   8, episode reward: 92.700, mean reward:  2.317 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 101.664083, mean_q: 45.349817, mean_eps: 0.100000\n","     270181/2000000000: episode: 7442, duration: 4.341s, episode steps:  33, steps per second:   8, episode reward: 125.300, mean reward:  3.797 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 93.995040, mean_q: 46.470423, mean_eps: 0.100000\n","     270218/2000000000: episode: 7443, duration: 4.822s, episode steps:  37, steps per second:   8, episode reward: 105.800, mean reward:  2.859 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 86.724290, mean_q: 44.925836, mean_eps: 0.100000\n","     270251/2000000000: episode: 7444, duration: 4.302s, episode steps:  33, steps per second:   8, episode reward: 71.300, mean reward:  2.161 [-20.000, 18.400], mean action: 1.091 [0.000, 2.000],  loss: 96.298618, mean_q: 44.806084, mean_eps: 0.100000\n","     270291/2000000000: episode: 7445, duration: 5.436s, episode steps:  40, steps per second:   7, episode reward: 60.900, mean reward:  1.522 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 89.682440, mean_q: 46.574505, mean_eps: 0.100000\n","     270327/2000000000: episode: 7446, duration: 4.726s, episode steps:  36, steps per second:   8, episode reward: 42.400, mean reward:  1.178 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 88.581618, mean_q: 45.787693, mean_eps: 0.100000\n","     270363/2000000000: episode: 7447, duration: 4.855s, episode steps:  36, steps per second:   7, episode reward: 164.400, mean reward:  4.567 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 79.680274, mean_q: 46.172934, mean_eps: 0.100000\n","     270400/2000000000: episode: 7448, duration: 5.263s, episode steps:  37, steps per second:   7, episode reward: 20.600, mean reward:  0.557 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 90.551586, mean_q: 45.261930, mean_eps: 0.100000\n","     270432/2000000000: episode: 7449, duration: 4.463s, episode steps:  32, steps per second:   7, episode reward: 117.200, mean reward:  3.662 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 86.078378, mean_q: 45.914071, mean_eps: 0.100000\n","     270461/2000000000: episode: 7450, duration: 3.815s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 88.391742, mean_q: 46.020926, mean_eps: 0.100000\n","     270499/2000000000: episode: 7451, duration: 5.252s, episode steps:  38, steps per second:   7, episode reward: 178.100, mean reward:  4.687 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 90.827802, mean_q: 46.907305, mean_eps: 0.100000\n","     270525/2000000000: episode: 7452, duration: 3.922s, episode steps:  26, steps per second:   7, episode reward: 60.400, mean reward:  2.323 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 90.037721, mean_q: 44.835136, mean_eps: 0.100000\n","     270559/2000000000: episode: 7453, duration: 4.628s, episode steps:  34, steps per second:   7, episode reward: 105.800, mean reward:  3.112 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 97.677065, mean_q: 46.049550, mean_eps: 0.100000\n","     270599/2000000000: episode: 7454, duration: 5.327s, episode steps:  40, steps per second:   8, episode reward: 181.400, mean reward:  4.535 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.200364, mean_q: 46.276662, mean_eps: 0.100000\n","     270627/2000000000: episode: 7455, duration: 3.780s, episode steps:  28, steps per second:   7, episode reward: 240.400, mean reward:  8.586 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.425703, mean_q: 46.878790, mean_eps: 0.100000\n","     270662/2000000000: episode: 7456, duration: 4.835s, episode steps:  35, steps per second:   7, episode reward: 42.200, mean reward:  1.206 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 87.818479, mean_q: 45.910417, mean_eps: 0.100000\n","     270691/2000000000: episode: 7457, duration: 4.084s, episode steps:  29, steps per second:   7, episode reward: 76.100, mean reward:  2.624 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 82.255828, mean_q: 45.869884, mean_eps: 0.100000\n","     270720/2000000000: episode: 7458, duration: 3.925s, episode steps:  29, steps per second:   7, episode reward: 182.500, mean reward:  6.293 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 90.866892, mean_q: 44.786004, mean_eps: 0.100000\n","     270754/2000000000: episode: 7459, duration: 4.461s, episode steps:  34, steps per second:   8, episode reward: 205.500, mean reward:  6.044 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 85.146108, mean_q: 45.443624, mean_eps: 0.100000\n","     270786/2000000000: episode: 7460, duration: 4.291s, episode steps:  32, steps per second:   7, episode reward: 61.200, mean reward:  1.912 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 84.935933, mean_q: 45.320886, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     270815/2000000000: episode: 7461, duration: 3.694s, episode steps:  29, steps per second:   8, episode reward: -39.500, mean reward: -1.362 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 85.213753, mean_q: 46.030063, mean_eps: 0.100000\n","     270844/2000000000: episode: 7462, duration: 3.713s, episode steps:  29, steps per second:   8, episode reward: 127.500, mean reward:  4.397 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 81.625442, mean_q: 45.803580, mean_eps: 0.100000\n","     270876/2000000000: episode: 7463, duration: 4.294s, episode steps:  32, steps per second:   7, episode reward: 145.400, mean reward:  4.544 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 88.390247, mean_q: 46.159325, mean_eps: 0.100000\n","     270907/2000000000: episode: 7464, duration: 3.944s, episode steps:  31, steps per second:   8, episode reward: -55.100, mean reward: -1.777 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 80.678581, mean_q: 46.440547, mean_eps: 0.100000\n","     270947/2000000000: episode: 7465, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.668759, mean_q: 45.048974, mean_eps: 0.100000\n","     270979/2000000000: episode: 7466, duration: 4.323s, episode steps:  32, steps per second:   7, episode reward: 181.300, mean reward:  5.666 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.291149, mean_q: 46.238028, mean_eps: 0.100000\n","     271019/2000000000: episode: 7467, duration: 4.937s, episode steps:  40, steps per second:   8, episode reward: 18.800, mean reward:  0.470 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 82.380790, mean_q: 45.461066, mean_eps: 0.100000\n","     271047/2000000000: episode: 7468, duration: 3.581s, episode steps:  28, steps per second:   8, episode reward: 131.900, mean reward:  4.711 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.879903, mean_q: 45.469608, mean_eps: 0.100000\n","     271085/2000000000: episode: 7469, duration: 4.653s, episode steps:  38, steps per second:   8, episode reward: 219.400, mean reward:  5.774 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 86.969904, mean_q: 45.305294, mean_eps: 0.100000\n","     271123/2000000000: episode: 7470, duration: 4.961s, episode steps:  38, steps per second:   8, episode reward: 77.600, mean reward:  2.042 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 81.622665, mean_q: 45.093210, mean_eps: 0.100000\n","     271157/2000000000: episode: 7471, duration: 4.392s, episode steps:  34, steps per second:   8, episode reward: 170.900, mean reward:  5.026 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 89.065099, mean_q: 46.036630, mean_eps: 0.100000\n","     271194/2000000000: episode: 7472, duration: 4.922s, episode steps:  37, steps per second:   8, episode reward: 134.400, mean reward:  3.632 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 88.677830, mean_q: 45.361908, mean_eps: 0.100000\n","     271226/2000000000: episode: 7473, duration: 4.235s, episode steps:  32, steps per second:   8, episode reward: 178.700, mean reward:  5.584 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 92.558528, mean_q: 45.102208, mean_eps: 0.100000\n","     271264/2000000000: episode: 7474, duration: 5.005s, episode steps:  38, steps per second:   8, episode reward: 88.400, mean reward:  2.326 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 87.831369, mean_q: 44.987330, mean_eps: 0.100000\n","     271301/2000000000: episode: 7475, duration: 4.718s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 88.754443, mean_q: 45.897886, mean_eps: 0.100000\n","     271339/2000000000: episode: 7476, duration: 5.189s, episode steps:  38, steps per second:   7, episode reward: -4.100, mean reward: -0.108 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 81.612033, mean_q: 46.569407, mean_eps: 0.100000\n","     271372/2000000000: episode: 7477, duration: 4.316s, episode steps:  33, steps per second:   8, episode reward: 118.500, mean reward:  3.591 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 86.735166, mean_q: 45.604724, mean_eps: 0.100000\n","     271410/2000000000: episode: 7478, duration: 4.937s, episode steps:  38, steps per second:   8, episode reward: 163.300, mean reward:  4.297 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 86.949511, mean_q: 45.571122, mean_eps: 0.100000\n","     271442/2000000000: episode: 7479, duration: 4.190s, episode steps:  32, steps per second:   8, episode reward: 112.300, mean reward:  3.509 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 82.578989, mean_q: 45.904768, mean_eps: 0.100000\n","     271481/2000000000: episode: 7480, duration: 5.082s, episode steps:  39, steps per second:   8, episode reward: 117.700, mean reward:  3.018 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 89.129222, mean_q: 45.272142, mean_eps: 0.100000\n","     271521/2000000000: episode: 7481, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 116.000, mean reward:  2.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 88.297345, mean_q: 46.026741, mean_eps: 0.100000\n","     271558/2000000000: episode: 7482, duration: 4.843s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 91.667693, mean_q: 45.943353, mean_eps: 0.100000\n","     271590/2000000000: episode: 7483, duration: 4.346s, episode steps:  32, steps per second:   7, episode reward: 75.900, mean reward:  2.372 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 92.787239, mean_q: 45.548107, mean_eps: 0.100000\n","     271628/2000000000: episode: 7484, duration: 5.111s, episode steps:  38, steps per second:   7, episode reward: 51.400, mean reward:  1.353 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 85.849773, mean_q: 45.717161, mean_eps: 0.100000\n","     271661/2000000000: episode: 7485, duration: 4.433s, episode steps:  33, steps per second:   7, episode reward: 146.000, mean reward:  4.424 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 79.688083, mean_q: 46.007846, mean_eps: 0.100000\n","     271694/2000000000: episode: 7486, duration: 4.383s, episode steps:  33, steps per second:   8, episode reward: 41.200, mean reward:  1.248 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.172066, mean_q: 45.027613, mean_eps: 0.100000\n","     271732/2000000000: episode: 7487, duration: 5.016s, episode steps:  38, steps per second:   8, episode reward: -32.400, mean reward: -0.853 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 80.063608, mean_q: 46.620371, mean_eps: 0.100000\n","     271764/2000000000: episode: 7488, duration: 4.219s, episode steps:  32, steps per second:   8, episode reward: 82.500, mean reward:  2.578 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 73.449413, mean_q: 46.283336, mean_eps: 0.100000\n","     271804/2000000000: episode: 7489, duration: 5.055s, episode steps:  40, steps per second:   8, episode reward: -11.600, mean reward: -0.290 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 84.395702, mean_q: 45.236028, mean_eps: 0.100000\n","     271841/2000000000: episode: 7490, duration: 4.612s, episode steps:  37, steps per second:   8, episode reward: 103.000, mean reward:  2.784 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 84.532591, mean_q: 45.261534, mean_eps: 0.100000\n","     271872/2000000000: episode: 7491, duration: 3.997s, episode steps:  31, steps per second:   8, episode reward: -5.300, mean reward: -0.171 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 78.841461, mean_q: 46.492494, mean_eps: 0.100000\n","     271906/2000000000: episode: 7492, duration: 4.368s, episode steps:  34, steps per second:   8, episode reward: 179.100, mean reward:  5.268 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 81.810785, mean_q: 45.172805, mean_eps: 0.100000\n","     271945/2000000000: episode: 7493, duration: 5.098s, episode steps:  39, steps per second:   8, episode reward: 77.400, mean reward:  1.985 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 89.146067, mean_q: 45.299120, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     271985/2000000000: episode: 7494, duration: 5.134s, episode steps:  40, steps per second:   8, episode reward: 25.000, mean reward:  0.625 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 85.710182, mean_q: 45.444491, mean_eps: 0.100000\n","     272025/2000000000: episode: 7495, duration: 5.409s, episode steps:  40, steps per second:   7, episode reward: 104.200, mean reward:  2.605 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.932614, mean_q: 45.385619, mean_eps: 0.100000\n","     272054/2000000000: episode: 7496, duration: 3.660s, episode steps:  29, steps per second:   8, episode reward: -15.500, mean reward: -0.534 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 78.236636, mean_q: 45.189191, mean_eps: 0.100000\n","     272087/2000000000: episode: 7497, duration: 4.292s, episode steps:  33, steps per second:   8, episode reward: 163.200, mean reward:  4.945 [-20.000, 18.600], mean action: 1.000 [0.000, 2.000],  loss: 90.717648, mean_q: 44.824346, mean_eps: 0.100000\n","     272119/2000000000: episode: 7498, duration: 4.300s, episode steps:  32, steps per second:   7, episode reward: 193.600, mean reward:  6.050 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 84.741759, mean_q: 46.302242, mean_eps: 0.100000\n","     272154/2000000000: episode: 7499, duration: 4.621s, episode steps:  35, steps per second:   8, episode reward: -13.500, mean reward: -0.386 [-20.000, 18.000], mean action: 1.286 [0.000, 2.000],  loss: 84.244762, mean_q: 45.102067, mean_eps: 0.100000\n","     272188/2000000000: episode: 7500, duration: 4.364s, episode steps:  34, steps per second:   8, episode reward: 141.300, mean reward:  4.156 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 81.425481, mean_q: 45.456907, mean_eps: 0.100000\n","     272221/2000000000: episode: 7501, duration: 3.999s, episode steps:  33, steps per second:   8, episode reward: 91.500, mean reward:  2.773 [-20.000, 18.000], mean action: 1.273 [0.000, 2.000],  loss: 86.782568, mean_q: 45.815224, mean_eps: 0.100000\n","     272250/2000000000: episode: 7502, duration: 3.766s, episode steps:  29, steps per second:   8, episode reward: 147.200, mean reward:  5.076 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 83.975304, mean_q: 45.447725, mean_eps: 0.100000\n","     272276/2000000000: episode: 7503, duration: 3.355s, episode steps:  26, steps per second:   8, episode reward: 71.900, mean reward:  2.765 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 89.840278, mean_q: 45.328402, mean_eps: 0.100000\n","     272313/2000000000: episode: 7504, duration: 4.759s, episode steps:  37, steps per second:   8, episode reward: 69.800, mean reward:  1.886 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.989645, mean_q: 46.279974, mean_eps: 0.100000\n","     272343/2000000000: episode: 7505, duration: 3.928s, episode steps:  30, steps per second:   8, episode reward: 235.300, mean reward:  7.843 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 84.199119, mean_q: 45.614664, mean_eps: 0.100000\n","     272380/2000000000: episode: 7506, duration: 4.838s, episode steps:  37, steps per second:   8, episode reward: 110.000, mean reward:  2.973 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 90.623854, mean_q: 46.333448, mean_eps: 0.100000\n","     272414/2000000000: episode: 7507, duration: 4.379s, episode steps:  34, steps per second:   8, episode reward: 233.600, mean reward:  6.871 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 85.499579, mean_q: 45.908552, mean_eps: 0.100000\n","     272445/2000000000: episode: 7508, duration: 3.891s, episode steps:  31, steps per second:   8, episode reward: 166.400, mean reward:  5.368 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.930458, mean_q: 44.186137, mean_eps: 0.100000\n","     272478/2000000000: episode: 7509, duration: 4.056s, episode steps:  33, steps per second:   8, episode reward: 65.400, mean reward:  1.982 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.845009, mean_q: 45.885225, mean_eps: 0.100000\n","     272515/2000000000: episode: 7510, duration: 4.689s, episode steps:  37, steps per second:   8, episode reward: 161.100, mean reward:  4.354 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 90.750856, mean_q: 45.393248, mean_eps: 0.100000\n","     272551/2000000000: episode: 7511, duration: 4.512s, episode steps:  36, steps per second:   8, episode reward: 153.700, mean reward:  4.269 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 88.192698, mean_q: 45.526863, mean_eps: 0.100000\n","     272582/2000000000: episode: 7512, duration: 3.906s, episode steps:  31, steps per second:   8, episode reward:  9.300, mean reward:  0.300 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 77.234930, mean_q: 45.938369, mean_eps: 0.100000\n","     272608/2000000000: episode: 7513, duration: 3.325s, episode steps:  26, steps per second:   8, episode reward: 176.500, mean reward:  6.788 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 77.929481, mean_q: 45.797365, mean_eps: 0.100000\n","     272648/2000000000: episode: 7514, duration: 5.097s, episode steps:  40, steps per second:   8, episode reward: 37.700, mean reward:  0.942 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.358699, mean_q: 44.869016, mean_eps: 0.100000\n","     272682/2000000000: episode: 7515, duration: 4.291s, episode steps:  34, steps per second:   8, episode reward: 87.600, mean reward:  2.576 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 83.799287, mean_q: 46.280427, mean_eps: 0.100000\n","     272714/2000000000: episode: 7516, duration: 3.948s, episode steps:  32, steps per second:   8, episode reward: -0.900, mean reward: -0.028 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 88.233371, mean_q: 45.932930, mean_eps: 0.100000\n","     272746/2000000000: episode: 7517, duration: 4.028s, episode steps:  32, steps per second:   8, episode reward: 174.100, mean reward:  5.441 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.103208, mean_q: 46.559518, mean_eps: 0.100000\n","     272783/2000000000: episode: 7518, duration: 4.719s, episode steps:  37, steps per second:   8, episode reward: 160.400, mean reward:  4.335 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 94.316391, mean_q: 44.941575, mean_eps: 0.100000\n","     272821/2000000000: episode: 7519, duration: 4.675s, episode steps:  38, steps per second:   8, episode reward: 110.400, mean reward:  2.905 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 80.383516, mean_q: 46.709940, mean_eps: 0.100000\n","     272861/2000000000: episode: 7520, duration: 5.079s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.927429, mean_q: 45.399538, mean_eps: 0.100000\n","     272889/2000000000: episode: 7521, duration: 3.645s, episode steps:  28, steps per second:   8, episode reward: 119.100, mean reward:  4.254 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 75.241543, mean_q: 46.128573, mean_eps: 0.100000\n","     272929/2000000000: episode: 7522, duration: 5.197s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.402442, mean_q: 45.514396, mean_eps: 0.100000\n","     272964/2000000000: episode: 7523, duration: 4.572s, episode steps:  35, steps per second:   8, episode reward: 263.700, mean reward:  7.534 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 88.506707, mean_q: 45.460419, mean_eps: 0.100000\n","     273004/2000000000: episode: 7524, duration: 5.142s, episode steps:  40, steps per second:   8, episode reward: 41.900, mean reward:  1.047 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.338382, mean_q: 45.170939, mean_eps: 0.100000\n","     273037/2000000000: episode: 7525, duration: 4.352s, episode steps:  33, steps per second:   8, episode reward: 149.700, mean reward:  4.536 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.548772, mean_q: 45.634842, mean_eps: 0.100000\n","     273076/2000000000: episode: 7526, duration: 4.967s, episode steps:  39, steps per second:   8, episode reward: 51.600, mean reward:  1.323 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 81.740132, mean_q: 45.763613, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     273105/2000000000: episode: 7527, duration: 3.719s, episode steps:  29, steps per second:   8, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 85.851882, mean_q: 44.660741, mean_eps: 0.100000\n","     273144/2000000000: episode: 7528, duration: 5.059s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 78.729842, mean_q: 46.500501, mean_eps: 0.100000\n","     273179/2000000000: episode: 7529, duration: 4.284s, episode steps:  35, steps per second:   8, episode reward: 54.800, mean reward:  1.566 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 86.136334, mean_q: 44.982413, mean_eps: 0.100000\n","     273219/2000000000: episode: 7530, duration: 5.530s, episode steps:  40, steps per second:   7, episode reward: -5.900, mean reward: -0.147 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 79.291642, mean_q: 46.547741, mean_eps: 0.100000\n","     273254/2000000000: episode: 7531, duration: 4.283s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 81.511334, mean_q: 45.784145, mean_eps: 0.100000\n","     273292/2000000000: episode: 7532, duration: 4.829s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 78.736216, mean_q: 46.149332, mean_eps: 0.100000\n","     273332/2000000000: episode: 7533, duration: 5.084s, episode steps:  40, steps per second:   8, episode reward: 38.200, mean reward:  0.955 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 88.708847, mean_q: 45.186847, mean_eps: 0.100000\n","     273371/2000000000: episode: 7534, duration: 5.114s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 88.339373, mean_q: 46.189374, mean_eps: 0.100000\n","     273408/2000000000: episode: 7535, duration: 4.706s, episode steps:  37, steps per second:   8, episode reward: -97.700, mean reward: -2.641 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.397452, mean_q: 45.965701, mean_eps: 0.100000\n","     273435/2000000000: episode: 7536, duration: 3.406s, episode steps:  27, steps per second:   8, episode reward: -58.000, mean reward: -2.148 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 81.627648, mean_q: 45.032088, mean_eps: 0.100000\n","     273467/2000000000: episode: 7537, duration: 4.054s, episode steps:  32, steps per second:   8, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 84.270207, mean_q: 45.640958, mean_eps: 0.100000\n","     273507/2000000000: episode: 7538, duration: 5.049s, episode steps:  40, steps per second:   8, episode reward: -74.400, mean reward: -1.860 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 92.363226, mean_q: 45.395987, mean_eps: 0.100000\n","     273537/2000000000: episode: 7539, duration: 4.162s, episode steps:  30, steps per second:   7, episode reward: 155.700, mean reward:  5.190 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 82.833262, mean_q: 46.086726, mean_eps: 0.100000\n","     273574/2000000000: episode: 7540, duration: 4.707s, episode steps:  37, steps per second:   8, episode reward: 48.800, mean reward:  1.319 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 84.499499, mean_q: 46.400012, mean_eps: 0.100000\n","     273614/2000000000: episode: 7541, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 82.287309, mean_q: 45.529519, mean_eps: 0.100000\n","     273644/2000000000: episode: 7542, duration: 3.876s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.851783, mean_q: 45.807164, mean_eps: 0.100000\n","     273680/2000000000: episode: 7543, duration: 4.689s, episode steps:  36, steps per second:   8, episode reward: 138.600, mean reward:  3.850 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 82.306579, mean_q: 46.644118, mean_eps: 0.100000\n","     273712/2000000000: episode: 7544, duration: 4.201s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 92.010230, mean_q: 45.387351, mean_eps: 0.100000\n","     273747/2000000000: episode: 7545, duration: 4.527s, episode steps:  35, steps per second:   8, episode reward: 45.100, mean reward:  1.289 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 82.399224, mean_q: 44.506848, mean_eps: 0.100000\n","     273776/2000000000: episode: 7546, duration: 3.756s, episode steps:  29, steps per second:   8, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 82.807215, mean_q: 46.034179, mean_eps: 0.100000\n","     273799/2000000000: episode: 7547, duration: 2.827s, episode steps:  23, steps per second:   8, episode reward: -3.100, mean reward: -0.135 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 80.682210, mean_q: 46.327337, mean_eps: 0.100000\n","     273839/2000000000: episode: 7548, duration: 4.822s, episode steps:  40, steps per second:   8, episode reward: 28.900, mean reward:  0.723 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 89.325667, mean_q: 45.995092, mean_eps: 0.100000\n","     273870/2000000000: episode: 7549, duration: 3.756s, episode steps:  31, steps per second:   8, episode reward: 196.500, mean reward:  6.339 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 86.883488, mean_q: 46.153226, mean_eps: 0.100000\n","     273910/2000000000: episode: 7550, duration: 4.903s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 94.832718, mean_q: 44.649401, mean_eps: 0.100000\n","     273942/2000000000: episode: 7551, duration: 3.886s, episode steps:  32, steps per second:   8, episode reward: 17.200, mean reward:  0.537 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 85.667973, mean_q: 46.364647, mean_eps: 0.100000\n","     273977/2000000000: episode: 7552, duration: 4.324s, episode steps:  35, steps per second:   8, episode reward: -28.000, mean reward: -0.800 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 88.502633, mean_q: 45.603143, mean_eps: 0.100000\n","     274015/2000000000: episode: 7553, duration: 4.729s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 84.386498, mean_q: 46.034667, mean_eps: 0.100000\n","     274049/2000000000: episode: 7554, duration: 4.517s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 90.947693, mean_q: 45.376643, mean_eps: 0.100000\n","     274083/2000000000: episode: 7555, duration: 4.321s, episode steps:  34, steps per second:   8, episode reward: -96.000, mean reward: -2.824 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 88.248319, mean_q: 44.799108, mean_eps: 0.100000\n","     274116/2000000000: episode: 7556, duration: 4.302s, episode steps:  33, steps per second:   8, episode reward:  7.600, mean reward:  0.230 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.545943, mean_q: 45.316566, mean_eps: 0.100000\n","     274153/2000000000: episode: 7557, duration: 4.906s, episode steps:  37, steps per second:   8, episode reward: 153.500, mean reward:  4.149 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 86.273716, mean_q: 46.296849, mean_eps: 0.100000\n","     274185/2000000000: episode: 7558, duration: 4.143s, episode steps:  32, steps per second:   8, episode reward: 42.000, mean reward:  1.312 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 79.382415, mean_q: 45.342632, mean_eps: 0.100000\n","     274222/2000000000: episode: 7559, duration: 4.682s, episode steps:  37, steps per second:   8, episode reward: 97.800, mean reward:  2.643 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 94.036758, mean_q: 45.909627, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     274248/2000000000: episode: 7560, duration: 3.127s, episode steps:  26, steps per second:   8, episode reward: 138.400, mean reward:  5.323 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 71.383599, mean_q: 45.568960, mean_eps: 0.100000\n","     274282/2000000000: episode: 7561, duration: 4.258s, episode steps:  34, steps per second:   8, episode reward: 97.700, mean reward:  2.874 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 78.319488, mean_q: 45.697923, mean_eps: 0.100000\n","     274311/2000000000: episode: 7562, duration: 3.636s, episode steps:  29, steps per second:   8, episode reward: 158.700, mean reward:  5.472 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 82.446616, mean_q: 45.024939, mean_eps: 0.100000\n","     274343/2000000000: episode: 7563, duration: 4.074s, episode steps:  32, steps per second:   8, episode reward: 158.000, mean reward:  4.937 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.882407, mean_q: 46.305251, mean_eps: 0.100000\n","     274383/2000000000: episode: 7564, duration: 5.192s, episode steps:  40, steps per second:   8, episode reward: -96.000, mean reward: -2.400 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 90.252613, mean_q: 45.728111, mean_eps: 0.100000\n","     274416/2000000000: episode: 7565, duration: 4.323s, episode steps:  33, steps per second:   8, episode reward: 69.500, mean reward:  2.106 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 86.581783, mean_q: 45.461815, mean_eps: 0.100000\n","     274454/2000000000: episode: 7566, duration: 4.715s, episode steps:  38, steps per second:   8, episode reward: 157.300, mean reward:  4.139 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 87.940837, mean_q: 45.989164, mean_eps: 0.100000\n","     274489/2000000000: episode: 7567, duration: 4.323s, episode steps:  35, steps per second:   8, episode reward: -59.800, mean reward: -1.709 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 93.335823, mean_q: 45.329240, mean_eps: 0.100000\n","     274523/2000000000: episode: 7568, duration: 4.252s, episode steps:  34, steps per second:   8, episode reward: 48.300, mean reward:  1.421 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 81.594218, mean_q: 45.957493, mean_eps: 0.100000\n","     274556/2000000000: episode: 7569, duration: 4.158s, episode steps:  33, steps per second:   8, episode reward: 26.500, mean reward:  0.803 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 76.855790, mean_q: 46.005868, mean_eps: 0.100000\n","     274590/2000000000: episode: 7570, duration: 4.301s, episode steps:  34, steps per second:   8, episode reward: -27.300, mean reward: -0.803 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 77.114424, mean_q: 46.025604, mean_eps: 0.100000\n","     274630/2000000000: episode: 7571, duration: 5.123s, episode steps:  40, steps per second:   8, episode reward: 91.900, mean reward:  2.298 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 91.982395, mean_q: 45.462978, mean_eps: 0.100000\n","     274668/2000000000: episode: 7572, duration: 4.886s, episode steps:  38, steps per second:   8, episode reward: -58.000, mean reward: -1.526 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 82.987700, mean_q: 46.236367, mean_eps: 0.100000\n","     274705/2000000000: episode: 7573, duration: 4.482s, episode steps:  37, steps per second:   8, episode reward: 162.400, mean reward:  4.389 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 82.369507, mean_q: 46.043350, mean_eps: 0.100000\n","     274727/2000000000: episode: 7574, duration: 2.717s, episode steps:  22, steps per second:   8, episode reward: -58.000, mean reward: -2.636 [-20.000, 18.000], mean action: 0.727 [0.000, 2.000],  loss: 88.704188, mean_q: 46.253489, mean_eps: 0.100000\n","     274761/2000000000: episode: 7575, duration: 4.425s, episode steps:  34, steps per second:   8, episode reward: -16.200, mean reward: -0.476 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 84.507546, mean_q: 44.808943, mean_eps: 0.100000\n","     274793/2000000000: episode: 7576, duration: 3.898s, episode steps:  32, steps per second:   8, episode reward: -1.800, mean reward: -0.056 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 88.771951, mean_q: 46.123088, mean_eps: 0.100000\n","     274833/2000000000: episode: 7577, duration: 4.767s, episode steps:  40, steps per second:   8, episode reward: 64.900, mean reward:  1.622 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 93.874910, mean_q: 44.900981, mean_eps: 0.100000\n","     274871/2000000000: episode: 7578, duration: 4.687s, episode steps:  38, steps per second:   8, episode reward: 68.500, mean reward:  1.803 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 85.271511, mean_q: 45.364865, mean_eps: 0.100000\n","     274900/2000000000: episode: 7579, duration: 3.655s, episode steps:  29, steps per second:   8, episode reward: 227.500, mean reward:  7.845 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 82.571095, mean_q: 45.364120, mean_eps: 0.100000\n","     274939/2000000000: episode: 7580, duration: 4.878s, episode steps:  39, steps per second:   8, episode reward:  2.400, mean reward:  0.062 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 87.088257, mean_q: 45.808225, mean_eps: 0.100000\n","     274973/2000000000: episode: 7581, duration: 4.277s, episode steps:  34, steps per second:   8, episode reward: 66.500, mean reward:  1.956 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.161181, mean_q: 45.818062, mean_eps: 0.100000\n","     275004/2000000000: episode: 7582, duration: 4.270s, episode steps:  31, steps per second:   7, episode reward: 24.200, mean reward:  0.781 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.279864, mean_q: 45.437311, mean_eps: 0.100000\n","     275037/2000000000: episode: 7583, duration: 4.331s, episode steps:  33, steps per second:   8, episode reward: -31.800, mean reward: -0.964 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 88.404014, mean_q: 45.241698, mean_eps: 0.100000\n","     275077/2000000000: episode: 7584, duration: 5.190s, episode steps:  40, steps per second:   8, episode reward: 91.100, mean reward:  2.278 [-20.000, 18.300], mean action: 1.200 [0.000, 2.000],  loss: 96.340578, mean_q: 45.297995, mean_eps: 0.100000\n","     275107/2000000000: episode: 7585, duration: 4.006s, episode steps:  30, steps per second:   7, episode reward: 95.400, mean reward:  3.180 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 87.193095, mean_q: 46.278464, mean_eps: 0.100000\n","     275139/2000000000: episode: 7586, duration: 4.283s, episode steps:  32, steps per second:   7, episode reward: 58.800, mean reward:  1.838 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.404083, mean_q: 46.304260, mean_eps: 0.100000\n","     275175/2000000000: episode: 7587, duration: 4.773s, episode steps:  36, steps per second:   8, episode reward: 80.400, mean reward:  2.233 [-20.000, 18.000], mean action: 1.361 [0.000, 2.000],  loss: 90.864393, mean_q: 44.848283, mean_eps: 0.100000\n","     275209/2000000000: episode: 7588, duration: 4.722s, episode steps:  34, steps per second:   7, episode reward: 241.100, mean reward:  7.091 [-20.000, 19.100], mean action: 1.088 [0.000, 2.000],  loss: 91.603043, mean_q: 45.537882, mean_eps: 0.100000\n","     275246/2000000000: episode: 7589, duration: 4.927s, episode steps:  37, steps per second:   8, episode reward: 46.600, mean reward:  1.259 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 87.440405, mean_q: 45.825148, mean_eps: 0.100000\n","     275278/2000000000: episode: 7590, duration: 4.257s, episode steps:  32, steps per second:   8, episode reward: 77.600, mean reward:  2.425 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 82.577767, mean_q: 46.445322, mean_eps: 0.100000\n","     275311/2000000000: episode: 7591, duration: 4.688s, episode steps:  33, steps per second:   7, episode reward: 38.000, mean reward:  1.152 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 84.892275, mean_q: 46.020508, mean_eps: 0.100000\n","     275343/2000000000: episode: 7592, duration: 4.325s, episode steps:  32, steps per second:   7, episode reward: 137.900, mean reward:  4.309 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 76.585002, mean_q: 46.088921, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     275369/2000000000: episode: 7593, duration: 3.441s, episode steps:  26, steps per second:   8, episode reward: 82.000, mean reward:  3.154 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 89.886411, mean_q: 45.321016, mean_eps: 0.100000\n","     275403/2000000000: episode: 7594, duration: 4.390s, episode steps:  34, steps per second:   8, episode reward: -20.800, mean reward: -0.612 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 82.324633, mean_q: 45.891609, mean_eps: 0.100000\n","     275443/2000000000: episode: 7595, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 100.300, mean reward:  2.507 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.658294, mean_q: 46.227318, mean_eps: 0.100000\n","     275468/2000000000: episode: 7596, duration: 3.145s, episode steps:  25, steps per second:   8, episode reward: 119.300, mean reward:  4.772 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 86.765076, mean_q: 46.432293, mean_eps: 0.100000\n","     275508/2000000000: episode: 7597, duration: 5.070s, episode steps:  40, steps per second:   8, episode reward: 17.600, mean reward:  0.440 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 85.304859, mean_q: 45.756164, mean_eps: 0.100000\n","     275546/2000000000: episode: 7598, duration: 4.705s, episode steps:  38, steps per second:   8, episode reward: 229.200, mean reward:  6.032 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 83.020910, mean_q: 45.568553, mean_eps: 0.100000\n","     275578/2000000000: episode: 7599, duration: 3.991s, episode steps:  32, steps per second:   8, episode reward: 235.900, mean reward:  7.372 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 85.320555, mean_q: 45.740799, mean_eps: 0.100000\n","     275606/2000000000: episode: 7600, duration: 3.515s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 85.849017, mean_q: 45.287408, mean_eps: 0.100000\n","     275638/2000000000: episode: 7601, duration: 4.131s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.823581, mean_q: 46.283209, mean_eps: 0.100000\n","     275669/2000000000: episode: 7602, duration: 4.028s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.440527, mean_q: 46.889163, mean_eps: 0.100000\n","     275690/2000000000: episode: 7603, duration: 2.780s, episode steps:  21, steps per second:   8, episode reward: 94.000, mean reward:  4.476 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 80.676436, mean_q: 46.059533, mean_eps: 0.100000\n","     275730/2000000000: episode: 7604, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 90.937566, mean_q: 45.651814, mean_eps: 0.100000\n","     275755/2000000000: episode: 7605, duration: 3.111s, episode steps:  25, steps per second:   8, episode reward: 139.200, mean reward:  5.568 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 78.788921, mean_q: 46.414040, mean_eps: 0.100000\n","     275793/2000000000: episode: 7606, duration: 4.868s, episode steps:  38, steps per second:   8, episode reward: 217.400, mean reward:  5.721 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 76.387148, mean_q: 45.859354, mean_eps: 0.100000\n","     275826/2000000000: episode: 7607, duration: 4.074s, episode steps:  33, steps per second:   8, episode reward: 114.500, mean reward:  3.470 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 88.485797, mean_q: 45.813395, mean_eps: 0.100000\n","     275855/2000000000: episode: 7608, duration: 3.598s, episode steps:  29, steps per second:   8, episode reward: 14.300, mean reward:  0.493 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 76.123147, mean_q: 47.674356, mean_eps: 0.100000\n","     275881/2000000000: episode: 7609, duration: 3.284s, episode steps:  26, steps per second:   8, episode reward: 207.600, mean reward:  7.985 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 86.937932, mean_q: 46.300158, mean_eps: 0.100000\n","     275913/2000000000: episode: 7610, duration: 4.064s, episode steps:  32, steps per second:   8, episode reward: 75.100, mean reward:  2.347 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.186306, mean_q: 45.595191, mean_eps: 0.100000\n","     275950/2000000000: episode: 7611, duration: 4.637s, episode steps:  37, steps per second:   8, episode reward: 152.500, mean reward:  4.122 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 87.270664, mean_q: 45.347010, mean_eps: 0.100000\n","     275982/2000000000: episode: 7612, duration: 3.940s, episode steps:  32, steps per second:   8, episode reward: 106.200, mean reward:  3.319 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 91.847897, mean_q: 46.047416, mean_eps: 0.100000\n","     276015/2000000000: episode: 7613, duration: 4.199s, episode steps:  33, steps per second:   8, episode reward: 81.900, mean reward:  2.482 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.082448, mean_q: 46.959649, mean_eps: 0.100000\n","     276040/2000000000: episode: 7614, duration: 3.270s, episode steps:  25, steps per second:   8, episode reward: 84.200, mean reward:  3.368 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 78.431126, mean_q: 46.440043, mean_eps: 0.100000\n","     276078/2000000000: episode: 7615, duration: 4.903s, episode steps:  38, steps per second:   8, episode reward: 91.800, mean reward:  2.416 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 82.131116, mean_q: 45.887057, mean_eps: 0.100000\n","     276107/2000000000: episode: 7616, duration: 3.701s, episode steps:  29, steps per second:   8, episode reward: -97.900, mean reward: -3.376 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 90.163176, mean_q: 45.277864, mean_eps: 0.100000\n","     276138/2000000000: episode: 7617, duration: 3.979s, episode steps:  31, steps per second:   8, episode reward: 214.900, mean reward:  6.932 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 85.084963, mean_q: 45.194185, mean_eps: 0.100000\n","     276167/2000000000: episode: 7618, duration: 3.943s, episode steps:  29, steps per second:   7, episode reward: 52.300, mean reward:  1.803 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 84.765597, mean_q: 45.416297, mean_eps: 0.100000\n","     276199/2000000000: episode: 7619, duration: 4.119s, episode steps:  32, steps per second:   8, episode reward:  4.700, mean reward:  0.147 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.113780, mean_q: 45.755906, mean_eps: 0.100000\n","     276230/2000000000: episode: 7620, duration: 4.066s, episode steps:  31, steps per second:   8, episode reward: 60.300, mean reward:  1.945 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 85.148345, mean_q: 45.042238, mean_eps: 0.100000\n","     276270/2000000000: episode: 7621, duration: 5.162s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.142982, mean_q: 46.081354, mean_eps: 0.100000\n","     276304/2000000000: episode: 7622, duration: 4.496s, episode steps:  34, steps per second:   8, episode reward: 37.600, mean reward:  1.106 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 87.296663, mean_q: 45.964238, mean_eps: 0.100000\n","     276332/2000000000: episode: 7623, duration: 3.793s, episode steps:  28, steps per second:   7, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 85.594547, mean_q: 45.834017, mean_eps: 0.100000\n","     276363/2000000000: episode: 7624, duration: 4.114s, episode steps:  31, steps per second:   8, episode reward: 178.600, mean reward:  5.761 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.484919, mean_q: 45.831786, mean_eps: 0.100000\n","     276393/2000000000: episode: 7625, duration: 3.931s, episode steps:  30, steps per second:   8, episode reward: 138.900, mean reward:  4.630 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 77.741235, mean_q: 45.881799, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     276425/2000000000: episode: 7626, duration: 4.073s, episode steps:  32, steps per second:   8, episode reward: 191.600, mean reward:  5.987 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.972572, mean_q: 44.952413, mean_eps: 0.100000\n","     276465/2000000000: episode: 7627, duration: 5.018s, episode steps:  40, steps per second:   8, episode reward: 35.500, mean reward:  0.887 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 78.040096, mean_q: 45.343280, mean_eps: 0.100000\n","     276505/2000000000: episode: 7628, duration: 5.065s, episode steps:  40, steps per second:   8, episode reward: -35.600, mean reward: -0.890 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 88.326941, mean_q: 46.585471, mean_eps: 0.100000\n","     276531/2000000000: episode: 7629, duration: 3.497s, episode steps:  26, steps per second:   7, episode reward: -66.400, mean reward: -2.554 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 85.792993, mean_q: 46.413479, mean_eps: 0.100000\n","     276561/2000000000: episode: 7630, duration: 3.912s, episode steps:  30, steps per second:   8, episode reward: 92.000, mean reward:  3.067 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 91.768938, mean_q: 45.032372, mean_eps: 0.100000\n","     276586/2000000000: episode: 7631, duration: 3.440s, episode steps:  25, steps per second:   7, episode reward: 164.000, mean reward:  6.560 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 90.737399, mean_q: 45.057866, mean_eps: 0.100000\n","     276613/2000000000: episode: 7632, duration: 3.759s, episode steps:  27, steps per second:   7, episode reward: 62.600, mean reward:  2.319 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 80.048945, mean_q: 46.468233, mean_eps: 0.100000\n","     276644/2000000000: episode: 7633, duration: 3.979s, episode steps:  31, steps per second:   8, episode reward: 138.000, mean reward:  4.452 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 88.461975, mean_q: 45.983987, mean_eps: 0.100000\n","     276678/2000000000: episode: 7634, duration: 4.293s, episode steps:  34, steps per second:   8, episode reward: 167.100, mean reward:  4.915 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 83.922015, mean_q: 44.918652, mean_eps: 0.100000\n","     276715/2000000000: episode: 7635, duration: 4.851s, episode steps:  37, steps per second:   8, episode reward: 100.100, mean reward:  2.705 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 84.062280, mean_q: 45.438484, mean_eps: 0.100000\n","     276750/2000000000: episode: 7636, duration: 4.613s, episode steps:  35, steps per second:   8, episode reward: 101.100, mean reward:  2.889 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 83.336152, mean_q: 46.607083, mean_eps: 0.100000\n","     276774/2000000000: episode: 7637, duration: 3.291s, episode steps:  24, steps per second:   7, episode reward: 165.200, mean reward:  6.883 [-20.000, 18.000], mean action: 0.792 [0.000, 2.000],  loss: 83.188725, mean_q: 46.667150, mean_eps: 0.100000\n","     276801/2000000000: episode: 7638, duration: 3.610s, episode steps:  27, steps per second:   7, episode reward: 54.600, mean reward:  2.022 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 91.937310, mean_q: 47.572836, mean_eps: 0.100000\n","     276834/2000000000: episode: 7639, duration: 4.260s, episode steps:  33, steps per second:   8, episode reward: 214.200, mean reward:  6.491 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 89.742315, mean_q: 45.160016, mean_eps: 0.100000\n","     276865/2000000000: episode: 7640, duration: 4.066s, episode steps:  31, steps per second:   8, episode reward: 158.400, mean reward:  5.110 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.105316, mean_q: 45.542795, mean_eps: 0.100000\n","     276896/2000000000: episode: 7641, duration: 4.155s, episode steps:  31, steps per second:   7, episode reward: 319.500, mean reward: 10.306 [-6.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 85.467833, mean_q: 45.622596, mean_eps: 0.100000\n","     276923/2000000000: episode: 7642, duration: 3.348s, episode steps:  27, steps per second:   8, episode reward: 89.900, mean reward:  3.330 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 79.986283, mean_q: 45.483504, mean_eps: 0.100000\n","     276946/2000000000: episode: 7643, duration: 3.011s, episode steps:  23, steps per second:   8, episode reward: 218.600, mean reward:  9.504 [-20.000, 19.100], mean action: 0.652 [0.000, 2.000],  loss: 79.180339, mean_q: 45.765062, mean_eps: 0.100000\n","     276974/2000000000: episode: 7644, duration: 3.804s, episode steps:  28, steps per second:   7, episode reward: 134.400, mean reward:  4.800 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 93.992069, mean_q: 44.906654, mean_eps: 0.100000\n","     277006/2000000000: episode: 7645, duration: 4.217s, episode steps:  32, steps per second:   8, episode reward: 102.000, mean reward:  3.188 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 78.243832, mean_q: 45.988640, mean_eps: 0.100000\n","     277038/2000000000: episode: 7646, duration: 3.961s, episode steps:  32, steps per second:   8, episode reward: 60.000, mean reward:  1.875 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.577693, mean_q: 45.629882, mean_eps: 0.100000\n","     277077/2000000000: episode: 7647, duration: 4.910s, episode steps:  39, steps per second:   8, episode reward: 80.200, mean reward:  2.056 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 80.435946, mean_q: 45.166380, mean_eps: 0.100000\n","     277117/2000000000: episode: 7648, duration: 5.271s, episode steps:  40, steps per second:   8, episode reward: 103.800, mean reward:  2.595 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 87.509919, mean_q: 45.369145, mean_eps: 0.100000\n","     277157/2000000000: episode: 7649, duration: 5.245s, episode steps:  40, steps per second:   8, episode reward: 89.900, mean reward:  2.248 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.193130, mean_q: 45.584216, mean_eps: 0.100000\n","     277191/2000000000: episode: 7650, duration: 4.633s, episode steps:  34, steps per second:   7, episode reward: 135.400, mean reward:  3.982 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 81.877788, mean_q: 45.469460, mean_eps: 0.100000\n","     277229/2000000000: episode: 7651, duration: 5.408s, episode steps:  38, steps per second:   7, episode reward: 124.900, mean reward:  3.287 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 76.523782, mean_q: 46.290653, mean_eps: 0.100000\n","     277264/2000000000: episode: 7652, duration: 4.738s, episode steps:  35, steps per second:   7, episode reward: 127.800, mean reward:  3.651 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 85.077829, mean_q: 46.295404, mean_eps: 0.100000\n","     277302/2000000000: episode: 7653, duration: 4.951s, episode steps:  38, steps per second:   8, episode reward: 135.500, mean reward:  3.566 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 84.848362, mean_q: 45.653665, mean_eps: 0.100000\n","     277342/2000000000: episode: 7654, duration: 5.143s, episode steps:  40, steps per second:   8, episode reward: 99.500, mean reward:  2.488 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 81.505372, mean_q: 46.272544, mean_eps: 0.100000\n","     277382/2000000000: episode: 7655, duration: 5.117s, episode steps:  40, steps per second:   8, episode reward: 29.900, mean reward:  0.748 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 79.863421, mean_q: 46.228246, mean_eps: 0.100000\n","     277422/2000000000: episode: 7656, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: 128.800, mean reward:  3.220 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.450306, mean_q: 45.095235, mean_eps: 0.100000\n","     277449/2000000000: episode: 7657, duration: 3.561s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 86.348679, mean_q: 46.689878, mean_eps: 0.100000\n","     277477/2000000000: episode: 7658, duration: 3.747s, episode steps:  28, steps per second:   7, episode reward: 141.900, mean reward:  5.068 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 80.033516, mean_q: 45.197158, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     277505/2000000000: episode: 7659, duration: 3.544s, episode steps:  28, steps per second:   8, episode reward: 107.200, mean reward:  3.829 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 81.629728, mean_q: 45.180877, mean_eps: 0.100000\n","     277545/2000000000: episode: 7660, duration: 5.344s, episode steps:  40, steps per second:   7, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 86.464215, mean_q: 45.592920, mean_eps: 0.100000\n","     277579/2000000000: episode: 7661, duration: 4.408s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 84.682446, mean_q: 45.095864, mean_eps: 0.100000\n","     277615/2000000000: episode: 7662, duration: 4.768s, episode steps:  36, steps per second:   8, episode reward: -63.100, mean reward: -1.753 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 86.950671, mean_q: 45.567383, mean_eps: 0.100000\n","     277649/2000000000: episode: 7663, duration: 4.199s, episode steps:  34, steps per second:   8, episode reward: 126.300, mean reward:  3.715 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.221109, mean_q: 46.653990, mean_eps: 0.100000\n","     277689/2000000000: episode: 7664, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: 65.700, mean reward:  1.643 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 88.856289, mean_q: 45.681106, mean_eps: 0.100000\n","     277729/2000000000: episode: 7665, duration: 5.130s, episode steps:  40, steps per second:   8, episode reward: 118.000, mean reward:  2.950 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 82.029302, mean_q: 46.212877, mean_eps: 0.100000\n","     277765/2000000000: episode: 7666, duration: 4.689s, episode steps:  36, steps per second:   8, episode reward: 51.100, mean reward:  1.419 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 86.636580, mean_q: 45.260567, mean_eps: 0.100000\n","     277795/2000000000: episode: 7667, duration: 4.009s, episode steps:  30, steps per second:   7, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 87.399920, mean_q: 46.345963, mean_eps: 0.100000\n","     277835/2000000000: episode: 7668, duration: 5.399s, episode steps:  40, steps per second:   7, episode reward: 138.000, mean reward:  3.450 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 87.082474, mean_q: 45.841695, mean_eps: 0.100000\n","     277874/2000000000: episode: 7669, duration: 5.211s, episode steps:  39, steps per second:   7, episode reward: 90.200, mean reward:  2.313 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 84.593195, mean_q: 45.257524, mean_eps: 0.100000\n","     277909/2000000000: episode: 7670, duration: 4.463s, episode steps:  35, steps per second:   8, episode reward: 208.000, mean reward:  5.943 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 79.455065, mean_q: 45.166854, mean_eps: 0.100000\n","     277945/2000000000: episode: 7671, duration: 4.341s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 88.123390, mean_q: 46.430368, mean_eps: 0.100000\n","     277966/2000000000: episode: 7672, duration: 2.675s, episode steps:  21, steps per second:   8, episode reward: 117.700, mean reward:  5.605 [-20.000, 18.000], mean action: 0.524 [0.000, 2.000],  loss: 84.355764, mean_q: 44.531848, mean_eps: 0.100000\n","     278003/2000000000: episode: 7673, duration: 4.833s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 86.786521, mean_q: 45.609618, mean_eps: 0.100000\n","     278035/2000000000: episode: 7674, duration: 4.169s, episode steps:  32, steps per second:   8, episode reward: 184.700, mean reward:  5.772 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 80.275873, mean_q: 45.379052, mean_eps: 0.100000\n","     278066/2000000000: episode: 7675, duration: 3.999s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 85.149864, mean_q: 45.494255, mean_eps: 0.100000\n","     278106/2000000000: episode: 7676, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 80.391630, mean_q: 45.519266, mean_eps: 0.100000\n","     278145/2000000000: episode: 7677, duration: 4.948s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 86.011420, mean_q: 45.410516, mean_eps: 0.100000\n","     278177/2000000000: episode: 7678, duration: 4.105s, episode steps:  32, steps per second:   8, episode reward: 123.900, mean reward:  3.872 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.055850, mean_q: 46.480697, mean_eps: 0.100000\n","     278217/2000000000: episode: 7679, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: 110.500, mean reward:  2.762 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 77.022764, mean_q: 46.107008, mean_eps: 0.100000\n","     278250/2000000000: episode: 7680, duration: 4.569s, episode steps:  33, steps per second:   7, episode reward: 127.400, mean reward:  3.861 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 88.139515, mean_q: 45.200250, mean_eps: 0.100000\n","     278290/2000000000: episode: 7681, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: 85.200, mean reward:  2.130 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 84.506006, mean_q: 45.381153, mean_eps: 0.100000\n","     278330/2000000000: episode: 7682, duration: 5.271s, episode steps:  40, steps per second:   8, episode reward:  9.900, mean reward:  0.248 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 91.009988, mean_q: 45.309383, mean_eps: 0.100000\n","     278364/2000000000: episode: 7683, duration: 4.495s, episode steps:  34, steps per second:   8, episode reward: 167.900, mean reward:  4.938 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 87.130859, mean_q: 46.157845, mean_eps: 0.100000\n","     278395/2000000000: episode: 7684, duration: 4.059s, episode steps:  31, steps per second:   8, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 87.516745, mean_q: 45.780665, mean_eps: 0.100000\n","     278426/2000000000: episode: 7685, duration: 4.027s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 79.611150, mean_q: 47.037021, mean_eps: 0.100000\n","     278454/2000000000: episode: 7686, duration: 3.921s, episode steps:  28, steps per second:   7, episode reward: 84.400, mean reward:  3.014 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 84.338116, mean_q: 44.933666, mean_eps: 0.100000\n","     278479/2000000000: episode: 7687, duration: 3.333s, episode steps:  25, steps per second:   8, episode reward: -96.000, mean reward: -3.840 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 86.556093, mean_q: 46.504956, mean_eps: 0.100000\n","     278507/2000000000: episode: 7688, duration: 3.624s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 90.573231, mean_q: 45.348972, mean_eps: 0.100000\n","     278547/2000000000: episode: 7689, duration: 5.359s, episode steps:  40, steps per second:   7, episode reward: 98.900, mean reward:  2.472 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.647386, mean_q: 45.139075, mean_eps: 0.100000\n","     278586/2000000000: episode: 7690, duration: 4.976s, episode steps:  39, steps per second:   8, episode reward: -3.000, mean reward: -0.077 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 86.388836, mean_q: 45.920203, mean_eps: 0.100000\n","     278624/2000000000: episode: 7691, duration: 4.994s, episode steps:  38, steps per second:   8, episode reward: -1.700, mean reward: -0.045 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 85.261900, mean_q: 45.237391, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     278649/2000000000: episode: 7692, duration: 3.369s, episode steps:  25, steps per second:   7, episode reward: 250.000, mean reward: 10.000 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 84.804018, mean_q: 46.993092, mean_eps: 0.100000\n","     278675/2000000000: episode: 7693, duration: 3.467s, episode steps:  26, steps per second:   7, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 96.572670, mean_q: 45.503974, mean_eps: 0.100000\n","     278715/2000000000: episode: 7694, duration: 5.231s, episode steps:  40, steps per second:   8, episode reward: 177.400, mean reward:  4.435 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 84.583782, mean_q: 46.456481, mean_eps: 0.100000\n","     278755/2000000000: episode: 7695, duration: 5.310s, episode steps:  40, steps per second:   8, episode reward: 69.400, mean reward:  1.735 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 83.685476, mean_q: 46.514456, mean_eps: 0.100000\n","     278787/2000000000: episode: 7696, duration: 4.055s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.027328, mean_q: 45.681530, mean_eps: 0.100000\n","     278823/2000000000: episode: 7697, duration: 4.484s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 81.558757, mean_q: 46.355977, mean_eps: 0.100000\n","     278851/2000000000: episode: 7698, duration: 3.624s, episode steps:  28, steps per second:   8, episode reward: 246.000, mean reward:  8.786 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 78.662283, mean_q: 45.608953, mean_eps: 0.100000\n","     278887/2000000000: episode: 7699, duration: 4.428s, episode steps:  36, steps per second:   8, episode reward: 192.000, mean reward:  5.333 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 86.647508, mean_q: 45.866263, mean_eps: 0.100000\n","     278918/2000000000: episode: 7700, duration: 3.882s, episode steps:  31, steps per second:   8, episode reward: 175.700, mean reward:  5.668 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.240899, mean_q: 45.843943, mean_eps: 0.100000\n","     278950/2000000000: episode: 7701, duration: 3.979s, episode steps:  32, steps per second:   8, episode reward: 99.500, mean reward:  3.109 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 86.566788, mean_q: 44.449063, mean_eps: 0.100000\n","     278984/2000000000: episode: 7702, duration: 4.098s, episode steps:  34, steps per second:   8, episode reward: 136.300, mean reward:  4.009 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 91.739431, mean_q: 45.027184, mean_eps: 0.100000\n","     279015/2000000000: episode: 7703, duration: 4.081s, episode steps:  31, steps per second:   8, episode reward: -7.600, mean reward: -0.245 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 86.853445, mean_q: 45.977234, mean_eps: 0.100000\n","     279052/2000000000: episode: 7704, duration: 4.717s, episode steps:  37, steps per second:   8, episode reward: 200.900, mean reward:  5.430 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 87.915444, mean_q: 45.386430, mean_eps: 0.100000\n","     279092/2000000000: episode: 7705, duration: 5.295s, episode steps:  40, steps per second:   8, episode reward: 116.800, mean reward:  2.920 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 74.912360, mean_q: 46.020416, mean_eps: 0.100000\n","     279124/2000000000: episode: 7706, duration: 4.137s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 87.422230, mean_q: 45.902229, mean_eps: 0.100000\n","     279163/2000000000: episode: 7707, duration: 5.276s, episode steps:  39, steps per second:   7, episode reward: -77.100, mean reward: -1.977 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 77.558135, mean_q: 45.023936, mean_eps: 0.100000\n","     279191/2000000000: episode: 7708, duration: 3.515s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 83.502709, mean_q: 45.293696, mean_eps: 0.100000\n","     279217/2000000000: episode: 7709, duration: 3.521s, episode steps:  26, steps per second:   7, episode reward: -3.900, mean reward: -0.150 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 78.164645, mean_q: 46.766341, mean_eps: 0.100000\n","     279252/2000000000: episode: 7710, duration: 4.673s, episode steps:  35, steps per second:   7, episode reward: -106.100, mean reward: -3.031 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 88.862490, mean_q: 45.709738, mean_eps: 0.100000\n","     279284/2000000000: episode: 7711, duration: 4.230s, episode steps:  32, steps per second:   8, episode reward: 36.000, mean reward:  1.125 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 91.577806, mean_q: 44.999581, mean_eps: 0.100000\n","     279314/2000000000: episode: 7712, duration: 3.902s, episode steps:  30, steps per second:   8, episode reward: 71.800, mean reward:  2.393 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 85.678566, mean_q: 47.016299, mean_eps: 0.100000\n","     279346/2000000000: episode: 7713, duration: 4.221s, episode steps:  32, steps per second:   8, episode reward: 153.700, mean reward:  4.803 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 84.527975, mean_q: 45.893037, mean_eps: 0.100000\n","     279373/2000000000: episode: 7714, duration: 3.769s, episode steps:  27, steps per second:   7, episode reward: 144.200, mean reward:  5.341 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 77.074725, mean_q: 45.680143, mean_eps: 0.100000\n","     279407/2000000000: episode: 7715, duration: 4.812s, episode steps:  34, steps per second:   7, episode reward: 96.700, mean reward:  2.844 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.914575, mean_q: 45.828670, mean_eps: 0.100000\n","     279447/2000000000: episode: 7716, duration: 5.265s, episode steps:  40, steps per second:   8, episode reward: 123.700, mean reward:  3.092 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.699469, mean_q: 45.100538, mean_eps: 0.100000\n","     279483/2000000000: episode: 7717, duration: 4.723s, episode steps:  36, steps per second:   8, episode reward: -48.900, mean reward: -1.358 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 88.233360, mean_q: 45.864617, mean_eps: 0.100000\n","     279511/2000000000: episode: 7718, duration: 3.889s, episode steps:  28, steps per second:   7, episode reward: 51.500, mean reward:  1.839 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 79.267925, mean_q: 46.798012, mean_eps: 0.100000\n","     279551/2000000000: episode: 7719, duration: 5.498s, episode steps:  40, steps per second:   7, episode reward: 181.200, mean reward:  4.530 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 81.005763, mean_q: 45.987737, mean_eps: 0.100000\n","     279584/2000000000: episode: 7720, duration: 4.504s, episode steps:  33, steps per second:   7, episode reward: -94.500, mean reward: -2.864 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 80.539907, mean_q: 45.565483, mean_eps: 0.100000\n","     279620/2000000000: episode: 7721, duration: 4.880s, episode steps:  36, steps per second:   7, episode reward: 148.900, mean reward:  4.136 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 84.692391, mean_q: 45.508115, mean_eps: 0.100000\n","     279649/2000000000: episode: 7722, duration: 3.750s, episode steps:  29, steps per second:   8, episode reward: 150.400, mean reward:  5.186 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.489678, mean_q: 45.132859, mean_eps: 0.100000\n","     279679/2000000000: episode: 7723, duration: 3.825s, episode steps:  30, steps per second:   8, episode reward: 104.500, mean reward:  3.483 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 83.864059, mean_q: 47.167330, mean_eps: 0.100000\n","     279711/2000000000: episode: 7724, duration: 4.096s, episode steps:  32, steps per second:   8, episode reward: 116.400, mean reward:  3.638 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.655004, mean_q: 45.689437, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     279751/2000000000: episode: 7725, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 186.700, mean reward:  4.668 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.134965, mean_q: 46.545012, mean_eps: 0.100000\n","     279777/2000000000: episode: 7726, duration: 3.511s, episode steps:  26, steps per second:   7, episode reward: 63.600, mean reward:  2.446 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 81.854597, mean_q: 45.920224, mean_eps: 0.100000\n","     279814/2000000000: episode: 7727, duration: 5.756s, episode steps:  37, steps per second:   6, episode reward: -63.400, mean reward: -1.714 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 80.824296, mean_q: 45.380626, mean_eps: 0.100000\n","     279846/2000000000: episode: 7728, duration: 4.767s, episode steps:  32, steps per second:   7, episode reward: -77.200, mean reward: -2.412 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 80.891964, mean_q: 45.578549, mean_eps: 0.100000\n","     279877/2000000000: episode: 7729, duration: 4.274s, episode steps:  31, steps per second:   7, episode reward: 220.900, mean reward:  7.126 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 76.848984, mean_q: 45.786813, mean_eps: 0.100000\n","     279915/2000000000: episode: 7730, duration: 5.175s, episode steps:  38, steps per second:   7, episode reward: 35.200, mean reward:  0.926 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 85.420551, mean_q: 45.437813, mean_eps: 0.100000\n","     279955/2000000000: episode: 7731, duration: 5.517s, episode steps:  40, steps per second:   7, episode reward: 21.500, mean reward:  0.538 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 82.907289, mean_q: 45.254239, mean_eps: 0.100000\n","     279984/2000000000: episode: 7732, duration: 4.557s, episode steps:  29, steps per second:   6, episode reward: 128.500, mean reward:  4.431 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 70.296453, mean_q: 46.938289, mean_eps: 0.100000\n","     280024/2000000000: episode: 7733, duration: 5.838s, episode steps:  40, steps per second:   7, episode reward: 168.900, mean reward:  4.222 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.974259, mean_q: 46.220669, mean_eps: 0.100000\n","     280052/2000000000: episode: 7734, duration: 4.000s, episode steps:  28, steps per second:   7, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 90.125262, mean_q: 46.849629, mean_eps: 0.100000\n","     280089/2000000000: episode: 7735, duration: 4.882s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 96.888397, mean_q: 47.291766, mean_eps: 0.100000\n","     280123/2000000000: episode: 7736, duration: 4.353s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 97.173022, mean_q: 47.004464, mean_eps: 0.100000\n","     280153/2000000000: episode: 7737, duration: 3.878s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 93.052820, mean_q: 47.381566, mean_eps: 0.100000\n","     280178/2000000000: episode: 7738, duration: 3.178s, episode steps:  25, steps per second:   8, episode reward: 241.600, mean reward:  9.664 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 91.847518, mean_q: 47.820022, mean_eps: 0.100000\n","     280210/2000000000: episode: 7739, duration: 4.069s, episode steps:  32, steps per second:   8, episode reward: 74.400, mean reward:  2.325 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 87.120024, mean_q: 47.297773, mean_eps: 0.100000\n","     280247/2000000000: episode: 7740, duration: 4.884s, episode steps:  37, steps per second:   8, episode reward: 75.900, mean reward:  2.051 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 100.561597, mean_q: 46.815608, mean_eps: 0.100000\n","     280279/2000000000: episode: 7741, duration: 4.343s, episode steps:  32, steps per second:   7, episode reward: 141.200, mean reward:  4.413 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.444468, mean_q: 47.384801, mean_eps: 0.100000\n","     280316/2000000000: episode: 7742, duration: 5.058s, episode steps:  37, steps per second:   7, episode reward: 109.500, mean reward:  2.959 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.064960, mean_q: 47.526307, mean_eps: 0.100000\n","     280342/2000000000: episode: 7743, duration: 3.359s, episode steps:  26, steps per second:   8, episode reward: -17.200, mean reward: -0.662 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 84.642803, mean_q: 48.309314, mean_eps: 0.100000\n","     280382/2000000000: episode: 7744, duration: 5.392s, episode steps:  40, steps per second:   7, episode reward: -58.000, mean reward: -1.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 89.596812, mean_q: 47.082166, mean_eps: 0.100000\n","     280408/2000000000: episode: 7745, duration: 3.762s, episode steps:  26, steps per second:   7, episode reward:  6.900, mean reward:  0.265 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 81.688960, mean_q: 47.859363, mean_eps: 0.100000\n","     280442/2000000000: episode: 7746, duration: 4.772s, episode steps:  34, steps per second:   7, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 88.127712, mean_q: 46.723806, mean_eps: 0.100000\n","     280474/2000000000: episode: 7747, duration: 4.485s, episode steps:  32, steps per second:   7, episode reward: 160.600, mean reward:  5.019 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 87.883187, mean_q: 47.239404, mean_eps: 0.100000\n","     280511/2000000000: episode: 7748, duration: 4.945s, episode steps:  37, steps per second:   7, episode reward: 71.600, mean reward:  1.935 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 78.590343, mean_q: 46.477655, mean_eps: 0.100000\n","     280545/2000000000: episode: 7749, duration: 4.488s, episode steps:  34, steps per second:   8, episode reward: 68.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 92.411357, mean_q: 46.713547, mean_eps: 0.100000\n","     280575/2000000000: episode: 7750, duration: 4.042s, episode steps:  30, steps per second:   7, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.233 [0.000, 2.000],  loss: 83.494204, mean_q: 47.526364, mean_eps: 0.100000\n","     280603/2000000000: episode: 7751, duration: 3.735s, episode steps:  28, steps per second:   7, episode reward: 143.200, mean reward:  5.114 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.963216, mean_q: 47.408210, mean_eps: 0.100000\n","     280640/2000000000: episode: 7752, duration: 4.987s, episode steps:  37, steps per second:   7, episode reward: 83.200, mean reward:  2.249 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 89.762494, mean_q: 48.188281, mean_eps: 0.100000\n","     280672/2000000000: episode: 7753, duration: 4.350s, episode steps:  32, steps per second:   7, episode reward: 59.900, mean reward:  1.872 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 88.172545, mean_q: 46.388420, mean_eps: 0.100000\n","     280707/2000000000: episode: 7754, duration: 4.568s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 88.574197, mean_q: 47.295369, mean_eps: 0.100000\n","     280739/2000000000: episode: 7755, duration: 4.107s, episode steps:  32, steps per second:   8, episode reward: 126.400, mean reward:  3.950 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 98.470730, mean_q: 47.219499, mean_eps: 0.100000\n","     280774/2000000000: episode: 7756, duration: 4.292s, episode steps:  35, steps per second:   8, episode reward: 34.600, mean reward:  0.989 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 90.578968, mean_q: 46.787196, mean_eps: 0.100000\n","     280814/2000000000: episode: 7757, duration: 5.074s, episode steps:  40, steps per second:   8, episode reward: 52.200, mean reward:  1.305 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 84.285828, mean_q: 47.837433, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     280852/2000000000: episode: 7758, duration: 4.867s, episode steps:  38, steps per second:   8, episode reward: 137.300, mean reward:  3.613 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 80.085506, mean_q: 47.118074, mean_eps: 0.100000\n","     280890/2000000000: episode: 7759, duration: 4.998s, episode steps:  38, steps per second:   8, episode reward: 95.100, mean reward:  2.503 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 83.694783, mean_q: 48.619036, mean_eps: 0.100000\n","     280927/2000000000: episode: 7760, duration: 4.888s, episode steps:  37, steps per second:   8, episode reward: 84.700, mean reward:  2.289 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 82.266716, mean_q: 47.192998, mean_eps: 0.100000\n","     280963/2000000000: episode: 7761, duration: 4.810s, episode steps:  36, steps per second:   7, episode reward: 13.500, mean reward:  0.375 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 87.994968, mean_q: 46.600118, mean_eps: 0.100000\n","     280993/2000000000: episode: 7762, duration: 4.253s, episode steps:  30, steps per second:   7, episode reward: 208.600, mean reward:  6.953 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.711877, mean_q: 46.220855, mean_eps: 0.100000\n","     281030/2000000000: episode: 7763, duration: 4.834s, episode steps:  37, steps per second:   8, episode reward: 30.300, mean reward:  0.819 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 94.368972, mean_q: 48.088074, mean_eps: 0.100000\n","     281063/2000000000: episode: 7764, duration: 4.307s, episode steps:  33, steps per second:   8, episode reward: 161.300, mean reward:  4.888 [-12.400, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 83.427298, mean_q: 48.239622, mean_eps: 0.100000\n","     281094/2000000000: episode: 7765, duration: 4.074s, episode steps:  31, steps per second:   8, episode reward: 234.500, mean reward:  7.565 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 92.280764, mean_q: 47.362302, mean_eps: 0.100000\n","     281134/2000000000: episode: 7766, duration: 5.066s, episode steps:  40, steps per second:   8, episode reward: 164.900, mean reward:  4.122 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 83.522802, mean_q: 47.940488, mean_eps: 0.100000\n","     281174/2000000000: episode: 7767, duration: 5.289s, episode steps:  40, steps per second:   8, episode reward: 53.900, mean reward:  1.347 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 88.095870, mean_q: 47.396268, mean_eps: 0.100000\n","     281207/2000000000: episode: 7768, duration: 4.196s, episode steps:  33, steps per second:   8, episode reward:  7.900, mean reward:  0.239 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 91.610976, mean_q: 47.073823, mean_eps: 0.100000\n","     281237/2000000000: episode: 7769, duration: 3.608s, episode steps:  30, steps per second:   8, episode reward: 65.500, mean reward:  2.183 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 83.955629, mean_q: 47.225653, mean_eps: 0.100000\n","     281273/2000000000: episode: 7770, duration: 4.725s, episode steps:  36, steps per second:   8, episode reward: 20.100, mean reward:  0.558 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 85.852797, mean_q: 47.770393, mean_eps: 0.100000\n","     281304/2000000000: episode: 7771, duration: 3.879s, episode steps:  31, steps per second:   8, episode reward: 111.500, mean reward:  3.597 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 75.723598, mean_q: 47.256425, mean_eps: 0.100000\n","     281334/2000000000: episode: 7772, duration: 3.755s, episode steps:  30, steps per second:   8, episode reward: 262.400, mean reward:  8.747 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 96.944234, mean_q: 48.035634, mean_eps: 0.100000\n","     281369/2000000000: episode: 7773, duration: 4.464s, episode steps:  35, steps per second:   8, episode reward: 158.200, mean reward:  4.520 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 84.617035, mean_q: 47.235500, mean_eps: 0.100000\n","     281398/2000000000: episode: 7774, duration: 3.874s, episode steps:  29, steps per second:   7, episode reward: -53.800, mean reward: -1.855 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 88.820118, mean_q: 47.355430, mean_eps: 0.100000\n","     281427/2000000000: episode: 7775, duration: 3.828s, episode steps:  29, steps per second:   8, episode reward: 235.200, mean reward:  8.110 [-19.700, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 92.436803, mean_q: 47.945981, mean_eps: 0.100000\n","     281466/2000000000: episode: 7776, duration: 5.032s, episode steps:  39, steps per second:   8, episode reward: 82.000, mean reward:  2.103 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 90.016205, mean_q: 46.963053, mean_eps: 0.100000\n","     281501/2000000000: episode: 7777, duration: 4.568s, episode steps:  35, steps per second:   8, episode reward: 198.700, mean reward:  5.677 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 88.729510, mean_q: 47.610968, mean_eps: 0.100000\n","     281534/2000000000: episode: 7778, duration: 4.226s, episode steps:  33, steps per second:   8, episode reward: 170.600, mean reward:  5.170 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.379754, mean_q: 48.483843, mean_eps: 0.100000\n","     281573/2000000000: episode: 7779, duration: 5.235s, episode steps:  39, steps per second:   7, episode reward: 111.100, mean reward:  2.849 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 87.147061, mean_q: 47.504843, mean_eps: 0.100000\n","     281607/2000000000: episode: 7780, duration: 4.956s, episode steps:  34, steps per second:   7, episode reward: 58.300, mean reward:  1.715 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.007667, mean_q: 47.553699, mean_eps: 0.100000\n","     281647/2000000000: episode: 7781, duration: 5.713s, episode steps:  40, steps per second:   7, episode reward:  7.500, mean reward:  0.188 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 91.778274, mean_q: 46.687579, mean_eps: 0.100000\n","     281682/2000000000: episode: 7782, duration: 4.761s, episode steps:  35, steps per second:   7, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 82.092416, mean_q: 46.902382, mean_eps: 0.100000\n","     281720/2000000000: episode: 7783, duration: 5.103s, episode steps:  38, steps per second:   7, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 85.303921, mean_q: 47.389473, mean_eps: 0.100000\n","     281756/2000000000: episode: 7784, duration: 4.835s, episode steps:  36, steps per second:   7, episode reward: 74.600, mean reward:  2.072 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 83.574677, mean_q: 47.950995, mean_eps: 0.100000\n","     281786/2000000000: episode: 7785, duration: 4.054s, episode steps:  30, steps per second:   7, episode reward: 39.100, mean reward:  1.303 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 93.630025, mean_q: 48.599544, mean_eps: 0.100000\n","     281814/2000000000: episode: 7786, duration: 3.913s, episode steps:  28, steps per second:   7, episode reward: 307.800, mean reward: 10.993 [-20.000, 19.800], mean action: 0.929 [0.000, 2.000],  loss: 81.613941, mean_q: 47.157238, mean_eps: 0.100000\n","     281848/2000000000: episode: 7787, duration: 4.320s, episode steps:  34, steps per second:   8, episode reward: 101.200, mean reward:  2.976 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 78.114277, mean_q: 47.253076, mean_eps: 0.100000\n","     281882/2000000000: episode: 7788, duration: 4.325s, episode steps:  34, steps per second:   8, episode reward: 107.000, mean reward:  3.147 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 78.573806, mean_q: 47.717542, mean_eps: 0.100000\n","     281909/2000000000: episode: 7789, duration: 3.475s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 87.755731, mean_q: 48.547155, mean_eps: 0.100000\n","     281949/2000000000: episode: 7790, duration: 5.066s, episode steps:  40, steps per second:   8, episode reward: -50.400, mean reward: -1.260 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.094692, mean_q: 46.797843, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     281986/2000000000: episode: 7791, duration: 4.705s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 88.455451, mean_q: 46.786441, mean_eps: 0.100000\n","     282026/2000000000: episode: 7792, duration: 5.129s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 82.056651, mean_q: 47.955461, mean_eps: 0.100000\n","     282056/2000000000: episode: 7793, duration: 3.988s, episode steps:  30, steps per second:   8, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 88.386311, mean_q: 48.016671, mean_eps: 0.100000\n","     282095/2000000000: episode: 7794, duration: 4.972s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 83.836052, mean_q: 47.890521, mean_eps: 0.100000\n","     282127/2000000000: episode: 7795, duration: 4.161s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 79.124521, mean_q: 47.418425, mean_eps: 0.100000\n","     282167/2000000000: episode: 7796, duration: 5.496s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 77.667549, mean_q: 47.738399, mean_eps: 0.100000\n","     282196/2000000000: episode: 7797, duration: 3.783s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 78.620174, mean_q: 48.500667, mean_eps: 0.100000\n","     282229/2000000000: episode: 7798, duration: 4.445s, episode steps:  33, steps per second:   7, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 86.529547, mean_q: 48.417151, mean_eps: 0.100000\n","     282260/2000000000: episode: 7799, duration: 3.927s, episode steps:  31, steps per second:   8, episode reward: -44.200, mean reward: -1.426 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.872441, mean_q: 47.511369, mean_eps: 0.100000\n","     282297/2000000000: episode: 7800, duration: 4.738s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 89.039621, mean_q: 47.415960, mean_eps: 0.100000\n","     282337/2000000000: episode: 7801, duration: 4.922s, episode steps:  40, steps per second:   8, episode reward: -72.200, mean reward: -1.805 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 86.886058, mean_q: 46.976681, mean_eps: 0.100000\n","     282370/2000000000: episode: 7802, duration: 4.114s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 84.285686, mean_q: 47.038998, mean_eps: 0.100000\n","     282397/2000000000: episode: 7803, duration: 3.572s, episode steps:  27, steps per second:   8, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 82.198489, mean_q: 47.246147, mean_eps: 0.100000\n","     282432/2000000000: episode: 7804, duration: 4.286s, episode steps:  35, steps per second:   8, episode reward: 164.000, mean reward:  4.686 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 83.917270, mean_q: 47.938100, mean_eps: 0.100000\n","     282455/2000000000: episode: 7805, duration: 2.955s, episode steps:  23, steps per second:   8, episode reward: 170.000, mean reward:  7.391 [-20.000, 18.000], mean action: 0.826 [0.000, 2.000],  loss: 94.783066, mean_q: 47.243593, mean_eps: 0.100000\n","     282486/2000000000: episode: 7806, duration: 4.032s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 84.786746, mean_q: 46.563137, mean_eps: 0.100000\n","     282523/2000000000: episode: 7807, duration: 4.739s, episode steps:  37, steps per second:   8, episode reward: -26.200, mean reward: -0.708 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 84.210455, mean_q: 47.168836, mean_eps: 0.100000\n","     282553/2000000000: episode: 7808, duration: 3.785s, episode steps:  30, steps per second:   8, episode reward: -29.400, mean reward: -0.980 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 82.353508, mean_q: 47.450309, mean_eps: 0.100000\n","     282580/2000000000: episode: 7809, duration: 3.520s, episode steps:  27, steps per second:   8, episode reward: 156.600, mean reward:  5.800 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 92.645345, mean_q: 47.727345, mean_eps: 0.100000\n","     282620/2000000000: episode: 7810, duration: 5.168s, episode steps:  40, steps per second:   8, episode reward: 71.000, mean reward:  1.775 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.418965, mean_q: 47.110291, mean_eps: 0.100000\n","     282655/2000000000: episode: 7811, duration: 4.628s, episode steps:  35, steps per second:   8, episode reward: 114.400, mean reward:  3.269 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 92.559499, mean_q: 47.054291, mean_eps: 0.100000\n","     282688/2000000000: episode: 7812, duration: 4.405s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 78.349654, mean_q: 47.475781, mean_eps: 0.100000\n","     282720/2000000000: episode: 7813, duration: 4.007s, episode steps:  32, steps per second:   8, episode reward: 101.000, mean reward:  3.156 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 85.457479, mean_q: 46.583607, mean_eps: 0.100000\n","     282757/2000000000: episode: 7814, duration: 4.607s, episode steps:  37, steps per second:   8, episode reward: 109.700, mean reward:  2.965 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 85.277549, mean_q: 47.363919, mean_eps: 0.100000\n","     282788/2000000000: episode: 7815, duration: 4.055s, episode steps:  31, steps per second:   8, episode reward: -0.400, mean reward: -0.013 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 73.654640, mean_q: 48.069809, mean_eps: 0.100000\n","     282820/2000000000: episode: 7816, duration: 4.035s, episode steps:  32, steps per second:   8, episode reward: -58.000, mean reward: -1.812 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 87.700354, mean_q: 48.193244, mean_eps: 0.100000\n","     282845/2000000000: episode: 7817, duration: 3.277s, episode steps:  25, steps per second:   8, episode reward: -69.200, mean reward: -2.768 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 76.075682, mean_q: 47.143689, mean_eps: 0.100000\n","     282883/2000000000: episode: 7818, duration: 4.855s, episode steps:  38, steps per second:   8, episode reward: 35.600, mean reward:  0.937 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 92.801281, mean_q: 47.276812, mean_eps: 0.100000\n","     282916/2000000000: episode: 7819, duration: 4.414s, episode steps:  33, steps per second:   7, episode reward: 170.000, mean reward:  5.152 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 87.114011, mean_q: 46.760810, mean_eps: 0.100000\n","     282948/2000000000: episode: 7820, duration: 3.954s, episode steps:  32, steps per second:   8, episode reward: 111.000, mean reward:  3.469 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 84.705357, mean_q: 48.187708, mean_eps: 0.100000\n","     282988/2000000000: episode: 7821, duration: 4.795s, episode steps:  40, steps per second:   8, episode reward: 172.000, mean reward:  4.300 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 89.449285, mean_q: 47.530945, mean_eps: 0.100000\n","     283028/2000000000: episode: 7822, duration: 5.240s, episode steps:  40, steps per second:   8, episode reward: -105.900, mean reward: -2.647 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 89.188054, mean_q: 46.884227, mean_eps: 0.100000\n","     283058/2000000000: episode: 7823, duration: 3.992s, episode steps:  30, steps per second:   8, episode reward: 72.500, mean reward:  2.417 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 79.338930, mean_q: 47.548568, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     283087/2000000000: episode: 7824, duration: 3.944s, episode steps:  29, steps per second:   7, episode reward: 68.900, mean reward:  2.376 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 81.705420, mean_q: 46.909772, mean_eps: 0.100000\n","     283113/2000000000: episode: 7825, duration: 3.465s, episode steps:  26, steps per second:   8, episode reward: 179.600, mean reward:  6.908 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 76.987526, mean_q: 48.760639, mean_eps: 0.100000\n","     283145/2000000000: episode: 7826, duration: 4.307s, episode steps:  32, steps per second:   7, episode reward: 50.900, mean reward:  1.591 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.521662, mean_q: 48.082655, mean_eps: 0.100000\n","     283184/2000000000: episode: 7827, duration: 5.051s, episode steps:  39, steps per second:   8, episode reward: 140.600, mean reward:  3.605 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 93.045327, mean_q: 46.546028, mean_eps: 0.100000\n","     283213/2000000000: episode: 7828, duration: 3.929s, episode steps:  29, steps per second:   7, episode reward: 188.300, mean reward:  6.493 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 89.087947, mean_q: 47.004022, mean_eps: 0.100000\n","     283244/2000000000: episode: 7829, duration: 3.923s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 82.509092, mean_q: 48.165563, mean_eps: 0.100000\n","     283276/2000000000: episode: 7830, duration: 4.034s, episode steps:  32, steps per second:   8, episode reward: 16.300, mean reward:  0.509 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 81.860511, mean_q: 48.467344, mean_eps: 0.100000\n","     283309/2000000000: episode: 7831, duration: 4.478s, episode steps:  33, steps per second:   7, episode reward: 100.800, mean reward:  3.055 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 87.216828, mean_q: 47.753672, mean_eps: 0.100000\n","     283349/2000000000: episode: 7832, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: -15.200, mean reward: -0.380 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.701624, mean_q: 47.646754, mean_eps: 0.100000\n","     283380/2000000000: episode: 7833, duration: 4.124s, episode steps:  31, steps per second:   8, episode reward: 255.100, mean reward:  8.229 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.919315, mean_q: 47.052407, mean_eps: 0.100000\n","     283418/2000000000: episode: 7834, duration: 4.877s, episode steps:  38, steps per second:   8, episode reward: 33.900, mean reward:  0.892 [-20.000, 18.600], mean action: 1.158 [0.000, 2.000],  loss: 82.332852, mean_q: 48.795190, mean_eps: 0.100000\n","     283443/2000000000: episode: 7835, duration: 3.319s, episode steps:  25, steps per second:   8, episode reward: -43.300, mean reward: -1.732 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 86.362424, mean_q: 47.907504, mean_eps: 0.100000\n","     283476/2000000000: episode: 7836, duration: 4.276s, episode steps:  33, steps per second:   8, episode reward: 63.000, mean reward:  1.909 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 85.194060, mean_q: 46.917053, mean_eps: 0.100000\n","     283513/2000000000: episode: 7837, duration: 4.788s, episode steps:  37, steps per second:   8, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 85.912080, mean_q: 46.728625, mean_eps: 0.100000\n","     283546/2000000000: episode: 7838, duration: 4.133s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.250843, mean_q: 47.461524, mean_eps: 0.100000\n","     283576/2000000000: episode: 7839, duration: 3.906s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.988206, mean_q: 46.969980, mean_eps: 0.100000\n","     283613/2000000000: episode: 7840, duration: 4.581s, episode steps:  37, steps per second:   8, episode reward: 98.800, mean reward:  2.670 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 86.599728, mean_q: 47.240989, mean_eps: 0.100000\n","     283651/2000000000: episode: 7841, duration: 4.569s, episode steps:  38, steps per second:   8, episode reward: 202.900, mean reward:  5.339 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 80.368227, mean_q: 47.163275, mean_eps: 0.100000\n","     283685/2000000000: episode: 7842, duration: 4.204s, episode steps:  34, steps per second:   8, episode reward: -109.500, mean reward: -3.221 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.284577, mean_q: 47.726165, mean_eps: 0.100000\n","     283712/2000000000: episode: 7843, duration: 3.409s, episode steps:  27, steps per second:   8, episode reward: 147.200, mean reward:  5.452 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 85.249479, mean_q: 47.910519, mean_eps: 0.100000\n","     283751/2000000000: episode: 7844, duration: 5.015s, episode steps:  39, steps per second:   8, episode reward: 37.100, mean reward:  0.951 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 88.866567, mean_q: 47.246702, mean_eps: 0.100000\n","     283790/2000000000: episode: 7845, duration: 4.934s, episode steps:  39, steps per second:   8, episode reward: 133.600, mean reward:  3.426 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 88.536426, mean_q: 46.905771, mean_eps: 0.100000\n","     283824/2000000000: episode: 7846, duration: 4.390s, episode steps:  34, steps per second:   8, episode reward: 64.300, mean reward:  1.891 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 80.969140, mean_q: 47.347101, mean_eps: 0.100000\n","     283861/2000000000: episode: 7847, duration: 4.770s, episode steps:  37, steps per second:   8, episode reward: 217.400, mean reward:  5.876 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 81.279514, mean_q: 47.626321, mean_eps: 0.100000\n","     283898/2000000000: episode: 7848, duration: 4.614s, episode steps:  37, steps per second:   8, episode reward: 117.600, mean reward:  3.178 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 95.993082, mean_q: 46.881192, mean_eps: 0.100000\n","     283931/2000000000: episode: 7849, duration: 4.358s, episode steps:  33, steps per second:   8, episode reward: 42.000, mean reward:  1.273 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 78.412711, mean_q: 48.310877, mean_eps: 0.100000\n","     283967/2000000000: episode: 7850, duration: 4.745s, episode steps:  36, steps per second:   8, episode reward: 156.600, mean reward:  4.350 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 92.030375, mean_q: 47.886781, mean_eps: 0.100000\n","     283991/2000000000: episode: 7851, duration: 3.144s, episode steps:  24, steps per second:   8, episode reward: 38.000, mean reward:  1.583 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 80.143346, mean_q: 47.818710, mean_eps: 0.100000\n","     284021/2000000000: episode: 7852, duration: 3.776s, episode steps:  30, steps per second:   8, episode reward: -72.600, mean reward: -2.420 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 86.738005, mean_q: 48.445571, mean_eps: 0.100000\n","     284058/2000000000: episode: 7853, duration: 4.442s, episode steps:  37, steps per second:   8, episode reward: 192.600, mean reward:  5.205 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 78.506463, mean_q: 48.351952, mean_eps: 0.100000\n","     284098/2000000000: episode: 7854, duration: 4.947s, episode steps:  40, steps per second:   8, episode reward: 22.400, mean reward:  0.560 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 92.995629, mean_q: 47.695321, mean_eps: 0.100000\n","     284130/2000000000: episode: 7855, duration: 3.958s, episode steps:  32, steps per second:   8, episode reward: 198.500, mean reward:  6.203 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 88.809411, mean_q: 46.912325, mean_eps: 0.100000\n","     284160/2000000000: episode: 7856, duration: 3.678s, episode steps:  30, steps per second:   8, episode reward: 114.200, mean reward:  3.807 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 89.755693, mean_q: 47.240177, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     284192/2000000000: episode: 7857, duration: 4.218s, episode steps:  32, steps per second:   8, episode reward: 64.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 92.847461, mean_q: 46.925017, mean_eps: 0.100000\n","     284232/2000000000: episode: 7858, duration: 5.115s, episode steps:  40, steps per second:   8, episode reward: 210.000, mean reward:  5.250 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 83.319714, mean_q: 47.171732, mean_eps: 0.100000\n","     284261/2000000000: episode: 7859, duration: 3.931s, episode steps:  29, steps per second:   7, episode reward: 59.000, mean reward:  2.034 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.650703, mean_q: 47.856848, mean_eps: 0.100000\n","     284294/2000000000: episode: 7860, duration: 4.152s, episode steps:  33, steps per second:   8, episode reward:  5.400, mean reward:  0.164 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 91.008594, mean_q: 46.684891, mean_eps: 0.100000\n","     284328/2000000000: episode: 7861, duration: 4.276s, episode steps:  34, steps per second:   8, episode reward: 110.900, mean reward:  3.262 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 81.275935, mean_q: 47.411179, mean_eps: 0.100000\n","     284352/2000000000: episode: 7862, duration: 3.102s, episode steps:  24, steps per second:   8, episode reward: 210.600, mean reward:  8.775 [-20.000, 18.000], mean action: 0.792 [0.000, 2.000],  loss: 75.256131, mean_q: 47.652902, mean_eps: 0.100000\n","     284379/2000000000: episode: 7863, duration: 3.499s, episode steps:  27, steps per second:   8, episode reward: 142.500, mean reward:  5.278 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 76.404806, mean_q: 47.813057, mean_eps: 0.100000\n","     284417/2000000000: episode: 7864, duration: 4.777s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 86.103549, mean_q: 48.399346, mean_eps: 0.100000\n","     284451/2000000000: episode: 7865, duration: 4.189s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 79.253537, mean_q: 48.320086, mean_eps: 0.100000\n","     284487/2000000000: episode: 7866, duration: 4.580s, episode steps:  36, steps per second:   8, episode reward: -42.000, mean reward: -1.167 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 88.071070, mean_q: 47.368722, mean_eps: 0.100000\n","     284515/2000000000: episode: 7867, duration: 3.575s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 81.946810, mean_q: 47.835072, mean_eps: 0.100000\n","     284549/2000000000: episode: 7868, duration: 4.304s, episode steps:  34, steps per second:   8, episode reward: 37.300, mean reward:  1.097 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 94.308444, mean_q: 47.566238, mean_eps: 0.100000\n","     284586/2000000000: episode: 7869, duration: 4.793s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 85.211221, mean_q: 47.438696, mean_eps: 0.100000\n","     284624/2000000000: episode: 7870, duration: 4.955s, episode steps:  38, steps per second:   8, episode reward: 140.700, mean reward:  3.703 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 83.813202, mean_q: 47.423163, mean_eps: 0.100000\n","     284654/2000000000: episode: 7871, duration: 3.850s, episode steps:  30, steps per second:   8, episode reward: -32.100, mean reward: -1.070 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 83.967403, mean_q: 47.730932, mean_eps: 0.100000\n","     284686/2000000000: episode: 7872, duration: 4.058s, episode steps:  32, steps per second:   8, episode reward: 178.100, mean reward:  5.566 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 77.019700, mean_q: 47.428838, mean_eps: 0.100000\n","     284717/2000000000: episode: 7873, duration: 3.887s, episode steps:  31, steps per second:   8, episode reward: -12.400, mean reward: -0.400 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.773900, mean_q: 48.151776, mean_eps: 0.100000\n","     284757/2000000000: episode: 7874, duration: 5.008s, episode steps:  40, steps per second:   8, episode reward: 90.100, mean reward:  2.253 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 86.415011, mean_q: 47.983835, mean_eps: 0.100000\n","     284797/2000000000: episode: 7875, duration: 4.868s, episode steps:  40, steps per second:   8, episode reward: 28.300, mean reward:  0.707 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.305641, mean_q: 48.018756, mean_eps: 0.100000\n","     284833/2000000000: episode: 7876, duration: 4.440s, episode steps:  36, steps per second:   8, episode reward: 243.700, mean reward:  6.769 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 79.187718, mean_q: 46.936077, mean_eps: 0.100000\n","     284868/2000000000: episode: 7877, duration: 4.344s, episode steps:  35, steps per second:   8, episode reward: 152.500, mean reward:  4.357 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 87.345652, mean_q: 47.802278, mean_eps: 0.100000\n","     284895/2000000000: episode: 7878, duration: 3.403s, episode steps:  27, steps per second:   8, episode reward: 220.000, mean reward:  8.148 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 90.993025, mean_q: 47.367639, mean_eps: 0.100000\n","     284927/2000000000: episode: 7879, duration: 4.028s, episode steps:  32, steps per second:   8, episode reward: 114.100, mean reward:  3.566 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.533749, mean_q: 46.778904, mean_eps: 0.100000\n","     284961/2000000000: episode: 7880, duration: 4.320s, episode steps:  34, steps per second:   8, episode reward: 160.800, mean reward:  4.729 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 74.396570, mean_q: 46.767267, mean_eps: 0.100000\n","     284994/2000000000: episode: 7881, duration: 4.168s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.945692, mean_q: 46.957756, mean_eps: 0.100000\n","     285019/2000000000: episode: 7882, duration: 3.258s, episode steps:  25, steps per second:   8, episode reward: 16.700, mean reward:  0.668 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 84.941872, mean_q: 47.455993, mean_eps: 0.100000\n","     285058/2000000000: episode: 7883, duration: 5.121s, episode steps:  39, steps per second:   8, episode reward: 104.800, mean reward:  2.687 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 85.387906, mean_q: 47.269945, mean_eps: 0.100000\n","     285085/2000000000: episode: 7884, duration: 3.414s, episode steps:  27, steps per second:   8, episode reward: -183.500, mean reward: -6.796 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.031017, mean_q: 47.503952, mean_eps: 0.100000\n","     285118/2000000000: episode: 7885, duration: 4.181s, episode steps:  33, steps per second:   8, episode reward:  8.600, mean reward:  0.261 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 84.037883, mean_q: 48.106419, mean_eps: 0.100000\n","     285158/2000000000: episode: 7886, duration: 4.862s, episode steps:  40, steps per second:   8, episode reward: 39.700, mean reward:  0.992 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.217845, mean_q: 48.140753, mean_eps: 0.100000\n","     285188/2000000000: episode: 7887, duration: 3.695s, episode steps:  30, steps per second:   8, episode reward: 127.100, mean reward:  4.237 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 100.749543, mean_q: 47.374480, mean_eps: 0.100000\n","     285220/2000000000: episode: 7888, duration: 4.038s, episode steps:  32, steps per second:   8, episode reward: 104.100, mean reward:  3.253 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 91.415439, mean_q: 45.911258, mean_eps: 0.100000\n","     285255/2000000000: episode: 7889, duration: 4.415s, episode steps:  35, steps per second:   8, episode reward: 87.300, mean reward:  2.494 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 85.927371, mean_q: 48.869969, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     285290/2000000000: episode: 7890, duration: 4.298s, episode steps:  35, steps per second:   8, episode reward: 54.200, mean reward:  1.549 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 95.093795, mean_q: 47.360237, mean_eps: 0.100000\n","     285322/2000000000: episode: 7891, duration: 4.068s, episode steps:  32, steps per second:   8, episode reward: 32.700, mean reward:  1.022 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 92.092871, mean_q: 47.342337, mean_eps: 0.100000\n","     285354/2000000000: episode: 7892, duration: 4.016s, episode steps:  32, steps per second:   8, episode reward: 104.400, mean reward:  3.263 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 89.209361, mean_q: 47.584568, mean_eps: 0.100000\n","     285394/2000000000: episode: 7893, duration: 5.072s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 89.036273, mean_q: 46.360041, mean_eps: 0.100000\n","     285428/2000000000: episode: 7894, duration: 4.140s, episode steps:  34, steps per second:   8, episode reward: 192.300, mean reward:  5.656 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 83.535509, mean_q: 47.728838, mean_eps: 0.100000\n","     285458/2000000000: episode: 7895, duration: 3.643s, episode steps:  30, steps per second:   8, episode reward: 150.700, mean reward:  5.023 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 87.975864, mean_q: 47.879688, mean_eps: 0.100000\n","     285498/2000000000: episode: 7896, duration: 4.828s, episode steps:  40, steps per second:   8, episode reward: 68.000, mean reward:  1.700 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 85.800258, mean_q: 47.286435, mean_eps: 0.100000\n","     285538/2000000000: episode: 7897, duration: 5.137s, episode steps:  40, steps per second:   8, episode reward: 172.700, mean reward:  4.317 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 75.559307, mean_q: 47.408909, mean_eps: 0.100000\n","     285563/2000000000: episode: 7898, duration: 3.213s, episode steps:  25, steps per second:   8, episode reward: 161.400, mean reward:  6.456 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 83.381259, mean_q: 48.596729, mean_eps: 0.100000\n","     285595/2000000000: episode: 7899, duration: 4.104s, episode steps:  32, steps per second:   8, episode reward: 75.200, mean reward:  2.350 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 93.593919, mean_q: 46.569888, mean_eps: 0.100000\n","     285619/2000000000: episode: 7900, duration: 3.329s, episode steps:  24, steps per second:   7, episode reward: 180.300, mean reward:  7.513 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 82.391555, mean_q: 47.697736, mean_eps: 0.100000\n","     285658/2000000000: episode: 7901, duration: 5.233s, episode steps:  39, steps per second:   7, episode reward: 212.700, mean reward:  5.454 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 74.740777, mean_q: 47.662981, mean_eps: 0.100000\n","     285681/2000000000: episode: 7902, duration: 3.103s, episode steps:  23, steps per second:   7, episode reward:  9.100, mean reward:  0.396 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 85.672410, mean_q: 47.618579, mean_eps: 0.100000\n","     285702/2000000000: episode: 7903, duration: 2.739s, episode steps:  21, steps per second:   8, episode reward: 238.000, mean reward: 11.333 [-20.000, 18.000], mean action: 0.571 [0.000, 2.000],  loss: 98.667773, mean_q: 46.805896, mean_eps: 0.100000\n","     285729/2000000000: episode: 7904, duration: 3.517s, episode steps:  27, steps per second:   8, episode reward: 237.100, mean reward:  8.781 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 94.177522, mean_q: 46.845401, mean_eps: 0.100000\n","     285761/2000000000: episode: 7905, duration: 4.244s, episode steps:  32, steps per second:   8, episode reward: 39.700, mean reward:  1.241 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 76.284590, mean_q: 47.994878, mean_eps: 0.100000\n","     285785/2000000000: episode: 7906, duration: 2.976s, episode steps:  24, steps per second:   8, episode reward: 159.700, mean reward:  6.654 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 88.588620, mean_q: 47.175814, mean_eps: 0.100000\n","     285811/2000000000: episode: 7907, duration: 3.274s, episode steps:  26, steps per second:   8, episode reward: 168.900, mean reward:  6.496 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 82.781682, mean_q: 48.382586, mean_eps: 0.100000\n","     285847/2000000000: episode: 7908, duration: 4.562s, episode steps:  36, steps per second:   8, episode reward: 195.200, mean reward:  5.422 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 89.796339, mean_q: 47.571317, mean_eps: 0.100000\n","     285884/2000000000: episode: 7909, duration: 4.702s, episode steps:  37, steps per second:   8, episode reward: 48.300, mean reward:  1.305 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.205760, mean_q: 46.739455, mean_eps: 0.100000\n","     285918/2000000000: episode: 7910, duration: 4.200s, episode steps:  34, steps per second:   8, episode reward: 69.700, mean reward:  2.050 [-20.000, 18.700], mean action: 1.029 [0.000, 2.000],  loss: 80.957158, mean_q: 47.778053, mean_eps: 0.100000\n","     285946/2000000000: episode: 7911, duration: 3.477s, episode steps:  28, steps per second:   8, episode reward: 124.900, mean reward:  4.461 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.792286, mean_q: 47.277670, mean_eps: 0.100000\n","     285986/2000000000: episode: 7912, duration: 4.887s, episode steps:  40, steps per second:   8, episode reward: 155.300, mean reward:  3.883 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 90.425968, mean_q: 47.246980, mean_eps: 0.100000\n","     286018/2000000000: episode: 7913, duration: 3.920s, episode steps:  32, steps per second:   8, episode reward: 111.900, mean reward:  3.497 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 73.536206, mean_q: 47.209612, mean_eps: 0.100000\n","     286049/2000000000: episode: 7914, duration: 3.821s, episode steps:  31, steps per second:   8, episode reward: 196.000, mean reward:  6.323 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 82.065976, mean_q: 46.126261, mean_eps: 0.100000\n","     286082/2000000000: episode: 7915, duration: 4.075s, episode steps:  33, steps per second:   8, episode reward: 239.100, mean reward:  7.245 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 85.324731, mean_q: 47.744315, mean_eps: 0.100000\n","     286110/2000000000: episode: 7916, duration: 3.436s, episode steps:  28, steps per second:   8, episode reward: 100.700, mean reward:  3.596 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 84.030739, mean_q: 47.671372, mean_eps: 0.100000\n","     286145/2000000000: episode: 7917, duration: 4.173s, episode steps:  35, steps per second:   8, episode reward: 106.500, mean reward:  3.043 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 94.983619, mean_q: 47.632153, mean_eps: 0.100000\n","     286185/2000000000: episode: 7918, duration: 4.970s, episode steps:  40, steps per second:   8, episode reward: -132.000, mean reward: -3.300 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.554037, mean_q: 47.832831, mean_eps: 0.100000\n","     286216/2000000000: episode: 7919, duration: 3.817s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 84.852339, mean_q: 47.731717, mean_eps: 0.100000\n","     286249/2000000000: episode: 7920, duration: 4.249s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 88.573778, mean_q: 46.789774, mean_eps: 0.100000\n","     286282/2000000000: episode: 7921, duration: 4.111s, episode steps:  33, steps per second:   8, episode reward: -10.700, mean reward: -0.324 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 80.760423, mean_q: 48.817374, mean_eps: 0.100000\n","     286313/2000000000: episode: 7922, duration: 3.877s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 84.531868, mean_q: 47.243389, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     286345/2000000000: episode: 7923, duration: 3.889s, episode steps:  32, steps per second:   8, episode reward: 17.100, mean reward:  0.534 [-20.000, 19.900], mean action: 1.031 [0.000, 2.000],  loss: 85.636121, mean_q: 47.243788, mean_eps: 0.100000\n","     286378/2000000000: episode: 7924, duration: 4.057s, episode steps:  33, steps per second:   8, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.461668, mean_q: 47.461209, mean_eps: 0.100000\n","     286418/2000000000: episode: 7925, duration: 5.280s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.410179, mean_q: 48.124505, mean_eps: 0.100000\n","     286453/2000000000: episode: 7926, duration: 4.924s, episode steps:  35, steps per second:   7, episode reward: -14.300, mean reward: -0.409 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 83.961109, mean_q: 46.417655, mean_eps: 0.100000\n","     286493/2000000000: episode: 7927, duration: 5.509s, episode steps:  40, steps per second:   7, episode reward: 124.600, mean reward:  3.115 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 85.539601, mean_q: 47.642427, mean_eps: 0.100000\n","     286533/2000000000: episode: 7928, duration: 5.016s, episode steps:  40, steps per second:   8, episode reward: 100.000, mean reward:  2.500 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 87.650785, mean_q: 47.585658, mean_eps: 0.100000\n","     286566/2000000000: episode: 7929, duration: 4.181s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 92.901936, mean_q: 47.265917, mean_eps: 0.100000\n","     286602/2000000000: episode: 7930, duration: 4.579s, episode steps:  36, steps per second:   8, episode reward: 41.800, mean reward:  1.161 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 79.106785, mean_q: 47.915839, mean_eps: 0.100000\n","     286637/2000000000: episode: 7931, duration: 4.388s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 78.095737, mean_q: 46.332344, mean_eps: 0.100000\n","     286667/2000000000: episode: 7932, duration: 3.629s, episode steps:  30, steps per second:   8, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 95.939735, mean_q: 46.712418, mean_eps: 0.100000\n","     286706/2000000000: episode: 7933, duration: 4.924s, episode steps:  39, steps per second:   8, episode reward: 18.000, mean reward:  0.462 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 81.893677, mean_q: 47.915174, mean_eps: 0.100000\n","     286742/2000000000: episode: 7934, duration: 4.383s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 85.641108, mean_q: 47.539739, mean_eps: 0.100000\n","     286778/2000000000: episode: 7935, duration: 4.488s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 87.262926, mean_q: 46.927536, mean_eps: 0.100000\n","     286815/2000000000: episode: 7936, duration: 4.648s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 87.986159, mean_q: 47.639578, mean_eps: 0.100000\n","     286847/2000000000: episode: 7937, duration: 3.914s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 81.058071, mean_q: 48.001330, mean_eps: 0.100000\n","     286883/2000000000: episode: 7938, duration: 4.461s, episode steps:  36, steps per second:   8, episode reward: -4.500, mean reward: -0.125 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 82.873876, mean_q: 47.835496, mean_eps: 0.100000\n","     286923/2000000000: episode: 7939, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.245704, mean_q: 47.016370, mean_eps: 0.100000\n","     286949/2000000000: episode: 7940, duration: 3.314s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 88.756665, mean_q: 47.673522, mean_eps: 0.100000\n","     286975/2000000000: episode: 7941, duration: 3.239s, episode steps:  26, steps per second:   8, episode reward: 123.200, mean reward:  4.738 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.782870, mean_q: 46.839956, mean_eps: 0.100000\n","     287007/2000000000: episode: 7942, duration: 3.957s, episode steps:  32, steps per second:   8, episode reward: 112.200, mean reward:  3.506 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 83.219001, mean_q: 48.402647, mean_eps: 0.100000\n","     287043/2000000000: episode: 7943, duration: 4.440s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 82.304356, mean_q: 47.889422, mean_eps: 0.100000\n","     287083/2000000000: episode: 7944, duration: 4.923s, episode steps:  40, steps per second:   8, episode reward: 42.500, mean reward:  1.062 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 83.282462, mean_q: 47.103636, mean_eps: 0.100000\n","     287109/2000000000: episode: 7945, duration: 3.185s, episode steps:  26, steps per second:   8, episode reward: 118.900, mean reward:  4.573 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 83.985681, mean_q: 47.760122, mean_eps: 0.100000\n","     287149/2000000000: episode: 7946, duration: 4.982s, episode steps:  40, steps per second:   8, episode reward: 194.700, mean reward:  4.867 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.272097, mean_q: 47.000974, mean_eps: 0.100000\n","     287186/2000000000: episode: 7947, duration: 4.750s, episode steps:  37, steps per second:   8, episode reward: 27.400, mean reward:  0.741 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 93.054371, mean_q: 47.683943, mean_eps: 0.100000\n","     287219/2000000000: episode: 7948, duration: 4.299s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 82.888144, mean_q: 48.236732, mean_eps: 0.100000\n","     287255/2000000000: episode: 7949, duration: 4.582s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 86.210473, mean_q: 47.894461, mean_eps: 0.100000\n","     287293/2000000000: episode: 7950, duration: 4.887s, episode steps:  38, steps per second:   8, episode reward: 58.000, mean reward:  1.526 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 93.664947, mean_q: 47.378805, mean_eps: 0.100000\n","     287321/2000000000: episode: 7951, duration: 3.593s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 78.363682, mean_q: 47.121718, mean_eps: 0.100000\n","     287353/2000000000: episode: 7952, duration: 4.085s, episode steps:  32, steps per second:   8, episode reward:  7.800, mean reward:  0.244 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 83.007934, mean_q: 47.841921, mean_eps: 0.100000\n","     287384/2000000000: episode: 7953, duration: 3.954s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 86.756290, mean_q: 47.576827, mean_eps: 0.100000\n","     287420/2000000000: episode: 7954, duration: 4.700s, episode steps:  36, steps per second:   8, episode reward: -38.400, mean reward: -1.067 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 86.465704, mean_q: 48.528104, mean_eps: 0.100000\n","     287459/2000000000: episode: 7955, duration: 5.165s, episode steps:  39, steps per second:   8, episode reward: 67.200, mean reward:  1.723 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 77.999152, mean_q: 47.792264, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     287494/2000000000: episode: 7956, duration: 4.514s, episode steps:  35, steps per second:   8, episode reward: 104.000, mean reward:  2.971 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 93.092602, mean_q: 46.373367, mean_eps: 0.100000\n","     287531/2000000000: episode: 7957, duration: 4.692s, episode steps:  37, steps per second:   8, episode reward: -15.800, mean reward: -0.427 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 85.887661, mean_q: 46.823599, mean_eps: 0.100000\n","     287566/2000000000: episode: 7958, duration: 4.216s, episode steps:  35, steps per second:   8, episode reward: 82.000, mean reward:  2.343 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 78.879708, mean_q: 47.436327, mean_eps: 0.100000\n","     287594/2000000000: episode: 7959, duration: 3.606s, episode steps:  28, steps per second:   8, episode reward: 207.100, mean reward:  7.396 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 86.300465, mean_q: 46.801836, mean_eps: 0.100000\n","     287628/2000000000: episode: 7960, duration: 4.464s, episode steps:  34, steps per second:   8, episode reward: 209.800, mean reward:  6.171 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 91.881610, mean_q: 47.614868, mean_eps: 0.100000\n","     287657/2000000000: episode: 7961, duration: 3.771s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 85.609319, mean_q: 47.307972, mean_eps: 0.100000\n","     287693/2000000000: episode: 7962, duration: 4.635s, episode steps:  36, steps per second:   8, episode reward: 152.200, mean reward:  4.228 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 79.454618, mean_q: 48.172648, mean_eps: 0.100000\n","     287726/2000000000: episode: 7963, duration: 4.452s, episode steps:  33, steps per second:   7, episode reward: 46.000, mean reward:  1.394 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.387058, mean_q: 46.708525, mean_eps: 0.100000\n","     287766/2000000000: episode: 7964, duration: 5.303s, episode steps:  40, steps per second:   8, episode reward: 117.900, mean reward:  2.948 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 76.436815, mean_q: 46.187136, mean_eps: 0.100000\n","     287792/2000000000: episode: 7965, duration: 3.463s, episode steps:  26, steps per second:   8, episode reward: 233.000, mean reward:  8.962 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 85.313522, mean_q: 48.229180, mean_eps: 0.100000\n","     287821/2000000000: episode: 7966, duration: 3.839s, episode steps:  29, steps per second:   8, episode reward: 157.600, mean reward:  5.434 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.264586, mean_q: 47.951662, mean_eps: 0.100000\n","     287850/2000000000: episode: 7967, duration: 3.665s, episode steps:  29, steps per second:   8, episode reward: -4.100, mean reward: -0.141 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.386674, mean_q: 47.827920, mean_eps: 0.100000\n","     287880/2000000000: episode: 7968, duration: 3.858s, episode steps:  30, steps per second:   8, episode reward: 25.300, mean reward:  0.843 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.125877, mean_q: 47.905146, mean_eps: 0.100000\n","     287916/2000000000: episode: 7969, duration: 4.338s, episode steps:  36, steps per second:   8, episode reward: 112.000, mean reward:  3.111 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 80.607720, mean_q: 47.979259, mean_eps: 0.100000\n","     287954/2000000000: episode: 7970, duration: 4.688s, episode steps:  38, steps per second:   8, episode reward: 70.300, mean reward:  1.850 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 85.838534, mean_q: 47.331140, mean_eps: 0.100000\n","     287988/2000000000: episode: 7971, duration: 4.360s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 81.207481, mean_q: 47.065532, mean_eps: 0.100000\n","     288019/2000000000: episode: 7972, duration: 3.995s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.226 [0.000, 2.000],  loss: 85.237004, mean_q: 47.422266, mean_eps: 0.100000\n","     288055/2000000000: episode: 7973, duration: 4.605s, episode steps:  36, steps per second:   8, episode reward: 170.800, mean reward:  4.744 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.787659, mean_q: 47.219270, mean_eps: 0.100000\n","     288089/2000000000: episode: 7974, duration: 4.224s, episode steps:  34, steps per second:   8, episode reward: 114.600, mean reward:  3.371 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 82.775664, mean_q: 47.065975, mean_eps: 0.100000\n","     288126/2000000000: episode: 7975, duration: 4.471s, episode steps:  37, steps per second:   8, episode reward: 61.600, mean reward:  1.665 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 81.905344, mean_q: 47.241452, mean_eps: 0.100000\n","     288159/2000000000: episode: 7976, duration: 3.977s, episode steps:  33, steps per second:   8, episode reward: 100.700, mean reward:  3.052 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 89.228121, mean_q: 46.667236, mean_eps: 0.100000\n","     288192/2000000000: episode: 7977, duration: 4.055s, episode steps:  33, steps per second:   8, episode reward: 154.400, mean reward:  4.679 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 86.516813, mean_q: 48.076901, mean_eps: 0.100000\n","     288232/2000000000: episode: 7978, duration: 4.854s, episode steps:  40, steps per second:   8, episode reward: 178.800, mean reward:  4.470 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.453162, mean_q: 47.392275, mean_eps: 0.100000\n","     288267/2000000000: episode: 7979, duration: 4.432s, episode steps:  35, steps per second:   8, episode reward: 73.500, mean reward:  2.100 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 101.373536, mean_q: 47.361082, mean_eps: 0.100000\n","     288307/2000000000: episode: 7980, duration: 5.218s, episode steps:  40, steps per second:   8, episode reward: -19.900, mean reward: -0.498 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.638342, mean_q: 47.608854, mean_eps: 0.100000\n","     288344/2000000000: episode: 7981, duration: 4.925s, episode steps:  37, steps per second:   8, episode reward: 199.700, mean reward:  5.397 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 84.539326, mean_q: 47.426428, mean_eps: 0.100000\n","     288379/2000000000: episode: 7982, duration: 4.520s, episode steps:  35, steps per second:   8, episode reward: 186.300, mean reward:  5.323 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 77.717490, mean_q: 47.464406, mean_eps: 0.100000\n","     288409/2000000000: episode: 7983, duration: 3.743s, episode steps:  30, steps per second:   8, episode reward: -51.000, mean reward: -1.700 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 84.650971, mean_q: 48.763658, mean_eps: 0.100000\n","     288439/2000000000: episode: 7984, duration: 3.799s, episode steps:  30, steps per second:   8, episode reward: 17.200, mean reward:  0.573 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 88.503674, mean_q: 49.450220, mean_eps: 0.100000\n","     288477/2000000000: episode: 7985, duration: 4.813s, episode steps:  38, steps per second:   8, episode reward:  0.900, mean reward:  0.024 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 82.676575, mean_q: 47.430284, mean_eps: 0.100000\n","     288512/2000000000: episode: 7986, duration: 4.350s, episode steps:  35, steps per second:   8, episode reward: 141.600, mean reward:  4.046 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 77.531133, mean_q: 47.530733, mean_eps: 0.100000\n","     288547/2000000000: episode: 7987, duration: 4.527s, episode steps:  35, steps per second:   8, episode reward: 208.000, mean reward:  5.943 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 78.657065, mean_q: 47.925282, mean_eps: 0.100000\n","     288583/2000000000: episode: 7988, duration: 4.598s, episode steps:  36, steps per second:   8, episode reward: -94.200, mean reward: -2.617 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 93.617346, mean_q: 47.429970, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     288615/2000000000: episode: 7989, duration: 4.008s, episode steps:  32, steps per second:   8, episode reward: 208.000, mean reward:  6.500 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 90.706490, mean_q: 47.076891, mean_eps: 0.100000\n","     288648/2000000000: episode: 7990, duration: 4.109s, episode steps:  33, steps per second:   8, episode reward: 144.300, mean reward:  4.373 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 85.354390, mean_q: 48.074936, mean_eps: 0.100000\n","     288679/2000000000: episode: 7991, duration: 3.917s, episode steps:  31, steps per second:   8, episode reward: 134.000, mean reward:  4.323 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 78.980095, mean_q: 48.067657, mean_eps: 0.100000\n","     288712/2000000000: episode: 7992, duration: 4.154s, episode steps:  33, steps per second:   8, episode reward: 47.700, mean reward:  1.445 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.796480, mean_q: 47.097146, mean_eps: 0.100000\n","     288741/2000000000: episode: 7993, duration: 3.683s, episode steps:  29, steps per second:   8, episode reward: 195.200, mean reward:  6.731 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 90.998188, mean_q: 47.815865, mean_eps: 0.100000\n","     288772/2000000000: episode: 7994, duration: 3.898s, episode steps:  31, steps per second:   8, episode reward: 56.600, mean reward:  1.826 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 85.952976, mean_q: 47.698583, mean_eps: 0.100000\n","     288810/2000000000: episode: 7995, duration: 4.838s, episode steps:  38, steps per second:   8, episode reward: 40.000, mean reward:  1.053 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 92.918382, mean_q: 47.826146, mean_eps: 0.100000\n","     288849/2000000000: episode: 7996, duration: 5.016s, episode steps:  39, steps per second:   8, episode reward: 65.500, mean reward:  1.679 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 79.751559, mean_q: 48.439925, mean_eps: 0.100000\n","     288884/2000000000: episode: 7997, duration: 4.795s, episode steps:  35, steps per second:   7, episode reward: 62.900, mean reward:  1.797 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 81.806678, mean_q: 47.364728, mean_eps: 0.100000\n","     288921/2000000000: episode: 7998, duration: 4.844s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 88.672461, mean_q: 46.691563, mean_eps: 0.100000\n","     288954/2000000000: episode: 7999, duration: 4.153s, episode steps:  33, steps per second:   8, episode reward: 170.000, mean reward:  5.152 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 82.703827, mean_q: 48.424256, mean_eps: 0.100000\n","     288985/2000000000: episode: 8000, duration: 3.904s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 82.692990, mean_q: 47.583194, mean_eps: 0.100000\n","     289018/2000000000: episode: 8001, duration: 4.502s, episode steps:  33, steps per second:   7, episode reward: 64.500, mean reward:  1.955 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 86.919222, mean_q: 48.214255, mean_eps: 0.100000\n","     289058/2000000000: episode: 8002, duration: 5.268s, episode steps:  40, steps per second:   8, episode reward: 66.100, mean reward:  1.653 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 86.511998, mean_q: 47.417674, mean_eps: 0.100000\n","     289095/2000000000: episode: 8003, duration: 4.881s, episode steps:  37, steps per second:   8, episode reward: 55.700, mean reward:  1.505 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 85.327855, mean_q: 47.720139, mean_eps: 0.100000\n","     289129/2000000000: episode: 8004, duration: 4.297s, episode steps:  34, steps per second:   8, episode reward: 246.000, mean reward:  7.235 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.069068, mean_q: 47.808861, mean_eps: 0.100000\n","     289169/2000000000: episode: 8005, duration: 5.020s, episode steps:  40, steps per second:   8, episode reward: 38.700, mean reward:  0.968 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 89.350031, mean_q: 48.454705, mean_eps: 0.100000\n","     289200/2000000000: episode: 8006, duration: 4.050s, episode steps:  31, steps per second:   8, episode reward: 77.600, mean reward:  2.503 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 89.020521, mean_q: 47.142073, mean_eps: 0.100000\n","     289226/2000000000: episode: 8007, duration: 3.494s, episode steps:  26, steps per second:   7, episode reward: 235.900, mean reward:  9.073 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 89.343360, mean_q: 46.980245, mean_eps: 0.100000\n","     289261/2000000000: episode: 8008, duration: 4.525s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 94.430311, mean_q: 46.973536, mean_eps: 0.100000\n","     289297/2000000000: episode: 8009, duration: 4.660s, episode steps:  36, steps per second:   8, episode reward: 111.200, mean reward:  3.089 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 79.263509, mean_q: 46.673588, mean_eps: 0.100000\n","     289325/2000000000: episode: 8010, duration: 3.594s, episode steps:  28, steps per second:   8, episode reward: 97.600, mean reward:  3.486 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 84.891229, mean_q: 47.358896, mean_eps: 0.100000\n","     289354/2000000000: episode: 8011, duration: 3.693s, episode steps:  29, steps per second:   8, episode reward: 185.200, mean reward:  6.386 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 89.921640, mean_q: 47.761917, mean_eps: 0.100000\n","     289385/2000000000: episode: 8012, duration: 4.064s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 84.807498, mean_q: 46.500741, mean_eps: 0.100000\n","     289413/2000000000: episode: 8013, duration: 3.785s, episode steps:  28, steps per second:   7, episode reward: 155.100, mean reward:  5.539 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 89.442955, mean_q: 48.185201, mean_eps: 0.100000\n","     289443/2000000000: episode: 8014, duration: 4.035s, episode steps:  30, steps per second:   7, episode reward: 88.900, mean reward:  2.963 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.792592, mean_q: 49.081251, mean_eps: 0.100000\n","     289472/2000000000: episode: 8015, duration: 3.749s, episode steps:  29, steps per second:   8, episode reward: 120.200, mean reward:  4.145 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 91.531535, mean_q: 47.508777, mean_eps: 0.100000\n","     289512/2000000000: episode: 8016, duration: 5.057s, episode steps:  40, steps per second:   8, episode reward: 15.300, mean reward:  0.382 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 87.274553, mean_q: 46.838143, mean_eps: 0.100000\n","     289540/2000000000: episode: 8017, duration: 3.852s, episode steps:  28, steps per second:   7, episode reward: 141.600, mean reward:  5.057 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 94.397572, mean_q: 46.940419, mean_eps: 0.100000\n","     289580/2000000000: episode: 8018, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: 74.100, mean reward:  1.852 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.766052, mean_q: 47.512340, mean_eps: 0.100000\n","     289610/2000000000: episode: 8019, duration: 3.820s, episode steps:  30, steps per second:   8, episode reward: 124.900, mean reward:  4.163 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 86.024569, mean_q: 47.078426, mean_eps: 0.100000\n","     289650/2000000000: episode: 8020, duration: 4.996s, episode steps:  40, steps per second:   8, episode reward: 44.900, mean reward:  1.122 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 78.161510, mean_q: 47.411172, mean_eps: 0.100000\n","     289690/2000000000: episode: 8021, duration: 4.967s, episode steps:  40, steps per second:   8, episode reward: -40.200, mean reward: -1.005 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 78.069295, mean_q: 47.641416, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     289723/2000000000: episode: 8022, duration: 4.125s, episode steps:  33, steps per second:   8, episode reward: 119.100, mean reward:  3.609 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 87.514887, mean_q: 47.324747, mean_eps: 0.100000\n","     289760/2000000000: episode: 8023, duration: 4.687s, episode steps:  37, steps per second:   8, episode reward: 102.000, mean reward:  2.757 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 85.271618, mean_q: 46.974467, mean_eps: 0.100000\n","     289800/2000000000: episode: 8024, duration: 4.772s, episode steps:  40, steps per second:   8, episode reward: 93.200, mean reward:  2.330 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.713174, mean_q: 47.730755, mean_eps: 0.100000\n","     289834/2000000000: episode: 8025, duration: 4.145s, episode steps:  34, steps per second:   8, episode reward: -124.900, mean reward: -3.674 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 90.550719, mean_q: 48.446985, mean_eps: 0.100000\n","     289863/2000000000: episode: 8026, duration: 3.504s, episode steps:  29, steps per second:   8, episode reward: 77.600, mean reward:  2.676 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 83.395277, mean_q: 47.019165, mean_eps: 0.100000\n","     289892/2000000000: episode: 8027, duration: 3.604s, episode steps:  29, steps per second:   8, episode reward: 106.600, mean reward:  3.676 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 80.339832, mean_q: 48.388871, mean_eps: 0.100000\n","     289922/2000000000: episode: 8028, duration: 3.819s, episode steps:  30, steps per second:   8, episode reward: 253.500, mean reward:  8.450 [-20.000, 19.600], mean action: 0.967 [0.000, 2.000],  loss: 78.454078, mean_q: 47.752600, mean_eps: 0.100000\n","     289953/2000000000: episode: 8029, duration: 4.164s, episode steps:  31, steps per second:   7, episode reward: 98.200, mean reward:  3.168 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 89.233683, mean_q: 48.482855, mean_eps: 0.100000\n","     289978/2000000000: episode: 8030, duration: 3.275s, episode steps:  25, steps per second:   8, episode reward: 214.500, mean reward:  8.580 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 88.124465, mean_q: 46.474056, mean_eps: 0.100000\n","     290006/2000000000: episode: 8031, duration: 3.674s, episode steps:  28, steps per second:   8, episode reward: 10.400, mean reward:  0.371 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 90.154446, mean_q: 47.250376, mean_eps: 0.100000\n","     290031/2000000000: episode: 8032, duration: 3.253s, episode steps:  25, steps per second:   8, episode reward: 180.100, mean reward:  7.204 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 92.788956, mean_q: 48.133441, mean_eps: 0.100000\n","     290067/2000000000: episode: 8033, duration: 4.740s, episode steps:  36, steps per second:   8, episode reward: 92.200, mean reward:  2.561 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 82.026594, mean_q: 49.924015, mean_eps: 0.100000\n","     290102/2000000000: episode: 8034, duration: 4.551s, episode steps:  35, steps per second:   8, episode reward: 177.500, mean reward:  5.071 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 96.049021, mean_q: 49.316257, mean_eps: 0.100000\n","     290131/2000000000: episode: 8035, duration: 3.694s, episode steps:  29, steps per second:   8, episode reward: -13.000, mean reward: -0.448 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 86.774246, mean_q: 49.237080, mean_eps: 0.100000\n","     290154/2000000000: episode: 8036, duration: 3.107s, episode steps:  23, steps per second:   7, episode reward: 247.500, mean reward: 10.761 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 86.405748, mean_q: 49.612796, mean_eps: 0.100000\n","     290179/2000000000: episode: 8037, duration: 3.240s, episode steps:  25, steps per second:   8, episode reward: 215.700, mean reward:  8.628 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 81.945674, mean_q: 48.361938, mean_eps: 0.100000\n","     290211/2000000000: episode: 8038, duration: 4.238s, episode steps:  32, steps per second:   8, episode reward: 89.600, mean reward:  2.800 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 96.027595, mean_q: 48.426946, mean_eps: 0.100000\n","     290244/2000000000: episode: 8039, duration: 4.217s, episode steps:  33, steps per second:   8, episode reward: 82.600, mean reward:  2.503 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 82.753469, mean_q: 49.103908, mean_eps: 0.100000\n","     290266/2000000000: episode: 8040, duration: 2.734s, episode steps:  22, steps per second:   8, episode reward: 182.700, mean reward:  8.305 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 86.030845, mean_q: 48.986388, mean_eps: 0.100000\n","     290293/2000000000: episode: 8041, duration: 3.480s, episode steps:  27, steps per second:   8, episode reward: 31.400, mean reward:  1.163 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 85.176927, mean_q: 50.141899, mean_eps: 0.100000\n","     290321/2000000000: episode: 8042, duration: 3.545s, episode steps:  28, steps per second:   8, episode reward: 145.400, mean reward:  5.193 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.093273, mean_q: 48.505600, mean_eps: 0.100000\n","     290351/2000000000: episode: 8043, duration: 3.922s, episode steps:  30, steps per second:   8, episode reward: 181.900, mean reward:  6.063 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 86.967614, mean_q: 49.256620, mean_eps: 0.100000\n","     290376/2000000000: episode: 8044, duration: 3.248s, episode steps:  25, steps per second:   8, episode reward: 180.400, mean reward:  7.216 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 97.554962, mean_q: 48.187195, mean_eps: 0.100000\n","     290409/2000000000: episode: 8045, duration: 4.182s, episode steps:  33, steps per second:   8, episode reward: 41.000, mean reward:  1.242 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 95.593037, mean_q: 48.663517, mean_eps: 0.100000\n","     290449/2000000000: episode: 8046, duration: 5.160s, episode steps:  40, steps per second:   8, episode reward: 126.600, mean reward:  3.165 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 88.934386, mean_q: 48.810636, mean_eps: 0.100000\n","     290483/2000000000: episode: 8047, duration: 4.119s, episode steps:  34, steps per second:   8, episode reward: 55.700, mean reward:  1.638 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 83.576809, mean_q: 48.338424, mean_eps: 0.100000\n","     290513/2000000000: episode: 8048, duration: 3.637s, episode steps:  30, steps per second:   8, episode reward: 213.100, mean reward:  7.103 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 95.343117, mean_q: 48.420006, mean_eps: 0.100000\n","     290553/2000000000: episode: 8049, duration: 5.216s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.549404, mean_q: 49.691008, mean_eps: 0.100000\n","     290584/2000000000: episode: 8050, duration: 4.126s, episode steps:  31, steps per second:   8, episode reward: 173.100, mean reward:  5.584 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 91.879064, mean_q: 49.526604, mean_eps: 0.100000\n","     290608/2000000000: episode: 8051, duration: 3.166s, episode steps:  24, steps per second:   8, episode reward: -110.500, mean reward: -4.604 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 85.893794, mean_q: 50.381600, mean_eps: 0.100000\n","     290638/2000000000: episode: 8052, duration: 3.885s, episode steps:  30, steps per second:   8, episode reward: 180.900, mean reward:  6.030 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.237894, mean_q: 48.624376, mean_eps: 0.100000\n","     290663/2000000000: episode: 8053, duration: 3.193s, episode steps:  25, steps per second:   8, episode reward: 132.000, mean reward:  5.280 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 91.850144, mean_q: 48.544235, mean_eps: 0.100000\n","     290696/2000000000: episode: 8054, duration: 4.296s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 87.316538, mean_q: 49.130426, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     290735/2000000000: episode: 8055, duration: 5.067s, episode steps:  39, steps per second:   8, episode reward: 34.000, mean reward:  0.872 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 94.304209, mean_q: 48.284715, mean_eps: 0.100000\n","     290772/2000000000: episode: 8056, duration: 5.052s, episode steps:  37, steps per second:   7, episode reward: 26.200, mean reward:  0.708 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 88.027059, mean_q: 49.656826, mean_eps: 0.100000\n","     290809/2000000000: episode: 8057, duration: 4.750s, episode steps:  37, steps per second:   8, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 85.494328, mean_q: 48.920610, mean_eps: 0.100000\n","     290843/2000000000: episode: 8058, duration: 4.601s, episode steps:  34, steps per second:   7, episode reward: 116.300, mean reward:  3.421 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 85.157777, mean_q: 48.025186, mean_eps: 0.100000\n","     290883/2000000000: episode: 8059, duration: 5.321s, episode steps:  40, steps per second:   8, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 79.842337, mean_q: 48.580781, mean_eps: 0.100000\n","     290923/2000000000: episode: 8060, duration: 5.098s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 83.469331, mean_q: 48.863197, mean_eps: 0.100000\n","     290963/2000000000: episode: 8061, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 89.785830, mean_q: 49.784024, mean_eps: 0.100000\n","     290995/2000000000: episode: 8062, duration: 4.191s, episode steps:  32, steps per second:   8, episode reward: 56.000, mean reward:  1.750 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 82.617319, mean_q: 48.912891, mean_eps: 0.100000\n","     291029/2000000000: episode: 8063, duration: 4.265s, episode steps:  34, steps per second:   8, episode reward: 197.400, mean reward:  5.806 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 85.918652, mean_q: 48.404277, mean_eps: 0.100000\n","     291058/2000000000: episode: 8064, duration: 3.782s, episode steps:  29, steps per second:   8, episode reward: 90.400, mean reward:  3.117 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 91.325580, mean_q: 48.919747, mean_eps: 0.100000\n","     291098/2000000000: episode: 8065, duration: 5.417s, episode steps:  40, steps per second:   7, episode reward: -56.000, mean reward: -1.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 89.261800, mean_q: 48.804491, mean_eps: 0.100000\n","     291131/2000000000: episode: 8066, duration: 4.190s, episode steps:  33, steps per second:   8, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 91.436396, mean_q: 48.260198, mean_eps: 0.100000\n","     291167/2000000000: episode: 8067, duration: 5.115s, episode steps:  36, steps per second:   7, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.538284, mean_q: 49.465755, mean_eps: 0.100000\n","     291207/2000000000: episode: 8068, duration: 5.280s, episode steps:  40, steps per second:   8, episode reward: 173.000, mean reward:  4.325 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.758952, mean_q: 48.993860, mean_eps: 0.100000\n","     291232/2000000000: episode: 8069, duration: 3.566s, episode steps:  25, steps per second:   7, episode reward: 65.200, mean reward:  2.608 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 86.977026, mean_q: 49.950264, mean_eps: 0.100000\n","     291258/2000000000: episode: 8070, duration: 3.515s, episode steps:  26, steps per second:   7, episode reward: -96.000, mean reward: -3.692 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 92.844499, mean_q: 48.192776, mean_eps: 0.100000\n","     291288/2000000000: episode: 8071, duration: 4.038s, episode steps:  30, steps per second:   7, episode reward: 37.600, mean reward:  1.253 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 80.634672, mean_q: 48.821378, mean_eps: 0.100000\n","     291323/2000000000: episode: 8072, duration: 4.800s, episode steps:  35, steps per second:   7, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 85.449665, mean_q: 49.109275, mean_eps: 0.100000\n","     291361/2000000000: episode: 8073, duration: 5.308s, episode steps:  38, steps per second:   7, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 81.534934, mean_q: 49.358005, mean_eps: 0.100000\n","     291394/2000000000: episode: 8074, duration: 4.201s, episode steps:  33, steps per second:   8, episode reward: 186.300, mean reward:  5.645 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.472553, mean_q: 48.361015, mean_eps: 0.100000\n","     291434/2000000000: episode: 8075, duration: 5.085s, episode steps:  40, steps per second:   8, episode reward: 126.600, mean reward:  3.165 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 86.430579, mean_q: 48.794148, mean_eps: 0.100000\n","     291466/2000000000: episode: 8076, duration: 4.295s, episode steps:  32, steps per second:   7, episode reward: 21.200, mean reward:  0.662 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 92.369171, mean_q: 49.545289, mean_eps: 0.100000\n","     291491/2000000000: episode: 8077, duration: 3.185s, episode steps:  25, steps per second:   8, episode reward: 170.000, mean reward:  6.800 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 90.213999, mean_q: 50.083305, mean_eps: 0.100000\n","     291526/2000000000: episode: 8078, duration: 4.376s, episode steps:  35, steps per second:   8, episode reward: 244.400, mean reward:  6.983 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 87.393148, mean_q: 48.711237, mean_eps: 0.100000\n","     291563/2000000000: episode: 8079, duration: 4.594s, episode steps:  37, steps per second:   8, episode reward: 47.600, mean reward:  1.286 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 87.471685, mean_q: 48.772142, mean_eps: 0.100000\n","     291603/2000000000: episode: 8080, duration: 4.997s, episode steps:  40, steps per second:   8, episode reward:  0.200, mean reward:  0.005 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 81.630524, mean_q: 49.183536, mean_eps: 0.100000\n","     291635/2000000000: episode: 8081, duration: 4.163s, episode steps:  32, steps per second:   8, episode reward: 57.600, mean reward:  1.800 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 86.769410, mean_q: 48.888660, mean_eps: 0.100000\n","     291664/2000000000: episode: 8082, duration: 3.661s, episode steps:  29, steps per second:   8, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 81.859706, mean_q: 50.628429, mean_eps: 0.100000\n","     291696/2000000000: episode: 8083, duration: 4.225s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 89.554128, mean_q: 48.471950, mean_eps: 0.100000\n","     291721/2000000000: episode: 8084, duration: 3.169s, episode steps:  25, steps per second:   8, episode reward: 63.900, mean reward:  2.556 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 98.114076, mean_q: 48.233955, mean_eps: 0.100000\n","     291761/2000000000: episode: 8085, duration: 4.965s, episode steps:  40, steps per second:   8, episode reward: 160.500, mean reward:  4.013 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.996771, mean_q: 48.598090, mean_eps: 0.100000\n","     291788/2000000000: episode: 8086, duration: 3.491s, episode steps:  27, steps per second:   8, episode reward: -91.300, mean reward: -3.381 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 92.349172, mean_q: 48.883755, mean_eps: 0.100000\n","     291826/2000000000: episode: 8087, duration: 5.298s, episode steps:  38, steps per second:   7, episode reward: 83.400, mean reward:  2.195 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 81.292121, mean_q: 48.724090, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     291866/2000000000: episode: 8088, duration: 5.159s, episode steps:  40, steps per second:   8, episode reward:  8.700, mean reward:  0.217 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 92.544736, mean_q: 47.735029, mean_eps: 0.100000\n","     291902/2000000000: episode: 8089, duration: 4.426s, episode steps:  36, steps per second:   8, episode reward: 118.700, mean reward:  3.297 [-20.000, 18.000], mean action: 1.306 [0.000, 2.000],  loss: 88.370176, mean_q: 48.856045, mean_eps: 0.100000\n","     291929/2000000000: episode: 8090, duration: 3.373s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 85.402882, mean_q: 48.166511, mean_eps: 0.100000\n","     291969/2000000000: episode: 8091, duration: 5.289s, episode steps:  40, steps per second:   8, episode reward: -8.500, mean reward: -0.213 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.634710, mean_q: 48.475185, mean_eps: 0.100000\n","     292009/2000000000: episode: 8092, duration: 5.222s, episode steps:  40, steps per second:   8, episode reward: 97.300, mean reward:  2.432 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 93.207814, mean_q: 49.592455, mean_eps: 0.100000\n","     292043/2000000000: episode: 8093, duration: 4.378s, episode steps:  34, steps per second:   8, episode reward:  6.500, mean reward:  0.191 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 91.958492, mean_q: 47.885663, mean_eps: 0.100000\n","     292076/2000000000: episode: 8094, duration: 4.200s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 92.888700, mean_q: 49.748075, mean_eps: 0.100000\n","     292111/2000000000: episode: 8095, duration: 4.590s, episode steps:  35, steps per second:   8, episode reward: 308.200, mean reward:  8.806 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 83.376134, mean_q: 48.168134, mean_eps: 0.100000\n","     292143/2000000000: episode: 8096, duration: 4.330s, episode steps:  32, steps per second:   7, episode reward: 214.500, mean reward:  6.703 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.746715, mean_q: 49.009485, mean_eps: 0.100000\n","     292178/2000000000: episode: 8097, duration: 4.636s, episode steps:  35, steps per second:   8, episode reward: 54.800, mean reward:  1.566 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 83.350146, mean_q: 48.267923, mean_eps: 0.100000\n","     292209/2000000000: episode: 8098, duration: 4.119s, episode steps:  31, steps per second:   8, episode reward: 57.500, mean reward:  1.855 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 90.883341, mean_q: 48.201527, mean_eps: 0.100000\n","     292245/2000000000: episode: 8099, duration: 4.959s, episode steps:  36, steps per second:   7, episode reward: 147.800, mean reward:  4.106 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 77.035215, mean_q: 48.284992, mean_eps: 0.100000\n","     292283/2000000000: episode: 8100, duration: 5.203s, episode steps:  38, steps per second:   7, episode reward: 162.300, mean reward:  4.271 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 88.194725, mean_q: 47.722545, mean_eps: 0.100000\n","     292311/2000000000: episode: 8101, duration: 3.752s, episode steps:  28, steps per second:   7, episode reward: 212.300, mean reward:  7.582 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 85.488444, mean_q: 48.710985, mean_eps: 0.100000\n","     292343/2000000000: episode: 8102, duration: 4.335s, episode steps:  32, steps per second:   7, episode reward: 174.600, mean reward:  5.456 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 86.130116, mean_q: 49.705828, mean_eps: 0.100000\n","     292373/2000000000: episode: 8103, duration: 3.933s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 90.014656, mean_q: 48.173560, mean_eps: 0.100000\n","     292400/2000000000: episode: 8104, duration: 3.739s, episode steps:  27, steps per second:   7, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 84.236315, mean_q: 48.349724, mean_eps: 0.100000\n","     292440/2000000000: episode: 8105, duration: 5.279s, episode steps:  40, steps per second:   8, episode reward: 60.000, mean reward:  1.500 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.772863, mean_q: 48.120226, mean_eps: 0.100000\n","     292480/2000000000: episode: 8106, duration: 5.270s, episode steps:  40, steps per second:   8, episode reward: 138.500, mean reward:  3.463 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.034725, mean_q: 48.848765, mean_eps: 0.100000\n","     292506/2000000000: episode: 8107, duration: 3.445s, episode steps:  26, steps per second:   8, episode reward: -157.100, mean reward: -6.042 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 94.359943, mean_q: 48.824560, mean_eps: 0.100000\n","     292542/2000000000: episode: 8108, duration: 4.773s, episode steps:  36, steps per second:   8, episode reward: 159.800, mean reward:  4.439 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 92.614654, mean_q: 47.918197, mean_eps: 0.100000\n","     292579/2000000000: episode: 8109, duration: 4.793s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 90.628440, mean_q: 47.760524, mean_eps: 0.100000\n","     292609/2000000000: episode: 8110, duration: 3.951s, episode steps:  30, steps per second:   8, episode reward: 164.400, mean reward:  5.480 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.336098, mean_q: 48.741746, mean_eps: 0.100000\n","     292649/2000000000: episode: 8111, duration: 5.373s, episode steps:  40, steps per second:   7, episode reward: 200.300, mean reward:  5.007 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 80.112509, mean_q: 48.790685, mean_eps: 0.100000\n","     292689/2000000000: episode: 8112, duration: 5.268s, episode steps:  40, steps per second:   8, episode reward: 108.900, mean reward:  2.723 [-20.000, 18.300], mean action: 1.225 [0.000, 2.000],  loss: 87.634269, mean_q: 48.444240, mean_eps: 0.100000\n","     292723/2000000000: episode: 8113, duration: 4.523s, episode steps:  34, steps per second:   8, episode reward: 33.100, mean reward:  0.974 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 90.152573, mean_q: 47.134278, mean_eps: 0.100000\n","     292755/2000000000: episode: 8114, duration: 4.168s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 98.651085, mean_q: 48.489749, mean_eps: 0.100000\n","     292795/2000000000: episode: 8115, duration: 5.060s, episode steps:  40, steps per second:   8, episode reward: 97.900, mean reward:  2.447 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 85.853445, mean_q: 49.452916, mean_eps: 0.100000\n","     292824/2000000000: episode: 8116, duration: 4.136s, episode steps:  29, steps per second:   7, episode reward: 16.900, mean reward:  0.583 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 87.364868, mean_q: 49.734890, mean_eps: 0.100000\n","     292861/2000000000: episode: 8117, duration: 5.184s, episode steps:  37, steps per second:   7, episode reward: 63.100, mean reward:  1.705 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 89.863752, mean_q: 48.400351, mean_eps: 0.100000\n","     292889/2000000000: episode: 8118, duration: 3.571s, episode steps:  28, steps per second:   8, episode reward: 91.000, mean reward:  3.250 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.007307, mean_q: 48.153465, mean_eps: 0.100000\n","     292920/2000000000: episode: 8119, duration: 4.208s, episode steps:  31, steps per second:   7, episode reward: -14.300, mean reward: -0.461 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 87.887044, mean_q: 49.187970, mean_eps: 0.100000\n","     292948/2000000000: episode: 8120, duration: 3.729s, episode steps:  28, steps per second:   8, episode reward: 123.700, mean reward:  4.418 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 84.526011, mean_q: 48.852819, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     292973/2000000000: episode: 8121, duration: 3.380s, episode steps:  25, steps per second:   7, episode reward: 80.200, mean reward:  3.208 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 77.490421, mean_q: 49.458884, mean_eps: 0.100000\n","     293013/2000000000: episode: 8122, duration: 5.435s, episode steps:  40, steps per second:   7, episode reward: -2.200, mean reward: -0.055 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 96.334225, mean_q: 48.842563, mean_eps: 0.100000\n","     293047/2000000000: episode: 8123, duration: 4.611s, episode steps:  34, steps per second:   7, episode reward: 121.700, mean reward:  3.579 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 85.599829, mean_q: 48.693981, mean_eps: 0.100000\n","     293083/2000000000: episode: 8124, duration: 5.321s, episode steps:  36, steps per second:   7, episode reward: 106.900, mean reward:  2.969 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 85.653960, mean_q: 48.355905, mean_eps: 0.100000\n","     293114/2000000000: episode: 8125, duration: 4.254s, episode steps:  31, steps per second:   7, episode reward: 117.800, mean reward:  3.800 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 80.520779, mean_q: 48.942671, mean_eps: 0.100000\n","     293142/2000000000: episode: 8126, duration: 3.766s, episode steps:  28, steps per second:   7, episode reward: 193.100, mean reward:  6.896 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 79.019115, mean_q: 49.088774, mean_eps: 0.100000\n","     293175/2000000000: episode: 8127, duration: 4.326s, episode steps:  33, steps per second:   8, episode reward: 104.200, mean reward:  3.158 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 84.568313, mean_q: 48.049318, mean_eps: 0.100000\n","     293203/2000000000: episode: 8128, duration: 3.637s, episode steps:  28, steps per second:   8, episode reward: 76.200, mean reward:  2.721 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 81.808202, mean_q: 48.267535, mean_eps: 0.100000\n","     293239/2000000000: episode: 8129, duration: 4.619s, episode steps:  36, steps per second:   8, episode reward: -10.800, mean reward: -0.300 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 93.235727, mean_q: 48.321990, mean_eps: 0.100000\n","     293279/2000000000: episode: 8130, duration: 5.288s, episode steps:  40, steps per second:   8, episode reward: 181.500, mean reward:  4.537 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.440342, mean_q: 49.750448, mean_eps: 0.100000\n","     293313/2000000000: episode: 8131, duration: 4.551s, episode steps:  34, steps per second:   7, episode reward: 65.600, mean reward:  1.929 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 88.166077, mean_q: 48.383779, mean_eps: 0.100000\n","     293348/2000000000: episode: 8132, duration: 4.547s, episode steps:  35, steps per second:   8, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 96.160453, mean_q: 48.119292, mean_eps: 0.100000\n","     293385/2000000000: episode: 8133, duration: 4.979s, episode steps:  37, steps per second:   7, episode reward: 123.500, mean reward:  3.338 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 85.726822, mean_q: 48.332190, mean_eps: 0.100000\n","     293414/2000000000: episode: 8134, duration: 3.913s, episode steps:  29, steps per second:   7, episode reward: -96.000, mean reward: -3.310 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 91.144993, mean_q: 47.978458, mean_eps: 0.100000\n","     293448/2000000000: episode: 8135, duration: 4.232s, episode steps:  34, steps per second:   8, episode reward: 104.200, mean reward:  3.065 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 82.094020, mean_q: 48.668079, mean_eps: 0.100000\n","     293474/2000000000: episode: 8136, duration: 3.176s, episode steps:  26, steps per second:   8, episode reward: 101.500, mean reward:  3.904 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 92.411071, mean_q: 48.544905, mean_eps: 0.100000\n","     293500/2000000000: episode: 8137, duration: 3.151s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 92.648485, mean_q: 48.834412, mean_eps: 0.100000\n","     293540/2000000000: episode: 8138, duration: 4.863s, episode steps:  40, steps per second:   8, episode reward: 95.900, mean reward:  2.397 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 88.207813, mean_q: 49.433447, mean_eps: 0.100000\n","     293577/2000000000: episode: 8139, duration: 4.622s, episode steps:  37, steps per second:   8, episode reward: -27.000, mean reward: -0.730 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 82.689008, mean_q: 48.944961, mean_eps: 0.100000\n","     293606/2000000000: episode: 8140, duration: 3.710s, episode steps:  29, steps per second:   8, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 90.469040, mean_q: 49.847283, mean_eps: 0.100000\n","     293641/2000000000: episode: 8141, duration: 4.698s, episode steps:  35, steps per second:   7, episode reward: 24.700, mean reward:  0.706 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 79.224635, mean_q: 48.767952, mean_eps: 0.100000\n","     293670/2000000000: episode: 8142, duration: 4.099s, episode steps:  29, steps per second:   7, episode reward: 143.500, mean reward:  4.948 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 88.895683, mean_q: 49.544248, mean_eps: 0.100000\n","     293690/2000000000: episode: 8143, duration: 2.804s, episode steps:  20, steps per second:   7, episode reward:  8.400, mean reward:  0.420 [-20.000, 18.000], mean action: 0.500 [0.000, 1.000],  loss: 91.644086, mean_q: 49.098159, mean_eps: 0.100000\n","     293729/2000000000: episode: 8144, duration: 5.337s, episode steps:  39, steps per second:   7, episode reward: 110.000, mean reward:  2.821 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 86.662542, mean_q: 48.052774, mean_eps: 0.100000\n","     293762/2000000000: episode: 8145, duration: 4.635s, episode steps:  33, steps per second:   7, episode reward: 263.200, mean reward:  7.976 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.425110, mean_q: 48.967887, mean_eps: 0.100000\n","     293796/2000000000: episode: 8146, duration: 4.720s, episode steps:  34, steps per second:   7, episode reward: 239.200, mean reward:  7.035 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 88.105537, mean_q: 48.612979, mean_eps: 0.100000\n","     293824/2000000000: episode: 8147, duration: 4.019s, episode steps:  28, steps per second:   7, episode reward: 180.800, mean reward:  6.457 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 96.582921, mean_q: 48.287015, mean_eps: 0.100000\n","     293846/2000000000: episode: 8148, duration: 3.169s, episode steps:  22, steps per second:   7, episode reward: 127.500, mean reward:  5.795 [-20.000, 18.000], mean action: 0.682 [0.000, 2.000],  loss: 93.357577, mean_q: 48.458929, mean_eps: 0.100000\n","     293877/2000000000: episode: 8149, duration: 4.557s, episode steps:  31, steps per second:   7, episode reward: 136.800, mean reward:  4.413 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 87.467025, mean_q: 49.331589, mean_eps: 0.100000\n","     293900/2000000000: episode: 8150, duration: 3.583s, episode steps:  23, steps per second:   6, episode reward: 199.700, mean reward:  8.683 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 84.010391, mean_q: 49.407604, mean_eps: 0.100000\n","     293933/2000000000: episode: 8151, duration: 4.579s, episode steps:  33, steps per second:   7, episode reward: 109.900, mean reward:  3.330 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.449242, mean_q: 48.281936, mean_eps: 0.100000\n","     293967/2000000000: episode: 8152, duration: 4.797s, episode steps:  34, steps per second:   7, episode reward: 36.300, mean reward:  1.068 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 85.119221, mean_q: 47.922729, mean_eps: 0.100000\n","     293999/2000000000: episode: 8153, duration: 4.452s, episode steps:  32, steps per second:   7, episode reward: 48.300, mean reward:  1.509 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 90.554131, mean_q: 48.216927, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     294039/2000000000: episode: 8154, duration: 5.387s, episode steps:  40, steps per second:   7, episode reward: 41.100, mean reward:  1.027 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 90.324985, mean_q: 48.051470, mean_eps: 0.100000\n","     294064/2000000000: episode: 8155, duration: 3.281s, episode steps:  25, steps per second:   8, episode reward: 101.800, mean reward:  4.072 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 80.900989, mean_q: 48.054271, mean_eps: 0.100000\n","     294091/2000000000: episode: 8156, duration: 3.859s, episode steps:  27, steps per second:   7, episode reward: 78.200, mean reward:  2.896 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.950978, mean_q: 48.212580, mean_eps: 0.100000\n","     294131/2000000000: episode: 8157, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: 70.700, mean reward:  1.767 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 85.605264, mean_q: 48.329471, mean_eps: 0.100000\n","     294156/2000000000: episode: 8158, duration: 3.197s, episode steps:  25, steps per second:   8, episode reward: 115.600, mean reward:  4.624 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 80.603793, mean_q: 48.619796, mean_eps: 0.100000\n","     294196/2000000000: episode: 8159, duration: 5.078s, episode steps:  40, steps per second:   8, episode reward: 44.100, mean reward:  1.103 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.620523, mean_q: 48.397783, mean_eps: 0.100000\n","     294227/2000000000: episode: 8160, duration: 4.047s, episode steps:  31, steps per second:   8, episode reward: 153.400, mean reward:  4.948 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 83.216259, mean_q: 49.204223, mean_eps: 0.100000\n","     294267/2000000000: episode: 8161, duration: 5.139s, episode steps:  40, steps per second:   8, episode reward: 143.200, mean reward:  3.580 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.477744, mean_q: 48.946759, mean_eps: 0.100000\n","     294301/2000000000: episode: 8162, duration: 4.600s, episode steps:  34, steps per second:   7, episode reward: 191.700, mean reward:  5.638 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 81.669801, mean_q: 49.941272, mean_eps: 0.100000\n","     294325/2000000000: episode: 8163, duration: 3.283s, episode steps:  24, steps per second:   7, episode reward: -13.400, mean reward: -0.558 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 85.485733, mean_q: 48.777087, mean_eps: 0.100000\n","     294355/2000000000: episode: 8164, duration: 3.978s, episode steps:  30, steps per second:   8, episode reward: 49.200, mean reward:  1.640 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 85.744428, mean_q: 48.767085, mean_eps: 0.100000\n","     294391/2000000000: episode: 8165, duration: 4.623s, episode steps:  36, steps per second:   8, episode reward: 109.700, mean reward:  3.047 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 92.118529, mean_q: 48.578111, mean_eps: 0.100000\n","     294420/2000000000: episode: 8166, duration: 3.746s, episode steps:  29, steps per second:   8, episode reward: 109.000, mean reward:  3.759 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 83.206443, mean_q: 48.814294, mean_eps: 0.100000\n","     294447/2000000000: episode: 8167, duration: 3.485s, episode steps:  27, steps per second:   8, episode reward: 141.300, mean reward:  5.233 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 81.970458, mean_q: 50.471538, mean_eps: 0.100000\n","     294480/2000000000: episode: 8168, duration: 4.555s, episode steps:  33, steps per second:   7, episode reward: 73.900, mean reward:  2.239 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.728062, mean_q: 48.738106, mean_eps: 0.100000\n","     294511/2000000000: episode: 8169, duration: 4.180s, episode steps:  31, steps per second:   7, episode reward: 30.500, mean reward:  0.984 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 91.578058, mean_q: 49.393227, mean_eps: 0.100000\n","     294549/2000000000: episode: 8170, duration: 4.901s, episode steps:  38, steps per second:   8, episode reward: 233.500, mean reward:  6.145 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 81.329435, mean_q: 48.673553, mean_eps: 0.100000\n","     294576/2000000000: episode: 8171, duration: 3.456s, episode steps:  27, steps per second:   8, episode reward: 233.800, mean reward:  8.659 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 79.701982, mean_q: 48.616667, mean_eps: 0.100000\n","     294605/2000000000: episode: 8172, duration: 3.703s, episode steps:  29, steps per second:   8, episode reward: 114.600, mean reward:  3.952 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 77.841752, mean_q: 49.146309, mean_eps: 0.100000\n","     294634/2000000000: episode: 8173, duration: 3.650s, episode steps:  29, steps per second:   8, episode reward: 123.000, mean reward:  4.241 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.186033, mean_q: 48.513731, mean_eps: 0.100000\n","     294661/2000000000: episode: 8174, duration: 3.442s, episode steps:  27, steps per second:   8, episode reward: 120.500, mean reward:  4.463 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 88.981736, mean_q: 48.802636, mean_eps: 0.100000\n","     294690/2000000000: episode: 8175, duration: 3.739s, episode steps:  29, steps per second:   8, episode reward: 178.400, mean reward:  6.152 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 93.319017, mean_q: 48.251946, mean_eps: 0.100000\n","     294715/2000000000: episode: 8176, duration: 3.237s, episode steps:  25, steps per second:   8, episode reward: 47.100, mean reward:  1.884 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 79.111723, mean_q: 48.581078, mean_eps: 0.100000\n","     294741/2000000000: episode: 8177, duration: 3.725s, episode steps:  26, steps per second:   7, episode reward: 83.000, mean reward:  3.192 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 86.953735, mean_q: 48.420372, mean_eps: 0.100000\n","     294773/2000000000: episode: 8178, duration: 4.331s, episode steps:  32, steps per second:   7, episode reward: 121.600, mean reward:  3.800 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 88.889704, mean_q: 47.872529, mean_eps: 0.100000\n","     294812/2000000000: episode: 8179, duration: 5.151s, episode steps:  39, steps per second:   8, episode reward: 256.600, mean reward:  6.579 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 81.210271, mean_q: 49.240734, mean_eps: 0.100000\n","     294850/2000000000: episode: 8180, duration: 4.991s, episode steps:  38, steps per second:   8, episode reward: 117.100, mean reward:  3.082 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 90.887531, mean_q: 48.228777, mean_eps: 0.100000\n","     294879/2000000000: episode: 8181, duration: 3.641s, episode steps:  29, steps per second:   8, episode reward: 87.900, mean reward:  3.031 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.038887, mean_q: 48.920650, mean_eps: 0.100000\n","     294906/2000000000: episode: 8182, duration: 3.549s, episode steps:  27, steps per second:   8, episode reward: 103.300, mean reward:  3.826 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 89.576149, mean_q: 49.895538, mean_eps: 0.100000\n","     294946/2000000000: episode: 8183, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: 124.700, mean reward:  3.117 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 87.563999, mean_q: 47.947944, mean_eps: 0.100000\n","     294972/2000000000: episode: 8184, duration: 3.286s, episode steps:  26, steps per second:   8, episode reward: 97.100, mean reward:  3.735 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 82.634467, mean_q: 48.832455, mean_eps: 0.100000\n","     295008/2000000000: episode: 8185, duration: 4.564s, episode steps:  36, steps per second:   8, episode reward: 115.200, mean reward:  3.200 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.639574, mean_q: 49.256600, mean_eps: 0.100000\n","     295041/2000000000: episode: 8186, duration: 4.135s, episode steps:  33, steps per second:   8, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 88.510235, mean_q: 49.535408, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     295064/2000000000: episode: 8187, duration: 2.983s, episode steps:  23, steps per second:   8, episode reward: 18.000, mean reward:  0.783 [-20.000, 18.000], mean action: 0.435 [0.000, 2.000],  loss: 84.322567, mean_q: 48.887822, mean_eps: 0.100000\n","     295104/2000000000: episode: 8188, duration: 5.156s, episode steps:  40, steps per second:   8, episode reward: -88.200, mean reward: -2.205 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.150126, mean_q: 48.951888, mean_eps: 0.100000\n","     295139/2000000000: episode: 8189, duration: 4.719s, episode steps:  35, steps per second:   7, episode reward: -77.800, mean reward: -2.223 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 83.604765, mean_q: 48.950486, mean_eps: 0.100000\n","     295174/2000000000: episode: 8190, duration: 4.397s, episode steps:  35, steps per second:   8, episode reward:  6.800, mean reward:  0.194 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.453669, mean_q: 48.130355, mean_eps: 0.100000\n","     295200/2000000000: episode: 8191, duration: 3.274s, episode steps:  26, steps per second:   8, episode reward: 89.200, mean reward:  3.431 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 84.931240, mean_q: 48.354083, mean_eps: 0.100000\n","     295236/2000000000: episode: 8192, duration: 4.672s, episode steps:  36, steps per second:   8, episode reward: 106.700, mean reward:  2.964 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 89.261396, mean_q: 48.332829, mean_eps: 0.100000\n","     295268/2000000000: episode: 8193, duration: 4.078s, episode steps:  32, steps per second:   8, episode reward: 58.800, mean reward:  1.838 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 91.401454, mean_q: 48.243326, mean_eps: 0.100000\n","     295301/2000000000: episode: 8194, duration: 4.086s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 86.116170, mean_q: 49.173987, mean_eps: 0.100000\n","     295337/2000000000: episode: 8195, duration: 4.581s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 82.716603, mean_q: 48.541613, mean_eps: 0.100000\n","     295376/2000000000: episode: 8196, duration: 4.963s, episode steps:  39, steps per second:   8, episode reward: 147.000, mean reward:  3.769 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 84.968184, mean_q: 48.097707, mean_eps: 0.100000\n","     295413/2000000000: episode: 8197, duration: 4.726s, episode steps:  37, steps per second:   8, episode reward: 114.900, mean reward:  3.105 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 82.216055, mean_q: 49.079858, mean_eps: 0.100000\n","     295437/2000000000: episode: 8198, duration: 3.072s, episode steps:  24, steps per second:   8, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 0.958 [0.000, 2.000],  loss: 85.189358, mean_q: 48.412363, mean_eps: 0.100000\n","     295477/2000000000: episode: 8199, duration: 5.209s, episode steps:  40, steps per second:   8, episode reward: 248.000, mean reward:  6.200 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.355783, mean_q: 48.430114, mean_eps: 0.100000\n","     295511/2000000000: episode: 8200, duration: 4.385s, episode steps:  34, steps per second:   8, episode reward: 246.000, mean reward:  7.235 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 85.876253, mean_q: 48.324905, mean_eps: 0.100000\n","     295543/2000000000: episode: 8201, duration: 4.498s, episode steps:  32, steps per second:   7, episode reward: -96.000, mean reward: -3.000 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.873695, mean_q: 49.049587, mean_eps: 0.100000\n","     295575/2000000000: episode: 8202, duration: 5.055s, episode steps:  32, steps per second:   6, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 81.262428, mean_q: 48.982335, mean_eps: 0.100000\n","     295615/2000000000: episode: 8203, duration: 5.100s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.933127, mean_q: 48.508305, mean_eps: 0.100000\n","     295652/2000000000: episode: 8204, duration: 4.707s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 86.242855, mean_q: 49.291571, mean_eps: 0.100000\n","     295681/2000000000: episode: 8205, duration: 3.590s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 77.394300, mean_q: 48.972035, mean_eps: 0.100000\n","     295708/2000000000: episode: 8206, duration: 3.686s, episode steps:  27, steps per second:   7, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 84.834489, mean_q: 48.721415, mean_eps: 0.100000\n","     295742/2000000000: episode: 8207, duration: 4.373s, episode steps:  34, steps per second:   8, episode reward: 40.100, mean reward:  1.179 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 86.822209, mean_q: 48.358629, mean_eps: 0.100000\n","     295782/2000000000: episode: 8208, duration: 5.335s, episode steps:  40, steps per second:   7, episode reward: 56.000, mean reward:  1.400 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 80.195618, mean_q: 48.195321, mean_eps: 0.100000\n","     295811/2000000000: episode: 8209, duration: 3.756s, episode steps:  29, steps per second:   8, episode reward: -76.400, mean reward: -2.634 [-20.000, 18.000], mean action: 0.724 [0.000, 2.000],  loss: 85.453595, mean_q: 47.943508, mean_eps: 0.100000\n","     295842/2000000000: episode: 8210, duration: 3.915s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 86.359252, mean_q: 48.604548, mean_eps: 0.100000\n","     295878/2000000000: episode: 8211, duration: 4.329s, episode steps:  36, steps per second:   8, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 85.787633, mean_q: 49.154257, mean_eps: 0.100000\n","     295912/2000000000: episode: 8212, duration: 4.213s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 87.890042, mean_q: 49.308193, mean_eps: 0.100000\n","     295942/2000000000: episode: 8213, duration: 3.937s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 92.625516, mean_q: 48.220008, mean_eps: 0.100000\n","     295970/2000000000: episode: 8214, duration: 3.664s, episode steps:  28, steps per second:   8, episode reward: 110.400, mean reward:  3.943 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 90.440704, mean_q: 49.564652, mean_eps: 0.100000\n","     296002/2000000000: episode: 8215, duration: 4.143s, episode steps:  32, steps per second:   8, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 84.589182, mean_q: 48.635412, mean_eps: 0.100000\n","     296035/2000000000: episode: 8216, duration: 4.506s, episode steps:  33, steps per second:   7, episode reward: -58.000, mean reward: -1.758 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.645850, mean_q: 48.639822, mean_eps: 0.100000\n","     296070/2000000000: episode: 8217, duration: 4.441s, episode steps:  35, steps per second:   8, episode reward: 284.000, mean reward:  8.114 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 84.188540, mean_q: 49.377680, mean_eps: 0.100000\n","     296102/2000000000: episode: 8218, duration: 4.274s, episode steps:  32, steps per second:   7, episode reward: 27.000, mean reward:  0.844 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 84.803541, mean_q: 49.216590, mean_eps: 0.100000\n","     296133/2000000000: episode: 8219, duration: 3.999s, episode steps:  31, steps per second:   8, episode reward: 109.000, mean reward:  3.516 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 81.388936, mean_q: 49.039971, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     296173/2000000000: episode: 8220, duration: 5.143s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 82.654892, mean_q: 48.652843, mean_eps: 0.100000\n","     296203/2000000000: episode: 8221, duration: 3.944s, episode steps:  30, steps per second:   8, episode reward: 124.800, mean reward:  4.160 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 78.757336, mean_q: 49.077230, mean_eps: 0.100000\n","     296243/2000000000: episode: 8222, duration: 5.080s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.227406, mean_q: 48.775173, mean_eps: 0.100000\n","     296275/2000000000: episode: 8223, duration: 3.869s, episode steps:  32, steps per second:   8, episode reward: 121.200, mean reward:  3.788 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 81.219540, mean_q: 49.128408, mean_eps: 0.100000\n","     296312/2000000000: episode: 8224, duration: 4.428s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 88.885675, mean_q: 48.821266, mean_eps: 0.100000\n","     296343/2000000000: episode: 8225, duration: 3.726s, episode steps:  31, steps per second:   8, episode reward: 189.600, mean reward:  6.116 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 90.248283, mean_q: 48.877438, mean_eps: 0.100000\n","     296379/2000000000: episode: 8226, duration: 4.514s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 91.942693, mean_q: 48.662057, mean_eps: 0.100000\n","     296413/2000000000: episode: 8227, duration: 4.261s, episode steps:  34, steps per second:   8, episode reward: -8.300, mean reward: -0.244 [-20.000, 18.000], mean action: 1.235 [0.000, 2.000],  loss: 88.128009, mean_q: 48.645915, mean_eps: 0.100000\n","     296441/2000000000: episode: 8228, duration: 3.563s, episode steps:  28, steps per second:   8, episode reward: 38.400, mean reward:  1.371 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 82.633357, mean_q: 49.357121, mean_eps: 0.100000\n","     296479/2000000000: episode: 8229, duration: 4.761s, episode steps:  38, steps per second:   8, episode reward: -66.400, mean reward: -1.747 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 92.875283, mean_q: 48.520356, mean_eps: 0.100000\n","     296509/2000000000: episode: 8230, duration: 3.832s, episode steps:  30, steps per second:   8, episode reward: 134.000, mean reward:  4.467 [-20.000, 19.000], mean action: 0.900 [0.000, 2.000],  loss: 83.894052, mean_q: 49.131849, mean_eps: 0.100000\n","     296544/2000000000: episode: 8231, duration: 4.584s, episode steps:  35, steps per second:   8, episode reward: -28.900, mean reward: -0.826 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 92.408814, mean_q: 48.501863, mean_eps: 0.100000\n","     296572/2000000000: episode: 8232, duration: 3.522s, episode steps:  28, steps per second:   8, episode reward: 154.200, mean reward:  5.507 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 79.434039, mean_q: 49.389599, mean_eps: 0.100000\n","     296609/2000000000: episode: 8233, duration: 4.699s, episode steps:  37, steps per second:   8, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 87.390819, mean_q: 47.802350, mean_eps: 0.100000\n","     296639/2000000000: episode: 8234, duration: 3.973s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 93.080833, mean_q: 49.762450, mean_eps: 0.100000\n","     296667/2000000000: episode: 8235, duration: 3.686s, episode steps:  28, steps per second:   8, episode reward: -76.600, mean reward: -2.736 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 82.005760, mean_q: 48.103830, mean_eps: 0.100000\n","     296693/2000000000: episode: 8236, duration: 3.333s, episode steps:  26, steps per second:   8, episode reward: 55.900, mean reward:  2.150 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 86.962470, mean_q: 49.820323, mean_eps: 0.100000\n","     296727/2000000000: episode: 8237, duration: 4.436s, episode steps:  34, steps per second:   8, episode reward: 121.300, mean reward:  3.568 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 90.319814, mean_q: 47.488438, mean_eps: 0.100000\n","     296752/2000000000: episode: 8238, duration: 3.236s, episode steps:  25, steps per second:   8, episode reward: 204.900, mean reward:  8.196 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 80.257892, mean_q: 49.632701, mean_eps: 0.100000\n","     296774/2000000000: episode: 8239, duration: 2.803s, episode steps:  22, steps per second:   8, episode reward: -20.000, mean reward: -0.909 [-20.000, 18.000], mean action: 0.727 [0.000, 2.000],  loss: 94.182736, mean_q: 49.762622, mean_eps: 0.100000\n","     296805/2000000000: episode: 8240, duration: 4.080s, episode steps:  31, steps per second:   8, episode reward: 152.700, mean reward:  4.926 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 90.704178, mean_q: 48.593578, mean_eps: 0.100000\n","     296835/2000000000: episode: 8241, duration: 4.226s, episode steps:  30, steps per second:   7, episode reward: 72.200, mean reward:  2.407 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 79.934460, mean_q: 48.987974, mean_eps: 0.100000\n","     296862/2000000000: episode: 8242, duration: 3.883s, episode steps:  27, steps per second:   7, episode reward: 60.000, mean reward:  2.222 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 93.641560, mean_q: 49.829315, mean_eps: 0.100000\n","     296891/2000000000: episode: 8243, duration: 3.778s, episode steps:  29, steps per second:   8, episode reward: 125.300, mean reward:  4.321 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 80.810465, mean_q: 48.644127, mean_eps: 0.100000\n","     296925/2000000000: episode: 8244, duration: 4.112s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 87.460103, mean_q: 48.871620, mean_eps: 0.100000\n","     296953/2000000000: episode: 8245, duration: 3.533s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 79.959618, mean_q: 49.945841, mean_eps: 0.100000\n","     296990/2000000000: episode: 8246, duration: 4.542s, episode steps:  37, steps per second:   8, episode reward: -31.000, mean reward: -0.838 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 80.018899, mean_q: 48.493950, mean_eps: 0.100000\n","     297028/2000000000: episode: 8247, duration: 4.649s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 86.236815, mean_q: 48.361389, mean_eps: 0.100000\n","     297068/2000000000: episode: 8248, duration: 5.071s, episode steps:  40, steps per second:   8, episode reward: 198.700, mean reward:  4.967 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.944170, mean_q: 48.895085, mean_eps: 0.100000\n","     297103/2000000000: episode: 8249, duration: 4.496s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 85.671076, mean_q: 48.605679, mean_eps: 0.100000\n","     297133/2000000000: episode: 8250, duration: 3.844s, episode steps:  30, steps per second:   8, episode reward: 73.200, mean reward:  2.440 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 84.066772, mean_q: 48.776501, mean_eps: 0.100000\n","     297165/2000000000: episode: 8251, duration: 4.251s, episode steps:  32, steps per second:   8, episode reward: 17.200, mean reward:  0.537 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 74.136402, mean_q: 49.294814, mean_eps: 0.100000\n","     297197/2000000000: episode: 8252, duration: 4.163s, episode steps:  32, steps per second:   8, episode reward: -37.400, mean reward: -1.169 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 92.704697, mean_q: 50.028530, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     297228/2000000000: episode: 8253, duration: 3.933s, episode steps:  31, steps per second:   8, episode reward: 166.200, mean reward:  5.361 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 87.408093, mean_q: 49.334478, mean_eps: 0.100000\n","     297256/2000000000: episode: 8254, duration: 3.645s, episode steps:  28, steps per second:   8, episode reward: 93.700, mean reward:  3.346 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 92.879994, mean_q: 49.100120, mean_eps: 0.100000\n","     297291/2000000000: episode: 8255, duration: 4.705s, episode steps:  35, steps per second:   7, episode reward: 13.500, mean reward:  0.386 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 85.942106, mean_q: 48.673677, mean_eps: 0.100000\n","     297319/2000000000: episode: 8256, duration: 3.396s, episode steps:  28, steps per second:   8, episode reward: -18.900, mean reward: -0.675 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 93.090243, mean_q: 49.317582, mean_eps: 0.100000\n","     297350/2000000000: episode: 8257, duration: 3.974s, episode steps:  31, steps per second:   8, episode reward: 27.400, mean reward:  0.884 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 89.084348, mean_q: 49.147116, mean_eps: 0.100000\n","     297375/2000000000: episode: 8258, duration: 3.162s, episode steps:  25, steps per second:   8, episode reward: 51.500, mean reward:  2.060 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 98.470553, mean_q: 48.346873, mean_eps: 0.100000\n","     297402/2000000000: episode: 8259, duration: 3.397s, episode steps:  27, steps per second:   8, episode reward: 101.600, mean reward:  3.763 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 84.870661, mean_q: 49.109890, mean_eps: 0.100000\n","     297434/2000000000: episode: 8260, duration: 3.857s, episode steps:  32, steps per second:   8, episode reward: 90.300, mean reward:  2.822 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 86.087977, mean_q: 47.721934, mean_eps: 0.100000\n","     297462/2000000000: episode: 8261, duration: 3.472s, episode steps:  28, steps per second:   8, episode reward: 148.600, mean reward:  5.307 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 89.582813, mean_q: 48.625537, mean_eps: 0.100000\n","     297497/2000000000: episode: 8262, duration: 4.651s, episode steps:  35, steps per second:   8, episode reward: 78.300, mean reward:  2.237 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 77.221205, mean_q: 49.361537, mean_eps: 0.100000\n","     297529/2000000000: episode: 8263, duration: 4.226s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.147843, mean_q: 49.445094, mean_eps: 0.100000\n","     297566/2000000000: episode: 8264, duration: 4.625s, episode steps:  37, steps per second:   8, episode reward: 164.200, mean reward:  4.438 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 78.207754, mean_q: 48.268347, mean_eps: 0.100000\n","     297598/2000000000: episode: 8265, duration: 3.993s, episode steps:  32, steps per second:   8, episode reward: 96.000, mean reward:  3.000 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 78.650791, mean_q: 50.342568, mean_eps: 0.100000\n","     297638/2000000000: episode: 8266, duration: 5.077s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.504287, mean_q: 48.115289, mean_eps: 0.100000\n","     297666/2000000000: episode: 8267, duration: 3.580s, episode steps:  28, steps per second:   8, episode reward: -19.400, mean reward: -0.693 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 81.255961, mean_q: 49.003713, mean_eps: 0.100000\n","     297696/2000000000: episode: 8268, duration: 3.703s, episode steps:  30, steps per second:   8, episode reward: 139.800, mean reward:  4.660 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 92.089757, mean_q: 49.684784, mean_eps: 0.100000\n","     297735/2000000000: episode: 8269, duration: 4.815s, episode steps:  39, steps per second:   8, episode reward: 187.800, mean reward:  4.815 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 83.893569, mean_q: 49.309072, mean_eps: 0.100000\n","     297761/2000000000: episode: 8270, duration: 3.376s, episode steps:  26, steps per second:   8, episode reward: 94.800, mean reward:  3.646 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 86.489008, mean_q: 49.826860, mean_eps: 0.100000\n","     297786/2000000000: episode: 8271, duration: 3.203s, episode steps:  25, steps per second:   8, episode reward: 94.200, mean reward:  3.768 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 85.328201, mean_q: 48.849403, mean_eps: 0.100000\n","     297823/2000000000: episode: 8272, duration: 4.859s, episode steps:  37, steps per second:   8, episode reward: 238.900, mean reward:  6.457 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 84.591770, mean_q: 48.313022, mean_eps: 0.100000\n","     297860/2000000000: episode: 8273, duration: 4.918s, episode steps:  37, steps per second:   8, episode reward: 118.300, mean reward:  3.197 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 85.618561, mean_q: 48.684080, mean_eps: 0.100000\n","     297885/2000000000: episode: 8274, duration: 3.359s, episode steps:  25, steps per second:   7, episode reward: 79.600, mean reward:  3.184 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 84.338274, mean_q: 48.811766, mean_eps: 0.100000\n","     297916/2000000000: episode: 8275, duration: 3.724s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 86.021329, mean_q: 48.721610, mean_eps: 0.100000\n","     297943/2000000000: episode: 8276, duration: 3.510s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 93.445167, mean_q: 47.869510, mean_eps: 0.100000\n","     297983/2000000000: episode: 8277, duration: 5.097s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 82.783467, mean_q: 49.060185, mean_eps: 0.100000\n","     298013/2000000000: episode: 8278, duration: 3.692s, episode steps:  30, steps per second:   8, episode reward: 39.400, mean reward:  1.313 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 84.036562, mean_q: 49.572767, mean_eps: 0.100000\n","     298045/2000000000: episode: 8279, duration: 3.937s, episode steps:  32, steps per second:   8, episode reward: 34.900, mean reward:  1.091 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 85.869529, mean_q: 49.227912, mean_eps: 0.100000\n","     298083/2000000000: episode: 8280, duration: 4.792s, episode steps:  38, steps per second:   8, episode reward: 164.900, mean reward:  4.339 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 79.819235, mean_q: 48.575821, mean_eps: 0.100000\n","     298118/2000000000: episode: 8281, duration: 4.313s, episode steps:  35, steps per second:   8, episode reward: 36.600, mean reward:  1.046 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 81.844693, mean_q: 48.942572, mean_eps: 0.100000\n","     298151/2000000000: episode: 8282, duration: 4.153s, episode steps:  33, steps per second:   8, episode reward: 155.200, mean reward:  4.703 [-20.000, 18.000], mean action: 0.909 [0.000, 2.000],  loss: 86.529214, mean_q: 48.267282, mean_eps: 0.100000\n","     298180/2000000000: episode: 8283, duration: 3.529s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 90.259920, mean_q: 48.657713, mean_eps: 0.100000\n","     298212/2000000000: episode: 8284, duration: 3.861s, episode steps:  32, steps per second:   8, episode reward: 162.200, mean reward:  5.069 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.109039, mean_q: 48.807963, mean_eps: 0.100000\n","     298247/2000000000: episode: 8285, duration: 4.344s, episode steps:  35, steps per second:   8, episode reward: 69.300, mean reward:  1.980 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 88.900676, mean_q: 47.948822, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     298287/2000000000: episode: 8286, duration: 5.037s, episode steps:  40, steps per second:   8, episode reward: 90.700, mean reward:  2.267 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 84.895033, mean_q: 48.839173, mean_eps: 0.100000\n","     298318/2000000000: episode: 8287, duration: 4.049s, episode steps:  31, steps per second:   8, episode reward: -12.500, mean reward: -0.403 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.262894, mean_q: 49.120942, mean_eps: 0.100000\n","     298346/2000000000: episode: 8288, duration: 3.634s, episode steps:  28, steps per second:   8, episode reward: 77.400, mean reward:  2.764 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.978598, mean_q: 47.271742, mean_eps: 0.100000\n","     298380/2000000000: episode: 8289, duration: 4.286s, episode steps:  34, steps per second:   8, episode reward: 109.800, mean reward:  3.229 [-20.000, 18.000], mean action: 0.971 [0.000, 2.000],  loss: 93.340511, mean_q: 49.010281, mean_eps: 0.100000\n","     298410/2000000000: episode: 8290, duration: 3.767s, episode steps:  30, steps per second:   8, episode reward: 188.800, mean reward:  6.293 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 92.728039, mean_q: 49.274456, mean_eps: 0.100000\n","     298441/2000000000: episode: 8291, duration: 4.035s, episode steps:  31, steps per second:   8, episode reward: 123.500, mean reward:  3.984 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.120887, mean_q: 48.552581, mean_eps: 0.100000\n","     298472/2000000000: episode: 8292, duration: 4.093s, episode steps:  31, steps per second:   8, episode reward: 214.400, mean reward:  6.916 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.860053, mean_q: 48.255537, mean_eps: 0.100000\n","     298500/2000000000: episode: 8293, duration: 3.800s, episode steps:  28, steps per second:   7, episode reward: 36.600, mean reward:  1.307 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.490894, mean_q: 49.590472, mean_eps: 0.100000\n","     298528/2000000000: episode: 8294, duration: 3.686s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 91.959001, mean_q: 50.312119, mean_eps: 0.100000\n","     298563/2000000000: episode: 8295, duration: 4.450s, episode steps:  35, steps per second:   8, episode reward: -134.000, mean reward: -3.829 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 87.064624, mean_q: 49.379548, mean_eps: 0.100000\n","     298600/2000000000: episode: 8296, duration: 4.626s, episode steps:  37, steps per second:   8, episode reward: -1.200, mean reward: -0.032 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 91.370220, mean_q: 48.876986, mean_eps: 0.100000\n","     298633/2000000000: episode: 8297, duration: 4.226s, episode steps:  33, steps per second:   8, episode reward: 73.000, mean reward:  2.212 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.233834, mean_q: 49.078821, mean_eps: 0.100000\n","     298668/2000000000: episode: 8298, duration: 4.206s, episode steps:  35, steps per second:   8, episode reward: 160.000, mean reward:  4.571 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 77.540084, mean_q: 49.361834, mean_eps: 0.100000\n","     298705/2000000000: episode: 8299, duration: 4.405s, episode steps:  37, steps per second:   8, episode reward: 177.200, mean reward:  4.789 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 79.447679, mean_q: 48.646771, mean_eps: 0.100000\n","     298743/2000000000: episode: 8300, duration: 4.541s, episode steps:  38, steps per second:   8, episode reward: 235.400, mean reward:  6.195 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 87.090826, mean_q: 49.233890, mean_eps: 0.100000\n","     298771/2000000000: episode: 8301, duration: 3.419s, episode steps:  28, steps per second:   8, episode reward: 116.400, mean reward:  4.157 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 87.501525, mean_q: 48.870971, mean_eps: 0.100000\n","     298802/2000000000: episode: 8302, duration: 3.800s, episode steps:  31, steps per second:   8, episode reward: 208.400, mean reward:  6.723 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 87.948435, mean_q: 49.333243, mean_eps: 0.100000\n","     298837/2000000000: episode: 8303, duration: 4.247s, episode steps:  35, steps per second:   8, episode reward: 113.700, mean reward:  3.249 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 88.028816, mean_q: 47.764528, mean_eps: 0.100000\n","     298870/2000000000: episode: 8304, duration: 4.276s, episode steps:  33, steps per second:   8, episode reward: 156.000, mean reward:  4.727 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 76.855854, mean_q: 48.816652, mean_eps: 0.100000\n","     298906/2000000000: episode: 8305, duration: 4.511s, episode steps:  36, steps per second:   8, episode reward: 86.300, mean reward:  2.397 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 92.396011, mean_q: 49.925492, mean_eps: 0.100000\n","     298935/2000000000: episode: 8306, duration: 3.573s, episode steps:  29, steps per second:   8, episode reward: 138.300, mean reward:  4.769 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 88.817976, mean_q: 48.544919, mean_eps: 0.100000\n","     298960/2000000000: episode: 8307, duration: 3.064s, episode steps:  25, steps per second:   8, episode reward: 97.300, mean reward:  3.892 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 92.519376, mean_q: 48.213992, mean_eps: 0.100000\n","     298991/2000000000: episode: 8308, duration: 3.858s, episode steps:  31, steps per second:   8, episode reward: 174.700, mean reward:  5.635 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.735496, mean_q: 49.536364, mean_eps: 0.100000\n","     299026/2000000000: episode: 8309, duration: 4.428s, episode steps:  35, steps per second:   8, episode reward: 108.800, mean reward:  3.109 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 85.214917, mean_q: 49.372312, mean_eps: 0.100000\n","     299056/2000000000: episode: 8310, duration: 3.823s, episode steps:  30, steps per second:   8, episode reward: 78.200, mean reward:  2.607 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 89.017920, mean_q: 49.492125, mean_eps: 0.100000\n","     299081/2000000000: episode: 8311, duration: 3.304s, episode steps:  25, steps per second:   8, episode reward: 183.600, mean reward:  7.344 [-20.000, 18.700], mean action: 0.880 [0.000, 2.000],  loss: 82.478076, mean_q: 49.794602, mean_eps: 0.100000\n","     299110/2000000000: episode: 8312, duration: 3.615s, episode steps:  29, steps per second:   8, episode reward: 30.600, mean reward:  1.055 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 75.935943, mean_q: 48.931139, mean_eps: 0.100000\n","     299148/2000000000: episode: 8313, duration: 4.682s, episode steps:  38, steps per second:   8, episode reward: 123.400, mean reward:  3.247 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 85.965552, mean_q: 48.617679, mean_eps: 0.100000\n","     299187/2000000000: episode: 8314, duration: 5.052s, episode steps:  39, steps per second:   8, episode reward: 142.400, mean reward:  3.651 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 89.320474, mean_q: 48.984930, mean_eps: 0.100000\n","     299223/2000000000: episode: 8315, duration: 4.498s, episode steps:  36, steps per second:   8, episode reward: 124.500, mean reward:  3.458 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.773399, mean_q: 48.604537, mean_eps: 0.100000\n","     299257/2000000000: episode: 8316, duration: 4.275s, episode steps:  34, steps per second:   8, episode reward: 36.300, mean reward:  1.068 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 88.471216, mean_q: 48.373906, mean_eps: 0.100000\n","     299288/2000000000: episode: 8317, duration: 3.714s, episode steps:  31, steps per second:   8, episode reward: 252.000, mean reward:  8.129 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 88.216841, mean_q: 47.950765, mean_eps: 0.100000\n","     299317/2000000000: episode: 8318, duration: 3.485s, episode steps:  29, steps per second:   8, episode reward: 68.100, mean reward:  2.348 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 96.126806, mean_q: 49.041366, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     299357/2000000000: episode: 8319, duration: 4.780s, episode steps:  40, steps per second:   8, episode reward: 150.300, mean reward:  3.758 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 92.775767, mean_q: 48.044005, mean_eps: 0.100000\n","     299383/2000000000: episode: 8320, duration: 3.186s, episode steps:  26, steps per second:   8, episode reward: -29.500, mean reward: -1.135 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 89.747752, mean_q: 48.467226, mean_eps: 0.100000\n","     299408/2000000000: episode: 8321, duration: 3.120s, episode steps:  25, steps per second:   8, episode reward: 131.500, mean reward:  5.260 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 97.918759, mean_q: 49.881257, mean_eps: 0.100000\n","     299438/2000000000: episode: 8322, duration: 3.807s, episode steps:  30, steps per second:   8, episode reward: 169.500, mean reward:  5.650 [-14.100, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 91.811154, mean_q: 48.627518, mean_eps: 0.100000\n","     299471/2000000000: episode: 8323, duration: 4.201s, episode steps:  33, steps per second:   8, episode reward: 239.200, mean reward:  7.248 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 87.273159, mean_q: 49.079245, mean_eps: 0.100000\n","     299511/2000000000: episode: 8324, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: 117.500, mean reward:  2.937 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 87.712624, mean_q: 48.522947, mean_eps: 0.100000\n","     299541/2000000000: episode: 8325, duration: 3.788s, episode steps:  30, steps per second:   8, episode reward: 120.200, mean reward:  4.007 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 89.391067, mean_q: 50.204080, mean_eps: 0.100000\n","     299569/2000000000: episode: 8326, duration: 3.371s, episode steps:  28, steps per second:   8, episode reward: 27.400, mean reward:  0.979 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 83.256088, mean_q: 48.608424, mean_eps: 0.100000\n","     299607/2000000000: episode: 8327, duration: 4.909s, episode steps:  38, steps per second:   8, episode reward: 59.800, mean reward:  1.574 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 92.856417, mean_q: 48.930623, mean_eps: 0.100000\n","     299647/2000000000: episode: 8328, duration: 4.748s, episode steps:  40, steps per second:   8, episode reward: 83.000, mean reward:  2.075 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 75.649494, mean_q: 49.140946, mean_eps: 0.100000\n","     299675/2000000000: episode: 8329, duration: 3.459s, episode steps:  28, steps per second:   8, episode reward: 193.400, mean reward:  6.907 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 78.971785, mean_q: 49.562940, mean_eps: 0.100000\n","     299702/2000000000: episode: 8330, duration: 3.428s, episode steps:  27, steps per second:   8, episode reward: -42.200, mean reward: -1.563 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 83.520760, mean_q: 48.572015, mean_eps: 0.100000\n","     299728/2000000000: episode: 8331, duration: 3.396s, episode steps:  26, steps per second:   8, episode reward: 154.700, mean reward:  5.950 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 90.359986, mean_q: 48.723543, mean_eps: 0.100000\n","     299757/2000000000: episode: 8332, duration: 3.755s, episode steps:  29, steps per second:   8, episode reward: 59.400, mean reward:  2.048 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 79.412039, mean_q: 48.027957, mean_eps: 0.100000\n","     299789/2000000000: episode: 8333, duration: 4.293s, episode steps:  32, steps per second:   7, episode reward: 246.000, mean reward:  7.688 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 94.234204, mean_q: 48.800370, mean_eps: 0.100000\n","     299820/2000000000: episode: 8334, duration: 4.087s, episode steps:  31, steps per second:   8, episode reward: 70.200, mean reward:  2.265 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 78.006970, mean_q: 47.929537, mean_eps: 0.100000\n","     299856/2000000000: episode: 8335, duration: 4.479s, episode steps:  36, steps per second:   8, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.028 [0.000, 2.000],  loss: 82.383929, mean_q: 49.879227, mean_eps: 0.100000\n","     299892/2000000000: episode: 8336, duration: 4.662s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 93.163252, mean_q: 48.776133, mean_eps: 0.100000\n","     299931/2000000000: episode: 8337, duration: 4.942s, episode steps:  39, steps per second:   8, episode reward: 98.400, mean reward:  2.523 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 91.528769, mean_q: 48.563766, mean_eps: 0.100000\n","     299962/2000000000: episode: 8338, duration: 3.998s, episode steps:  31, steps per second:   8, episode reward: 125.100, mean reward:  4.035 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.627023, mean_q: 50.431653, mean_eps: 0.100000\n","     300002/2000000000: episode: 8339, duration: 5.019s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 91.184379, mean_q: 48.943165, mean_eps: 0.100000\n","     300042/2000000000: episode: 8340, duration: 5.178s, episode steps:  40, steps per second:   8, episode reward: -152.000, mean reward: -3.800 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 84.795299, mean_q: 48.907510, mean_eps: 0.100000\n","     300067/2000000000: episode: 8341, duration: 3.289s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 80.675008, mean_q: 48.111778, mean_eps: 0.100000\n","     300097/2000000000: episode: 8342, duration: 3.786s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 92.980607, mean_q: 50.178455, mean_eps: 0.100000\n","     300131/2000000000: episode: 8343, duration: 4.327s, episode steps:  34, steps per second:   8, episode reward: -41.900, mean reward: -1.232 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 82.149336, mean_q: 48.462542, mean_eps: 0.100000\n","     300166/2000000000: episode: 8344, duration: 4.102s, episode steps:  35, steps per second:   9, episode reward: 103.300, mean reward:  2.951 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 89.448137, mean_q: 49.185594, mean_eps: 0.100000\n","     300206/2000000000: episode: 8345, duration: 4.741s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 89.801649, mean_q: 49.126903, mean_eps: 0.100000\n","     300244/2000000000: episode: 8346, duration: 4.512s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 95.852651, mean_q: 48.535255, mean_eps: 0.100000\n","     300275/2000000000: episode: 8347, duration: 3.830s, episode steps:  31, steps per second:   8, episode reward: -20.000, mean reward: -0.645 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 88.162677, mean_q: 48.985442, mean_eps: 0.100000\n","     300315/2000000000: episode: 8348, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: -136.000, mean reward: -3.400 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 87.405670, mean_q: 50.639846, mean_eps: 0.100000\n","     300348/2000000000: episode: 8349, duration: 4.108s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 85.038413, mean_q: 50.170116, mean_eps: 0.100000\n","     300388/2000000000: episode: 8350, duration: 4.983s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 95.774713, mean_q: 49.394012, mean_eps: 0.100000\n","     300415/2000000000: episode: 8351, duration: 3.559s, episode steps:  27, steps per second:   8, episode reward: 142.900, mean reward:  5.293 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 91.436767, mean_q: 49.673102, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     300441/2000000000: episode: 8352, duration: 3.530s, episode steps:  26, steps per second:   7, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 93.274824, mean_q: 50.315491, mean_eps: 0.100000\n","     300474/2000000000: episode: 8353, duration: 4.064s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 85.570617, mean_q: 48.665453, mean_eps: 0.100000\n","     300507/2000000000: episode: 8354, duration: 4.140s, episode steps:  33, steps per second:   8, episode reward: 55.700, mean reward:  1.688 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.246520, mean_q: 49.693726, mean_eps: 0.100000\n","     300538/2000000000: episode: 8355, duration: 4.042s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 92.504842, mean_q: 48.711231, mean_eps: 0.100000\n","     300567/2000000000: episode: 8356, duration: 3.808s, episode steps:  29, steps per second:   8, episode reward: 143.200, mean reward:  4.938 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 85.616705, mean_q: 48.059541, mean_eps: 0.100000\n","     300601/2000000000: episode: 8357, duration: 4.338s, episode steps:  34, steps per second:   8, episode reward: 167.200, mean reward:  4.918 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 93.230518, mean_q: 49.720175, mean_eps: 0.100000\n","     300641/2000000000: episode: 8358, duration: 4.988s, episode steps:  40, steps per second:   8, episode reward: 85.000, mean reward:  2.125 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 90.488321, mean_q: 49.221486, mean_eps: 0.100000\n","     300669/2000000000: episode: 8359, duration: 3.769s, episode steps:  28, steps per second:   7, episode reward: 219.300, mean reward:  7.832 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 91.883960, mean_q: 48.587774, mean_eps: 0.100000\n","     300702/2000000000: episode: 8360, duration: 4.394s, episode steps:  33, steps per second:   8, episode reward: -16.000, mean reward: -0.485 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.941543, mean_q: 49.154256, mean_eps: 0.100000\n","     300727/2000000000: episode: 8361, duration: 3.397s, episode steps:  25, steps per second:   7, episode reward: 132.000, mean reward:  5.280 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 90.820328, mean_q: 50.308156, mean_eps: 0.100000\n","     300758/2000000000: episode: 8362, duration: 3.947s, episode steps:  31, steps per second:   8, episode reward: -10.400, mean reward: -0.335 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 83.283052, mean_q: 48.491844, mean_eps: 0.100000\n","     300798/2000000000: episode: 8363, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 89.965891, mean_q: 50.366652, mean_eps: 0.100000\n","     300820/2000000000: episode: 8364, duration: 2.827s, episode steps:  22, steps per second:   8, episode reward: 18.000, mean reward:  0.818 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 94.111076, mean_q: 48.678395, mean_eps: 0.100000\n","     300860/2000000000: episode: 8365, duration: 5.088s, episode steps:  40, steps per second:   8, episode reward: 192.000, mean reward:  4.800 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 86.686812, mean_q: 49.712797, mean_eps: 0.100000\n","     300883/2000000000: episode: 8366, duration: 2.931s, episode steps:  23, steps per second:   8, episode reward: 132.000, mean reward:  5.739 [-20.000, 18.000], mean action: 0.826 [0.000, 2.000],  loss: 84.083284, mean_q: 48.771309, mean_eps: 0.100000\n","     300915/2000000000: episode: 8367, duration: 4.239s, episode steps:  32, steps per second:   8, episode reward: 53.800, mean reward:  1.681 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 85.186990, mean_q: 49.200816, mean_eps: 0.100000\n","     300942/2000000000: episode: 8368, duration: 3.373s, episode steps:  27, steps per second:   8, episode reward: -16.200, mean reward: -0.600 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 96.642198, mean_q: 50.085491, mean_eps: 0.100000\n","     300978/2000000000: episode: 8369, duration: 4.336s, episode steps:  36, steps per second:   8, episode reward:  6.300, mean reward:  0.175 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 82.320516, mean_q: 49.550497, mean_eps: 0.100000\n","     301007/2000000000: episode: 8370, duration: 3.625s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 94.557364, mean_q: 48.788182, mean_eps: 0.100000\n","     301036/2000000000: episode: 8371, duration: 3.654s, episode steps:  29, steps per second:   8, episode reward: 224.900, mean reward:  7.755 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 80.164112, mean_q: 50.139250, mean_eps: 0.100000\n","     301063/2000000000: episode: 8372, duration: 3.407s, episode steps:  27, steps per second:   8, episode reward: -81.600, mean reward: -3.022 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 93.281932, mean_q: 49.140910, mean_eps: 0.100000\n","     301103/2000000000: episode: 8373, duration: 4.801s, episode steps:  40, steps per second:   8, episode reward: 187.000, mean reward:  4.675 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 79.518569, mean_q: 49.434703, mean_eps: 0.100000\n","     301126/2000000000: episode: 8374, duration: 2.774s, episode steps:  23, steps per second:   8, episode reward: 81.900, mean reward:  3.561 [-20.000, 18.000], mean action: 0.391 [0.000, 2.000],  loss: 90.528387, mean_q: 48.981179, mean_eps: 0.100000\n","     301150/2000000000: episode: 8375, duration: 2.989s, episode steps:  24, steps per second:   8, episode reward: 281.900, mean reward: 11.746 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 87.021393, mean_q: 49.496390, mean_eps: 0.100000\n","     301190/2000000000: episode: 8376, duration: 4.781s, episode steps:  40, steps per second:   8, episode reward: -51.200, mean reward: -1.280 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 90.797147, mean_q: 49.368154, mean_eps: 0.100000\n","     301220/2000000000: episode: 8377, duration: 3.735s, episode steps:  30, steps per second:   8, episode reward: 74.600, mean reward:  2.487 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 93.189097, mean_q: 48.718609, mean_eps: 0.100000\n","     301257/2000000000: episode: 8378, duration: 4.739s, episode steps:  37, steps per second:   8, episode reward: -42.100, mean reward: -1.138 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 87.991906, mean_q: 49.198144, mean_eps: 0.100000\n","     301297/2000000000: episode: 8379, duration: 4.885s, episode steps:  40, steps per second:   8, episode reward: 176.800, mean reward:  4.420 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 91.330739, mean_q: 48.886828, mean_eps: 0.100000\n","     301333/2000000000: episode: 8380, duration: 4.652s, episode steps:  36, steps per second:   8, episode reward: 141.400, mean reward:  3.928 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 92.571696, mean_q: 49.088521, mean_eps: 0.100000\n","     301362/2000000000: episode: 8381, duration: 3.613s, episode steps:  29, steps per second:   8, episode reward:  5.900, mean reward:  0.203 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 93.342291, mean_q: 48.780453, mean_eps: 0.100000\n","     301397/2000000000: episode: 8382, duration: 4.481s, episode steps:  35, steps per second:   8, episode reward: 64.000, mean reward:  1.829 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 80.407190, mean_q: 50.156841, mean_eps: 0.100000\n","     301432/2000000000: episode: 8383, duration: 4.700s, episode steps:  35, steps per second:   7, episode reward: 257.600, mean reward:  7.360 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 81.129920, mean_q: 49.449338, mean_eps: 0.100000\n","     301461/2000000000: episode: 8384, duration: 4.015s, episode steps:  29, steps per second:   7, episode reward: 177.000, mean reward:  6.103 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 88.619669, mean_q: 49.474556, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     301489/2000000000: episode: 8385, duration: 3.551s, episode steps:  28, steps per second:   8, episode reward: -44.600, mean reward: -1.593 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 87.747285, mean_q: 49.620392, mean_eps: 0.100000\n","     301521/2000000000: episode: 8386, duration: 4.109s, episode steps:  32, steps per second:   8, episode reward: 204.700, mean reward:  6.397 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 95.975614, mean_q: 48.994000, mean_eps: 0.100000\n","     301557/2000000000: episode: 8387, duration: 4.696s, episode steps:  36, steps per second:   8, episode reward: 21.500, mean reward:  0.597 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 97.100085, mean_q: 49.247355, mean_eps: 0.100000\n","     301582/2000000000: episode: 8388, duration: 3.147s, episode steps:  25, steps per second:   8, episode reward: 132.000, mean reward:  5.280 [-20.000, 18.000], mean action: 0.960 [0.000, 2.000],  loss: 88.149156, mean_q: 50.056906, mean_eps: 0.100000\n","     301609/2000000000: episode: 8389, duration: 3.449s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.056626, mean_q: 50.534365, mean_eps: 0.100000\n","     301642/2000000000: episode: 8390, duration: 4.336s, episode steps:  33, steps per second:   8, episode reward: 116.200, mean reward:  3.521 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 88.944761, mean_q: 49.989054, mean_eps: 0.100000\n","     301676/2000000000: episode: 8391, duration: 4.381s, episode steps:  34, steps per second:   8, episode reward: -81.100, mean reward: -2.385 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 83.572701, mean_q: 49.466711, mean_eps: 0.100000\n","     301710/2000000000: episode: 8392, duration: 4.394s, episode steps:  34, steps per second:   8, episode reward: 73.100, mean reward:  2.150 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 81.759162, mean_q: 49.458139, mean_eps: 0.100000\n","     301738/2000000000: episode: 8393, duration: 3.599s, episode steps:  28, steps per second:   8, episode reward: 56.300, mean reward:  2.011 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 86.024760, mean_q: 49.486501, mean_eps: 0.100000\n","     301763/2000000000: episode: 8394, duration: 3.306s, episode steps:  25, steps per second:   8, episode reward: 206.400, mean reward:  8.256 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 82.459765, mean_q: 48.209379, mean_eps: 0.100000\n","     301796/2000000000: episode: 8395, duration: 4.374s, episode steps:  33, steps per second:   8, episode reward: 140.100, mean reward:  4.245 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.755750, mean_q: 49.206598, mean_eps: 0.100000\n","     301825/2000000000: episode: 8396, duration: 3.874s, episode steps:  29, steps per second:   7, episode reward: -53.700, mean reward: -1.852 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 81.033505, mean_q: 50.211026, mean_eps: 0.100000\n","     301854/2000000000: episode: 8397, duration: 3.645s, episode steps:  29, steps per second:   8, episode reward: 52.400, mean reward:  1.807 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 98.006515, mean_q: 49.416484, mean_eps: 0.100000\n","     301882/2000000000: episode: 8398, duration: 3.550s, episode steps:  28, steps per second:   8, episode reward: 103.200, mean reward:  3.686 [-20.000, 18.700], mean action: 0.786 [0.000, 2.000],  loss: 86.733710, mean_q: 49.411532, mean_eps: 0.100000\n","     301918/2000000000: episode: 8399, duration: 4.770s, episode steps:  36, steps per second:   8, episode reward: -105.900, mean reward: -2.942 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 90.663717, mean_q: 49.792231, mean_eps: 0.100000\n","     301951/2000000000: episode: 8400, duration: 4.199s, episode steps:  33, steps per second:   8, episode reward: 188.100, mean reward:  5.700 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 83.981200, mean_q: 49.364874, mean_eps: 0.100000\n","     301989/2000000000: episode: 8401, duration: 4.754s, episode steps:  38, steps per second:   8, episode reward: -3.100, mean reward: -0.082 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 89.924507, mean_q: 49.268537, mean_eps: 0.100000\n","     302028/2000000000: episode: 8402, duration: 4.946s, episode steps:  39, steps per second:   8, episode reward: 122.900, mean reward:  3.151 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 80.753294, mean_q: 48.968989, mean_eps: 0.100000\n","     302057/2000000000: episode: 8403, duration: 3.890s, episode steps:  29, steps per second:   7, episode reward: 145.800, mean reward:  5.028 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 89.126891, mean_q: 49.085658, mean_eps: 0.100000\n","     302097/2000000000: episode: 8404, duration: 5.269s, episode steps:  40, steps per second:   8, episode reward: -17.800, mean reward: -0.445 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.257298, mean_q: 49.638590, mean_eps: 0.100000\n","     302127/2000000000: episode: 8405, duration: 3.825s, episode steps:  30, steps per second:   8, episode reward: -13.600, mean reward: -0.453 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 82.309476, mean_q: 49.630858, mean_eps: 0.100000\n","     302159/2000000000: episode: 8406, duration: 4.475s, episode steps:  32, steps per second:   7, episode reward: 248.200, mean reward:  7.756 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 87.269603, mean_q: 48.260890, mean_eps: 0.100000\n","     302196/2000000000: episode: 8407, duration: 4.938s, episode steps:  37, steps per second:   7, episode reward: 111.900, mean reward:  3.024 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 86.017999, mean_q: 48.974384, mean_eps: 0.100000\n","     302224/2000000000: episode: 8408, duration: 3.565s, episode steps:  28, steps per second:   8, episode reward: 26.100, mean reward:  0.932 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 93.471925, mean_q: 48.576551, mean_eps: 0.100000\n","     302261/2000000000: episode: 8409, duration: 4.713s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 90.921970, mean_q: 50.389855, mean_eps: 0.100000\n","     302292/2000000000: episode: 8410, duration: 3.953s, episode steps:  31, steps per second:   8, episode reward: 287.500, mean reward:  9.274 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 89.500972, mean_q: 48.862645, mean_eps: 0.100000\n","     302329/2000000000: episode: 8411, duration: 4.545s, episode steps:  37, steps per second:   8, episode reward: 146.200, mean reward:  3.951 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 91.422850, mean_q: 49.881296, mean_eps: 0.100000\n","     302363/2000000000: episode: 8412, duration: 4.236s, episode steps:  34, steps per second:   8, episode reward: 202.700, mean reward:  5.962 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 88.935482, mean_q: 49.227881, mean_eps: 0.100000\n","     302391/2000000000: episode: 8413, duration: 3.814s, episode steps:  28, steps per second:   7, episode reward: 48.700, mean reward:  1.739 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 88.566613, mean_q: 48.896589, mean_eps: 0.100000\n","     302425/2000000000: episode: 8414, duration: 4.454s, episode steps:  34, steps per second:   8, episode reward: 57.600, mean reward:  1.694 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 86.900658, mean_q: 49.607101, mean_eps: 0.100000\n","     302458/2000000000: episode: 8415, duration: 4.323s, episode steps:  33, steps per second:   8, episode reward: 196.900, mean reward:  5.967 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 85.487203, mean_q: 48.594692, mean_eps: 0.100000\n","     302498/2000000000: episode: 8416, duration: 5.034s, episode steps:  40, steps per second:   8, episode reward: 157.600, mean reward:  3.940 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 90.579302, mean_q: 50.648759, mean_eps: 0.100000\n","     302524/2000000000: episode: 8417, duration: 3.353s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 94.172261, mean_q: 48.964986, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     302557/2000000000: episode: 8418, duration: 4.523s, episode steps:  33, steps per second:   7, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 93.347407, mean_q: 48.735204, mean_eps: 0.100000\n","     302596/2000000000: episode: 8419, duration: 5.410s, episode steps:  39, steps per second:   7, episode reward: 200.200, mean reward:  5.133 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 87.372014, mean_q: 49.286364, mean_eps: 0.100000\n","     302636/2000000000: episode: 8420, duration: 5.076s, episode steps:  40, steps per second:   8, episode reward: 44.100, mean reward:  1.103 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 90.721117, mean_q: 48.873463, mean_eps: 0.100000\n","     302674/2000000000: episode: 8421, duration: 4.651s, episode steps:  38, steps per second:   8, episode reward: 66.900, mean reward:  1.761 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 87.983268, mean_q: 49.498694, mean_eps: 0.100000\n","     302710/2000000000: episode: 8422, duration: 4.607s, episode steps:  36, steps per second:   8, episode reward: 123.900, mean reward:  3.442 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 85.378427, mean_q: 48.906287, mean_eps: 0.100000\n","     302743/2000000000: episode: 8423, duration: 4.328s, episode steps:  33, steps per second:   8, episode reward: 67.100, mean reward:  2.033 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.110800, mean_q: 48.912686, mean_eps: 0.100000\n","     302775/2000000000: episode: 8424, duration: 4.563s, episode steps:  32, steps per second:   7, episode reward: 212.400, mean reward:  6.638 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 88.888206, mean_q: 48.038631, mean_eps: 0.100000\n","     302810/2000000000: episode: 8425, duration: 4.561s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 80.373843, mean_q: 49.680986, mean_eps: 0.100000\n","     302848/2000000000: episode: 8426, duration: 4.975s, episode steps:  38, steps per second:   8, episode reward: 24.800, mean reward:  0.653 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 94.835753, mean_q: 48.884024, mean_eps: 0.100000\n","     302876/2000000000: episode: 8427, duration: 3.681s, episode steps:  28, steps per second:   8, episode reward: 90.900, mean reward:  3.246 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 85.409601, mean_q: 49.594995, mean_eps: 0.100000\n","     302907/2000000000: episode: 8428, duration: 3.822s, episode steps:  31, steps per second:   8, episode reward: 14.600, mean reward:  0.471 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 89.473539, mean_q: 49.054608, mean_eps: 0.100000\n","     302938/2000000000: episode: 8429, duration: 4.030s, episode steps:  31, steps per second:   8, episode reward: 129.800, mean reward:  4.187 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 97.264767, mean_q: 48.640628, mean_eps: 0.100000\n","     302963/2000000000: episode: 8430, duration: 3.260s, episode steps:  25, steps per second:   8, episode reward: 207.200, mean reward:  8.288 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 94.429590, mean_q: 50.934109, mean_eps: 0.100000\n","     302994/2000000000: episode: 8431, duration: 3.969s, episode steps:  31, steps per second:   8, episode reward: 19.200, mean reward:  0.619 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 92.956687, mean_q: 49.926370, mean_eps: 0.100000\n","     303026/2000000000: episode: 8432, duration: 4.257s, episode steps:  32, steps per second:   8, episode reward: 35.700, mean reward:  1.116 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 86.459594, mean_q: 50.817251, mean_eps: 0.100000\n","     303063/2000000000: episode: 8433, duration: 4.708s, episode steps:  37, steps per second:   8, episode reward: -29.100, mean reward: -0.786 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 82.184631, mean_q: 50.583829, mean_eps: 0.100000\n","     303101/2000000000: episode: 8434, duration: 4.809s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 74.260758, mean_q: 49.464282, mean_eps: 0.100000\n","     303133/2000000000: episode: 8435, duration: 4.284s, episode steps:  32, steps per second:   7, episode reward: 134.500, mean reward:  4.203 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 82.181889, mean_q: 49.674084, mean_eps: 0.100000\n","     303162/2000000000: episode: 8436, duration: 3.784s, episode steps:  29, steps per second:   8, episode reward: 110.600, mean reward:  3.814 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 80.787379, mean_q: 50.031643, mean_eps: 0.100000\n","     303194/2000000000: episode: 8437, duration: 4.175s, episode steps:  32, steps per second:   8, episode reward: 40.800, mean reward:  1.275 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 84.327867, mean_q: 49.709138, mean_eps: 0.100000\n","     303234/2000000000: episode: 8438, duration: 4.962s, episode steps:  40, steps per second:   8, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.081632, mean_q: 49.329184, mean_eps: 0.100000\n","     303261/2000000000: episode: 8439, duration: 3.455s, episode steps:  27, steps per second:   8, episode reward: 204.600, mean reward:  7.578 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 90.629827, mean_q: 48.905336, mean_eps: 0.100000\n","     303292/2000000000: episode: 8440, duration: 3.839s, episode steps:  31, steps per second:   8, episode reward: 115.000, mean reward:  3.710 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 76.932204, mean_q: 49.600196, mean_eps: 0.100000\n","     303322/2000000000: episode: 8441, duration: 3.735s, episode steps:  30, steps per second:   8, episode reward: 38.000, mean reward:  1.267 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 88.619758, mean_q: 48.854197, mean_eps: 0.100000\n","     303362/2000000000: episode: 8442, duration: 4.838s, episode steps:  40, steps per second:   8, episode reward: 101.400, mean reward:  2.535 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.780728, mean_q: 48.719249, mean_eps: 0.100000\n","     303397/2000000000: episode: 8443, duration: 4.429s, episode steps:  35, steps per second:   8, episode reward: 34.100, mean reward:  0.974 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 91.848619, mean_q: 49.922871, mean_eps: 0.100000\n","     303424/2000000000: episode: 8444, duration: 3.425s, episode steps:  27, steps per second:   8, episode reward: 85.700, mean reward:  3.174 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 79.210854, mean_q: 50.384953, mean_eps: 0.100000\n","     303456/2000000000: episode: 8445, duration: 4.045s, episode steps:  32, steps per second:   8, episode reward: 168.200, mean reward:  5.256 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 83.672139, mean_q: 49.848894, mean_eps: 0.100000\n","     303476/2000000000: episode: 8446, duration: 2.605s, episode steps:  20, steps per second:   8, episode reward: 56.100, mean reward:  2.805 [-20.000, 18.000], mean action: 0.450 [0.000, 1.000],  loss: 89.022186, mean_q: 50.040873, mean_eps: 0.100000\n","     303506/2000000000: episode: 8447, duration: 3.723s, episode steps:  30, steps per second:   8, episode reward: 166.300, mean reward:  5.543 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 83.599911, mean_q: 49.148348, mean_eps: 0.100000\n","     303537/2000000000: episode: 8448, duration: 3.844s, episode steps:  31, steps per second:   8, episode reward: 122.600, mean reward:  3.955 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 93.885531, mean_q: 49.540113, mean_eps: 0.100000\n","     303561/2000000000: episode: 8449, duration: 3.147s, episode steps:  24, steps per second:   8, episode reward: 131.300, mean reward:  5.471 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 94.030286, mean_q: 50.006284, mean_eps: 0.100000\n","     303593/2000000000: episode: 8450, duration: 4.179s, episode steps:  32, steps per second:   8, episode reward: 67.000, mean reward:  2.094 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 85.704858, mean_q: 49.939612, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     303621/2000000000: episode: 8451, duration: 3.712s, episode steps:  28, steps per second:   8, episode reward: 109.800, mean reward:  3.921 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 88.344183, mean_q: 49.720204, mean_eps: 0.100000\n","     303652/2000000000: episode: 8452, duration: 4.059s, episode steps:  31, steps per second:   8, episode reward: 97.100, mean reward:  3.132 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.615953, mean_q: 50.304567, mean_eps: 0.100000\n","     303673/2000000000: episode: 8453, duration: 3.009s, episode steps:  21, steps per second:   7, episode reward: 83.000, mean reward:  3.952 [-20.000, 18.000], mean action: 0.571 [0.000, 2.000],  loss: 83.382514, mean_q: 50.603748, mean_eps: 0.100000\n","     303704/2000000000: episode: 8454, duration: 4.500s, episode steps:  31, steps per second:   7, episode reward: 234.500, mean reward:  7.565 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 81.596308, mean_q: 49.642402, mean_eps: 0.100000\n","     303733/2000000000: episode: 8455, duration: 3.816s, episode steps:  29, steps per second:   8, episode reward: 167.600, mean reward:  5.779 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.796424, mean_q: 49.442560, mean_eps: 0.100000\n","     303768/2000000000: episode: 8456, duration: 4.605s, episode steps:  35, steps per second:   8, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 82.766532, mean_q: 50.494480, mean_eps: 0.100000\n","     303800/2000000000: episode: 8457, duration: 4.186s, episode steps:  32, steps per second:   8, episode reward: 137.500, mean reward:  4.297 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 85.606890, mean_q: 49.817758, mean_eps: 0.100000\n","     303840/2000000000: episode: 8458, duration: 5.043s, episode steps:  40, steps per second:   8, episode reward: 169.600, mean reward:  4.240 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.386885, mean_q: 49.346766, mean_eps: 0.100000\n","     303868/2000000000: episode: 8459, duration: 4.011s, episode steps:  28, steps per second:   7, episode reward: 137.600, mean reward:  4.914 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 88.633188, mean_q: 49.607040, mean_eps: 0.100000\n","     303891/2000000000: episode: 8460, duration: 3.061s, episode steps:  23, steps per second:   8, episode reward: 109.200, mean reward:  4.748 [-20.000, 18.000], mean action: 0.783 [0.000, 2.000],  loss: 89.224915, mean_q: 50.175223, mean_eps: 0.100000\n","     303920/2000000000: episode: 8461, duration: 3.864s, episode steps:  29, steps per second:   8, episode reward: 122.400, mean reward:  4.221 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 84.920249, mean_q: 49.786080, mean_eps: 0.100000\n","     303952/2000000000: episode: 8462, duration: 4.091s, episode steps:  32, steps per second:   8, episode reward: 46.600, mean reward:  1.456 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 88.679080, mean_q: 49.978257, mean_eps: 0.100000\n","     303987/2000000000: episode: 8463, duration: 4.481s, episode steps:  35, steps per second:   8, episode reward: 86.800, mean reward:  2.480 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 83.985706, mean_q: 48.796776, mean_eps: 0.100000\n","     304011/2000000000: episode: 8464, duration: 3.157s, episode steps:  24, steps per second:   8, episode reward: 172.300, mean reward:  7.179 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 89.568314, mean_q: 50.570881, mean_eps: 0.100000\n","     304039/2000000000: episode: 8465, duration: 3.471s, episode steps:  28, steps per second:   8, episode reward: 66.900, mean reward:  2.389 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 87.154996, mean_q: 49.620449, mean_eps: 0.100000\n","     304079/2000000000: episode: 8466, duration: 4.940s, episode steps:  40, steps per second:   8, episode reward: 189.000, mean reward:  4.725 [-20.000, 18.000], mean action: 1.150 [0.000, 2.000],  loss: 84.307573, mean_q: 49.192929, mean_eps: 0.100000\n","     304119/2000000000: episode: 8467, duration: 5.114s, episode steps:  40, steps per second:   8, episode reward: 87.700, mean reward:  2.192 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 94.539965, mean_q: 49.024011, mean_eps: 0.100000\n","     304153/2000000000: episode: 8468, duration: 4.308s, episode steps:  34, steps per second:   8, episode reward: -85.100, mean reward: -2.503 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 90.485300, mean_q: 49.666814, mean_eps: 0.100000\n","     304184/2000000000: episode: 8469, duration: 4.006s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 90.189950, mean_q: 50.630494, mean_eps: 0.100000\n","     304211/2000000000: episode: 8470, duration: 3.660s, episode steps:  27, steps per second:   7, episode reward: -38.700, mean reward: -1.433 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 70.257575, mean_q: 49.383500, mean_eps: 0.100000\n","     304248/2000000000: episode: 8471, duration: 4.972s, episode steps:  37, steps per second:   7, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 87.957643, mean_q: 49.756747, mean_eps: 0.100000\n","     304279/2000000000: episode: 8472, duration: 4.179s, episode steps:  31, steps per second:   7, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 83.945636, mean_q: 48.617038, mean_eps: 0.100000\n","     304309/2000000000: episode: 8473, duration: 4.350s, episode steps:  30, steps per second:   7, episode reward: 78.700, mean reward:  2.623 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 82.443683, mean_q: 48.813640, mean_eps: 0.100000\n","     304334/2000000000: episode: 8474, duration: 3.431s, episode steps:  25, steps per second:   7, episode reward: 152.900, mean reward:  6.116 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.571753, mean_q: 49.055799, mean_eps: 0.100000\n","     304366/2000000000: episode: 8475, duration: 4.207s, episode steps:  32, steps per second:   8, episode reward: 155.700, mean reward:  4.866 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 86.840316, mean_q: 50.357821, mean_eps: 0.100000\n","     304393/2000000000: episode: 8476, duration: 3.591s, episode steps:  27, steps per second:   8, episode reward: 149.200, mean reward:  5.526 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 83.288344, mean_q: 49.810062, mean_eps: 0.100000\n","     304424/2000000000: episode: 8477, duration: 3.899s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.508525, mean_q: 49.101943, mean_eps: 0.100000\n","     304464/2000000000: episode: 8478, duration: 5.107s, episode steps:  40, steps per second:   8, episode reward: -54.000, mean reward: -1.350 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 84.203337, mean_q: 49.418077, mean_eps: 0.100000\n","     304490/2000000000: episode: 8479, duration: 3.415s, episode steps:  26, steps per second:   8, episode reward: 47.900, mean reward:  1.842 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 83.300760, mean_q: 49.409293, mean_eps: 0.100000\n","     304530/2000000000: episode: 8480, duration: 5.161s, episode steps:  40, steps per second:   8, episode reward: 145.400, mean reward:  3.635 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.283556, mean_q: 50.165720, mean_eps: 0.100000\n","     304561/2000000000: episode: 8481, duration: 3.931s, episode steps:  31, steps per second:   8, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 83.140566, mean_q: 49.919367, mean_eps: 0.100000\n","     304590/2000000000: episode: 8482, duration: 3.572s, episode steps:  29, steps per second:   8, episode reward: -58.000, mean reward: -2.000 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 82.722546, mean_q: 49.360684, mean_eps: 0.100000\n","     304621/2000000000: episode: 8483, duration: 3.897s, episode steps:  31, steps per second:   8, episode reward: 110.300, mean reward:  3.558 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 79.512345, mean_q: 49.664992, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     304655/2000000000: episode: 8484, duration: 4.300s, episode steps:  34, steps per second:   8, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 82.218049, mean_q: 49.352816, mean_eps: 0.100000\n","     304678/2000000000: episode: 8485, duration: 3.208s, episode steps:  23, steps per second:   7, episode reward: 166.000, mean reward:  7.217 [-20.000, 18.000], mean action: 0.826 [0.000, 2.000],  loss: 80.339496, mean_q: 48.851392, mean_eps: 0.100000\n","     304713/2000000000: episode: 8486, duration: 4.489s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.257 [0.000, 2.000],  loss: 79.771428, mean_q: 48.710664, mean_eps: 0.100000\n","     304744/2000000000: episode: 8487, duration: 4.004s, episode steps:  31, steps per second:   8, episode reward: 68.100, mean reward:  2.197 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 80.717072, mean_q: 49.691993, mean_eps: 0.100000\n","     304784/2000000000: episode: 8488, duration: 5.216s, episode steps:  40, steps per second:   8, episode reward: -36.000, mean reward: -0.900 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 89.860332, mean_q: 49.211350, mean_eps: 0.100000\n","     304811/2000000000: episode: 8489, duration: 3.570s, episode steps:  27, steps per second:   8, episode reward: -20.000, mean reward: -0.741 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.310220, mean_q: 49.714815, mean_eps: 0.100000\n","     304840/2000000000: episode: 8490, duration: 3.621s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 84.385994, mean_q: 50.200068, mean_eps: 0.100000\n","     304870/2000000000: episode: 8491, duration: 3.811s, episode steps:  30, steps per second:   8, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 91.763599, mean_q: 48.509136, mean_eps: 0.100000\n","     304900/2000000000: episode: 8492, duration: 3.790s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 76.106893, mean_q: 50.034259, mean_eps: 0.100000\n","     304930/2000000000: episode: 8493, duration: 3.875s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 95.556374, mean_q: 49.600858, mean_eps: 0.100000\n","     304970/2000000000: episode: 8494, duration: 4.974s, episode steps:  40, steps per second:   8, episode reward: 20.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 94.346240, mean_q: 49.977796, mean_eps: 0.100000\n","     305010/2000000000: episode: 8495, duration: 5.230s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.566919, mean_q: 50.155912, mean_eps: 0.100000\n","     305034/2000000000: episode: 8496, duration: 3.060s, episode steps:  24, steps per second:   8, episode reward: 123.100, mean reward:  5.129 [-20.000, 18.000], mean action: 0.583 [0.000, 2.000],  loss: 96.128974, mean_q: 48.746237, mean_eps: 0.100000\n","     305074/2000000000: episode: 8497, duration: 4.730s, episode steps:  40, steps per second:   8, episode reward: 110.700, mean reward:  2.767 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 87.873482, mean_q: 50.475173, mean_eps: 0.100000\n","     305113/2000000000: episode: 8498, duration: 5.145s, episode steps:  39, steps per second:   8, episode reward: 132.000, mean reward:  3.385 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 91.474161, mean_q: 49.172841, mean_eps: 0.100000\n","     305153/2000000000: episode: 8499, duration: 5.410s, episode steps:  40, steps per second:   7, episode reward: 12.000, mean reward:  0.300 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 81.148639, mean_q: 48.981580, mean_eps: 0.100000\n","     305179/2000000000: episode: 8500, duration: 3.564s, episode steps:  26, steps per second:   7, episode reward: 138.500, mean reward:  5.327 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 85.902746, mean_q: 50.673804, mean_eps: 0.100000\n","     305208/2000000000: episode: 8501, duration: 3.840s, episode steps:  29, steps per second:   8, episode reward: 165.700, mean reward:  5.714 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.115120, mean_q: 49.069667, mean_eps: 0.100000\n","     305246/2000000000: episode: 8502, duration: 4.859s, episode steps:  38, steps per second:   8, episode reward: 36.300, mean reward:  0.955 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 88.540764, mean_q: 49.162397, mean_eps: 0.100000\n","     305284/2000000000: episode: 8503, duration: 4.736s, episode steps:  38, steps per second:   8, episode reward: -10.500, mean reward: -0.276 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 90.504447, mean_q: 49.650690, mean_eps: 0.100000\n","     305313/2000000000: episode: 8504, duration: 3.676s, episode steps:  29, steps per second:   8, episode reward:  3.400, mean reward:  0.117 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 84.429426, mean_q: 49.635807, mean_eps: 0.100000\n","     305345/2000000000: episode: 8505, duration: 4.241s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 82.352584, mean_q: 49.277804, mean_eps: 0.100000\n","     305383/2000000000: episode: 8506, duration: 5.021s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 80.778316, mean_q: 50.071669, mean_eps: 0.100000\n","     305413/2000000000: episode: 8507, duration: 3.659s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.524629, mean_q: 50.047825, mean_eps: 0.100000\n","     305452/2000000000: episode: 8508, duration: 4.688s, episode steps:  39, steps per second:   8, episode reward: 137.000, mean reward:  3.513 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 87.312766, mean_q: 49.215255, mean_eps: 0.100000\n","     305492/2000000000: episode: 8509, duration: 5.151s, episode steps:  40, steps per second:   8, episode reward: 184.400, mean reward:  4.610 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 84.290697, mean_q: 49.064238, mean_eps: 0.100000\n","     305519/2000000000: episode: 8510, duration: 3.451s, episode steps:  27, steps per second:   8, episode reward: 68.500, mean reward:  2.537 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 84.507830, mean_q: 49.305326, mean_eps: 0.100000\n","     305551/2000000000: episode: 8511, duration: 4.117s, episode steps:  32, steps per second:   8, episode reward: 109.300, mean reward:  3.416 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 92.289877, mean_q: 49.156233, mean_eps: 0.100000\n","     305591/2000000000: episode: 8512, duration: 5.259s, episode steps:  40, steps per second:   8, episode reward: 48.600, mean reward:  1.215 [-20.000, 18.000], mean action: 1.575 [0.000, 2.000],  loss: 85.177804, mean_q: 49.151561, mean_eps: 0.100000\n","     305614/2000000000: episode: 8513, duration: 3.282s, episode steps:  23, steps per second:   7, episode reward: -50.800, mean reward: -2.209 [-20.000, 18.000], mean action: 0.913 [0.000, 2.000],  loss: 82.844202, mean_q: 50.162187, mean_eps: 0.100000\n","     305651/2000000000: episode: 8514, duration: 4.803s, episode steps:  37, steps per second:   8, episode reward: 107.600, mean reward:  2.908 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 90.222204, mean_q: 49.690042, mean_eps: 0.100000\n","     305687/2000000000: episode: 8515, duration: 4.613s, episode steps:  36, steps per second:   8, episode reward: 95.300, mean reward:  2.647 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 90.035463, mean_q: 49.213483, mean_eps: 0.100000\n","     305721/2000000000: episode: 8516, duration: 4.629s, episode steps:  34, steps per second:   7, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 82.026370, mean_q: 49.361511, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     305758/2000000000: episode: 8517, duration: 4.920s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 88.964204, mean_q: 49.535351, mean_eps: 0.100000\n","     305785/2000000000: episode: 8518, duration: 3.596s, episode steps:  27, steps per second:   8, episode reward: 89.900, mean reward:  3.330 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 85.918134, mean_q: 50.482766, mean_eps: 0.100000\n","     305824/2000000000: episode: 8519, duration: 5.001s, episode steps:  39, steps per second:   8, episode reward: -32.100, mean reward: -0.823 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 89.281888, mean_q: 47.971387, mean_eps: 0.100000\n","     305847/2000000000: episode: 8520, duration: 3.137s, episode steps:  23, steps per second:   7, episode reward: 176.900, mean reward:  7.691 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 75.698888, mean_q: 51.028434, mean_eps: 0.100000\n","     305876/2000000000: episode: 8521, duration: 3.801s, episode steps:  29, steps per second:   8, episode reward: 161.600, mean reward:  5.572 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 84.011643, mean_q: 49.912788, mean_eps: 0.100000\n","     305914/2000000000: episode: 8522, duration: 4.825s, episode steps:  38, steps per second:   8, episode reward: 213.800, mean reward:  5.626 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 84.386208, mean_q: 48.640525, mean_eps: 0.100000\n","     305946/2000000000: episode: 8523, duration: 4.178s, episode steps:  32, steps per second:   8, episode reward: -14.800, mean reward: -0.462 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.416621, mean_q: 49.912713, mean_eps: 0.100000\n","     305974/2000000000: episode: 8524, duration: 3.738s, episode steps:  28, steps per second:   7, episode reward: 106.500, mean reward:  3.804 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 90.477718, mean_q: 49.084671, mean_eps: 0.100000\n","     305997/2000000000: episode: 8525, duration: 2.986s, episode steps:  23, steps per second:   8, episode reward: 92.900, mean reward:  4.039 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 91.477945, mean_q: 50.135295, mean_eps: 0.100000\n","     306022/2000000000: episode: 8526, duration: 3.240s, episode steps:  25, steps per second:   8, episode reward: 78.600, mean reward:  3.144 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 82.435837, mean_q: 49.831262, mean_eps: 0.100000\n","     306049/2000000000: episode: 8527, duration: 3.380s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.117913, mean_q: 50.967707, mean_eps: 0.100000\n","     306078/2000000000: episode: 8528, duration: 3.791s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 76.264935, mean_q: 49.665267, mean_eps: 0.100000\n","     306118/2000000000: episode: 8529, duration: 5.128s, episode steps:  40, steps per second:   8, episode reward: 47.800, mean reward:  1.195 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 82.846564, mean_q: 49.234752, mean_eps: 0.100000\n","     306143/2000000000: episode: 8530, duration: 3.261s, episode steps:  25, steps per second:   8, episode reward: 208.500, mean reward:  8.340 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 82.180439, mean_q: 49.399184, mean_eps: 0.100000\n","     306170/2000000000: episode: 8531, duration: 3.588s, episode steps:  27, steps per second:   8, episode reward: 77.700, mean reward:  2.878 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 86.025167, mean_q: 49.488915, mean_eps: 0.100000\n","     306197/2000000000: episode: 8532, duration: 3.536s, episode steps:  27, steps per second:   8, episode reward: -21.200, mean reward: -0.785 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 78.751027, mean_q: 49.538163, mean_eps: 0.100000\n","     306231/2000000000: episode: 8533, duration: 4.304s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 90.353553, mean_q: 49.307481, mean_eps: 0.100000\n","     306261/2000000000: episode: 8534, duration: 3.750s, episode steps:  30, steps per second:   8, episode reward: 201.700, mean reward:  6.723 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 90.587168, mean_q: 49.388594, mean_eps: 0.100000\n","     306297/2000000000: episode: 8535, duration: 4.618s, episode steps:  36, steps per second:   8, episode reward: 21.300, mean reward:  0.592 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.334832, mean_q: 49.776907, mean_eps: 0.100000\n","     306322/2000000000: episode: 8536, duration: 3.281s, episode steps:  25, steps per second:   8, episode reward: 204.800, mean reward:  8.192 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 87.545881, mean_q: 48.760146, mean_eps: 0.100000\n","     306353/2000000000: episode: 8537, duration: 3.915s, episode steps:  31, steps per second:   8, episode reward: -35.000, mean reward: -1.129 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 86.696486, mean_q: 49.569170, mean_eps: 0.100000\n","     306384/2000000000: episode: 8538, duration: 3.814s, episode steps:  31, steps per second:   8, episode reward: 52.500, mean reward:  1.694 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 75.835437, mean_q: 50.857013, mean_eps: 0.100000\n","     306414/2000000000: episode: 8539, duration: 3.850s, episode steps:  30, steps per second:   8, episode reward: 94.100, mean reward:  3.137 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.416978, mean_q: 50.519754, mean_eps: 0.100000\n","     306448/2000000000: episode: 8540, duration: 4.353s, episode steps:  34, steps per second:   8, episode reward: -56.700, mean reward: -1.668 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 78.954813, mean_q: 50.214235, mean_eps: 0.100000\n","     306483/2000000000: episode: 8541, duration: 4.369s, episode steps:  35, steps per second:   8, episode reward: 53.900, mean reward:  1.540 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 85.697641, mean_q: 49.475559, mean_eps: 0.100000\n","     306514/2000000000: episode: 8542, duration: 4.396s, episode steps:  31, steps per second:   7, episode reward: 53.400, mean reward:  1.723 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 85.752864, mean_q: 49.723959, mean_eps: 0.100000\n","     306541/2000000000: episode: 8543, duration: 3.722s, episode steps:  27, steps per second:   7, episode reward: 94.700, mean reward:  3.507 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 94.577777, mean_q: 50.454338, mean_eps: 0.100000\n","     306575/2000000000: episode: 8544, duration: 4.409s, episode steps:  34, steps per second:   8, episode reward: 183.100, mean reward:  5.385 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 87.479987, mean_q: 49.675246, mean_eps: 0.100000\n","     306613/2000000000: episode: 8545, duration: 4.861s, episode steps:  38, steps per second:   8, episode reward: 52.300, mean reward:  1.376 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 84.044133, mean_q: 50.457680, mean_eps: 0.100000\n","     306638/2000000000: episode: 8546, duration: 3.266s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 77.180927, mean_q: 50.613690, mean_eps: 0.100000\n","     306665/2000000000: episode: 8547, duration: 3.561s, episode steps:  27, steps per second:   8, episode reward: 50.500, mean reward:  1.870 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 86.833321, mean_q: 49.831581, mean_eps: 0.100000\n","     306692/2000000000: episode: 8548, duration: 3.551s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 86.338016, mean_q: 49.403881, mean_eps: 0.100000\n","     306732/2000000000: episode: 8549, duration: 5.274s, episode steps:  40, steps per second:   8, episode reward: 85.900, mean reward:  2.148 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 90.418894, mean_q: 49.833108, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     306768/2000000000: episode: 8550, duration: 4.598s, episode steps:  36, steps per second:   8, episode reward: 144.100, mean reward:  4.003 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 89.291472, mean_q: 49.115658, mean_eps: 0.100000\n","     306803/2000000000: episode: 8551, duration: 4.609s, episode steps:  35, steps per second:   8, episode reward: 109.700, mean reward:  3.134 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 89.635002, mean_q: 49.659929, mean_eps: 0.100000\n","     306842/2000000000: episode: 8552, duration: 5.227s, episode steps:  39, steps per second:   7, episode reward: 192.200, mean reward:  4.928 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 73.462214, mean_q: 49.719852, mean_eps: 0.100000\n","     306877/2000000000: episode: 8553, duration: 4.693s, episode steps:  35, steps per second:   7, episode reward: 80.400, mean reward:  2.297 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 87.443971, mean_q: 50.747667, mean_eps: 0.100000\n","     306904/2000000000: episode: 8554, duration: 3.542s, episode steps:  27, steps per second:   8, episode reward: 89.800, mean reward:  3.326 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 83.146219, mean_q: 49.570596, mean_eps: 0.100000\n","     306934/2000000000: episode: 8555, duration: 3.953s, episode steps:  30, steps per second:   8, episode reward: 232.000, mean reward:  7.733 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 82.318017, mean_q: 48.560045, mean_eps: 0.100000\n","     306971/2000000000: episode: 8556, duration: 4.758s, episode steps:  37, steps per second:   8, episode reward: 66.900, mean reward:  1.808 [-20.000, 18.000], mean action: 1.324 [0.000, 2.000],  loss: 88.391333, mean_q: 49.557337, mean_eps: 0.100000\n","     306999/2000000000: episode: 8557, duration: 3.829s, episode steps:  28, steps per second:   7, episode reward: 67.400, mean reward:  2.407 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 87.334817, mean_q: 49.681281, mean_eps: 0.100000\n","     307035/2000000000: episode: 8558, duration: 4.486s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 82.949730, mean_q: 49.054359, mean_eps: 0.100000\n","     307065/2000000000: episode: 8559, duration: 3.680s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 85.247078, mean_q: 49.258098, mean_eps: 0.100000\n","     307097/2000000000: episode: 8560, duration: 4.016s, episode steps:  32, steps per second:   8, episode reward: -20.000, mean reward: -0.625 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 86.602074, mean_q: 48.237165, mean_eps: 0.100000\n","     307124/2000000000: episode: 8561, duration: 3.462s, episode steps:  27, steps per second:   8, episode reward: 67.100, mean reward:  2.485 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 80.763476, mean_q: 49.748079, mean_eps: 0.100000\n","     307164/2000000000: episode: 8562, duration: 4.798s, episode steps:  40, steps per second:   8, episode reward: 148.700, mean reward:  3.717 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 83.322595, mean_q: 48.742271, mean_eps: 0.100000\n","     307189/2000000000: episode: 8563, duration: 3.113s, episode steps:  25, steps per second:   8, episode reward: 136.500, mean reward:  5.460 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 90.203932, mean_q: 50.025668, mean_eps: 0.100000\n","     307225/2000000000: episode: 8564, duration: 4.457s, episode steps:  36, steps per second:   8, episode reward: 72.600, mean reward:  2.017 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 87.895272, mean_q: 49.209677, mean_eps: 0.100000\n","     307251/2000000000: episode: 8565, duration: 3.245s, episode steps:  26, steps per second:   8, episode reward: 206.500, mean reward:  7.942 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 85.591708, mean_q: 48.972592, mean_eps: 0.100000\n","     307276/2000000000: episode: 8566, duration: 3.114s, episode steps:  25, steps per second:   8, episode reward: 195.300, mean reward:  7.812 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 78.110207, mean_q: 50.539210, mean_eps: 0.100000\n","     307310/2000000000: episode: 8567, duration: 4.125s, episode steps:  34, steps per second:   8, episode reward: 68.300, mean reward:  2.009 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 83.205244, mean_q: 48.684051, mean_eps: 0.100000\n","     307339/2000000000: episode: 8568, duration: 3.558s, episode steps:  29, steps per second:   8, episode reward: 203.900, mean reward:  7.031 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 73.365132, mean_q: 50.643534, mean_eps: 0.100000\n","     307369/2000000000: episode: 8569, duration: 3.612s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 88.665770, mean_q: 49.834059, mean_eps: 0.100000\n","     307396/2000000000: episode: 8570, duration: 3.230s, episode steps:  27, steps per second:   8, episode reward: 56.900, mean reward:  2.107 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 83.959678, mean_q: 50.025514, mean_eps: 0.100000\n","     307434/2000000000: episode: 8571, duration: 4.770s, episode steps:  38, steps per second:   8, episode reward: 47.800, mean reward:  1.258 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 82.895241, mean_q: 49.498688, mean_eps: 0.100000\n","     307464/2000000000: episode: 8572, duration: 3.673s, episode steps:  30, steps per second:   8, episode reward: 46.100, mean reward:  1.537 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.414279, mean_q: 49.666523, mean_eps: 0.100000\n","     307499/2000000000: episode: 8573, duration: 4.404s, episode steps:  35, steps per second:   8, episode reward: 263.200, mean reward:  7.520 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 88.969688, mean_q: 49.366272, mean_eps: 0.100000\n","     307532/2000000000: episode: 8574, duration: 3.964s, episode steps:  33, steps per second:   8, episode reward: 120.600, mean reward:  3.655 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 95.351860, mean_q: 50.641945, mean_eps: 0.100000\n","     307571/2000000000: episode: 8575, duration: 4.742s, episode steps:  39, steps per second:   8, episode reward: 208.000, mean reward:  5.333 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 92.274568, mean_q: 49.690741, mean_eps: 0.100000\n","     307603/2000000000: episode: 8576, duration: 3.969s, episode steps:  32, steps per second:   8, episode reward: 229.400, mean reward:  7.169 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 87.544427, mean_q: 48.794383, mean_eps: 0.100000\n","     307633/2000000000: episode: 8577, duration: 3.622s, episode steps:  30, steps per second:   8, episode reward: -20.700, mean reward: -0.690 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 79.961605, mean_q: 49.096598, mean_eps: 0.100000\n","     307670/2000000000: episode: 8578, duration: 4.521s, episode steps:  37, steps per second:   8, episode reward: 198.300, mean reward:  5.359 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 88.504955, mean_q: 49.505592, mean_eps: 0.100000\n","     307703/2000000000: episode: 8579, duration: 4.075s, episode steps:  33, steps per second:   8, episode reward: 148.600, mean reward:  4.503 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 88.687343, mean_q: 48.166429, mean_eps: 0.100000\n","     307732/2000000000: episode: 8580, duration: 3.907s, episode steps:  29, steps per second:   7, episode reward: 57.200, mean reward:  1.972 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 92.951228, mean_q: 50.419034, mean_eps: 0.100000\n","     307765/2000000000: episode: 8581, duration: 4.168s, episode steps:  33, steps per second:   8, episode reward: 145.300, mean reward:  4.403 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 80.072276, mean_q: 49.815047, mean_eps: 0.100000\n","     307793/2000000000: episode: 8582, duration: 3.548s, episode steps:  28, steps per second:   8, episode reward: 89.900, mean reward:  3.211 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 81.171453, mean_q: 49.316619, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     307819/2000000000: episode: 8583, duration: 3.366s, episode steps:  26, steps per second:   8, episode reward: 44.900, mean reward:  1.727 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 84.939190, mean_q: 49.694309, mean_eps: 0.100000\n","     307850/2000000000: episode: 8584, duration: 4.042s, episode steps:  31, steps per second:   8, episode reward: 120.000, mean reward:  3.871 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 85.040262, mean_q: 50.658732, mean_eps: 0.100000\n","     307874/2000000000: episode: 8585, duration: 3.217s, episode steps:  24, steps per second:   7, episode reward: 71.900, mean reward:  2.996 [-20.000, 18.000], mean action: 0.792 [0.000, 2.000],  loss: 84.441137, mean_q: 48.325319, mean_eps: 0.100000\n","     307914/2000000000: episode: 8586, duration: 5.365s, episode steps:  40, steps per second:   7, episode reward: 121.300, mean reward:  3.033 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 89.831556, mean_q: 50.199104, mean_eps: 0.100000\n","     307944/2000000000: episode: 8587, duration: 3.980s, episode steps:  30, steps per second:   8, episode reward: 194.200, mean reward:  6.473 [-20.000, 18.800], mean action: 0.900 [0.000, 2.000],  loss: 85.983710, mean_q: 49.664759, mean_eps: 0.100000\n","     307982/2000000000: episode: 8588, duration: 5.052s, episode steps:  38, steps per second:   8, episode reward: 155.000, mean reward:  4.079 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 86.901135, mean_q: 49.229365, mean_eps: 0.100000\n","     308022/2000000000: episode: 8589, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: 35.900, mean reward:  0.898 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 80.276212, mean_q: 49.658275, mean_eps: 0.100000\n","     308062/2000000000: episode: 8590, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: 130.500, mean reward:  3.262 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 88.114579, mean_q: 49.249070, mean_eps: 0.100000\n","     308095/2000000000: episode: 8591, duration: 4.407s, episode steps:  33, steps per second:   7, episode reward: 103.700, mean reward:  3.142 [-20.000, 18.000], mean action: 1.242 [0.000, 2.000],  loss: 86.532289, mean_q: 50.767721, mean_eps: 0.100000\n","     308123/2000000000: episode: 8592, duration: 3.920s, episode steps:  28, steps per second:   7, episode reward: 128.500, mean reward:  4.589 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 78.890866, mean_q: 50.008821, mean_eps: 0.100000\n","     308152/2000000000: episode: 8593, duration: 3.815s, episode steps:  29, steps per second:   8, episode reward: 148.600, mean reward:  5.124 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 94.450591, mean_q: 49.727068, mean_eps: 0.100000\n","     308185/2000000000: episode: 8594, duration: 4.351s, episode steps:  33, steps per second:   8, episode reward: 56.300, mean reward:  1.706 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 79.193473, mean_q: 49.223050, mean_eps: 0.100000\n","     308219/2000000000: episode: 8595, duration: 4.379s, episode steps:  34, steps per second:   8, episode reward: 55.400, mean reward:  1.629 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 85.929336, mean_q: 49.236240, mean_eps: 0.100000\n","     308246/2000000000: episode: 8596, duration: 3.462s, episode steps:  27, steps per second:   8, episode reward: 193.700, mean reward:  7.174 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 86.720304, mean_q: 49.197097, mean_eps: 0.100000\n","     308267/2000000000: episode: 8597, duration: 2.577s, episode steps:  21, steps per second:   8, episode reward: 270.500, mean reward: 12.881 [-20.000, 18.000], mean action: 0.524 [0.000, 2.000],  loss: 84.793868, mean_q: 49.153218, mean_eps: 0.100000\n","     308301/2000000000: episode: 8598, duration: 4.313s, episode steps:  34, steps per second:   8, episode reward: 205.300, mean reward:  6.038 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 86.146993, mean_q: 49.353986, mean_eps: 0.100000\n","     308331/2000000000: episode: 8599, duration: 3.788s, episode steps:  30, steps per second:   8, episode reward: -8.600, mean reward: -0.287 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 77.789526, mean_q: 49.411743, mean_eps: 0.100000\n","     308360/2000000000: episode: 8600, duration: 3.687s, episode steps:  29, steps per second:   8, episode reward: 88.700, mean reward:  3.059 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 89.530077, mean_q: 49.281570, mean_eps: 0.100000\n","     308395/2000000000: episode: 8601, duration: 4.334s, episode steps:  35, steps per second:   8, episode reward: 24.000, mean reward:  0.686 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 86.107045, mean_q: 49.301239, mean_eps: 0.100000\n","     308424/2000000000: episode: 8602, duration: 3.680s, episode steps:  29, steps per second:   8, episode reward: 124.400, mean reward:  4.290 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 95.947133, mean_q: 49.924659, mean_eps: 0.100000\n","     308449/2000000000: episode: 8603, duration: 3.058s, episode steps:  25, steps per second:   8, episode reward: 27.400, mean reward:  1.096 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 82.302771, mean_q: 49.099008, mean_eps: 0.100000\n","     308478/2000000000: episode: 8604, duration: 3.668s, episode steps:  29, steps per second:   8, episode reward: 163.900, mean reward:  5.652 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 90.419786, mean_q: 50.758844, mean_eps: 0.100000\n","     308509/2000000000: episode: 8605, duration: 3.806s, episode steps:  31, steps per second:   8, episode reward: 141.100, mean reward:  4.552 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 87.826346, mean_q: 49.892485, mean_eps: 0.100000\n","     308540/2000000000: episode: 8606, duration: 3.932s, episode steps:  31, steps per second:   8, episode reward: 107.900, mean reward:  3.481 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 93.536093, mean_q: 49.130564, mean_eps: 0.100000\n","     308567/2000000000: episode: 8607, duration: 3.484s, episode steps:  27, steps per second:   8, episode reward: 166.200, mean reward:  6.156 [-20.000, 18.600], mean action: 0.815 [0.000, 2.000],  loss: 92.358846, mean_q: 49.991503, mean_eps: 0.100000\n","     308594/2000000000: episode: 8608, duration: 3.506s, episode steps:  27, steps per second:   8, episode reward: 158.600, mean reward:  5.874 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 88.635724, mean_q: 50.128445, mean_eps: 0.100000\n","     308624/2000000000: episode: 8609, duration: 4.040s, episode steps:  30, steps per second:   7, episode reward: 83.900, mean reward:  2.797 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 85.641492, mean_q: 49.395992, mean_eps: 0.100000\n","     308657/2000000000: episode: 8610, duration: 4.609s, episode steps:  33, steps per second:   7, episode reward: 203.700, mean reward:  6.173 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 78.317138, mean_q: 50.143157, mean_eps: 0.100000\n","     308684/2000000000: episode: 8611, duration: 3.631s, episode steps:  27, steps per second:   7, episode reward: 189.400, mean reward:  7.015 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 83.688396, mean_q: 50.801532, mean_eps: 0.100000\n","     308711/2000000000: episode: 8612, duration: 3.567s, episode steps:  27, steps per second:   8, episode reward: 69.000, mean reward:  2.556 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 81.492695, mean_q: 50.389779, mean_eps: 0.100000\n","     308751/2000000000: episode: 8613, duration: 5.349s, episode steps:  40, steps per second:   7, episode reward: 40.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 92.085750, mean_q: 49.431008, mean_eps: 0.100000\n","     308784/2000000000: episode: 8614, duration: 4.368s, episode steps:  33, steps per second:   8, episode reward: 117.000, mean reward:  3.545 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 82.736385, mean_q: 48.776268, mean_eps: 0.100000\n","     308813/2000000000: episode: 8615, duration: 3.911s, episode steps:  29, steps per second:   7, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 80.432613, mean_q: 49.157746, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     308847/2000000000: episode: 8616, duration: 4.571s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 85.067597, mean_q: 49.298981, mean_eps: 0.100000\n","     308872/2000000000: episode: 8617, duration: 3.352s, episode steps:  25, steps per second:   7, episode reward: 106.500, mean reward:  4.260 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 88.540819, mean_q: 49.142986, mean_eps: 0.100000\n","     308903/2000000000: episode: 8618, duration: 4.144s, episode steps:  31, steps per second:   7, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 78.355298, mean_q: 49.192630, mean_eps: 0.100000\n","     308929/2000000000: episode: 8619, duration: 3.463s, episode steps:  26, steps per second:   8, episode reward: 180.000, mean reward:  6.923 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 84.358971, mean_q: 49.711057, mean_eps: 0.100000\n","     308969/2000000000: episode: 8620, duration: 5.349s, episode steps:  40, steps per second:   7, episode reward: 97.900, mean reward:  2.447 [-20.000, 19.200], mean action: 1.225 [0.000, 2.000],  loss: 88.691763, mean_q: 48.968030, mean_eps: 0.100000\n","     308996/2000000000: episode: 8621, duration: 3.693s, episode steps:  27, steps per second:   7, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 85.425330, mean_q: 50.169184, mean_eps: 0.100000\n","     309036/2000000000: episode: 8622, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.748914, mean_q: 48.928771, mean_eps: 0.100000\n","     309065/2000000000: episode: 8623, duration: 3.842s, episode steps:  29, steps per second:   8, episode reward: 69.700, mean reward:  2.403 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 84.764020, mean_q: 48.657896, mean_eps: 0.100000\n","     309105/2000000000: episode: 8624, duration: 4.909s, episode steps:  40, steps per second:   8, episode reward: 123.900, mean reward:  3.097 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.993382, mean_q: 49.808942, mean_eps: 0.100000\n","     309132/2000000000: episode: 8625, duration: 3.230s, episode steps:  27, steps per second:   8, episode reward: 90.000, mean reward:  3.333 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 84.805045, mean_q: 49.074572, mean_eps: 0.100000\n","     309166/2000000000: episode: 8626, duration: 4.161s, episode steps:  34, steps per second:   8, episode reward: 94.000, mean reward:  2.765 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 86.234488, mean_q: 48.353753, mean_eps: 0.100000\n","     309193/2000000000: episode: 8627, duration: 3.372s, episode steps:  27, steps per second:   8, episode reward: -3.200, mean reward: -0.119 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 86.090172, mean_q: 50.589592, mean_eps: 0.100000\n","     309217/2000000000: episode: 8628, duration: 3.103s, episode steps:  24, steps per second:   8, episode reward: 132.000, mean reward:  5.500 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 81.951431, mean_q: 49.559045, mean_eps: 0.100000\n","     309247/2000000000: episode: 8629, duration: 3.752s, episode steps:  30, steps per second:   8, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 82.992854, mean_q: 49.300277, mean_eps: 0.100000\n","     309280/2000000000: episode: 8630, duration: 4.183s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 77.931256, mean_q: 49.562444, mean_eps: 0.100000\n","     309311/2000000000: episode: 8631, duration: 3.894s, episode steps:  31, steps per second:   8, episode reward: -39.600, mean reward: -1.277 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 88.141109, mean_q: 49.973021, mean_eps: 0.100000\n","     309351/2000000000: episode: 8632, duration: 5.119s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 83.341203, mean_q: 49.737111, mean_eps: 0.100000\n","     309382/2000000000: episode: 8633, duration: 3.944s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 85.157210, mean_q: 49.683251, mean_eps: 0.100000\n","     309410/2000000000: episode: 8634, duration: 3.635s, episode steps:  28, steps per second:   8, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 92.717854, mean_q: 49.880825, mean_eps: 0.100000\n","     309449/2000000000: episode: 8635, duration: 4.958s, episode steps:  39, steps per second:   8, episode reward: 208.000, mean reward:  5.333 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 83.405318, mean_q: 48.972397, mean_eps: 0.100000\n","     309487/2000000000: episode: 8636, duration: 4.799s, episode steps:  38, steps per second:   8, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 88.558453, mean_q: 48.878042, mean_eps: 0.100000\n","     309518/2000000000: episode: 8637, duration: 4.624s, episode steps:  31, steps per second:   7, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 1.258 [0.000, 2.000],  loss: 82.593353, mean_q: 49.208985, mean_eps: 0.100000\n","     309552/2000000000: episode: 8638, duration: 4.691s, episode steps:  34, steps per second:   7, episode reward: -20.000, mean reward: -0.588 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 87.247810, mean_q: 49.557543, mean_eps: 0.100000\n","     309589/2000000000: episode: 8639, duration: 4.771s, episode steps:  37, steps per second:   8, episode reward: 94.000, mean reward:  2.541 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 85.650943, mean_q: 48.737782, mean_eps: 0.100000\n","     309626/2000000000: episode: 8640, duration: 4.759s, episode steps:  37, steps per second:   8, episode reward: 132.000, mean reward:  3.568 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 90.902128, mean_q: 49.499316, mean_eps: 0.100000\n","     309666/2000000000: episode: 8641, duration: 5.299s, episode steps:  40, steps per second:   8, episode reward: 257.800, mean reward:  6.445 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.202666, mean_q: 50.292484, mean_eps: 0.100000\n","     309698/2000000000: episode: 8642, duration: 4.308s, episode steps:  32, steps per second:   7, episode reward: 187.500, mean reward:  5.859 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 98.297823, mean_q: 48.996951, mean_eps: 0.100000\n","     309728/2000000000: episode: 8643, duration: 4.021s, episode steps:  30, steps per second:   7, episode reward: 178.400, mean reward:  5.947 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 95.499902, mean_q: 50.135414, mean_eps: 0.100000\n","     309767/2000000000: episode: 8644, duration: 5.175s, episode steps:  39, steps per second:   8, episode reward: 107.700, mean reward:  2.762 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 84.043362, mean_q: 49.851996, mean_eps: 0.100000\n","     309801/2000000000: episode: 8645, duration: 4.643s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 79.297055, mean_q: 49.516346, mean_eps: 0.100000\n","     309827/2000000000: episode: 8646, duration: 3.303s, episode steps:  26, steps per second:   8, episode reward: 124.100, mean reward:  4.773 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 84.834196, mean_q: 50.049271, mean_eps: 0.100000\n","     309853/2000000000: episode: 8647, duration: 3.334s, episode steps:  26, steps per second:   8, episode reward: 72.900, mean reward:  2.804 [-20.000, 18.000], mean action: 1.038 [0.000, 2.000],  loss: 84.211527, mean_q: 51.004355, mean_eps: 0.100000\n","     309887/2000000000: episode: 8648, duration: 4.406s, episode steps:  34, steps per second:   8, episode reward: 183.400, mean reward:  5.394 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 86.156379, mean_q: 47.995874, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     309912/2000000000: episode: 8649, duration: 3.506s, episode steps:  25, steps per second:   7, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 84.096806, mean_q: 50.132179, mean_eps: 0.100000\n","     309952/2000000000: episode: 8650, duration: 5.221s, episode steps:  40, steps per second:   8, episode reward: 58.700, mean reward:  1.468 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.088730, mean_q: 50.369222, mean_eps: 0.100000\n","     309984/2000000000: episode: 8651, duration: 4.205s, episode steps:  32, steps per second:   8, episode reward: 161.900, mean reward:  5.059 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 82.546364, mean_q: 49.779902, mean_eps: 0.100000\n","     310021/2000000000: episode: 8652, duration: 5.251s, episode steps:  37, steps per second:   7, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 84.332771, mean_q: 48.370719, mean_eps: 0.100000\n","     310057/2000000000: episode: 8653, duration: 4.669s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 89.322084, mean_q: 48.820384, mean_eps: 0.100000\n","     310089/2000000000: episode: 8654, duration: 4.210s, episode steps:  32, steps per second:   8, episode reward: 80.100, mean reward:  2.503 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 87.815152, mean_q: 50.073049, mean_eps: 0.100000\n","     310129/2000000000: episode: 8655, duration: 4.852s, episode steps:  40, steps per second:   8, episode reward: -37.800, mean reward: -0.945 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 89.742442, mean_q: 50.285783, mean_eps: 0.100000\n","     310156/2000000000: episode: 8656, duration: 3.365s, episode steps:  27, steps per second:   8, episode reward: 59.200, mean reward:  2.193 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 85.014429, mean_q: 50.526802, mean_eps: 0.100000\n","     310190/2000000000: episode: 8657, duration: 4.175s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 87.191894, mean_q: 49.708874, mean_eps: 0.100000\n","     310216/2000000000: episode: 8658, duration: 3.292s, episode steps:  26, steps per second:   8, episode reward: 161.900, mean reward:  6.227 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 85.672940, mean_q: 49.158107, mean_eps: 0.100000\n","     310243/2000000000: episode: 8659, duration: 3.597s, episode steps:  27, steps per second:   8, episode reward: -32.200, mean reward: -1.193 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 90.440611, mean_q: 48.743074, mean_eps: 0.100000\n","     310272/2000000000: episode: 8660, duration: 3.931s, episode steps:  29, steps per second:   7, episode reward: 247.600, mean reward:  8.538 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 80.439532, mean_q: 50.807480, mean_eps: 0.100000\n","     310306/2000000000: episode: 8661, duration: 4.445s, episode steps:  34, steps per second:   8, episode reward: 119.200, mean reward:  3.506 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 92.163060, mean_q: 49.967095, mean_eps: 0.100000\n","     310342/2000000000: episode: 8662, duration: 4.976s, episode steps:  36, steps per second:   7, episode reward: 94.000, mean reward:  2.611 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 89.063463, mean_q: 50.280509, mean_eps: 0.100000\n","     310373/2000000000: episode: 8663, duration: 4.618s, episode steps:  31, steps per second:   7, episode reward: 56.000, mean reward:  1.806 [-20.000, 18.000], mean action: 1.226 [0.000, 2.000],  loss: 96.484138, mean_q: 50.084888, mean_eps: 0.100000\n","     310401/2000000000: episode: 8664, duration: 3.826s, episode steps:  28, steps per second:   7, episode reward: 154.900, mean reward:  5.532 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 97.837069, mean_q: 49.994243, mean_eps: 0.100000\n","     310438/2000000000: episode: 8665, duration: 4.760s, episode steps:  37, steps per second:   8, episode reward: 118.600, mean reward:  3.205 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 99.512249, mean_q: 49.704305, mean_eps: 0.100000\n","     310477/2000000000: episode: 8666, duration: 5.156s, episode steps:  39, steps per second:   8, episode reward: 96.000, mean reward:  2.462 [-20.000, 18.000], mean action: 1.154 [0.000, 2.000],  loss: 97.975195, mean_q: 49.639860, mean_eps: 0.100000\n","     310506/2000000000: episode: 8667, duration: 3.969s, episode steps:  29, steps per second:   7, episode reward: 152.000, mean reward:  5.241 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 83.273606, mean_q: 50.146801, mean_eps: 0.100000\n","     310540/2000000000: episode: 8668, duration: 4.725s, episode steps:  34, steps per second:   7, episode reward: -28.900, mean reward: -0.850 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 95.261908, mean_q: 49.718034, mean_eps: 0.100000\n","     310564/2000000000: episode: 8669, duration: 3.088s, episode steps:  24, steps per second:   8, episode reward: -13.000, mean reward: -0.542 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 91.350998, mean_q: 48.960308, mean_eps: 0.100000\n","     310599/2000000000: episode: 8670, duration: 4.394s, episode steps:  35, steps per second:   8, episode reward: 178.900, mean reward:  5.111 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 86.163951, mean_q: 49.939158, mean_eps: 0.100000\n","     310622/2000000000: episode: 8671, duration: 2.871s, episode steps:  23, steps per second:   8, episode reward: 203.700, mean reward:  8.857 [-20.000, 18.000], mean action: 0.565 [0.000, 2.000],  loss: 90.821180, mean_q: 50.470638, mean_eps: 0.100000\n","     310653/2000000000: episode: 8672, duration: 4.074s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.284694, mean_q: 50.308088, mean_eps: 0.100000\n","     310684/2000000000: episode: 8673, duration: 3.842s, episode steps:  31, steps per second:   8, episode reward: 190.900, mean reward:  6.158 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 88.882282, mean_q: 49.726013, mean_eps: 0.100000\n","     310718/2000000000: episode: 8674, duration: 4.417s, episode steps:  34, steps per second:   8, episode reward: 77.100, mean reward:  2.268 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 78.729442, mean_q: 50.587871, mean_eps: 0.100000\n","     310757/2000000000: episode: 8675, duration: 5.022s, episode steps:  39, steps per second:   8, episode reward: 185.100, mean reward:  4.746 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 81.537507, mean_q: 50.839411, mean_eps: 0.100000\n","     310789/2000000000: episode: 8676, duration: 4.162s, episode steps:  32, steps per second:   8, episode reward: 86.000, mean reward:  2.688 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 90.107775, mean_q: 49.522308, mean_eps: 0.100000\n","     310816/2000000000: episode: 8677, duration: 3.570s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 82.621067, mean_q: 50.471334, mean_eps: 0.100000\n","     310849/2000000000: episode: 8678, duration: 4.408s, episode steps:  33, steps per second:   7, episode reward: 30.900, mean reward:  0.936 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 93.296990, mean_q: 50.418762, mean_eps: 0.100000\n","     310878/2000000000: episode: 8679, duration: 3.755s, episode steps:  29, steps per second:   8, episode reward: 192.400, mean reward:  6.634 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 82.859631, mean_q: 50.457785, mean_eps: 0.100000\n","     310904/2000000000: episode: 8680, duration: 3.454s, episode steps:  26, steps per second:   8, episode reward: 26.000, mean reward:  1.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.530917, mean_q: 51.002134, mean_eps: 0.100000\n","     310940/2000000000: episode: 8681, duration: 4.760s, episode steps:  36, steps per second:   8, episode reward: 192.400, mean reward:  5.344 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 90.954948, mean_q: 48.829169, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     310966/2000000000: episode: 8682, duration: 3.453s, episode steps:  26, steps per second:   8, episode reward: 177.400, mean reward:  6.823 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 83.259992, mean_q: 49.948809, mean_eps: 0.100000\n","     310993/2000000000: episode: 8683, duration: 3.482s, episode steps:  27, steps per second:   8, episode reward: 83.400, mean reward:  3.089 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 85.604786, mean_q: 49.994683, mean_eps: 0.100000\n","     311020/2000000000: episode: 8684, duration: 3.472s, episode steps:  27, steps per second:   8, episode reward:  4.500, mean reward:  0.167 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 86.629172, mean_q: 51.609452, mean_eps: 0.100000\n","     311048/2000000000: episode: 8685, duration: 3.797s, episode steps:  28, steps per second:   7, episode reward: 154.700, mean reward:  5.525 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 89.522925, mean_q: 49.650578, mean_eps: 0.100000\n","     311077/2000000000: episode: 8686, duration: 3.538s, episode steps:  29, steps per second:   8, episode reward: 175.300, mean reward:  6.045 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 89.815239, mean_q: 48.999909, mean_eps: 0.100000\n","     311108/2000000000: episode: 8687, duration: 4.277s, episode steps:  31, steps per second:   7, episode reward: 154.900, mean reward:  4.997 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 82.093626, mean_q: 50.300142, mean_eps: 0.100000\n","     311142/2000000000: episode: 8688, duration: 4.447s, episode steps:  34, steps per second:   8, episode reward: 54.200, mean reward:  1.594 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 90.698602, mean_q: 48.178919, mean_eps: 0.100000\n","     311166/2000000000: episode: 8689, duration: 3.201s, episode steps:  24, steps per second:   7, episode reward: 36.900, mean reward:  1.537 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 85.606829, mean_q: 49.576184, mean_eps: 0.100000\n","     311202/2000000000: episode: 8690, duration: 4.356s, episode steps:  36, steps per second:   8, episode reward: 160.400, mean reward:  4.456 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 86.878869, mean_q: 50.452980, mean_eps: 0.100000\n","     311232/2000000000: episode: 8691, duration: 3.615s, episode steps:  30, steps per second:   8, episode reward: 146.000, mean reward:  4.867 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 92.957605, mean_q: 49.657411, mean_eps: 0.100000\n","     311259/2000000000: episode: 8692, duration: 3.313s, episode steps:  27, steps per second:   8, episode reward: 88.100, mean reward:  3.263 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 77.558207, mean_q: 50.099360, mean_eps: 0.100000\n","     311289/2000000000: episode: 8693, duration: 3.757s, episode steps:  30, steps per second:   8, episode reward: 155.900, mean reward:  5.197 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 97.092164, mean_q: 50.454008, mean_eps: 0.100000\n","     311316/2000000000: episode: 8694, duration: 3.047s, episode steps:  27, steps per second:   9, episode reward: 172.300, mean reward:  6.381 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 83.212088, mean_q: 50.480546, mean_eps: 0.100000\n","     311341/2000000000: episode: 8695, duration: 2.867s, episode steps:  25, steps per second:   9, episode reward: 122.800, mean reward:  4.912 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 82.833066, mean_q: 50.367723, mean_eps: 0.100000\n","     311381/2000000000: episode: 8696, duration: 4.796s, episode steps:  40, steps per second:   8, episode reward: 160.700, mean reward:  4.018 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.322593, mean_q: 51.064020, mean_eps: 0.100000\n","     311406/2000000000: episode: 8697, duration: 3.065s, episode steps:  25, steps per second:   8, episode reward: 123.800, mean reward:  4.952 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 88.265130, mean_q: 49.777217, mean_eps: 0.100000\n","     311430/2000000000: episode: 8698, duration: 3.014s, episode steps:  24, steps per second:   8, episode reward: 221.000, mean reward:  9.208 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 96.023081, mean_q: 49.425855, mean_eps: 0.100000\n","     311465/2000000000: episode: 8699, duration: 4.211s, episode steps:  35, steps per second:   8, episode reward: 147.600, mean reward:  4.217 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 82.995489, mean_q: 49.988369, mean_eps: 0.100000\n","     311494/2000000000: episode: 8700, duration: 3.518s, episode steps:  29, steps per second:   8, episode reward: 74.000, mean reward:  2.552 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 84.698319, mean_q: 50.452914, mean_eps: 0.100000\n","     311534/2000000000: episode: 8701, duration: 4.706s, episode steps:  40, steps per second:   8, episode reward: 69.400, mean reward:  1.735 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 80.214382, mean_q: 50.937690, mean_eps: 0.100000\n","     311564/2000000000: episode: 8702, duration: 3.707s, episode steps:  30, steps per second:   8, episode reward: -43.000, mean reward: -1.433 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 90.034497, mean_q: 49.483994, mean_eps: 0.100000\n","     311596/2000000000: episode: 8703, duration: 3.880s, episode steps:  32, steps per second:   8, episode reward: 73.600, mean reward:  2.300 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.571024, mean_q: 50.020224, mean_eps: 0.100000\n","     311634/2000000000: episode: 8704, duration: 4.265s, episode steps:  38, steps per second:   9, episode reward: -22.200, mean reward: -0.584 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 86.879787, mean_q: 50.338109, mean_eps: 0.100000\n","     311661/2000000000: episode: 8705, duration: 3.120s, episode steps:  27, steps per second:   9, episode reward: 208.000, mean reward:  7.704 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 88.732845, mean_q: 50.303251, mean_eps: 0.100000\n","     311696/2000000000: episode: 8706, duration: 4.332s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 85.739528, mean_q: 50.711830, mean_eps: 0.100000\n","     311736/2000000000: episode: 8707, duration: 4.933s, episode steps:  40, steps per second:   8, episode reward: 78.000, mean reward:  1.950 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 95.642013, mean_q: 50.883006, mean_eps: 0.100000\n","     311764/2000000000: episode: 8708, duration: 3.439s, episode steps:  28, steps per second:   8, episode reward: 113.700, mean reward:  4.061 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 81.398313, mean_q: 49.364764, mean_eps: 0.100000\n","     311792/2000000000: episode: 8709, duration: 3.578s, episode steps:  28, steps per second:   8, episode reward: -4.800, mean reward: -0.171 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 76.309972, mean_q: 50.996571, mean_eps: 0.100000\n","     311827/2000000000: episode: 8710, duration: 4.487s, episode steps:  35, steps per second:   8, episode reward: 243.200, mean reward:  6.949 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 91.161421, mean_q: 49.204204, mean_eps: 0.100000\n","     311867/2000000000: episode: 8711, duration: 4.928s, episode steps:  40, steps per second:   8, episode reward: 138.500, mean reward:  3.462 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 92.997742, mean_q: 50.103144, mean_eps: 0.100000\n","     311892/2000000000: episode: 8712, duration: 3.251s, episode steps:  25, steps per second:   8, episode reward: 208.000, mean reward:  8.320 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 84.874668, mean_q: 49.897983, mean_eps: 0.100000\n","     311925/2000000000: episode: 8713, duration: 4.351s, episode steps:  33, steps per second:   8, episode reward: 54.400, mean reward:  1.648 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.237845, mean_q: 50.102630, mean_eps: 0.100000\n","     311954/2000000000: episode: 8714, duration: 3.468s, episode steps:  29, steps per second:   8, episode reward: 208.000, mean reward:  7.172 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 89.486026, mean_q: 50.577006, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     311980/2000000000: episode: 8715, duration: 3.229s, episode steps:  26, steps per second:   8, episode reward: 46.400, mean reward:  1.785 [-20.000, 18.000], mean action: 1.115 [0.000, 2.000],  loss: 86.483686, mean_q: 49.642616, mean_eps: 0.100000\n","     312018/2000000000: episode: 8716, duration: 4.546s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 81.171698, mean_q: 49.932833, mean_eps: 0.100000\n","     312047/2000000000: episode: 8717, duration: 3.624s, episode steps:  29, steps per second:   8, episode reward: 128.900, mean reward:  4.445 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 81.223896, mean_q: 50.927118, mean_eps: 0.100000\n","     312072/2000000000: episode: 8718, duration: 3.262s, episode steps:  25, steps per second:   8, episode reward: 108.000, mean reward:  4.320 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 90.811284, mean_q: 50.857345, mean_eps: 0.100000\n","     312104/2000000000: episode: 8719, duration: 4.410s, episode steps:  32, steps per second:   7, episode reward: 174.500, mean reward:  5.453 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.881108, mean_q: 49.273946, mean_eps: 0.100000\n","     312144/2000000000: episode: 8720, duration: 5.590s, episode steps:  40, steps per second:   7, episode reward: 67.200, mean reward:  1.680 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.064585, mean_q: 49.634833, mean_eps: 0.100000\n","     312167/2000000000: episode: 8721, duration: 3.284s, episode steps:  23, steps per second:   7, episode reward: 241.100, mean reward: 10.483 [-20.000, 18.000], mean action: 0.783 [0.000, 2.000],  loss: 88.073523, mean_q: 49.951600, mean_eps: 0.100000\n","     312198/2000000000: episode: 8722, duration: 4.127s, episode steps:  31, steps per second:   8, episode reward: -12.200, mean reward: -0.394 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 83.482295, mean_q: 50.534559, mean_eps: 0.100000\n","     312233/2000000000: episode: 8723, duration: 4.569s, episode steps:  35, steps per second:   8, episode reward: 54.200, mean reward:  1.549 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 93.320852, mean_q: 49.521816, mean_eps: 0.100000\n","     312263/2000000000: episode: 8724, duration: 3.811s, episode steps:  30, steps per second:   8, episode reward: 32.800, mean reward:  1.093 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.119233, mean_q: 49.916936, mean_eps: 0.100000\n","     312300/2000000000: episode: 8725, duration: 4.481s, episode steps:  37, steps per second:   8, episode reward: 29.400, mean reward:  0.795 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 75.071510, mean_q: 50.181933, mean_eps: 0.100000\n","     312329/2000000000: episode: 8726, duration: 3.764s, episode steps:  29, steps per second:   8, episode reward: 163.200, mean reward:  5.628 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 80.267315, mean_q: 49.821706, mean_eps: 0.100000\n","     312354/2000000000: episode: 8727, duration: 3.127s, episode steps:  25, steps per second:   8, episode reward: 169.800, mean reward:  6.792 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 87.468495, mean_q: 50.585358, mean_eps: 0.100000\n","     312379/2000000000: episode: 8728, duration: 3.357s, episode steps:  25, steps per second:   7, episode reward: 228.900, mean reward:  9.156 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 75.933843, mean_q: 50.224949, mean_eps: 0.100000\n","     312410/2000000000: episode: 8729, duration: 4.254s, episode steps:  31, steps per second:   7, episode reward: 195.900, mean reward:  6.319 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 91.298964, mean_q: 49.609752, mean_eps: 0.100000\n","     312448/2000000000: episode: 8730, duration: 5.353s, episode steps:  38, steps per second:   7, episode reward: 227.200, mean reward:  5.979 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 88.015477, mean_q: 48.726310, mean_eps: 0.100000\n","     312477/2000000000: episode: 8731, duration: 3.985s, episode steps:  29, steps per second:   7, episode reward: 246.000, mean reward:  8.483 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.672680, mean_q: 51.071428, mean_eps: 0.100000\n","     312504/2000000000: episode: 8732, duration: 3.799s, episode steps:  27, steps per second:   7, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.342013, mean_q: 50.311638, mean_eps: 0.100000\n","     312543/2000000000: episode: 8733, duration: 5.146s, episode steps:  39, steps per second:   8, episode reward: 48.600, mean reward:  1.246 [-20.000, 18.000], mean action: 1.333 [0.000, 2.000],  loss: 88.557027, mean_q: 50.350037, mean_eps: 0.100000\n","     312567/2000000000: episode: 8734, duration: 3.159s, episode steps:  24, steps per second:   8, episode reward: 58.400, mean reward:  2.433 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 89.603626, mean_q: 50.150687, mean_eps: 0.100000\n","     312600/2000000000: episode: 8735, duration: 4.449s, episode steps:  33, steps per second:   7, episode reward: 220.300, mean reward:  6.676 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 87.181547, mean_q: 49.672446, mean_eps: 0.100000\n","     312636/2000000000: episode: 8736, duration: 4.766s, episode steps:  36, steps per second:   8, episode reward: 168.900, mean reward:  4.692 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 91.463313, mean_q: 50.284175, mean_eps: 0.100000\n","     312664/2000000000: episode: 8737, duration: 3.729s, episode steps:  28, steps per second:   8, episode reward: 149.700, mean reward:  5.346 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 84.099805, mean_q: 48.649332, mean_eps: 0.100000\n","     312691/2000000000: episode: 8738, duration: 3.691s, episode steps:  27, steps per second:   7, episode reward: 140.100, mean reward:  5.189 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 85.549145, mean_q: 49.987318, mean_eps: 0.100000\n","     312731/2000000000: episode: 8739, duration: 5.001s, episode steps:  40, steps per second:   8, episode reward: 25.200, mean reward:  0.630 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 90.959289, mean_q: 50.331491, mean_eps: 0.100000\n","     312764/2000000000: episode: 8740, duration: 3.920s, episode steps:  33, steps per second:   8, episode reward: 209.600, mean reward:  6.352 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 82.680292, mean_q: 50.747857, mean_eps: 0.100000\n","     312796/2000000000: episode: 8741, duration: 3.947s, episode steps:  32, steps per second:   8, episode reward: 84.300, mean reward:  2.634 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 82.748436, mean_q: 50.179013, mean_eps: 0.100000\n","     312824/2000000000: episode: 8742, duration: 3.706s, episode steps:  28, steps per second:   8, episode reward: 153.900, mean reward:  5.496 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.068254, mean_q: 49.986308, mean_eps: 0.100000\n","     312850/2000000000: episode: 8743, duration: 3.422s, episode steps:  26, steps per second:   8, episode reward: 105.600, mean reward:  4.062 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 79.652531, mean_q: 49.427574, mean_eps: 0.100000\n","     312877/2000000000: episode: 8744, duration: 3.413s, episode steps:  27, steps per second:   8, episode reward: 213.400, mean reward:  7.904 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 83.957744, mean_q: 49.489241, mean_eps: 0.100000\n","     312910/2000000000: episode: 8745, duration: 4.099s, episode steps:  33, steps per second:   8, episode reward: 179.000, mean reward:  5.424 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.854309, mean_q: 50.473573, mean_eps: 0.100000\n","     312938/2000000000: episode: 8746, duration: 3.662s, episode steps:  28, steps per second:   8, episode reward: 60.400, mean reward:  2.157 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.447543, mean_q: 49.602737, mean_eps: 0.100000\n","     312978/2000000000: episode: 8747, duration: 5.118s, episode steps:  40, steps per second:   8, episode reward: 152.500, mean reward:  3.813 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 88.803285, mean_q: 50.180081, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     313013/2000000000: episode: 8748, duration: 4.278s, episode steps:  35, steps per second:   8, episode reward: -51.800, mean reward: -1.480 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 88.043473, mean_q: 50.988495, mean_eps: 0.100000\n","     313045/2000000000: episode: 8749, duration: 4.003s, episode steps:  32, steps per second:   8, episode reward: 215.300, mean reward:  6.728 [-6.700, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 88.018524, mean_q: 50.759977, mean_eps: 0.100000\n","     313074/2000000000: episode: 8750, duration: 3.612s, episode steps:  29, steps per second:   8, episode reward: 160.900, mean reward:  5.548 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 86.954249, mean_q: 50.020721, mean_eps: 0.100000\n","     313109/2000000000: episode: 8751, duration: 4.210s, episode steps:  35, steps per second:   8, episode reward: 173.600, mean reward:  4.960 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 88.936580, mean_q: 49.471776, mean_eps: 0.100000\n","     313136/2000000000: episode: 8752, duration: 3.334s, episode steps:  27, steps per second:   8, episode reward: 89.700, mean reward:  3.322 [-20.000, 19.900], mean action: 0.815 [0.000, 2.000],  loss: 83.043519, mean_q: 50.421733, mean_eps: 0.100000\n","     313164/2000000000: episode: 8753, duration: 3.615s, episode steps:  28, steps per second:   8, episode reward: 121.200, mean reward:  4.329 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 70.333185, mean_q: 50.028799, mean_eps: 0.100000\n","     313204/2000000000: episode: 8754, duration: 5.285s, episode steps:  40, steps per second:   8, episode reward: 18.000, mean reward:  0.450 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.495517, mean_q: 50.992532, mean_eps: 0.100000\n","     313230/2000000000: episode: 8755, duration: 3.353s, episode steps:  26, steps per second:   8, episode reward: 150.700, mean reward:  5.796 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 80.467978, mean_q: 50.806733, mean_eps: 0.100000\n","     313266/2000000000: episode: 8756, duration: 4.705s, episode steps:  36, steps per second:   8, episode reward: 164.700, mean reward:  4.575 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 89.918845, mean_q: 50.644562, mean_eps: 0.100000\n","     313289/2000000000: episode: 8757, duration: 3.011s, episode steps:  23, steps per second:   8, episode reward: 115.600, mean reward:  5.026 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 75.594504, mean_q: 50.876480, mean_eps: 0.100000\n","     313327/2000000000: episode: 8758, duration: 4.454s, episode steps:  38, steps per second:   9, episode reward: 124.100, mean reward:  3.266 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 82.543142, mean_q: 49.254052, mean_eps: 0.100000\n","     313367/2000000000: episode: 8759, duration: 5.029s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 97.142150, mean_q: 50.483394, mean_eps: 0.100000\n","     313404/2000000000: episode: 8760, duration: 5.041s, episode steps:  37, steps per second:   7, episode reward: 69.500, mean reward:  1.878 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 87.592639, mean_q: 49.551655, mean_eps: 0.100000\n","     313439/2000000000: episode: 8761, duration: 4.708s, episode steps:  35, steps per second:   7, episode reward: -20.000, mean reward: -0.571 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 81.702502, mean_q: 50.400438, mean_eps: 0.100000\n","     313460/2000000000: episode: 8762, duration: 2.852s, episode steps:  21, steps per second:   7, episode reward: 196.800, mean reward:  9.371 [-20.000, 18.000], mean action: 0.476 [0.000, 2.000],  loss: 88.379596, mean_q: 50.008059, mean_eps: 0.100000\n","     313497/2000000000: episode: 8763, duration: 4.986s, episode steps:  37, steps per second:   7, episode reward: 148.500, mean reward:  4.014 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 82.956517, mean_q: 50.272944, mean_eps: 0.100000\n","     313533/2000000000: episode: 8764, duration: 4.783s, episode steps:  36, steps per second:   8, episode reward: 128.300, mean reward:  3.564 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 83.120868, mean_q: 50.720840, mean_eps: 0.100000\n","     313564/2000000000: episode: 8765, duration: 4.215s, episode steps:  31, steps per second:   7, episode reward:  3.000, mean reward:  0.097 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 96.394488, mean_q: 50.352028, mean_eps: 0.100000\n","     313604/2000000000: episode: 8766, duration: 5.375s, episode steps:  40, steps per second:   7, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 90.937051, mean_q: 49.701993, mean_eps: 0.100000\n","     313633/2000000000: episode: 8767, duration: 4.130s, episode steps:  29, steps per second:   7, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 85.317794, mean_q: 51.385900, mean_eps: 0.100000\n","     313665/2000000000: episode: 8768, duration: 4.334s, episode steps:  32, steps per second:   7, episode reward: -77.300, mean reward: -2.416 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.463060, mean_q: 50.584752, mean_eps: 0.100000\n","     313702/2000000000: episode: 8769, duration: 4.884s, episode steps:  37, steps per second:   8, episode reward: 141.200, mean reward:  3.816 [-20.000, 18.000], mean action: 1.054 [0.000, 2.000],  loss: 83.430220, mean_q: 50.096400, mean_eps: 0.100000\n","     313730/2000000000: episode: 8770, duration: 3.800s, episode steps:  28, steps per second:   7, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 81.511554, mean_q: 49.753726, mean_eps: 0.100000\n","     313757/2000000000: episode: 8771, duration: 3.637s, episode steps:  27, steps per second:   7, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 74.157534, mean_q: 51.876471, mean_eps: 0.100000\n","     313782/2000000000: episode: 8772, duration: 3.301s, episode steps:  25, steps per second:   8, episode reward: 157.400, mean reward:  6.296 [-20.000, 18.000], mean action: 0.960 [0.000, 2.000],  loss: 83.586628, mean_q: 51.657868, mean_eps: 0.100000\n","     313812/2000000000: episode: 8773, duration: 3.885s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 80.750949, mean_q: 50.953999, mean_eps: 0.100000\n","     313848/2000000000: episode: 8774, duration: 4.775s, episode steps:  36, steps per second:   8, episode reward: 18.000, mean reward:  0.500 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 92.467767, mean_q: 50.707674, mean_eps: 0.100000\n","     313874/2000000000: episode: 8775, duration: 3.335s, episode steps:  26, steps per second:   8, episode reward: 103.800, mean reward:  3.992 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 86.017008, mean_q: 51.218349, mean_eps: 0.100000\n","     313914/2000000000: episode: 8776, duration: 4.879s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.092825, mean_q: 50.183899, mean_eps: 0.100000\n","     313945/2000000000: episode: 8777, duration: 3.832s, episode steps:  31, steps per second:   8, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 84.170355, mean_q: 50.239157, mean_eps: 0.100000\n","     313980/2000000000: episode: 8778, duration: 4.345s, episode steps:  35, steps per second:   8, episode reward: 45.200, mean reward:  1.291 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 78.432560, mean_q: 50.211092, mean_eps: 0.100000\n","     314019/2000000000: episode: 8779, duration: 5.094s, episode steps:  39, steps per second:   8, episode reward: 208.000, mean reward:  5.333 [-20.000, 18.000], mean action: 1.256 [0.000, 2.000],  loss: 77.938058, mean_q: 50.493002, mean_eps: 0.100000\n","     314051/2000000000: episode: 8780, duration: 4.124s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 86.919931, mean_q: 49.442305, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     314081/2000000000: episode: 8781, duration: 3.955s, episode steps:  30, steps per second:   8, episode reward: 246.000, mean reward:  8.200 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 83.480982, mean_q: 50.068210, mean_eps: 0.100000\n","     314110/2000000000: episode: 8782, duration: 3.934s, episode steps:  29, steps per second:   7, episode reward: 195.900, mean reward:  6.755 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 89.692695, mean_q: 50.548407, mean_eps: 0.100000\n","     314139/2000000000: episode: 8783, duration: 3.818s, episode steps:  29, steps per second:   8, episode reward: 132.000, mean reward:  4.552 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 83.719464, mean_q: 50.727448, mean_eps: 0.100000\n","     314167/2000000000: episode: 8784, duration: 3.872s, episode steps:  28, steps per second:   7, episode reward: 75.900, mean reward:  2.711 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.222057, mean_q: 49.712303, mean_eps: 0.100000\n","     314191/2000000000: episode: 8785, duration: 3.130s, episode steps:  24, steps per second:   8, episode reward: 170.000, mean reward:  7.083 [-20.000, 18.000], mean action: 0.583 [0.000, 2.000],  loss: 73.964168, mean_q: 51.859582, mean_eps: 0.100000\n","     314213/2000000000: episode: 8786, duration: 3.125s, episode steps:  22, steps per second:   7, episode reward: 28.900, mean reward:  1.314 [-20.000, 18.000], mean action: 0.773 [0.000, 2.000],  loss: 83.292136, mean_q: 50.341952, mean_eps: 0.100000\n","     314247/2000000000: episode: 8787, duration: 4.497s, episode steps:  34, steps per second:   8, episode reward: 18.000, mean reward:  0.529 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.407325, mean_q: 50.353033, mean_eps: 0.100000\n","     314285/2000000000: episode: 8788, duration: 4.814s, episode steps:  38, steps per second:   8, episode reward: 154.900, mean reward:  4.076 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 86.046582, mean_q: 49.960953, mean_eps: 0.100000\n","     314316/2000000000: episode: 8789, duration: 3.991s, episode steps:  31, steps per second:   8, episode reward: 233.800, mean reward:  7.542 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 82.468709, mean_q: 49.640858, mean_eps: 0.100000\n","     314346/2000000000: episode: 8790, duration: 3.681s, episode steps:  30, steps per second:   8, episode reward: 129.900, mean reward:  4.330 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 82.883715, mean_q: 49.443999, mean_eps: 0.100000\n","     314379/2000000000: episode: 8791, duration: 4.223s, episode steps:  33, steps per second:   8, episode reward: 63.100, mean reward:  1.912 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 80.354526, mean_q: 51.009819, mean_eps: 0.100000\n","     314415/2000000000: episode: 8792, duration: 4.532s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 83.499842, mean_q: 50.179755, mean_eps: 0.100000\n","     314450/2000000000: episode: 8793, duration: 4.429s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 82.062187, mean_q: 50.684210, mean_eps: 0.100000\n","     314482/2000000000: episode: 8794, duration: 4.085s, episode steps:  32, steps per second:   8, episode reward: 143.800, mean reward:  4.494 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 86.325182, mean_q: 50.123307, mean_eps: 0.100000\n","     314518/2000000000: episode: 8795, duration: 4.542s, episode steps:  36, steps per second:   8, episode reward: 107.100, mean reward:  2.975 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 87.892216, mean_q: 50.045087, mean_eps: 0.100000\n","     314558/2000000000: episode: 8796, duration: 5.337s, episode steps:  40, steps per second:   7, episode reward: 144.200, mean reward:  3.605 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 83.791538, mean_q: 50.685523, mean_eps: 0.100000\n","     314590/2000000000: episode: 8797, duration: 4.433s, episode steps:  32, steps per second:   7, episode reward:  1.900, mean reward:  0.059 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 87.076276, mean_q: 51.056145, mean_eps: 0.100000\n","     314616/2000000000: episode: 8798, duration: 3.588s, episode steps:  26, steps per second:   7, episode reward: 253.900, mean reward:  9.765 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.212800, mean_q: 50.955620, mean_eps: 0.100000\n","     314644/2000000000: episode: 8799, duration: 3.334s, episode steps:  28, steps per second:   8, episode reward: 220.000, mean reward:  7.857 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 83.277617, mean_q: 50.631874, mean_eps: 0.100000\n","     314671/2000000000: episode: 8800, duration: 3.368s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 83.664719, mean_q: 51.605244, mean_eps: 0.100000\n","     314698/2000000000: episode: 8801, duration: 3.407s, episode steps:  27, steps per second:   8, episode reward: 129.400, mean reward:  4.793 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 90.760923, mean_q: 50.306838, mean_eps: 0.100000\n","     314724/2000000000: episode: 8802, duration: 3.345s, episode steps:  26, steps per second:   8, episode reward: 208.000, mean reward:  8.000 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 86.431476, mean_q: 50.516991, mean_eps: 0.100000\n","     314762/2000000000: episode: 8803, duration: 4.619s, episode steps:  38, steps per second:   8, episode reward:  1.200, mean reward:  0.032 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 79.866517, mean_q: 50.244123, mean_eps: 0.100000\n","     314802/2000000000: episode: 8804, duration: 4.999s, episode steps:  40, steps per second:   8, episode reward: 106.500, mean reward:  2.663 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 80.750007, mean_q: 50.165179, mean_eps: 0.100000\n","     314832/2000000000: episode: 8805, duration: 3.780s, episode steps:  30, steps per second:   8, episode reward: 251.000, mean reward:  8.367 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 82.877277, mean_q: 50.691708, mean_eps: 0.100000\n","     314864/2000000000: episode: 8806, duration: 4.150s, episode steps:  32, steps per second:   8, episode reward: 208.000, mean reward:  6.500 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 82.337482, mean_q: 49.521365, mean_eps: 0.100000\n","     314892/2000000000: episode: 8807, duration: 3.619s, episode steps:  28, steps per second:   8, episode reward: -54.300, mean reward: -1.939 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 76.898887, mean_q: 49.140266, mean_eps: 0.100000\n","     314932/2000000000: episode: 8808, duration: 5.248s, episode steps:  40, steps per second:   8, episode reward: 183.200, mean reward:  4.580 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 96.176909, mean_q: 49.769861, mean_eps: 0.100000\n","     314965/2000000000: episode: 8809, duration: 4.177s, episode steps:  33, steps per second:   8, episode reward:  3.300, mean reward:  0.100 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 87.599395, mean_q: 49.973372, mean_eps: 0.100000\n","     315005/2000000000: episode: 8810, duration: 4.957s, episode steps:  40, steps per second:   8, episode reward: 170.000, mean reward:  4.250 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 84.898539, mean_q: 49.078544, mean_eps: 0.100000\n","     315045/2000000000: episode: 8811, duration: 5.078s, episode steps:  40, steps per second:   8, episode reward: 174.200, mean reward:  4.355 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.935292, mean_q: 50.367156, mean_eps: 0.100000\n","     315077/2000000000: episode: 8812, duration: 4.151s, episode steps:  32, steps per second:   8, episode reward: 16.200, mean reward:  0.506 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 76.294501, mean_q: 49.828639, mean_eps: 0.100000\n","     315109/2000000000: episode: 8813, duration: 4.099s, episode steps:  32, steps per second:   8, episode reward: 86.800, mean reward:  2.712 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 90.493289, mean_q: 49.560815, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     315139/2000000000: episode: 8814, duration: 3.747s, episode steps:  30, steps per second:   8, episode reward: 59.500, mean reward:  1.983 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.368845, mean_q: 50.226300, mean_eps: 0.100000\n","     315162/2000000000: episode: 8815, duration: 3.055s, episode steps:  23, steps per second:   8, episode reward: 109.000, mean reward:  4.739 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 74.085911, mean_q: 49.806102, mean_eps: 0.100000\n","     315201/2000000000: episode: 8816, duration: 4.994s, episode steps:  39, steps per second:   8, episode reward: 178.900, mean reward:  4.587 [-20.000, 18.000], mean action: 1.205 [0.000, 2.000],  loss: 82.912565, mean_q: 50.241997, mean_eps: 0.100000\n","     315230/2000000000: episode: 8817, duration: 3.735s, episode steps:  29, steps per second:   8, episode reward: 85.800, mean reward:  2.959 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 82.302439, mean_q: 50.192422, mean_eps: 0.100000\n","     315270/2000000000: episode: 8818, duration: 5.571s, episode steps:  40, steps per second:   7, episode reward: 33.600, mean reward:  0.840 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 80.522641, mean_q: 49.920846, mean_eps: 0.100000\n","     315296/2000000000: episode: 8819, duration: 3.590s, episode steps:  26, steps per second:   7, episode reward: 18.000, mean reward:  0.692 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 87.776013, mean_q: 49.751919, mean_eps: 0.100000\n","     315331/2000000000: episode: 8820, duration: 4.715s, episode steps:  35, steps per second:   7, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 93.146170, mean_q: 50.239886, mean_eps: 0.100000\n","     315371/2000000000: episode: 8821, duration: 5.064s, episode steps:  40, steps per second:   8, episode reward: 46.300, mean reward:  1.157 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 83.692031, mean_q: 50.156718, mean_eps: 0.100000\n","     315401/2000000000: episode: 8822, duration: 3.697s, episode steps:  30, steps per second:   8, episode reward: 64.800, mean reward:  2.160 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 83.922279, mean_q: 50.370953, mean_eps: 0.100000\n","     315429/2000000000: episode: 8823, duration: 3.464s, episode steps:  28, steps per second:   8, episode reward: 28.800, mean reward:  1.029 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 87.768051, mean_q: 50.617559, mean_eps: 0.100000\n","     315464/2000000000: episode: 8824, duration: 4.409s, episode steps:  35, steps per second:   8, episode reward: 28.200, mean reward:  0.806 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.889186, mean_q: 50.636163, mean_eps: 0.100000\n","     315490/2000000000: episode: 8825, duration: 3.370s, episode steps:  26, steps per second:   8, episode reward: 141.100, mean reward:  5.427 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 92.455883, mean_q: 50.370855, mean_eps: 0.100000\n","     315522/2000000000: episode: 8826, duration: 4.150s, episode steps:  32, steps per second:   8, episode reward: 17.300, mean reward:  0.541 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 79.613078, mean_q: 50.175575, mean_eps: 0.100000\n","     315548/2000000000: episode: 8827, duration: 3.568s, episode steps:  26, steps per second:   7, episode reward: 111.600, mean reward:  4.292 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 83.863944, mean_q: 49.899644, mean_eps: 0.100000\n","     315579/2000000000: episode: 8828, duration: 4.314s, episode steps:  31, steps per second:   7, episode reward: -83.200, mean reward: -2.684 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 89.018701, mean_q: 49.979875, mean_eps: 0.100000\n","     315608/2000000000: episode: 8829, duration: 4.034s, episode steps:  29, steps per second:   7, episode reward: 143.600, mean reward:  4.952 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 83.543633, mean_q: 49.864465, mean_eps: 0.100000\n","     315640/2000000000: episode: 8830, duration: 4.627s, episode steps:  32, steps per second:   7, episode reward: 190.100, mean reward:  5.941 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 78.116860, mean_q: 49.936177, mean_eps: 0.100000\n","     315663/2000000000: episode: 8831, duration: 3.141s, episode steps:  23, steps per second:   7, episode reward: -6.100, mean reward: -0.265 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 85.787516, mean_q: 50.041512, mean_eps: 0.100000\n","     315700/2000000000: episode: 8832, duration: 4.792s, episode steps:  37, steps per second:   8, episode reward: 39.400, mean reward:  1.065 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 85.803739, mean_q: 49.808982, mean_eps: 0.100000\n","     315733/2000000000: episode: 8833, duration: 4.302s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 81.482492, mean_q: 49.941280, mean_eps: 0.100000\n","     315764/2000000000: episode: 8834, duration: 4.044s, episode steps:  31, steps per second:   8, episode reward: 143.100, mean reward:  4.616 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 91.391529, mean_q: 49.558911, mean_eps: 0.100000\n","     315798/2000000000: episode: 8835, duration: 4.518s, episode steps:  34, steps per second:   8, episode reward: 64.000, mean reward:  1.882 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 80.545008, mean_q: 49.895642, mean_eps: 0.100000\n","     315832/2000000000: episode: 8836, duration: 4.647s, episode steps:  34, steps per second:   7, episode reward: -43.900, mean reward: -1.291 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.261892, mean_q: 50.632851, mean_eps: 0.100000\n","     315858/2000000000: episode: 8837, duration: 3.412s, episode steps:  26, steps per second:   8, episode reward: 249.600, mean reward:  9.600 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 79.946324, mean_q: 51.178877, mean_eps: 0.100000\n","     315891/2000000000: episode: 8838, duration: 4.021s, episode steps:  33, steps per second:   8, episode reward: 207.600, mean reward:  6.291 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 80.568387, mean_q: 50.022045, mean_eps: 0.100000\n","     315915/2000000000: episode: 8839, duration: 2.967s, episode steps:  24, steps per second:   8, episode reward: 278.600, mean reward: 11.608 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 77.200229, mean_q: 49.413757, mean_eps: 0.100000\n","     315947/2000000000: episode: 8840, duration: 4.027s, episode steps:  32, steps per second:   8, episode reward: 78.900, mean reward:  2.466 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 87.675890, mean_q: 50.308563, mean_eps: 0.100000\n","     315987/2000000000: episode: 8841, duration: 4.956s, episode steps:  40, steps per second:   8, episode reward: 84.800, mean reward:  2.120 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 85.144166, mean_q: 49.295975, mean_eps: 0.100000\n","     316016/2000000000: episode: 8842, duration: 3.825s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 80.401297, mean_q: 49.722918, mean_eps: 0.100000\n","     316036/2000000000: episode: 8843, duration: 2.556s, episode steps:  20, steps per second:   8, episode reward: 89.400, mean reward:  4.470 [-20.000, 18.000], mean action: 0.550 [0.000, 1.000],  loss: 90.288440, mean_q: 48.740173, mean_eps: 0.100000\n","     316061/2000000000: episode: 8844, duration: 3.169s, episode steps:  25, steps per second:   8, episode reward: 145.700, mean reward:  5.828 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 80.984897, mean_q: 49.622032, mean_eps: 0.100000\n","     316088/2000000000: episode: 8845, duration: 3.452s, episode steps:  27, steps per second:   8, episode reward: -6.600, mean reward: -0.244 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 85.005837, mean_q: 49.529654, mean_eps: 0.100000\n","     316116/2000000000: episode: 8846, duration: 3.520s, episode steps:  28, steps per second:   8, episode reward: -37.300, mean reward: -1.332 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 81.211526, mean_q: 49.573586, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     316137/2000000000: episode: 8847, duration: 2.668s, episode steps:  21, steps per second:   8, episode reward: 110.600, mean reward:  5.267 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 82.193796, mean_q: 50.444310, mean_eps: 0.100000\n","     316164/2000000000: episode: 8848, duration: 3.489s, episode steps:  27, steps per second:   8, episode reward: 124.900, mean reward:  4.626 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.407997, mean_q: 50.371996, mean_eps: 0.100000\n","     316204/2000000000: episode: 8849, duration: 4.952s, episode steps:  40, steps per second:   8, episode reward: 146.100, mean reward:  3.653 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 85.706352, mean_q: 50.814903, mean_eps: 0.100000\n","     316228/2000000000: episode: 8850, duration: 3.034s, episode steps:  24, steps per second:   8, episode reward: 116.800, mean reward:  4.867 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 77.399020, mean_q: 50.167405, mean_eps: 0.100000\n","     316268/2000000000: episode: 8851, duration: 4.977s, episode steps:  40, steps per second:   8, episode reward: -23.600, mean reward: -0.590 [-20.000, 19.200], mean action: 1.350 [0.000, 2.000],  loss: 82.925452, mean_q: 50.803965, mean_eps: 0.100000\n","     316295/2000000000: episode: 8852, duration: 3.392s, episode steps:  27, steps per second:   8, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 75.142746, mean_q: 51.020742, mean_eps: 0.100000\n","     316325/2000000000: episode: 8853, duration: 3.884s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 82.373706, mean_q: 50.606133, mean_eps: 0.100000\n","     316357/2000000000: episode: 8854, duration: 4.018s, episode steps:  32, steps per second:   8, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 81.719155, mean_q: 49.599128, mean_eps: 0.100000\n","     316381/2000000000: episode: 8855, duration: 3.178s, episode steps:  24, steps per second:   8, episode reward: 105.900, mean reward:  4.413 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 89.302140, mean_q: 49.892389, mean_eps: 0.100000\n","     316412/2000000000: episode: 8856, duration: 4.155s, episode steps:  31, steps per second:   7, episode reward: -20.600, mean reward: -0.665 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 87.598567, mean_q: 50.653149, mean_eps: 0.100000\n","     316441/2000000000: episode: 8857, duration: 3.676s, episode steps:  29, steps per second:   8, episode reward: 142.800, mean reward:  4.924 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.589455, mean_q: 50.806944, mean_eps: 0.100000\n","     316468/2000000000: episode: 8858, duration: 3.224s, episode steps:  27, steps per second:   8, episode reward: 172.700, mean reward:  6.396 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 88.444286, mean_q: 48.944070, mean_eps: 0.100000\n","     316502/2000000000: episode: 8859, duration: 4.182s, episode steps:  34, steps per second:   8, episode reward: 55.200, mean reward:  1.624 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 91.109670, mean_q: 51.372065, mean_eps: 0.100000\n","     316531/2000000000: episode: 8860, duration: 3.718s, episode steps:  29, steps per second:   8, episode reward: 148.000, mean reward:  5.103 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 73.683490, mean_q: 50.377856, mean_eps: 0.100000\n","     316556/2000000000: episode: 8861, duration: 3.167s, episode steps:  25, steps per second:   8, episode reward: 21.900, mean reward:  0.876 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 76.807125, mean_q: 49.117596, mean_eps: 0.100000\n","     316596/2000000000: episode: 8862, duration: 5.185s, episode steps:  40, steps per second:   8, episode reward: 33.200, mean reward:  0.830 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 86.365627, mean_q: 50.714686, mean_eps: 0.100000\n","     316636/2000000000: episode: 8863, duration: 5.447s, episode steps:  40, steps per second:   7, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 86.027553, mean_q: 51.421111, mean_eps: 0.100000\n","     316676/2000000000: episode: 8864, duration: 5.271s, episode steps:  40, steps per second:   8, episode reward: 88.100, mean reward:  2.202 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.210453, mean_q: 49.873712, mean_eps: 0.100000\n","     316710/2000000000: episode: 8865, duration: 4.681s, episode steps:  34, steps per second:   7, episode reward: 40.500, mean reward:  1.191 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 87.431856, mean_q: 50.476365, mean_eps: 0.100000\n","     316742/2000000000: episode: 8866, duration: 4.186s, episode steps:  32, steps per second:   8, episode reward: 208.800, mean reward:  6.525 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 89.193781, mean_q: 50.167920, mean_eps: 0.100000\n","     316770/2000000000: episode: 8867, duration: 3.772s, episode steps:  28, steps per second:   7, episode reward: 131.000, mean reward:  4.679 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 79.585216, mean_q: 50.147837, mean_eps: 0.100000\n","     316800/2000000000: episode: 8868, duration: 3.712s, episode steps:  30, steps per second:   8, episode reward: 13.200, mean reward:  0.440 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 82.993323, mean_q: 49.223274, mean_eps: 0.100000\n","     316834/2000000000: episode: 8869, duration: 4.280s, episode steps:  34, steps per second:   8, episode reward: 128.200, mean reward:  3.771 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 83.016209, mean_q: 50.163769, mean_eps: 0.100000\n","     316856/2000000000: episode: 8870, duration: 2.797s, episode steps:  22, steps per second:   8, episode reward: 85.800, mean reward:  3.900 [-20.000, 18.000], mean action: 0.682 [0.000, 2.000],  loss: 88.682854, mean_q: 50.604124, mean_eps: 0.100000\n","     316886/2000000000: episode: 8871, duration: 3.677s, episode steps:  30, steps per second:   8, episode reward: 80.500, mean reward:  2.683 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.265724, mean_q: 49.480578, mean_eps: 0.100000\n","     316923/2000000000: episode: 8872, duration: 4.584s, episode steps:  37, steps per second:   8, episode reward: 197.900, mean reward:  5.349 [-20.000, 18.000], mean action: 1.162 [0.000, 2.000],  loss: 90.264541, mean_q: 50.163030, mean_eps: 0.100000\n","     316963/2000000000: episode: 8873, duration: 4.943s, episode steps:  40, steps per second:   8, episode reward: 125.300, mean reward:  3.132 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 79.586974, mean_q: 50.034630, mean_eps: 0.100000\n","     316990/2000000000: episode: 8874, duration: 3.266s, episode steps:  27, steps per second:   8, episode reward: 57.700, mean reward:  2.137 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 82.008862, mean_q: 50.326953, mean_eps: 0.100000\n","     317023/2000000000: episode: 8875, duration: 4.089s, episode steps:  33, steps per second:   8, episode reward: 92.300, mean reward:  2.797 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 86.623906, mean_q: 50.549582, mean_eps: 0.100000\n","     317051/2000000000: episode: 8876, duration: 3.472s, episode steps:  28, steps per second:   8, episode reward: 148.000, mean reward:  5.286 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 81.462832, mean_q: 50.533971, mean_eps: 0.100000\n","     317081/2000000000: episode: 8877, duration: 3.960s, episode steps:  30, steps per second:   8, episode reward: 28.500, mean reward:  0.950 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 88.731220, mean_q: 50.554649, mean_eps: 0.100000\n","     317116/2000000000: episode: 8878, duration: 4.548s, episode steps:  35, steps per second:   8, episode reward: 112.800, mean reward:  3.223 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 80.163452, mean_q: 49.514927, mean_eps: 0.100000\n","     317142/2000000000: episode: 8879, duration: 3.341s, episode steps:  26, steps per second:   8, episode reward: 102.800, mean reward:  3.954 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.284459, mean_q: 50.747459, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     317173/2000000000: episode: 8880, duration: 4.071s, episode steps:  31, steps per second:   8, episode reward: 48.000, mean reward:  1.548 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.854440, mean_q: 49.589370, mean_eps: 0.100000\n","     317199/2000000000: episode: 8881, duration: 3.220s, episode steps:  26, steps per second:   8, episode reward: -35.900, mean reward: -1.381 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 82.496872, mean_q: 50.879549, mean_eps: 0.100000\n","     317226/2000000000: episode: 8882, duration: 3.623s, episode steps:  27, steps per second:   7, episode reward: 91.300, mean reward:  3.381 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.988344, mean_q: 49.897682, mean_eps: 0.100000\n","     317252/2000000000: episode: 8883, duration: 3.334s, episode steps:  26, steps per second:   8, episode reward: 37.000, mean reward:  1.423 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 92.082595, mean_q: 50.712668, mean_eps: 0.100000\n","     317290/2000000000: episode: 8884, duration: 4.731s, episode steps:  38, steps per second:   8, episode reward: 229.000, mean reward:  6.026 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 89.513727, mean_q: 49.740207, mean_eps: 0.100000\n","     317316/2000000000: episode: 8885, duration: 3.448s, episode steps:  26, steps per second:   8, episode reward: 115.100, mean reward:  4.427 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 81.245803, mean_q: 50.094693, mean_eps: 0.100000\n","     317349/2000000000: episode: 8886, duration: 4.063s, episode steps:  33, steps per second:   8, episode reward: 118.600, mean reward:  3.594 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 87.358285, mean_q: 50.373975, mean_eps: 0.100000\n","     317383/2000000000: episode: 8887, duration: 4.075s, episode steps:  34, steps per second:   8, episode reward: 120.200, mean reward:  3.535 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.178434, mean_q: 50.680597, mean_eps: 0.100000\n","     317409/2000000000: episode: 8888, duration: 3.180s, episode steps:  26, steps per second:   8, episode reward: 205.300, mean reward:  7.896 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 76.037723, mean_q: 50.223936, mean_eps: 0.100000\n","     317440/2000000000: episode: 8889, duration: 3.661s, episode steps:  31, steps per second:   8, episode reward: 77.800, mean reward:  2.510 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 91.199677, mean_q: 50.348180, mean_eps: 0.100000\n","     317474/2000000000: episode: 8890, duration: 4.137s, episode steps:  34, steps per second:   8, episode reward: 257.900, mean reward:  7.585 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 88.746216, mean_q: 50.523623, mean_eps: 0.100000\n","     317497/2000000000: episode: 8891, duration: 3.012s, episode steps:  23, steps per second:   8, episode reward: 106.300, mean reward:  4.622 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 90.814534, mean_q: 49.370810, mean_eps: 0.100000\n","     317527/2000000000: episode: 8892, duration: 3.664s, episode steps:  30, steps per second:   8, episode reward: 132.700, mean reward:  4.423 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 79.851713, mean_q: 49.731302, mean_eps: 0.100000\n","     317562/2000000000: episode: 8893, duration: 4.157s, episode steps:  35, steps per second:   8, episode reward: 295.500, mean reward:  8.443 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 85.412086, mean_q: 50.446722, mean_eps: 0.100000\n","     317588/2000000000: episode: 8894, duration: 3.213s, episode steps:  26, steps per second:   8, episode reward: 285.900, mean reward: 10.996 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 79.231920, mean_q: 50.068118, mean_eps: 0.100000\n","     317615/2000000000: episode: 8895, duration: 3.417s, episode steps:  27, steps per second:   8, episode reward: 150.000, mean reward:  5.556 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 81.147121, mean_q: 50.709843, mean_eps: 0.100000\n","     317655/2000000000: episode: 8896, duration: 4.890s, episode steps:  40, steps per second:   8, episode reward: 43.800, mean reward:  1.095 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.594278, mean_q: 50.612630, mean_eps: 0.100000\n","     317684/2000000000: episode: 8897, duration: 3.612s, episode steps:  29, steps per second:   8, episode reward: 126.500, mean reward:  4.362 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 86.592890, mean_q: 50.403180, mean_eps: 0.100000\n","     317710/2000000000: episode: 8898, duration: 3.235s, episode steps:  26, steps per second:   8, episode reward: 111.200, mean reward:  4.277 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 77.423900, mean_q: 50.298208, mean_eps: 0.100000\n","     317740/2000000000: episode: 8899, duration: 3.717s, episode steps:  30, steps per second:   8, episode reward: 193.200, mean reward:  6.440 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 80.340672, mean_q: 49.905394, mean_eps: 0.100000\n","     317764/2000000000: episode: 8900, duration: 2.907s, episode steps:  24, steps per second:   8, episode reward: 255.100, mean reward: 10.629 [-15.200, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 84.164978, mean_q: 50.189216, mean_eps: 0.100000\n","     317791/2000000000: episode: 8901, duration: 3.250s, episode steps:  27, steps per second:   8, episode reward: 80.900, mean reward:  2.996 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 86.131726, mean_q: 50.919308, mean_eps: 0.100000\n","     317826/2000000000: episode: 8902, duration: 4.260s, episode steps:  35, steps per second:   8, episode reward: 192.600, mean reward:  5.503 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 86.339102, mean_q: 50.753358, mean_eps: 0.100000\n","     317856/2000000000: episode: 8903, duration: 4.115s, episode steps:  30, steps per second:   7, episode reward: 27.400, mean reward:  0.913 [-20.000, 18.000], mean action: 0.767 [0.000, 2.000],  loss: 89.959919, mean_q: 50.013991, mean_eps: 0.100000\n","     317882/2000000000: episode: 8904, duration: 3.429s, episode steps:  26, steps per second:   8, episode reward: 148.800, mean reward:  5.723 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 85.368627, mean_q: 49.958198, mean_eps: 0.100000\n","     317918/2000000000: episode: 8905, duration: 4.624s, episode steps:  36, steps per second:   8, episode reward: -37.300, mean reward: -1.036 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.981636, mean_q: 50.123378, mean_eps: 0.100000\n","     317951/2000000000: episode: 8906, duration: 4.167s, episode steps:  33, steps per second:   8, episode reward: 288.200, mean reward:  8.733 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.178630, mean_q: 49.960522, mean_eps: 0.100000\n","     317980/2000000000: episode: 8907, duration: 3.691s, episode steps:  29, steps per second:   8, episode reward: 303.400, mean reward: 10.462 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.644735, mean_q: 51.012951, mean_eps: 0.100000\n","     318006/2000000000: episode: 8908, duration: 3.432s, episode steps:  26, steps per second:   8, episode reward: 171.000, mean reward:  6.577 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 81.285747, mean_q: 50.038537, mean_eps: 0.100000\n","     318042/2000000000: episode: 8909, duration: 4.768s, episode steps:  36, steps per second:   8, episode reward: -20.000, mean reward: -0.556 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.974534, mean_q: 49.657349, mean_eps: 0.100000\n","     318070/2000000000: episode: 8910, duration: 3.817s, episode steps:  28, steps per second:   7, episode reward: 69.000, mean reward:  2.464 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 91.171523, mean_q: 50.198233, mean_eps: 0.100000\n","     318097/2000000000: episode: 8911, duration: 3.574s, episode steps:  27, steps per second:   8, episode reward: 30.300, mean reward:  1.122 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 77.575369, mean_q: 50.604709, mean_eps: 0.100000\n","     318137/2000000000: episode: 8912, duration: 5.147s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 77.357125, mean_q: 50.578427, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     318175/2000000000: episode: 8913, duration: 5.004s, episode steps:  38, steps per second:   8, episode reward: 26.900, mean reward:  0.708 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 90.626133, mean_q: 49.698817, mean_eps: 0.100000\n","     318206/2000000000: episode: 8914, duration: 4.247s, episode steps:  31, steps per second:   7, episode reward: -75.900, mean reward: -2.448 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 89.296457, mean_q: 50.676776, mean_eps: 0.100000\n","     318240/2000000000: episode: 8915, duration: 4.347s, episode steps:  34, steps per second:   8, episode reward: 22.500, mean reward:  0.662 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 80.918299, mean_q: 50.659358, mean_eps: 0.100000\n","     318280/2000000000: episode: 8916, duration: 5.047s, episode steps:  40, steps per second:   8, episode reward: 192.000, mean reward:  4.800 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.670576, mean_q: 50.086602, mean_eps: 0.100000\n","     318315/2000000000: episode: 8917, duration: 4.555s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 82.879492, mean_q: 51.055054, mean_eps: 0.100000\n","     318350/2000000000: episode: 8918, duration: 4.519s, episode steps:  35, steps per second:   8, episode reward: 76.100, mean reward:  2.174 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 82.002126, mean_q: 50.705138, mean_eps: 0.100000\n","     318381/2000000000: episode: 8919, duration: 4.120s, episode steps:  31, steps per second:   8, episode reward: 145.400, mean reward:  4.690 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.272153, mean_q: 50.091440, mean_eps: 0.100000\n","     318421/2000000000: episode: 8920, duration: 5.272s, episode steps:  40, steps per second:   8, episode reward: -76.000, mean reward: -1.900 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 92.964901, mean_q: 49.677547, mean_eps: 0.100000\n","     318450/2000000000: episode: 8921, duration: 4.151s, episode steps:  29, steps per second:   7, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 83.156612, mean_q: 50.491980, mean_eps: 0.100000\n","     318478/2000000000: episode: 8922, duration: 3.708s, episode steps:  28, steps per second:   8, episode reward: 246.000, mean reward:  8.786 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 76.567663, mean_q: 50.391473, mean_eps: 0.100000\n","     318507/2000000000: episode: 8923, duration: 3.601s, episode steps:  29, steps per second:   8, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 95.406423, mean_q: 50.954310, mean_eps: 0.100000\n","     318547/2000000000: episode: 8924, duration: 4.969s, episode steps:  40, steps per second:   8, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 89.814967, mean_q: 50.403316, mean_eps: 0.100000\n","     318586/2000000000: episode: 8925, duration: 4.999s, episode steps:  39, steps per second:   8, episode reward: -35.800, mean reward: -0.918 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 87.493999, mean_q: 49.577409, mean_eps: 0.100000\n","     318615/2000000000: episode: 8926, duration: 3.653s, episode steps:  29, steps per second:   8, episode reward: -20.000, mean reward: -0.690 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 85.864205, mean_q: 50.212296, mean_eps: 0.100000\n","     318645/2000000000: episode: 8927, duration: 3.989s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 88.225652, mean_q: 50.654298, mean_eps: 0.100000\n","     318671/2000000000: episode: 8928, duration: 3.521s, episode steps:  26, steps per second:   7, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 83.861036, mean_q: 50.273848, mean_eps: 0.100000\n","     318706/2000000000: episode: 8929, duration: 4.522s, episode steps:  35, steps per second:   8, episode reward: 116.400, mean reward:  3.326 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 77.812735, mean_q: 51.399992, mean_eps: 0.100000\n","     318733/2000000000: episode: 8930, duration: 3.545s, episode steps:  27, steps per second:   8, episode reward: 18.000, mean reward:  0.667 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 88.053831, mean_q: 49.129241, mean_eps: 0.100000\n","     318763/2000000000: episode: 8931, duration: 4.035s, episode steps:  30, steps per second:   7, episode reward: 284.000, mean reward:  9.467 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.888330, mean_q: 50.060836, mean_eps: 0.100000\n","     318794/2000000000: episode: 8932, duration: 3.946s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 92.847026, mean_q: 50.533750, mean_eps: 0.100000\n","     318825/2000000000: episode: 8933, duration: 4.047s, episode steps:  31, steps per second:   8, episode reward: 180.200, mean reward:  5.813 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 80.689683, mean_q: 49.902980, mean_eps: 0.100000\n","     318849/2000000000: episode: 8934, duration: 3.078s, episode steps:  24, steps per second:   8, episode reward: 63.000, mean reward:  2.625 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 88.847159, mean_q: 49.545980, mean_eps: 0.100000\n","     318876/2000000000: episode: 8935, duration: 3.621s, episode steps:  27, steps per second:   7, episode reward: 66.400, mean reward:  2.459 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 77.094611, mean_q: 50.105810, mean_eps: 0.100000\n","     318901/2000000000: episode: 8936, duration: 3.174s, episode steps:  25, steps per second:   8, episode reward: 132.000, mean reward:  5.280 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 78.246763, mean_q: 51.437611, mean_eps: 0.100000\n","     318924/2000000000: episode: 8937, duration: 3.124s, episode steps:  23, steps per second:   7, episode reward: 94.000, mean reward:  4.087 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 95.507308, mean_q: 50.492295, mean_eps: 0.100000\n","     318962/2000000000: episode: 8938, duration: 4.783s, episode steps:  38, steps per second:   8, episode reward: 112.400, mean reward:  2.958 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 76.332097, mean_q: 50.116859, mean_eps: 0.100000\n","     318991/2000000000: episode: 8939, duration: 3.862s, episode steps:  29, steps per second:   8, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 88.265220, mean_q: 50.511154, mean_eps: 0.100000\n","     319017/2000000000: episode: 8940, duration: 3.418s, episode steps:  26, steps per second:   8, episode reward: 148.400, mean reward:  5.708 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 96.978200, mean_q: 49.655577, mean_eps: 0.100000\n","     319051/2000000000: episode: 8941, duration: 4.621s, episode steps:  34, steps per second:   7, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 81.474427, mean_q: 49.999412, mean_eps: 0.100000\n","     319088/2000000000: episode: 8942, duration: 4.874s, episode steps:  37, steps per second:   8, episode reward: 268.400, mean reward:  7.254 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 79.481259, mean_q: 51.092432, mean_eps: 0.100000\n","     319121/2000000000: episode: 8943, duration: 4.404s, episode steps:  33, steps per second:   7, episode reward: -56.200, mean reward: -1.703 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 87.189201, mean_q: 49.749555, mean_eps: 0.100000\n","     319152/2000000000: episode: 8944, duration: 4.146s, episode steps:  31, steps per second:   7, episode reward: -40.600, mean reward: -1.310 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 83.041695, mean_q: 51.291860, mean_eps: 0.100000\n","     319190/2000000000: episode: 8945, duration: 5.058s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.105 [0.000, 2.000],  loss: 87.441916, mean_q: 51.522847, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     319222/2000000000: episode: 8946, duration: 4.350s, episode steps:  32, steps per second:   7, episode reward: 70.200, mean reward:  2.194 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 87.440962, mean_q: 50.178078, mean_eps: 0.100000\n","     319248/2000000000: episode: 8947, duration: 3.332s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.699550, mean_q: 50.342466, mean_eps: 0.100000\n","     319288/2000000000: episode: 8948, duration: 5.281s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 82.263272, mean_q: 50.161687, mean_eps: 0.100000\n","     319328/2000000000: episode: 8949, duration: 5.062s, episode steps:  40, steps per second:   8, episode reward:  9.200, mean reward:  0.230 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 83.273452, mean_q: 49.901431, mean_eps: 0.100000\n","     319354/2000000000: episode: 8950, duration: 3.262s, episode steps:  26, steps per second:   8, episode reward: -1.000, mean reward: -0.038 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 78.748003, mean_q: 50.854447, mean_eps: 0.100000\n","     319383/2000000000: episode: 8951, duration: 3.590s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 78.419900, mean_q: 49.912618, mean_eps: 0.100000\n","     319423/2000000000: episode: 8952, duration: 5.144s, episode steps:  40, steps per second:   8, episode reward: 106.100, mean reward:  2.652 [-20.000, 19.100], mean action: 1.300 [0.000, 2.000],  loss: 87.578142, mean_q: 50.961943, mean_eps: 0.100000\n","     319455/2000000000: episode: 8953, duration: 4.167s, episode steps:  32, steps per second:   8, episode reward: 107.000, mean reward:  3.344 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 82.316784, mean_q: 50.581990, mean_eps: 0.100000\n","     319484/2000000000: episode: 8954, duration: 3.735s, episode steps:  29, steps per second:   8, episode reward: 246.000, mean reward:  8.483 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 92.953907, mean_q: 49.645723, mean_eps: 0.100000\n","     319515/2000000000: episode: 8955, duration: 4.120s, episode steps:  31, steps per second:   8, episode reward: 43.000, mean reward:  1.387 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 80.790247, mean_q: 50.003690, mean_eps: 0.100000\n","     319550/2000000000: episode: 8956, duration: 4.584s, episode steps:  35, steps per second:   8, episode reward: 169.200, mean reward:  4.834 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 89.798180, mean_q: 49.719722, mean_eps: 0.100000\n","     319584/2000000000: episode: 8957, duration: 4.554s, episode steps:  34, steps per second:   7, episode reward: 244.700, mean reward:  7.197 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 85.738225, mean_q: 51.121443, mean_eps: 0.100000\n","     319609/2000000000: episode: 8958, duration: 3.353s, episode steps:  25, steps per second:   7, episode reward: 256.000, mean reward: 10.240 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 94.982972, mean_q: 50.824094, mean_eps: 0.100000\n","     319635/2000000000: episode: 8959, duration: 3.463s, episode steps:  26, steps per second:   8, episode reward: 278.500, mean reward: 10.712 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 79.157793, mean_q: 51.375043, mean_eps: 0.100000\n","     319668/2000000000: episode: 8960, duration: 4.436s, episode steps:  33, steps per second:   7, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 88.966861, mean_q: 50.032012, mean_eps: 0.100000\n","     319696/2000000000: episode: 8961, duration: 3.584s, episode steps:  28, steps per second:   8, episode reward: 97.600, mean reward:  3.486 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 88.135918, mean_q: 51.608427, mean_eps: 0.100000\n","     319732/2000000000: episode: 8962, duration: 4.934s, episode steps:  36, steps per second:   7, episode reward: 170.000, mean reward:  4.722 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 82.136266, mean_q: 50.270703, mean_eps: 0.100000\n","     319761/2000000000: episode: 8963, duration: 3.726s, episode steps:  29, steps per second:   8, episode reward: 103.800, mean reward:  3.579 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 83.823361, mean_q: 50.125390, mean_eps: 0.100000\n","     319799/2000000000: episode: 8964, duration: 5.261s, episode steps:  38, steps per second:   7, episode reward: 130.200, mean reward:  3.426 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 82.525921, mean_q: 49.078129, mean_eps: 0.100000\n","     319829/2000000000: episode: 8965, duration: 3.974s, episode steps:  30, steps per second:   8, episode reward: 15.400, mean reward:  0.513 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 90.597877, mean_q: 50.626035, mean_eps: 0.100000\n","     319852/2000000000: episode: 8966, duration: 3.077s, episode steps:  23, steps per second:   7, episode reward: 48.000, mean reward:  2.087 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 77.437559, mean_q: 50.432054, mean_eps: 0.100000\n","     319878/2000000000: episode: 8967, duration: 3.474s, episode steps:  26, steps per second:   7, episode reward: 187.700, mean reward:  7.219 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 79.735607, mean_q: 51.339694, mean_eps: 0.100000\n","     319907/2000000000: episode: 8968, duration: 3.897s, episode steps:  29, steps per second:   7, episode reward: 72.800, mean reward:  2.510 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 91.267357, mean_q: 50.303650, mean_eps: 0.100000\n","     319940/2000000000: episode: 8969, duration: 4.186s, episode steps:  33, steps per second:   8, episode reward: 142.400, mean reward:  4.315 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 93.326279, mean_q: 50.644998, mean_eps: 0.100000\n","     319971/2000000000: episode: 8970, duration: 4.124s, episode steps:  31, steps per second:   8, episode reward: -28.300, mean reward: -0.913 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.916301, mean_q: 50.436778, mean_eps: 0.100000\n","     320001/2000000000: episode: 8971, duration: 3.931s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 87.847163, mean_q: 51.177861, mean_eps: 0.100000\n","     320031/2000000000: episode: 8972, duration: 4.283s, episode steps:  30, steps per second:   7, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.294330, mean_q: 50.896995, mean_eps: 0.100000\n","     320060/2000000000: episode: 8973, duration: 3.715s, episode steps:  29, steps per second:   8, episode reward: 127.100, mean reward:  4.383 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 91.192029, mean_q: 50.568411, mean_eps: 0.100000\n","     320093/2000000000: episode: 8974, duration: 4.216s, episode steps:  33, steps per second:   8, episode reward: -122.700, mean reward: -3.718 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 79.473211, mean_q: 51.346917, mean_eps: 0.100000\n","     320125/2000000000: episode: 8975, duration: 4.222s, episode steps:  32, steps per second:   8, episode reward: 228.400, mean reward:  7.138 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 89.215210, mean_q: 50.465659, mean_eps: 0.100000\n","     320153/2000000000: episode: 8976, duration: 3.736s, episode steps:  28, steps per second:   7, episode reward: -96.000, mean reward: -3.429 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.520044, mean_q: 49.572049, mean_eps: 0.100000\n","     320185/2000000000: episode: 8977, duration: 3.972s, episode steps:  32, steps per second:   8, episode reward: 140.400, mean reward:  4.388 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.075984, mean_q: 50.699855, mean_eps: 0.100000\n","     320213/2000000000: episode: 8978, duration: 3.467s, episode steps:  28, steps per second:   8, episode reward: 39.000, mean reward:  1.393 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 91.245703, mean_q: 50.498263, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     320239/2000000000: episode: 8979, duration: 3.289s, episode steps:  26, steps per second:   8, episode reward: 72.900, mean reward:  2.804 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 82.985329, mean_q: 51.644141, mean_eps: 0.100000\n","     320273/2000000000: episode: 8980, duration: 4.083s, episode steps:  34, steps per second:   8, episode reward: 225.100, mean reward:  6.621 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 79.287643, mean_q: 51.207888, mean_eps: 0.100000\n","     320312/2000000000: episode: 8981, duration: 4.808s, episode steps:  39, steps per second:   8, episode reward: -14.600, mean reward: -0.374 [-20.000, 18.000], mean action: 1.308 [0.000, 2.000],  loss: 82.623805, mean_q: 50.498136, mean_eps: 0.100000\n","     320336/2000000000: episode: 8982, duration: 3.081s, episode steps:  24, steps per second:   8, episode reward: -28.500, mean reward: -1.188 [-20.000, 18.100], mean action: 0.667 [0.000, 2.000],  loss: 86.286891, mean_q: 51.379801, mean_eps: 0.100000\n","     320366/2000000000: episode: 8983, duration: 3.914s, episode steps:  30, steps per second:   8, episode reward: -5.600, mean reward: -0.187 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 86.911312, mean_q: 50.639337, mean_eps: 0.100000\n","     320399/2000000000: episode: 8984, duration: 4.414s, episode steps:  33, steps per second:   7, episode reward: 143.400, mean reward:  4.345 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 90.826793, mean_q: 50.259888, mean_eps: 0.100000\n","     320426/2000000000: episode: 8985, duration: 3.614s, episode steps:  27, steps per second:   7, episode reward: 71.400, mean reward:  2.644 [-20.000, 18.000], mean action: 1.074 [0.000, 2.000],  loss: 84.529840, mean_q: 51.997229, mean_eps: 0.100000\n","     320460/2000000000: episode: 8986, duration: 4.355s, episode steps:  34, steps per second:   8, episode reward: 106.800, mean reward:  3.141 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 81.383437, mean_q: 51.448599, mean_eps: 0.100000\n","     320492/2000000000: episode: 8987, duration: 4.241s, episode steps:  32, steps per second:   8, episode reward: 144.500, mean reward:  4.516 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 86.965154, mean_q: 51.313007, mean_eps: 0.100000\n","     320523/2000000000: episode: 8988, duration: 3.972s, episode steps:  31, steps per second:   8, episode reward: 220.700, mean reward:  7.119 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 80.432216, mean_q: 50.033129, mean_eps: 0.100000\n","     320552/2000000000: episode: 8989, duration: 3.769s, episode steps:  29, steps per second:   8, episode reward: 170.800, mean reward:  5.890 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 88.993808, mean_q: 50.559006, mean_eps: 0.100000\n","     320580/2000000000: episode: 8990, duration: 3.770s, episode steps:  28, steps per second:   7, episode reward: 227.800, mean reward:  8.136 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 86.416397, mean_q: 50.979578, mean_eps: 0.100000\n","     320613/2000000000: episode: 8991, duration: 4.469s, episode steps:  33, steps per second:   7, episode reward: 17.600, mean reward:  0.533 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 100.216275, mean_q: 50.005975, mean_eps: 0.100000\n","     320642/2000000000: episode: 8992, duration: 3.681s, episode steps:  29, steps per second:   8, episode reward: -68.100, mean reward: -2.348 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 87.874510, mean_q: 50.770742, mean_eps: 0.100000\n","     320676/2000000000: episode: 8993, duration: 4.466s, episode steps:  34, steps per second:   8, episode reward: 161.700, mean reward:  4.756 [-20.000, 18.000], mean action: 1.176 [0.000, 2.000],  loss: 82.833135, mean_q: 51.233925, mean_eps: 0.100000\n","     320712/2000000000: episode: 8994, duration: 4.768s, episode steps:  36, steps per second:   8, episode reward: 78.500, mean reward:  2.181 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 85.420419, mean_q: 50.376049, mean_eps: 0.100000\n","     320743/2000000000: episode: 8995, duration: 3.910s, episode steps:  31, steps per second:   8, episode reward: 146.600, mean reward:  4.729 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 83.331991, mean_q: 51.363217, mean_eps: 0.100000\n","     320775/2000000000: episode: 8996, duration: 3.965s, episode steps:  32, steps per second:   8, episode reward: 194.400, mean reward:  6.075 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 80.717363, mean_q: 51.598548, mean_eps: 0.100000\n","     320799/2000000000: episode: 8997, duration: 3.097s, episode steps:  24, steps per second:   8, episode reward: 207.700, mean reward:  8.654 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 81.492961, mean_q: 49.775437, mean_eps: 0.100000\n","     320827/2000000000: episode: 8998, duration: 3.555s, episode steps:  28, steps per second:   8, episode reward: 176.700, mean reward:  6.311 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 78.145046, mean_q: 50.876451, mean_eps: 0.100000\n","     320859/2000000000: episode: 8999, duration: 4.166s, episode steps:  32, steps per second:   8, episode reward: -38.500, mean reward: -1.203 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 83.092068, mean_q: 51.915511, mean_eps: 0.100000\n","     320882/2000000000: episode: 9000, duration: 3.119s, episode steps:  23, steps per second:   7, episode reward: 117.900, mean reward:  5.126 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 80.567653, mean_q: 51.707569, mean_eps: 0.100000\n","     320914/2000000000: episode: 9001, duration: 4.164s, episode steps:  32, steps per second:   8, episode reward: 214.300, mean reward:  6.697 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 91.420525, mean_q: 51.498987, mean_eps: 0.100000\n","     320952/2000000000: episode: 9002, duration: 5.028s, episode steps:  38, steps per second:   8, episode reward: 166.400, mean reward:  4.379 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 83.720040, mean_q: 50.214961, mean_eps: 0.100000\n","     320984/2000000000: episode: 9003, duration: 4.111s, episode steps:  32, steps per second:   8, episode reward: 222.000, mean reward:  6.937 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.185343, mean_q: 51.065470, mean_eps: 0.100000\n","     321015/2000000000: episode: 9004, duration: 4.110s, episode steps:  31, steps per second:   8, episode reward: 100.800, mean reward:  3.252 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 81.878229, mean_q: 51.775617, mean_eps: 0.100000\n","     321044/2000000000: episode: 9005, duration: 3.849s, episode steps:  29, steps per second:   8, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.737864, mean_q: 50.344122, mean_eps: 0.100000\n","     321079/2000000000: episode: 9006, duration: 4.524s, episode steps:  35, steps per second:   8, episode reward: -172.000, mean reward: -4.914 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 79.150616, mean_q: 51.018768, mean_eps: 0.100000\n","     321113/2000000000: episode: 9007, duration: 4.450s, episode steps:  34, steps per second:   8, episode reward: 284.000, mean reward:  8.353 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 86.973610, mean_q: 50.036696, mean_eps: 0.100000\n","     321149/2000000000: episode: 9008, duration: 4.448s, episode steps:  36, steps per second:   8, episode reward: 48.400, mean reward:  1.344 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.700979, mean_q: 51.525162, mean_eps: 0.100000\n","     321182/2000000000: episode: 9009, duration: 4.280s, episode steps:  33, steps per second:   8, episode reward: 158.600, mean reward:  4.806 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 85.105970, mean_q: 51.468040, mean_eps: 0.100000\n","     321211/2000000000: episode: 9010, duration: 3.826s, episode steps:  29, steps per second:   8, episode reward: 59.700, mean reward:  2.059 [-20.000, 18.400], mean action: 0.966 [0.000, 2.000],  loss: 77.812285, mean_q: 51.638157, mean_eps: 0.100000\n","     321251/2000000000: episode: 9011, duration: 5.081s, episode steps:  40, steps per second:   8, episode reward: 85.200, mean reward:  2.130 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 92.914858, mean_q: 50.990159, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     321278/2000000000: episode: 9012, duration: 3.487s, episode steps:  27, steps per second:   8, episode reward: 102.000, mean reward:  3.778 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 87.902323, mean_q: 51.137785, mean_eps: 0.100000\n","     321318/2000000000: episode: 9013, duration: 5.292s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 89.514957, mean_q: 51.386045, mean_eps: 0.100000\n","     321346/2000000000: episode: 9014, duration: 3.824s, episode steps:  28, steps per second:   7, episode reward: -1.100, mean reward: -0.039 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 90.820894, mean_q: 50.942974, mean_eps: 0.100000\n","     321380/2000000000: episode: 9015, duration: 4.545s, episode steps:  34, steps per second:   7, episode reward: 144.500, mean reward:  4.250 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 86.459849, mean_q: 49.657479, mean_eps: 0.100000\n","     321419/2000000000: episode: 9016, duration: 5.337s, episode steps:  39, steps per second:   7, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.077 [0.000, 2.000],  loss: 82.707285, mean_q: 49.809083, mean_eps: 0.100000\n","     321440/2000000000: episode: 9017, duration: 2.701s, episode steps:  21, steps per second:   8, episode reward: 16.800, mean reward:  0.800 [-20.000, 18.000], mean action: 0.571 [0.000, 2.000],  loss: 85.167684, mean_q: 50.086758, mean_eps: 0.100000\n","     321473/2000000000: episode: 9018, duration: 4.356s, episode steps:  33, steps per second:   8, episode reward: 284.000, mean reward:  8.606 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 96.494757, mean_q: 50.693946, mean_eps: 0.100000\n","     321506/2000000000: episode: 9019, duration: 4.439s, episode steps:  33, steps per second:   7, episode reward: 208.100, mean reward:  6.306 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.021868, mean_q: 50.790753, mean_eps: 0.100000\n","     321546/2000000000: episode: 9020, duration: 5.024s, episode steps:  40, steps per second:   8, episode reward: -31.100, mean reward: -0.778 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 80.824365, mean_q: 50.395693, mean_eps: 0.100000\n","     321578/2000000000: episode: 9021, duration: 4.039s, episode steps:  32, steps per second:   8, episode reward: 59.600, mean reward:  1.863 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 86.819317, mean_q: 51.218111, mean_eps: 0.100000\n","     321613/2000000000: episode: 9022, duration: 4.251s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 77.151462, mean_q: 50.638823, mean_eps: 0.100000\n","     321644/2000000000: episode: 9023, duration: 4.058s, episode steps:  31, steps per second:   8, episode reward: 67.300, mean reward:  2.171 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 89.862576, mean_q: 52.471290, mean_eps: 0.100000\n","     321680/2000000000: episode: 9024, duration: 4.659s, episode steps:  36, steps per second:   8, episode reward: 124.500, mean reward:  3.458 [-20.000, 18.000], mean action: 1.083 [0.000, 2.000],  loss: 97.184705, mean_q: 50.835049, mean_eps: 0.100000\n","     321712/2000000000: episode: 9025, duration: 4.130s, episode steps:  32, steps per second:   8, episode reward: 171.800, mean reward:  5.369 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 89.970796, mean_q: 50.483001, mean_eps: 0.100000\n","     321741/2000000000: episode: 9026, duration: 3.679s, episode steps:  29, steps per second:   8, episode reward: 59.700, mean reward:  2.059 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 99.107705, mean_q: 50.377698, mean_eps: 0.100000\n","     321773/2000000000: episode: 9027, duration: 3.962s, episode steps:  32, steps per second:   8, episode reward: 173.100, mean reward:  5.409 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 84.312170, mean_q: 50.539999, mean_eps: 0.100000\n","     321798/2000000000: episode: 9028, duration: 3.351s, episode steps:  25, steps per second:   7, episode reward: 175.600, mean reward:  7.024 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 84.011478, mean_q: 52.062028, mean_eps: 0.100000\n","     321838/2000000000: episode: 9029, duration: 5.279s, episode steps:  40, steps per second:   8, episode reward: 152.100, mean reward:  3.802 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 81.705839, mean_q: 49.684485, mean_eps: 0.100000\n","     321869/2000000000: episode: 9030, duration: 4.002s, episode steps:  31, steps per second:   8, episode reward: -6.700, mean reward: -0.216 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 80.747368, mean_q: 52.018641, mean_eps: 0.100000\n","     321906/2000000000: episode: 9031, duration: 4.734s, episode steps:  37, steps per second:   8, episode reward: 188.700, mean reward:  5.100 [-15.500, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 85.325538, mean_q: 50.247012, mean_eps: 0.100000\n","     321937/2000000000: episode: 9032, duration: 4.156s, episode steps:  31, steps per second:   7, episode reward: 78.100, mean reward:  2.519 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 93.986014, mean_q: 51.901168, mean_eps: 0.100000\n","     321974/2000000000: episode: 9033, duration: 5.249s, episode steps:  37, steps per second:   7, episode reward: 190.700, mean reward:  5.154 [-20.000, 18.300], mean action: 1.324 [0.000, 2.000],  loss: 87.331698, mean_q: 50.454356, mean_eps: 0.100000\n","     322004/2000000000: episode: 9034, duration: 4.198s, episode steps:  30, steps per second:   7, episode reward: 15.400, mean reward:  0.513 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.244016, mean_q: 52.004272, mean_eps: 0.100000\n","     322029/2000000000: episode: 9035, duration: 3.195s, episode steps:  25, steps per second:   8, episode reward: 100.600, mean reward:  4.024 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 87.247079, mean_q: 50.529450, mean_eps: 0.100000\n","     322058/2000000000: episode: 9036, duration: 3.781s, episode steps:  29, steps per second:   8, episode reward: 166.400, mean reward:  5.738 [-8.900, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 78.401025, mean_q: 51.972526, mean_eps: 0.100000\n","     322090/2000000000: episode: 9037, duration: 3.940s, episode steps:  32, steps per second:   8, episode reward: 142.800, mean reward:  4.462 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 87.830468, mean_q: 50.224876, mean_eps: 0.100000\n","     322127/2000000000: episode: 9038, duration: 4.646s, episode steps:  37, steps per second:   8, episode reward: 40.900, mean reward:  1.105 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 85.985096, mean_q: 50.393907, mean_eps: 0.100000\n","     322150/2000000000: episode: 9039, duration: 3.026s, episode steps:  23, steps per second:   8, episode reward: 151.700, mean reward:  6.596 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 76.929416, mean_q: 52.431776, mean_eps: 0.100000\n","     322184/2000000000: episode: 9040, duration: 4.162s, episode steps:  34, steps per second:   8, episode reward: 116.300, mean reward:  3.421 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 87.005633, mean_q: 50.759297, mean_eps: 0.100000\n","     322218/2000000000: episode: 9041, duration: 4.500s, episode steps:  34, steps per second:   8, episode reward: 45.800, mean reward:  1.347 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 84.977231, mean_q: 50.342299, mean_eps: 0.100000\n","     322244/2000000000: episode: 9042, duration: 3.662s, episode steps:  26, steps per second:   7, episode reward: 47.700, mean reward:  1.835 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 81.587764, mean_q: 51.049631, mean_eps: 0.100000\n","     322266/2000000000: episode: 9043, duration: 2.974s, episode steps:  22, steps per second:   7, episode reward: 176.200, mean reward:  8.009 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 96.355568, mean_q: 50.726412, mean_eps: 0.100000\n","     322295/2000000000: episode: 9044, duration: 3.728s, episode steps:  29, steps per second:   8, episode reward: 136.000, mean reward:  4.690 [-20.000, 18.000], mean action: 0.759 [0.000, 2.000],  loss: 84.924640, mean_q: 50.235249, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     322322/2000000000: episode: 9045, duration: 3.311s, episode steps:  27, steps per second:   8, episode reward: 144.500, mean reward:  5.352 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 79.250314, mean_q: 51.629259, mean_eps: 0.100000\n","     322344/2000000000: episode: 9046, duration: 2.802s, episode steps:  22, steps per second:   8, episode reward: 167.600, mean reward:  7.618 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 77.891420, mean_q: 51.740355, mean_eps: 0.100000\n","     322376/2000000000: episode: 9047, duration: 3.958s, episode steps:  32, steps per second:   8, episode reward: 242.000, mean reward:  7.562 [-20.000, 18.000], mean action: 1.219 [0.000, 2.000],  loss: 93.246602, mean_q: 50.815462, mean_eps: 0.100000\n","     322404/2000000000: episode: 9048, duration: 3.494s, episode steps:  28, steps per second:   8, episode reward: 120.400, mean reward:  4.300 [-20.000, 18.200], mean action: 0.893 [0.000, 2.000],  loss: 84.230966, mean_q: 51.629449, mean_eps: 0.100000\n","     322435/2000000000: episode: 9049, duration: 4.145s, episode steps:  31, steps per second:   7, episode reward: 52.000, mean reward:  1.677 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 91.126300, mean_q: 50.700026, mean_eps: 0.100000\n","     322464/2000000000: episode: 9050, duration: 3.849s, episode steps:  29, steps per second:   8, episode reward: 146.200, mean reward:  5.041 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 83.649625, mean_q: 51.273107, mean_eps: 0.100000\n","     322492/2000000000: episode: 9051, duration: 3.645s, episode steps:  28, steps per second:   8, episode reward: 138.000, mean reward:  4.929 [-20.000, 19.000], mean action: 0.857 [0.000, 2.000],  loss: 87.168628, mean_q: 50.750085, mean_eps: 0.100000\n","     322524/2000000000: episode: 9052, duration: 4.071s, episode steps:  32, steps per second:   8, episode reward: 159.100, mean reward:  4.972 [-20.000, 19.400], mean action: 1.219 [0.000, 2.000],  loss: 75.311328, mean_q: 52.143645, mean_eps: 0.100000\n","     322550/2000000000: episode: 9053, duration: 3.259s, episode steps:  26, steps per second:   8, episode reward: 214.900, mean reward:  8.265 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 86.491738, mean_q: 51.601896, mean_eps: 0.100000\n","     322577/2000000000: episode: 9054, duration: 3.532s, episode steps:  27, steps per second:   8, episode reward: 179.700, mean reward:  6.656 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 85.023896, mean_q: 51.312696, mean_eps: 0.100000\n","     322608/2000000000: episode: 9055, duration: 4.094s, episode steps:  31, steps per second:   8, episode reward: -10.300, mean reward: -0.332 [-20.000, 18.300], mean action: 1.097 [0.000, 2.000],  loss: 80.471404, mean_q: 50.750367, mean_eps: 0.100000\n","     322637/2000000000: episode: 9056, duration: 3.498s, episode steps:  29, steps per second:   8, episode reward: 144.600, mean reward:  4.986 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 86.440429, mean_q: 51.576860, mean_eps: 0.100000\n","     322667/2000000000: episode: 9057, duration: 3.707s, episode steps:  30, steps per second:   8, episode reward: -1.300, mean reward: -0.043 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 80.319189, mean_q: 51.909916, mean_eps: 0.100000\n","     322699/2000000000: episode: 9058, duration: 3.990s, episode steps:  32, steps per second:   8, episode reward: 126.900, mean reward:  3.966 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 80.839432, mean_q: 50.873696, mean_eps: 0.100000\n","     322739/2000000000: episode: 9059, duration: 4.961s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 85.750995, mean_q: 51.130144, mean_eps: 0.100000\n","     322779/2000000000: episode: 9060, duration: 4.963s, episode steps:  40, steps per second:   8, episode reward: 96.000, mean reward:  2.400 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 87.427796, mean_q: 51.360485, mean_eps: 0.100000\n","     322807/2000000000: episode: 9061, duration: 3.882s, episode steps:  28, steps per second:   7, episode reward: 193.900, mean reward:  6.925 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 84.667004, mean_q: 51.228342, mean_eps: 0.100000\n","     322834/2000000000: episode: 9062, duration: 3.906s, episode steps:  27, steps per second:   7, episode reward: -60.600, mean reward: -2.244 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 74.296050, mean_q: 51.194960, mean_eps: 0.100000\n","     322860/2000000000: episode: 9063, duration: 3.357s, episode steps:  26, steps per second:   8, episode reward: 208.100, mean reward:  8.004 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 76.224478, mean_q: 50.795107, mean_eps: 0.100000\n","     322900/2000000000: episode: 9064, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward: 38.000, mean reward:  0.950 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 75.476259, mean_q: 51.847378, mean_eps: 0.100000\n","     322934/2000000000: episode: 9065, duration: 4.383s, episode steps:  34, steps per second:   8, episode reward: 106.700, mean reward:  3.138 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.089986, mean_q: 50.869768, mean_eps: 0.100000\n","     322965/2000000000: episode: 9066, duration: 3.882s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.898775, mean_q: 51.212753, mean_eps: 0.100000\n","     322991/2000000000: episode: 9067, duration: 3.269s, episode steps:  26, steps per second:   8, episode reward: 170.000, mean reward:  6.538 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 86.279661, mean_q: 50.865783, mean_eps: 0.100000\n","     323018/2000000000: episode: 9068, duration: 3.564s, episode steps:  27, steps per second:   8, episode reward: 246.000, mean reward:  9.111 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 95.299332, mean_q: 50.258655, mean_eps: 0.100000\n","     323047/2000000000: episode: 9069, duration: 3.791s, episode steps:  29, steps per second:   8, episode reward: 51.200, mean reward:  1.766 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 87.244735, mean_q: 51.461767, mean_eps: 0.100000\n","     323079/2000000000: episode: 9070, duration: 4.083s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.823267, mean_q: 50.816194, mean_eps: 0.100000\n","     323114/2000000000: episode: 9071, duration: 4.358s, episode steps:  35, steps per second:   8, episode reward: 23.500, mean reward:  0.671 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 87.083941, mean_q: 51.242810, mean_eps: 0.100000\n","     323148/2000000000: episode: 9072, duration: 4.225s, episode steps:  34, steps per second:   8, episode reward: 208.000, mean reward:  6.118 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 89.270524, mean_q: 51.938256, mean_eps: 0.100000\n","     323179/2000000000: episode: 9073, duration: 3.902s, episode steps:  31, steps per second:   8, episode reward: 62.700, mean reward:  2.023 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.128112, mean_q: 50.797598, mean_eps: 0.100000\n","     323216/2000000000: episode: 9074, duration: 4.621s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 88.621078, mean_q: 50.895204, mean_eps: 0.100000\n","     323256/2000000000: episode: 9075, duration: 5.017s, episode steps:  40, steps per second:   8, episode reward:  0.000, mean reward:  0.000 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 90.706433, mean_q: 51.452748, mean_eps: 0.100000\n","     323276/2000000000: episode: 9076, duration: 2.669s, episode steps:  20, steps per second:   7, episode reward: 34.400, mean reward:  1.720 [-20.000, 18.000], mean action: 0.550 [0.000, 1.000],  loss: 82.617436, mean_q: 51.044813, mean_eps: 0.100000\n","     323307/2000000000: episode: 9077, duration: 3.867s, episode steps:  31, steps per second:   8, episode reward: 137.400, mean reward:  4.432 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 84.984550, mean_q: 50.673280, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     323338/2000000000: episode: 9078, duration: 3.892s, episode steps:  31, steps per second:   8, episode reward: -3.500, mean reward: -0.113 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.296905, mean_q: 50.158453, mean_eps: 0.100000\n","     323370/2000000000: episode: 9079, duration: 4.031s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 76.826425, mean_q: 50.954112, mean_eps: 0.100000\n","     323410/2000000000: episode: 9080, duration: 5.306s, episode steps:  40, steps per second:   8, episode reward: 58.000, mean reward:  1.450 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 85.428834, mean_q: 50.941347, mean_eps: 0.100000\n","     323440/2000000000: episode: 9081, duration: 3.990s, episode steps:  30, steps per second:   8, episode reward: 126.000, mean reward:  4.200 [-20.000, 18.000], mean action: 1.067 [0.000, 2.000],  loss: 75.111727, mean_q: 51.068944, mean_eps: 0.100000\n","     323480/2000000000: episode: 9082, duration: 5.290s, episode steps:  40, steps per second:   8, episode reward: 208.000, mean reward:  5.200 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.263296, mean_q: 51.133877, mean_eps: 0.100000\n","     323515/2000000000: episode: 9083, duration: 4.819s, episode steps:  35, steps per second:   7, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 75.687635, mean_q: 51.039671, mean_eps: 0.100000\n","     323549/2000000000: episode: 9084, duration: 4.584s, episode steps:  34, steps per second:   7, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 84.588761, mean_q: 50.577085, mean_eps: 0.100000\n","     323575/2000000000: episode: 9085, duration: 3.542s, episode steps:  26, steps per second:   7, episode reward: -20.000, mean reward: -0.769 [-20.000, 18.000], mean action: 1.038 [0.000, 2.000],  loss: 88.992526, mean_q: 51.244748, mean_eps: 0.100000\n","     323608/2000000000: episode: 9086, duration: 4.426s, episode steps:  33, steps per second:   7, episode reward: 218.100, mean reward:  6.609 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.643613, mean_q: 51.444484, mean_eps: 0.100000\n","     323648/2000000000: episode: 9087, duration: 5.303s, episode steps:  40, steps per second:   8, episode reward: 152.000, mean reward:  3.800 [-20.000, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 85.648586, mean_q: 50.944982, mean_eps: 0.100000\n","     323680/2000000000: episode: 9088, duration: 4.346s, episode steps:  32, steps per second:   7, episode reward: 223.100, mean reward:  6.972 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.545253, mean_q: 51.696148, mean_eps: 0.100000\n","     323708/2000000000: episode: 9089, duration: 3.515s, episode steps:  28, steps per second:   8, episode reward: 89.200, mean reward:  3.186 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 85.476048, mean_q: 50.154801, mean_eps: 0.100000\n","     323737/2000000000: episode: 9090, duration: 3.594s, episode steps:  29, steps per second:   8, episode reward: -36.400, mean reward: -1.255 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.043670, mean_q: 51.132927, mean_eps: 0.100000\n","     323762/2000000000: episode: 9091, duration: 3.040s, episode steps:  25, steps per second:   8, episode reward: 53.900, mean reward:  2.156 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 78.242996, mean_q: 50.380295, mean_eps: 0.100000\n","     323802/2000000000: episode: 9092, duration: 4.859s, episode steps:  40, steps per second:   8, episode reward: 25.900, mean reward:  0.647 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 87.282198, mean_q: 49.828560, mean_eps: 0.100000\n","     323831/2000000000: episode: 9093, duration: 3.605s, episode steps:  29, steps per second:   8, episode reward: 155.700, mean reward:  5.369 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.496045, mean_q: 51.858084, mean_eps: 0.100000\n","     323864/2000000000: episode: 9094, duration: 4.198s, episode steps:  33, steps per second:   8, episode reward: 43.700, mean reward:  1.324 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 75.254683, mean_q: 51.282607, mean_eps: 0.100000\n","     323891/2000000000: episode: 9095, duration: 3.349s, episode steps:  27, steps per second:   8, episode reward: 58.000, mean reward:  2.148 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 87.898394, mean_q: 52.603665, mean_eps: 0.100000\n","     323918/2000000000: episode: 9096, duration: 3.368s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 83.527845, mean_q: 49.360618, mean_eps: 0.100000\n","     323957/2000000000: episode: 9097, duration: 4.687s, episode steps:  39, steps per second:   8, episode reward: 122.100, mean reward:  3.131 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 92.915600, mean_q: 51.067901, mean_eps: 0.100000\n","     323980/2000000000: episode: 9098, duration: 2.854s, episode steps:  23, steps per second:   8, episode reward: 94.000, mean reward:  4.087 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 89.417668, mean_q: 51.208794, mean_eps: 0.100000\n","     324015/2000000000: episode: 9099, duration: 4.268s, episode steps:  35, steps per second:   8, episode reward: 199.800, mean reward:  5.709 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 80.253216, mean_q: 50.678384, mean_eps: 0.100000\n","     324043/2000000000: episode: 9100, duration: 3.406s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 88.717785, mean_q: 52.165634, mean_eps: 0.100000\n","     324073/2000000000: episode: 9101, duration: 3.696s, episode steps:  30, steps per second:   8, episode reward: 149.300, mean reward:  4.977 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.081475, mean_q: 50.678547, mean_eps: 0.100000\n","     324105/2000000000: episode: 9102, duration: 4.037s, episode steps:  32, steps per second:   8, episode reward: 59.900, mean reward:  1.872 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 77.436403, mean_q: 51.957527, mean_eps: 0.100000\n","     324139/2000000000: episode: 9103, duration: 4.154s, episode steps:  34, steps per second:   8, episode reward: -18.000, mean reward: -0.529 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 82.002912, mean_q: 51.324900, mean_eps: 0.100000\n","     324179/2000000000: episode: 9104, duration: 4.843s, episode steps:  40, steps per second:   8, episode reward: 85.500, mean reward:  2.137 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 88.604754, mean_q: 50.371064, mean_eps: 0.100000\n","     324203/2000000000: episode: 9105, duration: 3.054s, episode steps:  24, steps per second:   8, episode reward: 208.000, mean reward:  8.667 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 86.703477, mean_q: 51.086130, mean_eps: 0.100000\n","     324242/2000000000: episode: 9106, duration: 4.790s, episode steps:  39, steps per second:   8, episode reward: 56.000, mean reward:  1.436 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 82.820254, mean_q: 51.524697, mean_eps: 0.100000\n","     324267/2000000000: episode: 9107, duration: 3.224s, episode steps:  25, steps per second:   8, episode reward: 104.300, mean reward:  4.172 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 86.909961, mean_q: 51.011061, mean_eps: 0.100000\n","     324294/2000000000: episode: 9108, duration: 3.399s, episode steps:  27, steps per second:   8, episode reward: 129.800, mean reward:  4.807 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 87.265645, mean_q: 51.554634, mean_eps: 0.100000\n","     324326/2000000000: episode: 9109, duration: 4.409s, episode steps:  32, steps per second:   7, episode reward: 102.500, mean reward:  3.203 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 85.764162, mean_q: 51.395818, mean_eps: 0.100000\n","     324364/2000000000: episode: 9110, duration: 5.133s, episode steps:  38, steps per second:   7, episode reward: 144.700, mean reward:  3.808 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 88.574658, mean_q: 51.356353, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     324391/2000000000: episode: 9111, duration: 3.560s, episode steps:  27, steps per second:   8, episode reward: 61.700, mean reward:  2.285 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 84.298286, mean_q: 51.452772, mean_eps: 0.100000\n","     324418/2000000000: episode: 9112, duration: 3.567s, episode steps:  27, steps per second:   8, episode reward: 183.500, mean reward:  6.796 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 90.746414, mean_q: 50.634821, mean_eps: 0.100000\n","     324453/2000000000: episode: 9113, duration: 4.535s, episode steps:  35, steps per second:   8, episode reward: 101.500, mean reward:  2.900 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 88.282294, mean_q: 50.476033, mean_eps: 0.100000\n","     324480/2000000000: episode: 9114, duration: 3.640s, episode steps:  27, steps per second:   7, episode reward: 98.400, mean reward:  3.644 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 83.504534, mean_q: 50.150538, mean_eps: 0.100000\n","     324516/2000000000: episode: 9115, duration: 4.705s, episode steps:  36, steps per second:   8, episode reward: 64.100, mean reward:  1.781 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.918393, mean_q: 51.672775, mean_eps: 0.100000\n","     324542/2000000000: episode: 9116, duration: 3.517s, episode steps:  26, steps per second:   7, episode reward: 52.400, mean reward:  2.015 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 99.591801, mean_q: 51.171260, mean_eps: 0.100000\n","     324572/2000000000: episode: 9117, duration: 4.084s, episode steps:  30, steps per second:   7, episode reward: 185.300, mean reward:  6.177 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 94.046691, mean_q: 51.307121, mean_eps: 0.100000\n","     324606/2000000000: episode: 9118, duration: 4.641s, episode steps:  34, steps per second:   7, episode reward: 54.500, mean reward:  1.603 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.398367, mean_q: 50.262383, mean_eps: 0.100000\n","     324641/2000000000: episode: 9119, duration: 4.400s, episode steps:  35, steps per second:   8, episode reward: -54.300, mean reward: -1.551 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 79.311364, mean_q: 51.855688, mean_eps: 0.100000\n","     324672/2000000000: episode: 9120, duration: 4.019s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 87.074412, mean_q: 50.647206, mean_eps: 0.100000\n","     324704/2000000000: episode: 9121, duration: 4.326s, episode steps:  32, steps per second:   7, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 82.987193, mean_q: 52.429361, mean_eps: 0.100000\n","     324739/2000000000: episode: 9122, duration: 4.552s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 81.850893, mean_q: 50.087961, mean_eps: 0.100000\n","     324772/2000000000: episode: 9123, duration: 4.480s, episode steps:  33, steps per second:   7, episode reward: 322.000, mean reward:  9.758 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 81.341012, mean_q: 50.194328, mean_eps: 0.100000\n","     324802/2000000000: episode: 9124, duration: 3.799s, episode steps:  30, steps per second:   8, episode reward: -20.000, mean reward: -0.667 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 76.631015, mean_q: 51.144842, mean_eps: 0.100000\n","     324835/2000000000: episode: 9125, duration: 4.102s, episode steps:  33, steps per second:   8, episode reward: 42.500, mean reward:  1.288 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.517984, mean_q: 51.135707, mean_eps: 0.100000\n","     324872/2000000000: episode: 9126, duration: 4.571s, episode steps:  37, steps per second:   8, episode reward: -3.300, mean reward: -0.089 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.207933, mean_q: 50.332129, mean_eps: 0.100000\n","     324903/2000000000: episode: 9127, duration: 4.134s, episode steps:  31, steps per second:   7, episode reward: 172.200, mean reward:  5.555 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 85.063373, mean_q: 49.793003, mean_eps: 0.100000\n","     324928/2000000000: episode: 9128, duration: 3.253s, episode steps:  25, steps per second:   8, episode reward: 67.100, mean reward:  2.684 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 89.904131, mean_q: 50.981534, mean_eps: 0.100000\n","     324955/2000000000: episode: 9129, duration: 3.582s, episode steps:  27, steps per second:   8, episode reward: -13.300, mean reward: -0.493 [-20.000, 18.300], mean action: 0.926 [0.000, 2.000],  loss: 78.860332, mean_q: 50.923551, mean_eps: 0.100000\n","     324981/2000000000: episode: 9130, duration: 3.542s, episode steps:  26, steps per second:   7, episode reward: 49.700, mean reward:  1.912 [-20.000, 18.400], mean action: 0.808 [0.000, 2.000],  loss: 99.094986, mean_q: 50.301811, mean_eps: 0.100000\n","     325021/2000000000: episode: 9131, duration: 5.562s, episode steps:  40, steps per second:   7, episode reward: 14.500, mean reward:  0.362 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 90.669252, mean_q: 50.739586, mean_eps: 0.100000\n","     325049/2000000000: episode: 9132, duration: 3.721s, episode steps:  28, steps per second:   8, episode reward: 20.700, mean reward:  0.739 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 81.390397, mean_q: 51.202637, mean_eps: 0.100000\n","     325079/2000000000: episode: 9133, duration: 3.892s, episode steps:  30, steps per second:   8, episode reward: 127.700, mean reward:  4.257 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.452968, mean_q: 51.125197, mean_eps: 0.100000\n","     325106/2000000000: episode: 9134, duration: 3.599s, episode steps:  27, steps per second:   8, episode reward: 170.000, mean reward:  6.296 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 89.532134, mean_q: 51.698513, mean_eps: 0.100000\n","     325134/2000000000: episode: 9135, duration: 3.672s, episode steps:  28, steps per second:   8, episode reward: 244.700, mean reward:  8.739 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 86.387260, mean_q: 50.020013, mean_eps: 0.100000\n","     325168/2000000000: episode: 9136, duration: 4.235s, episode steps:  34, steps per second:   8, episode reward: 65.700, mean reward:  1.932 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 76.738563, mean_q: 51.830817, mean_eps: 0.100000\n","     325193/2000000000: episode: 9137, duration: 3.199s, episode steps:  25, steps per second:   8, episode reward: 91.500, mean reward:  3.660 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 84.184026, mean_q: 51.437855, mean_eps: 0.100000\n","     325220/2000000000: episode: 9138, duration: 3.457s, episode steps:  27, steps per second:   8, episode reward: 147.100, mean reward:  5.448 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 86.045260, mean_q: 50.927055, mean_eps: 0.100000\n","     325257/2000000000: episode: 9139, duration: 4.612s, episode steps:  37, steps per second:   8, episode reward: -32.100, mean reward: -0.868 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 76.924607, mean_q: 51.195488, mean_eps: 0.100000\n","     325283/2000000000: episode: 9140, duration: 3.276s, episode steps:  26, steps per second:   8, episode reward: 233.300, mean reward:  8.973 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 90.578053, mean_q: 51.180576, mean_eps: 0.100000\n","     325315/2000000000: episode: 9141, duration: 4.044s, episode steps:  32, steps per second:   8, episode reward: 75.600, mean reward:  2.362 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 81.714864, mean_q: 51.429493, mean_eps: 0.100000\n","     325348/2000000000: episode: 9142, duration: 4.088s, episode steps:  33, steps per second:   8, episode reward: 38.000, mean reward:  1.152 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 93.477505, mean_q: 50.754906, mean_eps: 0.100000\n","     325381/2000000000: episode: 9143, duration: 4.466s, episode steps:  33, steps per second:   7, episode reward: 154.700, mean reward:  4.688 [-20.000, 18.000], mean action: 1.303 [0.000, 2.000],  loss: 86.577711, mean_q: 50.031364, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     325410/2000000000: episode: 9144, duration: 3.754s, episode steps:  29, steps per second:   8, episode reward: 208.600, mean reward:  7.193 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 86.605842, mean_q: 51.476059, mean_eps: 0.100000\n","     325443/2000000000: episode: 9145, duration: 4.524s, episode steps:  33, steps per second:   7, episode reward: 113.700, mean reward:  3.445 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.715912, mean_q: 50.536324, mean_eps: 0.100000\n","     325471/2000000000: episode: 9146, duration: 3.807s, episode steps:  28, steps per second:   7, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 82.533532, mean_q: 51.319300, mean_eps: 0.100000\n","     325500/2000000000: episode: 9147, duration: 3.960s, episode steps:  29, steps per second:   7, episode reward: 246.000, mean reward:  8.483 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 82.014360, mean_q: 51.234460, mean_eps: 0.100000\n","     325538/2000000000: episode: 9148, duration: 5.196s, episode steps:  38, steps per second:   7, episode reward: 73.700, mean reward:  1.939 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 88.680969, mean_q: 52.043477, mean_eps: 0.100000\n","     325575/2000000000: episode: 9149, duration: 4.679s, episode steps:  37, steps per second:   8, episode reward: 17.300, mean reward:  0.468 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 85.397401, mean_q: 50.547421, mean_eps: 0.100000\n","     325606/2000000000: episode: 9150, duration: 4.082s, episode steps:  31, steps per second:   8, episode reward: 148.800, mean reward:  4.800 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 84.606317, mean_q: 50.949645, mean_eps: 0.100000\n","     325637/2000000000: episode: 9151, duration: 3.959s, episode steps:  31, steps per second:   8, episode reward: 159.200, mean reward:  5.135 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 77.798867, mean_q: 51.778326, mean_eps: 0.100000\n","     325676/2000000000: episode: 9152, duration: 4.966s, episode steps:  39, steps per second:   8, episode reward: 170.000, mean reward:  4.359 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 91.653148, mean_q: 51.170323, mean_eps: 0.100000\n","     325706/2000000000: episode: 9153, duration: 3.940s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 83.260257, mean_q: 50.911536, mean_eps: 0.100000\n","     325729/2000000000: episode: 9154, duration: 3.042s, episode steps:  23, steps per second:   8, episode reward: 18.000, mean reward:  0.783 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 72.451874, mean_q: 49.629163, mean_eps: 0.100000\n","     325760/2000000000: episode: 9155, duration: 3.803s, episode steps:  31, steps per second:   8, episode reward: 94.000, mean reward:  3.032 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.816913, mean_q: 51.813318, mean_eps: 0.100000\n","     325792/2000000000: episode: 9156, duration: 3.845s, episode steps:  32, steps per second:   8, episode reward: 144.100, mean reward:  4.503 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 84.918530, mean_q: 51.734508, mean_eps: 0.100000\n","     325822/2000000000: episode: 9157, duration: 3.848s, episode steps:  30, steps per second:   8, episode reward: 251.500, mean reward:  8.383 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 81.358766, mean_q: 50.961732, mean_eps: 0.100000\n","     325854/2000000000: episode: 9158, duration: 4.002s, episode steps:  32, steps per second:   8, episode reward: -4.000, mean reward: -0.125 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.713087, mean_q: 51.322365, mean_eps: 0.100000\n","     325882/2000000000: episode: 9159, duration: 3.466s, episode steps:  28, steps per second:   8, episode reward: 120.800, mean reward:  4.314 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 84.901998, mean_q: 50.402969, mean_eps: 0.100000\n","     325921/2000000000: episode: 9160, duration: 5.381s, episode steps:  39, steps per second:   7, episode reward: 53.900, mean reward:  1.382 [-20.000, 18.000], mean action: 1.231 [0.000, 2.000],  loss: 84.477798, mean_q: 50.303963, mean_eps: 0.100000\n","     325959/2000000000: episode: 9161, duration: 5.108s, episode steps:  38, steps per second:   7, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 82.636562, mean_q: 51.433609, mean_eps: 0.100000\n","     325992/2000000000: episode: 9162, duration: 4.301s, episode steps:  33, steps per second:   8, episode reward: -31.200, mean reward: -0.945 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 92.037063, mean_q: 50.605035, mean_eps: 0.100000\n","     326024/2000000000: episode: 9163, duration: 4.283s, episode steps:  32, steps per second:   7, episode reward: 118.500, mean reward:  3.703 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.093394, mean_q: 50.729268, mean_eps: 0.100000\n","     326063/2000000000: episode: 9164, duration: 5.272s, episode steps:  39, steps per second:   7, episode reward: -81.100, mean reward: -2.079 [-20.000, 18.000], mean action: 1.128 [0.000, 2.000],  loss: 85.105599, mean_q: 50.679656, mean_eps: 0.100000\n","     326090/2000000000: episode: 9165, duration: 3.605s, episode steps:  27, steps per second:   7, episode reward: 86.500, mean reward:  3.204 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 85.863491, mean_q: 50.925747, mean_eps: 0.100000\n","     326119/2000000000: episode: 9166, duration: 3.816s, episode steps:  29, steps per second:   8, episode reward: 83.700, mean reward:  2.886 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 82.023380, mean_q: 51.920458, mean_eps: 0.100000\n","     326146/2000000000: episode: 9167, duration: 3.836s, episode steps:  27, steps per second:   7, episode reward: 143.700, mean reward:  5.322 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 67.655129, mean_q: 50.767342, mean_eps: 0.100000\n","     326179/2000000000: episode: 9168, duration: 4.333s, episode steps:  33, steps per second:   8, episode reward: 127.100, mean reward:  3.852 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.850992, mean_q: 50.806540, mean_eps: 0.100000\n","     326208/2000000000: episode: 9169, duration: 3.882s, episode steps:  29, steps per second:   7, episode reward: 173.200, mean reward:  5.972 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.743362, mean_q: 50.286231, mean_eps: 0.100000\n","     326243/2000000000: episode: 9170, duration: 4.414s, episode steps:  35, steps per second:   8, episode reward: 150.600, mean reward:  4.303 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 85.162548, mean_q: 51.867058, mean_eps: 0.100000\n","     326276/2000000000: episode: 9171, duration: 4.083s, episode steps:  33, steps per second:   8, episode reward: 310.200, mean reward:  9.400 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 89.394696, mean_q: 50.494482, mean_eps: 0.100000\n","     326316/2000000000: episode: 9172, duration: 4.802s, episode steps:  40, steps per second:   8, episode reward: -52.800, mean reward: -1.320 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 83.608755, mean_q: 50.340894, mean_eps: 0.100000\n","     326356/2000000000: episode: 9173, duration: 4.728s, episode steps:  40, steps per second:   8, episode reward: 104.000, mean reward:  2.600 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 89.872516, mean_q: 52.382092, mean_eps: 0.100000\n","     326380/2000000000: episode: 9174, duration: 3.165s, episode steps:  24, steps per second:   8, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 0.917 [0.000, 2.000],  loss: 81.265484, mean_q: 52.133049, mean_eps: 0.100000\n","     326414/2000000000: episode: 9175, duration: 4.426s, episode steps:  34, steps per second:   8, episode reward: 133.300, mean reward:  3.921 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 88.837338, mean_q: 50.844770, mean_eps: 0.100000\n","     326448/2000000000: episode: 9176, duration: 4.532s, episode steps:  34, steps per second:   8, episode reward: 52.900, mean reward:  1.556 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 82.184734, mean_q: 51.503433, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     326474/2000000000: episode: 9177, duration: 3.398s, episode steps:  26, steps per second:   8, episode reward: 165.400, mean reward:  6.362 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 81.655854, mean_q: 50.642759, mean_eps: 0.100000\n","     326514/2000000000: episode: 9178, duration: 5.109s, episode steps:  40, steps per second:   8, episode reward: 144.200, mean reward:  3.605 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.324827, mean_q: 50.926348, mean_eps: 0.100000\n","     326544/2000000000: episode: 9179, duration: 3.781s, episode steps:  30, steps per second:   8, episode reward: 182.500, mean reward:  6.083 [-20.000, 19.600], mean action: 1.100 [0.000, 2.000],  loss: 99.943222, mean_q: 51.251512, mean_eps: 0.100000\n","     326570/2000000000: episode: 9180, duration: 3.244s, episode steps:  26, steps per second:   8, episode reward: 118.300, mean reward:  4.550 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 89.132875, mean_q: 51.526293, mean_eps: 0.100000\n","     326601/2000000000: episode: 9181, duration: 4.229s, episode steps:  31, steps per second:   7, episode reward: 250.400, mean reward:  8.077 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 78.846542, mean_q: 50.236078, mean_eps: 0.100000\n","     326636/2000000000: episode: 9182, duration: 4.504s, episode steps:  35, steps per second:   8, episode reward: 218.500, mean reward:  6.243 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 80.151532, mean_q: 51.183918, mean_eps: 0.100000\n","     326676/2000000000: episode: 9183, duration: 5.471s, episode steps:  40, steps per second:   7, episode reward: 112.500, mean reward:  2.812 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 83.353168, mean_q: 51.211329, mean_eps: 0.100000\n","     326703/2000000000: episode: 9184, duration: 3.983s, episode steps:  27, steps per second:   7, episode reward: 128.900, mean reward:  4.774 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 86.508723, mean_q: 51.158963, mean_eps: 0.100000\n","     326727/2000000000: episode: 9185, duration: 3.440s, episode steps:  24, steps per second:   7, episode reward: 223.900, mean reward:  9.329 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 83.743766, mean_q: 50.525649, mean_eps: 0.100000\n","     326757/2000000000: episode: 9186, duration: 4.095s, episode steps:  30, steps per second:   7, episode reward: 168.000, mean reward:  5.600 [-16.600, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 79.607921, mean_q: 51.140679, mean_eps: 0.100000\n","     326791/2000000000: episode: 9187, duration: 4.483s, episode steps:  34, steps per second:   8, episode reward: 168.800, mean reward:  4.965 [-20.000, 18.000], mean action: 1.206 [0.000, 2.000],  loss: 90.267842, mean_q: 50.669487, mean_eps: 0.100000\n","     326816/2000000000: episode: 9188, duration: 3.406s, episode steps:  25, steps per second:   7, episode reward: 250.800, mean reward: 10.032 [-5.500, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 89.320526, mean_q: 50.374394, mean_eps: 0.100000\n","     326841/2000000000: episode: 9189, duration: 3.292s, episode steps:  25, steps per second:   8, episode reward: 64.100, mean reward:  2.564 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 84.172101, mean_q: 50.773091, mean_eps: 0.100000\n","     326861/2000000000: episode: 9190, duration: 2.633s, episode steps:  20, steps per second:   8, episode reward:  3.100, mean reward:  0.155 [-20.000, 18.000], mean action: 0.500 [0.000, 1.000],  loss: 81.088444, mean_q: 50.679046, mean_eps: 0.100000\n","     326888/2000000000: episode: 9191, duration: 3.560s, episode steps:  27, steps per second:   8, episode reward: 300.500, mean reward: 11.130 [-7.800, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.044995, mean_q: 51.892840, mean_eps: 0.100000\n","     326926/2000000000: episode: 9192, duration: 5.104s, episode steps:  38, steps per second:   7, episode reward: 154.600, mean reward:  4.068 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 81.010451, mean_q: 49.937133, mean_eps: 0.100000\n","     326955/2000000000: episode: 9193, duration: 3.965s, episode steps:  29, steps per second:   7, episode reward: 154.400, mean reward:  5.324 [-20.000, 18.000], mean action: 0.828 [0.000, 2.000],  loss: 87.200152, mean_q: 51.712203, mean_eps: 0.100000\n","     326987/2000000000: episode: 9194, duration: 4.140s, episode steps:  32, steps per second:   8, episode reward: 120.000, mean reward:  3.750 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 77.353247, mean_q: 50.632246, mean_eps: 0.100000\n","     327019/2000000000: episode: 9195, duration: 4.106s, episode steps:  32, steps per second:   8, episode reward: 147.300, mean reward:  4.603 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 78.911465, mean_q: 51.213339, mean_eps: 0.100000\n","     327052/2000000000: episode: 9196, duration: 4.207s, episode steps:  33, steps per second:   8, episode reward: 198.700, mean reward:  6.021 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 87.527170, mean_q: 51.097768, mean_eps: 0.100000\n","     327086/2000000000: episode: 9197, duration: 4.507s, episode steps:  34, steps per second:   8, episode reward: -9.900, mean reward: -0.291 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 85.209622, mean_q: 51.340805, mean_eps: 0.100000\n","     327118/2000000000: episode: 9198, duration: 4.145s, episode steps:  32, steps per second:   8, episode reward: 67.100, mean reward:  2.097 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 84.012230, mean_q: 51.583448, mean_eps: 0.100000\n","     327144/2000000000: episode: 9199, duration: 3.421s, episode steps:  26, steps per second:   8, episode reward: -76.000, mean reward: -2.923 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 76.298959, mean_q: 50.143115, mean_eps: 0.100000\n","     327175/2000000000: episode: 9200, duration: 4.345s, episode steps:  31, steps per second:   7, episode reward: 111.400, mean reward:  3.594 [-20.000, 18.000], mean action: 0.871 [0.000, 2.000],  loss: 80.640816, mean_q: 51.119831, mean_eps: 0.100000\n","     327208/2000000000: episode: 9201, duration: 4.318s, episode steps:  33, steps per second:   8, episode reward: 195.800, mean reward:  5.933 [-20.000, 18.000], mean action: 0.970 [0.000, 2.000],  loss: 80.756529, mean_q: 51.200105, mean_eps: 0.100000\n","     327232/2000000000: episode: 9202, duration: 3.220s, episode steps:  24, steps per second:   7, episode reward: 153.500, mean reward:  6.396 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 80.068353, mean_q: 51.135166, mean_eps: 0.100000\n","     327270/2000000000: episode: 9203, duration: 4.768s, episode steps:  38, steps per second:   8, episode reward: 149.300, mean reward:  3.929 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 90.264261, mean_q: 51.295194, mean_eps: 0.100000\n","     327298/2000000000: episode: 9204, duration: 3.688s, episode steps:  28, steps per second:   8, episode reward: 144.500, mean reward:  5.161 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 77.843833, mean_q: 51.728027, mean_eps: 0.100000\n","     327328/2000000000: episode: 9205, duration: 3.835s, episode steps:  30, steps per second:   8, episode reward: -3.600, mean reward: -0.120 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 80.662768, mean_q: 51.743369, mean_eps: 0.100000\n","     327362/2000000000: episode: 9206, duration: 4.324s, episode steps:  34, steps per second:   8, episode reward: 109.000, mean reward:  3.206 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 89.224049, mean_q: 51.064464, mean_eps: 0.100000\n","     327397/2000000000: episode: 9207, duration: 4.416s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 83.875007, mean_q: 50.863339, mean_eps: 0.100000\n","     327435/2000000000: episode: 9208, duration: 4.950s, episode steps:  38, steps per second:   8, episode reward: 81.900, mean reward:  2.155 [-20.000, 18.000], mean action: 1.158 [0.000, 2.000],  loss: 77.511474, mean_q: 51.888996, mean_eps: 0.100000\n","     327466/2000000000: episode: 9209, duration: 3.801s, episode steps:  31, steps per second:   8, episode reward: 239.100, mean reward:  7.713 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 93.508184, mean_q: 52.031374, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     327492/2000000000: episode: 9210, duration: 3.424s, episode steps:  26, steps per second:   8, episode reward: 75.200, mean reward:  2.892 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.859059, mean_q: 51.390885, mean_eps: 0.100000\n","     327525/2000000000: episode: 9211, duration: 4.126s, episode steps:  33, steps per second:   8, episode reward: 176.300, mean reward:  5.342 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 84.793384, mean_q: 51.162303, mean_eps: 0.100000\n","     327550/2000000000: episode: 9212, duration: 3.241s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 79.614228, mean_q: 51.311531, mean_eps: 0.100000\n","     327576/2000000000: episode: 9213, duration: 3.241s, episode steps:  26, steps per second:   8, episode reward: -9.900, mean reward: -0.381 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 93.912228, mean_q: 50.182091, mean_eps: 0.100000\n","     327612/2000000000: episode: 9214, duration: 4.547s, episode steps:  36, steps per second:   8, episode reward: 56.000, mean reward:  1.556 [-20.000, 18.000], mean action: 1.056 [0.000, 2.000],  loss: 86.072298, mean_q: 51.729810, mean_eps: 0.100000\n","     327638/2000000000: episode: 9215, duration: 3.351s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 74.507789, mean_q: 50.993049, mean_eps: 0.100000\n","     327670/2000000000: episode: 9216, duration: 4.198s, episode steps:  32, steps per second:   8, episode reward: 170.000, mean reward:  5.312 [-20.000, 18.000], mean action: 0.938 [0.000, 2.000],  loss: 86.737584, mean_q: 51.417841, mean_eps: 0.100000\n","     327702/2000000000: episode: 9217, duration: 4.097s, episode steps:  32, steps per second:   8, episode reward: 136.800, mean reward:  4.275 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 84.732831, mean_q: 50.951327, mean_eps: 0.100000\n","     327735/2000000000: episode: 9218, duration: 4.230s, episode steps:  33, steps per second:   8, episode reward: 155.900, mean reward:  4.724 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 80.928075, mean_q: 51.554297, mean_eps: 0.100000\n","     327765/2000000000: episode: 9219, duration: 4.029s, episode steps:  30, steps per second:   7, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 82.224749, mean_q: 50.267900, mean_eps: 0.100000\n","     327805/2000000000: episode: 9220, duration: 5.194s, episode steps:  40, steps per second:   8, episode reward: 134.000, mean reward:  3.350 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 91.049385, mean_q: 50.490604, mean_eps: 0.100000\n","     327839/2000000000: episode: 9221, duration: 4.415s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 81.343596, mean_q: 51.781581, mean_eps: 0.100000\n","     327874/2000000000: episode: 9222, duration: 4.397s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 80.778824, mean_q: 50.854766, mean_eps: 0.100000\n","     327910/2000000000: episode: 9223, duration: 4.532s, episode steps:  36, steps per second:   8, episode reward: 29.500, mean reward:  0.819 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 86.020563, mean_q: 51.556973, mean_eps: 0.100000\n","     327931/2000000000: episode: 9224, duration: 2.546s, episode steps:  21, steps per second:   8, episode reward: 56.000, mean reward:  2.667 [-20.000, 18.000], mean action: 0.810 [0.000, 2.000],  loss: 90.475178, mean_q: 51.598928, mean_eps: 0.100000\n","     327969/2000000000: episode: 9225, duration: 4.527s, episode steps:  38, steps per second:   8, episode reward: 18.000, mean reward:  0.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 84.454351, mean_q: 51.492348, mean_eps: 0.100000\n","     327999/2000000000: episode: 9226, duration: 3.784s, episode steps:  30, steps per second:   8, episode reward: -49.700, mean reward: -1.657 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 88.771721, mean_q: 50.798454, mean_eps: 0.100000\n","     328030/2000000000: episode: 9227, duration: 3.898s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 81.784647, mean_q: 51.405554, mean_eps: 0.100000\n","     328058/2000000000: episode: 9228, duration: 3.724s, episode steps:  28, steps per second:   8, episode reward: 18.000, mean reward:  0.643 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 90.658064, mean_q: 51.932149, mean_eps: 0.100000\n","     328084/2000000000: episode: 9229, duration: 3.178s, episode steps:  26, steps per second:   8, episode reward: 186.500, mean reward:  7.173 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 81.187120, mean_q: 51.714991, mean_eps: 0.100000\n","     328115/2000000000: episode: 9230, duration: 3.860s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 93.231826, mean_q: 49.794502, mean_eps: 0.100000\n","     328144/2000000000: episode: 9231, duration: 3.870s, episode steps:  29, steps per second:   7, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 83.765538, mean_q: 51.274061, mean_eps: 0.100000\n","     328184/2000000000: episode: 9232, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.288942, mean_q: 51.195086, mean_eps: 0.100000\n","     328215/2000000000: episode: 9233, duration: 3.870s, episode steps:  31, steps per second:   8, episode reward: 296.000, mean reward:  9.548 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 92.701711, mean_q: 51.321520, mean_eps: 0.100000\n","     328241/2000000000: episode: 9234, duration: 3.295s, episode steps:  26, steps per second:   8, episode reward: 208.000, mean reward:  8.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.849052, mean_q: 52.172401, mean_eps: 0.100000\n","     328266/2000000000: episode: 9235, duration: 3.113s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 86.681398, mean_q: 51.850872, mean_eps: 0.100000\n","     328291/2000000000: episode: 9236, duration: 3.148s, episode steps:  25, steps per second:   8, episode reward: 94.000, mean reward:  3.760 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 82.864150, mean_q: 50.789736, mean_eps: 0.100000\n","     328325/2000000000: episode: 9237, duration: 4.268s, episode steps:  34, steps per second:   8, episode reward: 70.700, mean reward:  2.079 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 82.918677, mean_q: 51.654693, mean_eps: 0.100000\n","     328352/2000000000: episode: 9238, duration: 3.308s, episode steps:  27, steps per second:   8, episode reward: 189.800, mean reward:  7.030 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 82.913478, mean_q: 52.490399, mean_eps: 0.100000\n","     328386/2000000000: episode: 9239, duration: 4.457s, episode steps:  34, steps per second:   8, episode reward: 284.000, mean reward:  8.353 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 88.287812, mean_q: 50.684295, mean_eps: 0.100000\n","     328426/2000000000: episode: 9240, duration: 5.406s, episode steps:  40, steps per second:   7, episode reward: 204.000, mean reward:  5.100 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 84.861541, mean_q: 50.833852, mean_eps: 0.100000\n","     328451/2000000000: episode: 9241, duration: 3.532s, episode steps:  25, steps per second:   7, episode reward: 60.500, mean reward:  2.420 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 82.077924, mean_q: 51.888946, mean_eps: 0.100000\n","     328485/2000000000: episode: 9242, duration: 4.529s, episode steps:  34, steps per second:   8, episode reward: 170.000, mean reward:  5.000 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.488376, mean_q: 50.784326, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     328506/2000000000: episode: 9243, duration: 2.771s, episode steps:  21, steps per second:   8, episode reward: 199.500, mean reward:  9.500 [-20.000, 18.000], mean action: 0.524 [0.000, 2.000],  loss: 89.587177, mean_q: 51.793103, mean_eps: 0.100000\n","     328538/2000000000: episode: 9244, duration: 4.224s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 0.906 [0.000, 2.000],  loss: 81.053103, mean_q: 51.124877, mean_eps: 0.100000\n","     328570/2000000000: episode: 9245, duration: 4.085s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 90.010854, mean_q: 50.519301, mean_eps: 0.100000\n","     328610/2000000000: episode: 9246, duration: 4.914s, episode steps:  40, steps per second:   8, episode reward: 116.600, mean reward:  2.915 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 88.946537, mean_q: 50.793547, mean_eps: 0.100000\n","     328645/2000000000: episode: 9247, duration: 4.348s, episode steps:  35, steps per second:   8, episode reward: 16.000, mean reward:  0.457 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 84.454046, mean_q: 51.068178, mean_eps: 0.100000\n","     328673/2000000000: episode: 9248, duration: 3.691s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 79.701249, mean_q: 50.925929, mean_eps: 0.100000\n","     328708/2000000000: episode: 9249, duration: 4.661s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 80.145684, mean_q: 51.700743, mean_eps: 0.100000\n","     328736/2000000000: episode: 9250, duration: 3.544s, episode steps:  28, steps per second:   8, episode reward: -30.800, mean reward: -1.100 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.116572, mean_q: 51.261293, mean_eps: 0.100000\n","     328776/2000000000: episode: 9251, duration: 5.111s, episode steps:  40, steps per second:   8, episode reward: 147.700, mean reward:  3.692 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 76.755632, mean_q: 50.816608, mean_eps: 0.100000\n","     328809/2000000000: episode: 9252, duration: 3.999s, episode steps:  33, steps per second:   8, episode reward: 235.500, mean reward:  7.136 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 82.826369, mean_q: 51.913608, mean_eps: 0.100000\n","     328834/2000000000: episode: 9253, duration: 3.196s, episode steps:  25, steps per second:   8, episode reward: 53.900, mean reward:  2.156 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 78.636876, mean_q: 51.520835, mean_eps: 0.100000\n","     328874/2000000000: episode: 9254, duration: 5.033s, episode steps:  40, steps per second:   8, episode reward: 13.500, mean reward:  0.337 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 89.374787, mean_q: 51.092208, mean_eps: 0.100000\n","     328903/2000000000: episode: 9255, duration: 3.660s, episode steps:  29, steps per second:   8, episode reward: 199.800, mean reward:  6.890 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 81.002571, mean_q: 51.436996, mean_eps: 0.100000\n","     328936/2000000000: episode: 9256, duration: 4.074s, episode steps:  33, steps per second:   8, episode reward: 56.300, mean reward:  1.706 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.348230, mean_q: 50.976295, mean_eps: 0.100000\n","     328976/2000000000: episode: 9257, duration: 4.611s, episode steps:  40, steps per second:   9, episode reward: 146.000, mean reward:  3.650 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 79.349513, mean_q: 52.042066, mean_eps: 0.100000\n","     329011/2000000000: episode: 9258, duration: 4.270s, episode steps:  35, steps per second:   8, episode reward: 208.000, mean reward:  5.943 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 82.560603, mean_q: 51.116962, mean_eps: 0.100000\n","     329036/2000000000: episode: 9259, duration: 3.116s, episode steps:  25, steps per second:   8, episode reward: 18.000, mean reward:  0.720 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 77.981815, mean_q: 51.838623, mean_eps: 0.100000\n","     329065/2000000000: episode: 9260, duration: 3.692s, episode steps:  29, steps per second:   8, episode reward: 155.800, mean reward:  5.372 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.241274, mean_q: 51.448701, mean_eps: 0.100000\n","     329095/2000000000: episode: 9261, duration: 3.665s, episode steps:  30, steps per second:   8, episode reward: 75.800, mean reward:  2.527 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 78.867652, mean_q: 51.975419, mean_eps: 0.100000\n","     329132/2000000000: episode: 9262, duration: 4.475s, episode steps:  37, steps per second:   8, episode reward: 99.900, mean reward:  2.700 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 75.474710, mean_q: 51.375807, mean_eps: 0.100000\n","     329157/2000000000: episode: 9263, duration: 3.118s, episode steps:  25, steps per second:   8, episode reward: 11.700, mean reward:  0.468 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 85.083303, mean_q: 51.705528, mean_eps: 0.100000\n","     329179/2000000000: episode: 9264, duration: 2.797s, episode steps:  22, steps per second:   8, episode reward: 211.900, mean reward:  9.632 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 82.809351, mean_q: 51.301731, mean_eps: 0.100000\n","     329207/2000000000: episode: 9265, duration: 3.549s, episode steps:  28, steps per second:   8, episode reward: -24.200, mean reward: -0.864 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 81.045919, mean_q: 52.646171, mean_eps: 0.100000\n","     329240/2000000000: episode: 9266, duration: 4.148s, episode steps:  33, steps per second:   8, episode reward: 168.700, mean reward:  5.112 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 84.817606, mean_q: 51.408179, mean_eps: 0.100000\n","     329269/2000000000: episode: 9267, duration: 3.765s, episode steps:  29, steps per second:   8, episode reward: 181.800, mean reward:  6.269 [-20.000, 18.000], mean action: 0.724 [0.000, 2.000],  loss: 78.274920, mean_q: 49.751121, mean_eps: 0.100000\n","     329299/2000000000: episode: 9268, duration: 3.761s, episode steps:  30, steps per second:   8, episode reward: 54.800, mean reward:  1.827 [-20.000, 18.900], mean action: 0.967 [0.000, 2.000],  loss: 95.619790, mean_q: 50.993578, mean_eps: 0.100000\n","     329328/2000000000: episode: 9269, duration: 3.568s, episode steps:  29, steps per second:   8, episode reward: 120.000, mean reward:  4.138 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 82.218844, mean_q: 51.645982, mean_eps: 0.100000\n","     329366/2000000000: episode: 9270, duration: 4.598s, episode steps:  38, steps per second:   8, episode reward: 94.000, mean reward:  2.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 81.854616, mean_q: 51.763386, mean_eps: 0.100000\n","     329393/2000000000: episode: 9271, duration: 3.379s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 80.311838, mean_q: 50.535657, mean_eps: 0.100000\n","     329422/2000000000: episode: 9272, duration: 3.656s, episode steps:  29, steps per second:   8, episode reward: 193.500, mean reward:  6.672 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 89.782749, mean_q: 50.060270, mean_eps: 0.100000\n","     329450/2000000000: episode: 9273, duration: 3.529s, episode steps:  28, steps per second:   8, episode reward: 199.500, mean reward:  7.125 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 89.300386, mean_q: 51.693558, mean_eps: 0.100000\n","     329475/2000000000: episode: 9274, duration: 3.314s, episode steps:  25, steps per second:   8, episode reward: 236.000, mean reward:  9.440 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 88.738629, mean_q: 50.121384, mean_eps: 0.100000\n","     329512/2000000000: episode: 9275, duration: 4.766s, episode steps:  37, steps per second:   8, episode reward: 77.000, mean reward:  2.081 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 84.780920, mean_q: 50.373137, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     329552/2000000000: episode: 9276, duration: 4.954s, episode steps:  40, steps per second:   8, episode reward: 76.000, mean reward:  1.900 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 81.880543, mean_q: 51.275164, mean_eps: 0.100000\n","     329578/2000000000: episode: 9277, duration: 3.451s, episode steps:  26, steps per second:   8, episode reward: 117.900, mean reward:  4.535 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 84.712273, mean_q: 50.770345, mean_eps: 0.100000\n","     329604/2000000000: episode: 9278, duration: 3.406s, episode steps:  26, steps per second:   8, episode reward: 212.400, mean reward:  8.169 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 84.260940, mean_q: 52.866217, mean_eps: 0.100000\n","     329644/2000000000: episode: 9279, duration: 5.083s, episode steps:  40, steps per second:   8, episode reward: 38.100, mean reward:  0.953 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 81.847278, mean_q: 51.802958, mean_eps: 0.100000\n","     329674/2000000000: episode: 9280, duration: 3.920s, episode steps:  30, steps per second:   8, episode reward: 51.700, mean reward:  1.723 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.934980, mean_q: 50.612566, mean_eps: 0.100000\n","     329707/2000000000: episode: 9281, duration: 4.195s, episode steps:  33, steps per second:   8, episode reward: 108.900, mean reward:  3.300 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.094338, mean_q: 50.953284, mean_eps: 0.100000\n","     329747/2000000000: episode: 9282, duration: 5.104s, episode steps:  40, steps per second:   8, episode reward: 116.700, mean reward:  2.917 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 86.138415, mean_q: 51.092538, mean_eps: 0.100000\n","     329783/2000000000: episode: 9283, duration: 4.504s, episode steps:  36, steps per second:   8, episode reward: 19.800, mean reward:  0.550 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 77.824609, mean_q: 50.718845, mean_eps: 0.100000\n","     329810/2000000000: episode: 9284, duration: 3.653s, episode steps:  27, steps per second:   7, episode reward: 209.700, mean reward:  7.767 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 80.336551, mean_q: 52.138197, mean_eps: 0.100000\n","     329844/2000000000: episode: 9285, duration: 4.483s, episode steps:  34, steps per second:   8, episode reward:  2.000, mean reward:  0.059 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 87.374965, mean_q: 51.337075, mean_eps: 0.100000\n","     329873/2000000000: episode: 9286, duration: 4.058s, episode steps:  29, steps per second:   7, episode reward: 87.600, mean reward:  3.021 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 82.068251, mean_q: 52.056481, mean_eps: 0.100000\n","     329911/2000000000: episode: 9287, duration: 5.056s, episode steps:  38, steps per second:   8, episode reward: 24.700, mean reward:  0.650 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 81.724025, mean_q: 52.138729, mean_eps: 0.100000\n","     329944/2000000000: episode: 9288, duration: 4.113s, episode steps:  33, steps per second:   8, episode reward: 97.300, mean reward:  2.948 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 93.173100, mean_q: 50.089325, mean_eps: 0.100000\n","     329974/2000000000: episode: 9289, duration: 3.716s, episode steps:  30, steps per second:   8, episode reward: 128.000, mean reward:  4.267 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.669126, mean_q: 51.977534, mean_eps: 0.100000\n","     329998/2000000000: episode: 9290, duration: 3.129s, episode steps:  24, steps per second:   8, episode reward: 157.600, mean reward:  6.567 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 85.124433, mean_q: 50.759709, mean_eps: 0.100000\n","     330033/2000000000: episode: 9291, duration: 4.308s, episode steps:  35, steps per second:   8, episode reward: 56.000, mean reward:  1.600 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 99.067069, mean_q: 51.594983, mean_eps: 0.100000\n","     330062/2000000000: episode: 9292, duration: 3.623s, episode steps:  29, steps per second:   8, episode reward: -29.600, mean reward: -1.021 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 98.202621, mean_q: 51.982456, mean_eps: 0.100000\n","     330088/2000000000: episode: 9293, duration: 3.124s, episode steps:  26, steps per second:   8, episode reward: 111.600, mean reward:  4.292 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 93.021246, mean_q: 51.086304, mean_eps: 0.100000\n","     330112/2000000000: episode: 9294, duration: 3.061s, episode steps:  24, steps per second:   8, episode reward: 199.700, mean reward:  8.321 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 87.545208, mean_q: 51.689135, mean_eps: 0.100000\n","     330139/2000000000: episode: 9295, duration: 3.783s, episode steps:  27, steps per second:   7, episode reward: 132.100, mean reward:  4.893 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 96.774414, mean_q: 52.098916, mean_eps: 0.100000\n","     330179/2000000000: episode: 9296, duration: 5.242s, episode steps:  40, steps per second:   8, episode reward: 88.600, mean reward:  2.215 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 93.163820, mean_q: 51.150095, mean_eps: 0.100000\n","     330215/2000000000: episode: 9297, duration: 4.878s, episode steps:  36, steps per second:   7, episode reward: 71.700, mean reward:  1.992 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 88.208695, mean_q: 51.815851, mean_eps: 0.100000\n","     330255/2000000000: episode: 9298, duration: 5.146s, episode steps:  40, steps per second:   8, episode reward: 11.000, mean reward:  0.275 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 87.894928, mean_q: 52.478738, mean_eps: 0.100000\n","     330281/2000000000: episode: 9299, duration: 3.467s, episode steps:  26, steps per second:   7, episode reward:  4.600, mean reward:  0.177 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 87.649168, mean_q: 53.181117, mean_eps: 0.100000\n","     330313/2000000000: episode: 9300, duration: 3.971s, episode steps:  32, steps per second:   8, episode reward: 209.600, mean reward:  6.550 [-20.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 94.509460, mean_q: 52.416683, mean_eps: 0.100000\n","     330351/2000000000: episode: 9301, duration: 4.845s, episode steps:  38, steps per second:   8, episode reward: -20.000, mean reward: -0.526 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 92.093960, mean_q: 51.415446, mean_eps: 0.100000\n","     330379/2000000000: episode: 9302, duration: 3.669s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 84.430473, mean_q: 51.046425, mean_eps: 0.100000\n","     330406/2000000000: episode: 9303, duration: 3.461s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 88.659040, mean_q: 51.218014, mean_eps: 0.100000\n","     330442/2000000000: episode: 9304, duration: 4.789s, episode steps:  36, steps per second:   8, episode reward: 92.100, mean reward:  2.558 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 80.799014, mean_q: 51.980778, mean_eps: 0.100000\n","     330478/2000000000: episode: 9305, duration: 4.456s, episode steps:  36, steps per second:   8, episode reward: 125.200, mean reward:  3.478 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 84.257622, mean_q: 51.549551, mean_eps: 0.100000\n","     330507/2000000000: episode: 9306, duration: 3.624s, episode steps:  29, steps per second:   8, episode reward: 154.900, mean reward:  5.341 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 88.089527, mean_q: 52.772985, mean_eps: 0.100000\n","     330534/2000000000: episode: 9307, duration: 3.430s, episode steps:  27, steps per second:   8, episode reward: 194.800, mean reward:  7.215 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.174418, mean_q: 52.186229, mean_eps: 0.100000\n","     330562/2000000000: episode: 9308, duration: 3.427s, episode steps:  28, steps per second:   8, episode reward: 33.800, mean reward:  1.207 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 96.703834, mean_q: 52.124728, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     330590/2000000000: episode: 9309, duration: 3.749s, episode steps:  28, steps per second:   7, episode reward: 22.300, mean reward:  0.796 [-20.000, 19.000], mean action: 0.893 [0.000, 2.000],  loss: 87.215439, mean_q: 51.495082, mean_eps: 0.100000\n","     330615/2000000000: episode: 9310, duration: 3.131s, episode steps:  25, steps per second:   8, episode reward: 123.100, mean reward:  4.924 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 86.680092, mean_q: 51.006729, mean_eps: 0.100000\n","     330652/2000000000: episode: 9311, duration: 4.616s, episode steps:  37, steps per second:   8, episode reward: 153.200, mean reward:  4.141 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 92.447408, mean_q: 52.857058, mean_eps: 0.100000\n","     330692/2000000000: episode: 9312, duration: 4.930s, episode steps:  40, steps per second:   8, episode reward: 51.300, mean reward:  1.283 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.936767, mean_q: 51.220261, mean_eps: 0.100000\n","     330728/2000000000: episode: 9313, duration: 4.468s, episode steps:  36, steps per second:   8, episode reward: 196.100, mean reward:  5.447 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 89.033720, mean_q: 52.438839, mean_eps: 0.100000\n","     330756/2000000000: episode: 9314, duration: 3.524s, episode steps:  28, steps per second:   8, episode reward: 154.900, mean reward:  5.532 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 104.587877, mean_q: 51.176957, mean_eps: 0.100000\n","     330786/2000000000: episode: 9315, duration: 3.684s, episode steps:  30, steps per second:   8, episode reward: 156.800, mean reward:  5.227 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 92.720881, mean_q: 52.119474, mean_eps: 0.100000\n","     330816/2000000000: episode: 9316, duration: 3.683s, episode steps:  30, steps per second:   8, episode reward: 91.900, mean reward:  3.063 [-20.000, 18.000], mean action: 0.767 [0.000, 2.000],  loss: 81.263477, mean_q: 52.146574, mean_eps: 0.100000\n","     330843/2000000000: episode: 9317, duration: 3.349s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 95.362107, mean_q: 52.661510, mean_eps: 0.100000\n","     330871/2000000000: episode: 9318, duration: 3.425s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 83.383075, mean_q: 52.451865, mean_eps: 0.100000\n","     330894/2000000000: episode: 9319, duration: 2.891s, episode steps:  23, steps per second:   8, episode reward: 188.200, mean reward:  8.183 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 88.554836, mean_q: 51.807337, mean_eps: 0.100000\n","     330927/2000000000: episode: 9320, duration: 4.318s, episode steps:  33, steps per second:   8, episode reward: -22.900, mean reward: -0.694 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 81.477166, mean_q: 51.611171, mean_eps: 0.100000\n","     330958/2000000000: episode: 9321, duration: 3.962s, episode steps:  31, steps per second:   8, episode reward: 98.400, mean reward:  3.174 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 80.514430, mean_q: 52.361443, mean_eps: 0.100000\n","     330988/2000000000: episode: 9322, duration: 3.764s, episode steps:  30, steps per second:   8, episode reward: 44.900, mean reward:  1.497 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 83.546199, mean_q: 52.651355, mean_eps: 0.100000\n","     331018/2000000000: episode: 9323, duration: 3.581s, episode steps:  30, steps per second:   8, episode reward: 115.700, mean reward:  3.857 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 83.689472, mean_q: 51.440206, mean_eps: 0.100000\n","     331050/2000000000: episode: 9324, duration: 3.821s, episode steps:  32, steps per second:   8, episode reward: 199.500, mean reward:  6.234 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 99.946470, mean_q: 51.200200, mean_eps: 0.100000\n","     331085/2000000000: episode: 9325, duration: 4.320s, episode steps:  35, steps per second:   8, episode reward: 72.200, mean reward:  2.063 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 87.184787, mean_q: 51.477504, mean_eps: 0.100000\n","     331110/2000000000: episode: 9326, duration: 2.986s, episode steps:  25, steps per second:   8, episode reward: 82.100, mean reward:  3.284 [-20.000, 18.000], mean action: 0.880 [0.000, 2.000],  loss: 81.971114, mean_q: 52.517923, mean_eps: 0.100000\n","     331142/2000000000: episode: 9327, duration: 3.920s, episode steps:  32, steps per second:   8, episode reward: 173.400, mean reward:  5.419 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 83.384088, mean_q: 51.187429, mean_eps: 0.100000\n","     331167/2000000000: episode: 9328, duration: 2.969s, episode steps:  25, steps per second:   8, episode reward: 101.000, mean reward:  4.040 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 90.592857, mean_q: 51.262943, mean_eps: 0.100000\n","     331195/2000000000: episode: 9329, duration: 3.380s, episode steps:  28, steps per second:   8, episode reward: 183.200, mean reward:  6.543 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 88.907939, mean_q: 51.596321, mean_eps: 0.100000\n","     331225/2000000000: episode: 9330, duration: 3.599s, episode steps:  30, steps per second:   8, episode reward: 64.700, mean reward:  2.157 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 98.845987, mean_q: 52.234362, mean_eps: 0.100000\n","     331257/2000000000: episode: 9331, duration: 3.951s, episode steps:  32, steps per second:   8, episode reward: 269.400, mean reward:  8.419 [-13.000, 18.000], mean action: 1.188 [0.000, 2.000],  loss: 91.808827, mean_q: 52.566919, mean_eps: 0.100000\n","     331294/2000000000: episode: 9332, duration: 4.560s, episode steps:  37, steps per second:   8, episode reward: -29.900, mean reward: -0.808 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 88.857552, mean_q: 50.883599, mean_eps: 0.100000\n","     331325/2000000000: episode: 9333, duration: 3.719s, episode steps:  31, steps per second:   8, episode reward: -31.800, mean reward: -1.026 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 85.423690, mean_q: 52.336010, mean_eps: 0.100000\n","     331363/2000000000: episode: 9334, duration: 4.533s, episode steps:  38, steps per second:   8, episode reward: 101.600, mean reward:  2.674 [-20.000, 18.000], mean action: 1.316 [0.000, 2.000],  loss: 90.949626, mean_q: 52.061100, mean_eps: 0.100000\n","     331388/2000000000: episode: 9335, duration: 3.059s, episode steps:  25, steps per second:   8, episode reward: 134.300, mean reward:  5.372 [-20.000, 18.000], mean action: 0.960 [0.000, 2.000],  loss: 80.553221, mean_q: 52.907139, mean_eps: 0.100000\n","     331413/2000000000: episode: 9336, duration: 3.204s, episode steps:  25, steps per second:   8, episode reward: 67.200, mean reward:  2.688 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 85.098760, mean_q: 52.128107, mean_eps: 0.100000\n","     331453/2000000000: episode: 9337, duration: 5.169s, episode steps:  40, steps per second:   8, episode reward: 99.300, mean reward:  2.483 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 81.894479, mean_q: 51.269979, mean_eps: 0.100000\n","     331489/2000000000: episode: 9338, duration: 4.421s, episode steps:  36, steps per second:   8, episode reward: 115.700, mean reward:  3.214 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.285841, mean_q: 52.754249, mean_eps: 0.100000\n","     331526/2000000000: episode: 9339, duration: 4.581s, episode steps:  37, steps per second:   8, episode reward: 148.200, mean reward:  4.005 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 90.038438, mean_q: 51.693626, mean_eps: 0.100000\n","     331549/2000000000: episode: 9340, duration: 2.856s, episode steps:  23, steps per second:   8, episode reward: 157.800, mean reward:  6.861 [-20.000, 18.000], mean action: 0.609 [0.000, 2.000],  loss: 92.260158, mean_q: 52.528812, mean_eps: 0.100000\n","     331583/2000000000: episode: 9341, duration: 4.202s, episode steps:  34, steps per second:   8, episode reward: 239.100, mean reward:  7.032 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 91.518409, mean_q: 51.609285, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     331612/2000000000: episode: 9342, duration: 3.523s, episode steps:  29, steps per second:   8, episode reward: 123.700, mean reward:  4.266 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 87.551934, mean_q: 50.687622, mean_eps: 0.100000\n","     331633/2000000000: episode: 9343, duration: 2.737s, episode steps:  21, steps per second:   8, episode reward: 116.800, mean reward:  5.562 [-20.000, 18.000], mean action: 0.619 [0.000, 2.000],  loss: 87.090325, mean_q: 51.618745, mean_eps: 0.100000\n","     331654/2000000000: episode: 9344, duration: 2.816s, episode steps:  21, steps per second:   7, episode reward: 81.900, mean reward:  3.900 [-20.000, 18.000], mean action: 0.381 [0.000, 2.000],  loss: 86.728499, mean_q: 52.018376, mean_eps: 0.100000\n","     331685/2000000000: episode: 9345, duration: 4.000s, episode steps:  31, steps per second:   8, episode reward: 228.700, mean reward:  7.377 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 90.805993, mean_q: 52.475835, mean_eps: 0.100000\n","     331714/2000000000: episode: 9346, duration: 3.563s, episode steps:  29, steps per second:   8, episode reward: 43.300, mean reward:  1.493 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 96.338709, mean_q: 51.822677, mean_eps: 0.100000\n","     331742/2000000000: episode: 9347, duration: 3.393s, episode steps:  28, steps per second:   8, episode reward: 108.800, mean reward:  3.886 [-20.000, 18.000], mean action: 1.107 [0.000, 2.000],  loss: 77.557997, mean_q: 52.337900, mean_eps: 0.100000\n","     331765/2000000000: episode: 9348, duration: 2.895s, episode steps:  23, steps per second:   8, episode reward: 210.600, mean reward:  9.157 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 87.142519, mean_q: 52.523487, mean_eps: 0.100000\n","     331793/2000000000: episode: 9349, duration: 3.647s, episode steps:  28, steps per second:   8, episode reward: 31.200, mean reward:  1.114 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 94.638151, mean_q: 51.267685, mean_eps: 0.100000\n","     331824/2000000000: episode: 9350, duration: 3.867s, episode steps:  31, steps per second:   8, episode reward: -60.100, mean reward: -1.939 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 81.010476, mean_q: 52.329418, mean_eps: 0.100000\n","     331859/2000000000: episode: 9351, duration: 4.461s, episode steps:  35, steps per second:   8, episode reward: 199.800, mean reward:  5.709 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 90.313412, mean_q: 51.991992, mean_eps: 0.100000\n","     331884/2000000000: episode: 9352, duration: 3.393s, episode steps:  25, steps per second:   7, episode reward: 38.000, mean reward:  1.520 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 94.603115, mean_q: 51.877515, mean_eps: 0.100000\n","     331920/2000000000: episode: 9353, duration: 4.933s, episode steps:  36, steps per second:   7, episode reward: 184.500, mean reward:  5.125 [-9.400, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 90.315668, mean_q: 51.115353, mean_eps: 0.100000\n","     331949/2000000000: episode: 9354, duration: 3.771s, episode steps:  29, steps per second:   8, episode reward: 148.300, mean reward:  5.114 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 86.993617, mean_q: 52.517983, mean_eps: 0.100000\n","     331984/2000000000: episode: 9355, duration: 4.314s, episode steps:  35, steps per second:   8, episode reward: 102.500, mean reward:  2.929 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 88.403975, mean_q: 51.698752, mean_eps: 0.100000\n","     332010/2000000000: episode: 9356, duration: 3.418s, episode steps:  26, steps per second:   8, episode reward: 96.300, mean reward:  3.704 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 78.722738, mean_q: 53.285914, mean_eps: 0.100000\n","     332041/2000000000: episode: 9357, duration: 3.805s, episode steps:  31, steps per second:   8, episode reward: 253.400, mean reward:  8.174 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 86.379578, mean_q: 52.292757, mean_eps: 0.100000\n","     332081/2000000000: episode: 9358, duration: 4.904s, episode steps:  40, steps per second:   8, episode reward: -16.500, mean reward: -0.413 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 89.352455, mean_q: 52.512473, mean_eps: 0.100000\n","     332107/2000000000: episode: 9359, duration: 3.203s, episode steps:  26, steps per second:   8, episode reward: -20.000, mean reward: -0.769 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 97.382380, mean_q: 51.130956, mean_eps: 0.100000\n","     332129/2000000000: episode: 9360, duration: 2.817s, episode steps:  22, steps per second:   8, episode reward: 73.900, mean reward:  3.359 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 89.816774, mean_q: 51.292852, mean_eps: 0.100000\n","     332151/2000000000: episode: 9361, duration: 2.805s, episode steps:  22, steps per second:   8, episode reward: 33.300, mean reward:  1.514 [-20.000, 18.000], mean action: 0.682 [0.000, 2.000],  loss: 76.839219, mean_q: 51.876137, mean_eps: 0.100000\n","     332178/2000000000: episode: 9362, duration: 3.481s, episode steps:  27, steps per second:   8, episode reward: -20.000, mean reward: -0.741 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 86.955694, mean_q: 51.732958, mean_eps: 0.100000\n","     332204/2000000000: episode: 9363, duration: 3.450s, episode steps:  26, steps per second:   8, episode reward: 71.200, mean reward:  2.738 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 73.906647, mean_q: 52.148935, mean_eps: 0.100000\n","     332234/2000000000: episode: 9364, duration: 3.883s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 89.154799, mean_q: 50.750840, mean_eps: 0.100000\n","     332269/2000000000: episode: 9365, duration: 4.547s, episode steps:  35, steps per second:   8, episode reward: 143.400, mean reward:  4.097 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 96.070872, mean_q: 51.482072, mean_eps: 0.100000\n","     332299/2000000000: episode: 9366, duration: 3.824s, episode steps:  30, steps per second:   8, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 84.519283, mean_q: 53.188548, mean_eps: 0.100000\n","     332332/2000000000: episode: 9367, duration: 4.091s, episode steps:  33, steps per second:   8, episode reward: 119.100, mean reward:  3.609 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 97.774326, mean_q: 51.987989, mean_eps: 0.100000\n","     332365/2000000000: episode: 9368, duration: 4.252s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 94.570938, mean_q: 52.323461, mean_eps: 0.100000\n","     332395/2000000000: episode: 9369, duration: 3.738s, episode steps:  30, steps per second:   8, episode reward: 170.000, mean reward:  5.667 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 90.221499, mean_q: 51.253057, mean_eps: 0.100000\n","     332419/2000000000: episode: 9370, duration: 3.089s, episode steps:  24, steps per second:   8, episode reward: -20.000, mean reward: -0.833 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 86.191751, mean_q: 53.466992, mean_eps: 0.100000\n","     332450/2000000000: episode: 9371, duration: 3.745s, episode steps:  31, steps per second:   8, episode reward: 61.800, mean reward:  1.994 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 96.255000, mean_q: 52.277751, mean_eps: 0.100000\n","     332477/2000000000: episode: 9372, duration: 3.419s, episode steps:  27, steps per second:   8, episode reward: 187.700, mean reward:  6.952 [-20.000, 18.000], mean action: 0.704 [0.000, 2.000],  loss: 84.275119, mean_q: 52.659312, mean_eps: 0.100000\n","     332508/2000000000: episode: 9373, duration: 3.932s, episode steps:  31, steps per second:   8, episode reward: 193.200, mean reward:  6.232 [-20.000, 18.000], mean action: 0.806 [0.000, 2.000],  loss: 81.367500, mean_q: 51.762077, mean_eps: 0.100000\n","     332548/2000000000: episode: 9374, duration: 4.745s, episode steps:  40, steps per second:   8, episode reward: 103.700, mean reward:  2.593 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 83.718476, mean_q: 52.124621, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     332579/2000000000: episode: 9375, duration: 3.812s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 81.370668, mean_q: 52.964025, mean_eps: 0.100000\n","     332610/2000000000: episode: 9376, duration: 3.884s, episode steps:  31, steps per second:   8, episode reward: 81.000, mean reward:  2.613 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 86.644990, mean_q: 51.416339, mean_eps: 0.100000\n","     332642/2000000000: episode: 9377, duration: 4.214s, episode steps:  32, steps per second:   8, episode reward: -124.400, mean reward: -3.888 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 89.092045, mean_q: 51.545899, mean_eps: 0.100000\n","     332672/2000000000: episode: 9378, duration: 3.777s, episode steps:  30, steps per second:   8, episode reward: 94.000, mean reward:  3.133 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 84.232769, mean_q: 51.231374, mean_eps: 0.100000\n","     332703/2000000000: episode: 9379, duration: 3.954s, episode steps:  31, steps per second:   8, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 93.449691, mean_q: 52.416112, mean_eps: 0.100000\n","     332737/2000000000: episode: 9380, duration: 4.520s, episode steps:  34, steps per second:   8, episode reward: -117.700, mean reward: -3.462 [-20.000, 18.000], mean action: 1.029 [0.000, 2.000],  loss: 84.953678, mean_q: 52.973656, mean_eps: 0.100000\n","     332774/2000000000: episode: 9381, duration: 5.282s, episode steps:  37, steps per second:   7, episode reward: -20.000, mean reward: -0.541 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 86.666960, mean_q: 51.550010, mean_eps: 0.100000\n","     332804/2000000000: episode: 9382, duration: 4.399s, episode steps:  30, steps per second:   7, episode reward: 132.000, mean reward:  4.400 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 87.724097, mean_q: 51.778784, mean_eps: 0.100000\n","     332844/2000000000: episode: 9383, duration: 5.655s, episode steps:  40, steps per second:   7, episode reward: 214.500, mean reward:  5.363 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 99.030946, mean_q: 51.645956, mean_eps: 0.100000\n","     332871/2000000000: episode: 9384, duration: 3.342s, episode steps:  27, steps per second:   8, episode reward: 184.000, mean reward:  6.815 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 74.207610, mean_q: 51.519006, mean_eps: 0.100000\n","     332894/2000000000: episode: 9385, duration: 2.757s, episode steps:  23, steps per second:   8, episode reward: -20.000, mean reward: -0.870 [-20.000, 18.000], mean action: 0.391 [0.000, 2.000],  loss: 79.849241, mean_q: 53.477011, mean_eps: 0.100000\n","     332934/2000000000: episode: 9386, duration: 4.895s, episode steps:  40, steps per second:   8, episode reward: 54.400, mean reward:  1.360 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 90.407903, mean_q: 52.405180, mean_eps: 0.100000\n","     332962/2000000000: episode: 9387, duration: 3.566s, episode steps:  28, steps per second:   8, episode reward: 208.000, mean reward:  7.429 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 82.010409, mean_q: 52.790986, mean_eps: 0.100000\n","     332987/2000000000: episode: 9388, duration: 3.384s, episode steps:  25, steps per second:   7, episode reward: -58.000, mean reward: -2.320 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 93.832159, mean_q: 53.382931, mean_eps: 0.100000\n","     333014/2000000000: episode: 9389, duration: 3.584s, episode steps:  27, steps per second:   8, episode reward: 132.000, mean reward:  4.889 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 87.184572, mean_q: 52.536518, mean_eps: 0.100000\n","     333046/2000000000: episode: 9390, duration: 4.860s, episode steps:  32, steps per second:   7, episode reward: 191.800, mean reward:  5.994 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 83.102003, mean_q: 52.622077, mean_eps: 0.100000\n","     333071/2000000000: episode: 9391, duration: 3.575s, episode steps:  25, steps per second:   7, episode reward: 208.000, mean reward:  8.320 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 83.886703, mean_q: 52.626080, mean_eps: 0.100000\n","     333102/2000000000: episode: 9392, duration: 4.118s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 90.378780, mean_q: 51.775750, mean_eps: 0.100000\n","     333129/2000000000: episode: 9393, duration: 3.368s, episode steps:  27, steps per second:   8, episode reward: 172.000, mean reward:  6.370 [-20.000, 18.000], mean action: 0.963 [0.000, 2.000],  loss: 88.744971, mean_q: 51.974443, mean_eps: 0.100000\n","     333156/2000000000: episode: 9394, duration: 3.517s, episode steps:  27, steps per second:   8, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 90.445747, mean_q: 52.442935, mean_eps: 0.100000\n","     333193/2000000000: episode: 9395, duration: 4.544s, episode steps:  37, steps per second:   8, episode reward: 179.900, mean reward:  4.862 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 87.566089, mean_q: 52.133357, mean_eps: 0.100000\n","     333233/2000000000: episode: 9396, duration: 4.846s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 90.848129, mean_q: 52.019605, mean_eps: 0.100000\n","     333261/2000000000: episode: 9397, duration: 3.382s, episode steps:  28, steps per second:   8, episode reward: 72.600, mean reward:  2.593 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 84.649808, mean_q: 52.511349, mean_eps: 0.100000\n","     333301/2000000000: episode: 9398, duration: 4.900s, episode steps:  40, steps per second:   8, episode reward: 137.100, mean reward:  3.428 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 86.726696, mean_q: 52.372932, mean_eps: 0.100000\n","     333329/2000000000: episode: 9399, duration: 3.499s, episode steps:  28, steps per second:   8, episode reward: 76.400, mean reward:  2.729 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 90.698148, mean_q: 51.460721, mean_eps: 0.100000\n","     333354/2000000000: episode: 9400, duration: 3.204s, episode steps:  25, steps per second:   8, episode reward: 40.300, mean reward:  1.612 [-20.000, 18.000], mean action: 0.560 [0.000, 2.000],  loss: 88.103488, mean_q: 51.015296, mean_eps: 0.100000\n","     333389/2000000000: episode: 9401, duration: 4.554s, episode steps:  35, steps per second:   8, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 91.003171, mean_q: 51.128870, mean_eps: 0.100000\n","     333418/2000000000: episode: 9402, duration: 4.064s, episode steps:  29, steps per second:   7, episode reward: 99.500, mean reward:  3.431 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 82.796160, mean_q: 51.975958, mean_eps: 0.100000\n","     333443/2000000000: episode: 9403, duration: 3.305s, episode steps:  25, steps per second:   8, episode reward: 56.000, mean reward:  2.240 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 93.522857, mean_q: 52.923052, mean_eps: 0.100000\n","     333471/2000000000: episode: 9404, duration: 3.457s, episode steps:  28, steps per second:   8, episode reward: 150.300, mean reward:  5.368 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 79.278724, mean_q: 52.053198, mean_eps: 0.100000\n","     333494/2000000000: episode: 9405, duration: 3.124s, episode steps:  23, steps per second:   7, episode reward: 85.800, mean reward:  3.730 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 84.735716, mean_q: 52.032430, mean_eps: 0.100000\n","     333531/2000000000: episode: 9406, duration: 4.803s, episode steps:  37, steps per second:   8, episode reward: 208.000, mean reward:  5.622 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 93.561008, mean_q: 51.458983, mean_eps: 0.100000\n","     333571/2000000000: episode: 9407, duration: 5.312s, episode steps:  40, steps per second:   8, episode reward: 71.700, mean reward:  1.792 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 88.533206, mean_q: 50.883664, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     333593/2000000000: episode: 9408, duration: 2.821s, episode steps:  22, steps per second:   8, episode reward: 45.200, mean reward:  2.055 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 93.341278, mean_q: 52.988881, mean_eps: 0.100000\n","     333633/2000000000: episode: 9409, duration: 4.909s, episode steps:  40, steps per second:   8, episode reward: 60.300, mean reward:  1.508 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 89.237660, mean_q: 50.751582, mean_eps: 0.100000\n","     333662/2000000000: episode: 9410, duration: 3.579s, episode steps:  29, steps per second:   8, episode reward: 237.500, mean reward:  8.190 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 90.963652, mean_q: 51.436927, mean_eps: 0.100000\n","     333684/2000000000: episode: 9411, duration: 2.728s, episode steps:  22, steps per second:   8, episode reward: 231.700, mean reward: 10.532 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 73.527851, mean_q: 52.104481, mean_eps: 0.100000\n","     333715/2000000000: episode: 9412, duration: 3.947s, episode steps:  31, steps per second:   8, episode reward: 100.700, mean reward:  3.248 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 82.380021, mean_q: 52.228349, mean_eps: 0.100000\n","     333753/2000000000: episode: 9413, duration: 4.873s, episode steps:  38, steps per second:   8, episode reward: 133.300, mean reward:  3.508 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 87.511609, mean_q: 52.461684, mean_eps: 0.100000\n","     333784/2000000000: episode: 9414, duration: 3.836s, episode steps:  31, steps per second:   8, episode reward: 67.100, mean reward:  2.165 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 99.355037, mean_q: 52.390005, mean_eps: 0.100000\n","     333811/2000000000: episode: 9415, duration: 3.648s, episode steps:  27, steps per second:   7, episode reward: 198.500, mean reward:  7.352 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 92.099517, mean_q: 51.955646, mean_eps: 0.100000\n","     333840/2000000000: episode: 9416, duration: 3.861s, episode steps:  29, steps per second:   8, episode reward: 82.800, mean reward:  2.855 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.875292, mean_q: 52.456678, mean_eps: 0.100000\n","     333880/2000000000: episode: 9417, duration: 4.972s, episode steps:  40, steps per second:   8, episode reward: 32.900, mean reward:  0.823 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 91.248038, mean_q: 52.220933, mean_eps: 0.100000\n","     333910/2000000000: episode: 9418, duration: 3.785s, episode steps:  30, steps per second:   8, episode reward: 64.700, mean reward:  2.157 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 92.349077, mean_q: 52.875581, mean_eps: 0.100000\n","     333942/2000000000: episode: 9419, duration: 4.183s, episode steps:  32, steps per second:   8, episode reward: 244.700, mean reward:  7.647 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 86.398574, mean_q: 52.786560, mean_eps: 0.100000\n","     333972/2000000000: episode: 9420, duration: 3.878s, episode steps:  30, steps per second:   8, episode reward: 56.800, mean reward:  1.893 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 83.915257, mean_q: 51.835189, mean_eps: 0.100000\n","     334002/2000000000: episode: 9421, duration: 3.708s, episode steps:  30, steps per second:   8, episode reward: 102.200, mean reward:  3.407 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 84.205056, mean_q: 52.167239, mean_eps: 0.100000\n","     334031/2000000000: episode: 9422, duration: 3.801s, episode steps:  29, steps per second:   8, episode reward: 85.800, mean reward:  2.959 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 95.793405, mean_q: 50.817737, mean_eps: 0.100000\n","     334057/2000000000: episode: 9423, duration: 3.249s, episode steps:  26, steps per second:   8, episode reward: -52.700, mean reward: -2.027 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 91.177192, mean_q: 51.536961, mean_eps: 0.100000\n","     334092/2000000000: episode: 9424, duration: 4.422s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 76.663457, mean_q: 51.741241, mean_eps: 0.100000\n","     334121/2000000000: episode: 9425, duration: 3.749s, episode steps:  29, steps per second:   8, episode reward: 11.300, mean reward:  0.390 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 86.710137, mean_q: 52.535380, mean_eps: 0.100000\n","     334151/2000000000: episode: 9426, duration: 4.048s, episode steps:  30, steps per second:   7, episode reward: 153.100, mean reward:  5.103 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.621858, mean_q: 51.330469, mean_eps: 0.100000\n","     334191/2000000000: episode: 9427, duration: 5.041s, episode steps:  40, steps per second:   8, episode reward: 42.300, mean reward:  1.057 [-20.000, 18.000], mean action: 1.550 [0.000, 2.000],  loss: 82.533455, mean_q: 52.640213, mean_eps: 0.100000\n","     334222/2000000000: episode: 9428, duration: 3.886s, episode steps:  31, steps per second:   8, episode reward: 88.300, mean reward:  2.848 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 90.467597, mean_q: 53.244176, mean_eps: 0.100000\n","     334253/2000000000: episode: 9429, duration: 3.913s, episode steps:  31, steps per second:   8, episode reward: 235.800, mean reward:  7.606 [-20.000, 19.300], mean action: 1.000 [0.000, 2.000],  loss: 86.306689, mean_q: 51.489010, mean_eps: 0.100000\n","     334289/2000000000: episode: 9430, duration: 4.554s, episode steps:  36, steps per second:   8, episode reward: -60.700, mean reward: -1.686 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 77.553496, mean_q: 52.484765, mean_eps: 0.100000\n","     334327/2000000000: episode: 9431, duration: 4.748s, episode steps:  38, steps per second:   8, episode reward: 86.800, mean reward:  2.284 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 91.759212, mean_q: 51.631807, mean_eps: 0.100000\n","     334352/2000000000: episode: 9432, duration: 3.184s, episode steps:  25, steps per second:   8, episode reward: 38.000, mean reward:  1.520 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 89.544136, mean_q: 51.541387, mean_eps: 0.100000\n","     334378/2000000000: episode: 9433, duration: 3.326s, episode steps:  26, steps per second:   8, episode reward: 27.400, mean reward:  1.054 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 83.255044, mean_q: 52.070484, mean_eps: 0.100000\n","     334407/2000000000: episode: 9434, duration: 3.734s, episode steps:  29, steps per second:   8, episode reward: 119.400, mean reward:  4.117 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 88.176381, mean_q: 53.736784, mean_eps: 0.100000\n","     334444/2000000000: episode: 9435, duration: 4.798s, episode steps:  37, steps per second:   8, episode reward: 65.600, mean reward:  1.773 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 83.189161, mean_q: 52.730821, mean_eps: 0.100000\n","     334472/2000000000: episode: 9436, duration: 3.494s, episode steps:  28, steps per second:   8, episode reward: 142.100, mean reward:  5.075 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 86.141966, mean_q: 50.955167, mean_eps: 0.100000\n","     334498/2000000000: episode: 9437, duration: 3.154s, episode steps:  26, steps per second:   8, episode reward: 171.400, mean reward:  6.592 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 88.353235, mean_q: 51.410330, mean_eps: 0.100000\n","     334535/2000000000: episode: 9438, duration: 4.466s, episode steps:  37, steps per second:   8, episode reward: 100.700, mean reward:  2.722 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 95.281073, mean_q: 52.319644, mean_eps: 0.100000\n","     334563/2000000000: episode: 9439, duration: 3.440s, episode steps:  28, steps per second:   8, episode reward: 52.300, mean reward:  1.868 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 89.499356, mean_q: 51.240826, mean_eps: 0.100000\n","     334597/2000000000: episode: 9440, duration: 4.110s, episode steps:  34, steps per second:   8, episode reward: 139.900, mean reward:  4.115 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 85.004282, mean_q: 52.647840, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     334629/2000000000: episode: 9441, duration: 3.708s, episode steps:  32, steps per second:   9, episode reward: 17.000, mean reward:  0.531 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 83.153391, mean_q: 51.793230, mean_eps: 0.100000\n","     334662/2000000000: episode: 9442, duration: 4.133s, episode steps:  33, steps per second:   8, episode reward: 153.000, mean reward:  4.636 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 87.767056, mean_q: 52.409489, mean_eps: 0.100000\n","     334694/2000000000: episode: 9443, duration: 3.934s, episode steps:  32, steps per second:   8, episode reward: 98.400, mean reward:  3.075 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 88.929142, mean_q: 51.362639, mean_eps: 0.100000\n","     334719/2000000000: episode: 9444, duration: 2.925s, episode steps:  25, steps per second:   9, episode reward: 210.900, mean reward:  8.436 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 89.731290, mean_q: 51.414134, mean_eps: 0.100000\n","     334754/2000000000: episode: 9445, duration: 4.287s, episode steps:  35, steps per second:   8, episode reward: 94.000, mean reward:  2.686 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 84.551807, mean_q: 51.764286, mean_eps: 0.100000\n","     334779/2000000000: episode: 9446, duration: 3.034s, episode steps:  25, steps per second:   8, episode reward: 41.400, mean reward:  1.656 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 88.631917, mean_q: 51.025100, mean_eps: 0.100000\n","     334807/2000000000: episode: 9447, duration: 3.396s, episode steps:  28, steps per second:   8, episode reward: 127.500, mean reward:  4.554 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 83.512769, mean_q: 52.432014, mean_eps: 0.100000\n","     334840/2000000000: episode: 9448, duration: 3.938s, episode steps:  33, steps per second:   8, episode reward: 11.500, mean reward:  0.348 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.230553, mean_q: 52.542452, mean_eps: 0.100000\n","     334868/2000000000: episode: 9449, duration: 3.576s, episode steps:  28, steps per second:   8, episode reward: 89.400, mean reward:  3.193 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 75.922064, mean_q: 52.158435, mean_eps: 0.100000\n","     334899/2000000000: episode: 9450, duration: 4.094s, episode steps:  31, steps per second:   8, episode reward: 159.300, mean reward:  5.139 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 90.916132, mean_q: 50.719096, mean_eps: 0.100000\n","     334923/2000000000: episode: 9451, duration: 3.183s, episode steps:  24, steps per second:   8, episode reward: -5.300, mean reward: -0.221 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 94.188685, mean_q: 53.236434, mean_eps: 0.100000\n","     334956/2000000000: episode: 9452, duration: 4.093s, episode steps:  33, steps per second:   8, episode reward: 209.000, mean reward:  6.333 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 84.377005, mean_q: 53.007331, mean_eps: 0.100000\n","     334981/2000000000: episode: 9453, duration: 3.256s, episode steps:  25, steps per second:   8, episode reward: 72.900, mean reward:  2.916 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 80.718528, mean_q: 52.323847, mean_eps: 0.100000\n","     335005/2000000000: episode: 9454, duration: 3.136s, episode steps:  24, steps per second:   8, episode reward: 132.000, mean reward:  5.500 [-20.000, 18.000], mean action: 0.792 [0.000, 2.000],  loss: 91.603604, mean_q: 52.941861, mean_eps: 0.100000\n","     335041/2000000000: episode: 9455, duration: 4.476s, episode steps:  36, steps per second:   8, episode reward: 120.600, mean reward:  3.350 [-20.000, 19.000], mean action: 1.139 [0.000, 2.000],  loss: 83.144677, mean_q: 52.131011, mean_eps: 0.100000\n","     335081/2000000000: episode: 9456, duration: 4.797s, episode steps:  40, steps per second:   8, episode reward: 112.900, mean reward:  2.822 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 91.372200, mean_q: 51.526198, mean_eps: 0.100000\n","     335107/2000000000: episode: 9457, duration: 3.161s, episode steps:  26, steps per second:   8, episode reward: 94.000, mean reward:  3.615 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 93.697119, mean_q: 53.045197, mean_eps: 0.100000\n","     335137/2000000000: episode: 9458, duration: 3.653s, episode steps:  30, steps per second:   8, episode reward: 18.000, mean reward:  0.600 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 86.734395, mean_q: 51.578985, mean_eps: 0.100000\n","     335165/2000000000: episode: 9459, duration: 3.655s, episode steps:  28, steps per second:   8, episode reward: 197.000, mean reward:  7.036 [-20.000, 18.000], mean action: 0.643 [0.000, 2.000],  loss: 89.595970, mean_q: 53.121398, mean_eps: 0.100000\n","     335194/2000000000: episode: 9460, duration: 3.882s, episode steps:  29, steps per second:   7, episode reward: 170.000, mean reward:  5.862 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 81.926502, mean_q: 51.410278, mean_eps: 0.100000\n","     335222/2000000000: episode: 9461, duration: 3.959s, episode steps:  28, steps per second:   7, episode reward: 132.700, mean reward:  4.739 [-20.000, 18.000], mean action: 0.679 [0.000, 2.000],  loss: 91.152851, mean_q: 52.174869, mean_eps: 0.100000\n","     335253/2000000000: episode: 9462, duration: 4.017s, episode steps:  31, steps per second:   8, episode reward: -96.000, mean reward: -3.097 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 76.597325, mean_q: 51.459300, mean_eps: 0.100000\n","     335283/2000000000: episode: 9463, duration: 4.022s, episode steps:  30, steps per second:   7, episode reward: 192.000, mean reward:  6.400 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 81.437027, mean_q: 53.044163, mean_eps: 0.100000\n","     335312/2000000000: episode: 9464, duration: 3.837s, episode steps:  29, steps per second:   8, episode reward: 91.100, mean reward:  3.141 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 81.725282, mean_q: 52.549415, mean_eps: 0.100000\n","     335339/2000000000: episode: 9465, duration: 3.458s, episode steps:  27, steps per second:   8, episode reward: 159.100, mean reward:  5.893 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.884926, mean_q: 51.505037, mean_eps: 0.100000\n","     335375/2000000000: episode: 9466, duration: 4.541s, episode steps:  36, steps per second:   8, episode reward: 84.400, mean reward:  2.344 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 87.817997, mean_q: 52.106581, mean_eps: 0.100000\n","     335405/2000000000: episode: 9467, duration: 3.802s, episode steps:  30, steps per second:   8, episode reward: 105.800, mean reward:  3.527 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.866056, mean_q: 52.493801, mean_eps: 0.100000\n","     335437/2000000000: episode: 9468, duration: 4.195s, episode steps:  32, steps per second:   8, episode reward: 54.400, mean reward:  1.700 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 88.559038, mean_q: 52.676505, mean_eps: 0.100000\n","     335473/2000000000: episode: 9469, duration: 4.714s, episode steps:  36, steps per second:   8, episode reward: 161.200, mean reward:  4.478 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 87.551340, mean_q: 52.011887, mean_eps: 0.100000\n","     335506/2000000000: episode: 9470, duration: 4.443s, episode steps:  33, steps per second:   7, episode reward: 180.000, mean reward:  5.455 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 78.372011, mean_q: 52.966406, mean_eps: 0.100000\n","     335534/2000000000: episode: 9471, duration: 3.617s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 82.039148, mean_q: 52.220658, mean_eps: 0.100000\n","     335559/2000000000: episode: 9472, duration: 3.423s, episode steps:  25, steps per second:   7, episode reward: 176.000, mean reward:  7.040 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 87.347523, mean_q: 52.002495, mean_eps: 0.100000\n","     335587/2000000000: episode: 9473, duration: 3.727s, episode steps:  28, steps per second:   8, episode reward: 27.200, mean reward:  0.971 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 84.618617, mean_q: 50.970423, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     335612/2000000000: episode: 9474, duration: 3.432s, episode steps:  25, steps per second:   7, episode reward: 137.500, mean reward:  5.500 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 79.834029, mean_q: 51.339017, mean_eps: 0.100000\n","     335652/2000000000: episode: 9475, duration: 5.568s, episode steps:  40, steps per second:   7, episode reward: 114.000, mean reward:  2.850 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 81.049736, mean_q: 51.662886, mean_eps: 0.100000\n","     335684/2000000000: episode: 9476, duration: 4.334s, episode steps:  32, steps per second:   7, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.754100, mean_q: 52.670041, mean_eps: 0.100000\n","     335720/2000000000: episode: 9477, duration: 4.673s, episode steps:  36, steps per second:   8, episode reward: 172.700, mean reward:  4.797 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 87.654498, mean_q: 51.314410, mean_eps: 0.100000\n","     335758/2000000000: episode: 9478, duration: 5.033s, episode steps:  38, steps per second:   8, episode reward: -68.700, mean reward: -1.808 [-20.000, 18.000], mean action: 1.289 [0.000, 2.000],  loss: 89.496761, mean_q: 52.078585, mean_eps: 0.100000\n","     335789/2000000000: episode: 9479, duration: 4.134s, episode steps:  31, steps per second:   7, episode reward: 198.000, mean reward:  6.387 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 103.678111, mean_q: 51.718606, mean_eps: 0.100000\n","     335824/2000000000: episode: 9480, duration: 4.590s, episode steps:  35, steps per second:   8, episode reward: 258.900, mean reward:  7.397 [-20.000, 18.000], mean action: 1.086 [0.000, 2.000],  loss: 82.561715, mean_q: 52.604731, mean_eps: 0.100000\n","     335853/2000000000: episode: 9481, duration: 4.048s, episode steps:  29, steps per second:   7, episode reward: 226.100, mean reward:  7.797 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.144430, mean_q: 51.825864, mean_eps: 0.100000\n","     335891/2000000000: episode: 9482, duration: 5.141s, episode steps:  38, steps per second:   7, episode reward: 37.700, mean reward:  0.992 [-20.000, 18.000], mean action: 1.237 [0.000, 2.000],  loss: 85.054312, mean_q: 52.307369, mean_eps: 0.100000\n","     335917/2000000000: episode: 9483, duration: 3.625s, episode steps:  26, steps per second:   7, episode reward: 137.500, mean reward:  5.288 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 81.189610, mean_q: 52.115962, mean_eps: 0.100000\n","     335957/2000000000: episode: 9484, duration: 5.466s, episode steps:  40, steps per second:   7, episode reward: 89.400, mean reward:  2.235 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 82.520085, mean_q: 52.023963, mean_eps: 0.100000\n","     335982/2000000000: episode: 9485, duration: 3.588s, episode steps:  25, steps per second:   7, episode reward: -55.700, mean reward: -2.228 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 88.898949, mean_q: 52.739456, mean_eps: 0.100000\n","     336010/2000000000: episode: 9486, duration: 4.183s, episode steps:  28, steps per second:   7, episode reward: 127.200, mean reward:  4.543 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.006527, mean_q: 51.709925, mean_eps: 0.100000\n","     336038/2000000000: episode: 9487, duration: 3.906s, episode steps:  28, steps per second:   7, episode reward: 273.700, mean reward:  9.775 [-20.000, 18.000], mean action: 0.714 [0.000, 2.000],  loss: 85.667176, mean_q: 51.609294, mean_eps: 0.100000\n","     336064/2000000000: episode: 9488, duration: 3.728s, episode steps:  26, steps per second:   7, episode reward: 130.900, mean reward:  5.035 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 86.929824, mean_q: 50.287464, mean_eps: 0.100000\n","     336094/2000000000: episode: 9489, duration: 4.398s, episode steps:  30, steps per second:   7, episode reward: 136.400, mean reward:  4.547 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 80.615487, mean_q: 52.396725, mean_eps: 0.100000\n","     336121/2000000000: episode: 9490, duration: 3.815s, episode steps:  27, steps per second:   7, episode reward: 125.800, mean reward:  4.659 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.770060, mean_q: 52.283964, mean_eps: 0.100000\n","     336155/2000000000: episode: 9491, duration: 4.596s, episode steps:  34, steps per second:   7, episode reward: 137.100, mean reward:  4.032 [-20.000, 18.000], mean action: 1.088 [0.000, 2.000],  loss: 79.889746, mean_q: 51.576770, mean_eps: 0.100000\n","     336177/2000000000: episode: 9492, duration: 3.183s, episode steps:  22, steps per second:   7, episode reward: 85.300, mean reward:  3.877 [-20.000, 18.000], mean action: 0.409 [0.000, 2.000],  loss: 85.859094, mean_q: 52.560277, mean_eps: 0.100000\n","     336198/2000000000: episode: 9493, duration: 2.986s, episode steps:  21, steps per second:   7, episode reward: 198.900, mean reward:  9.471 [-20.000, 18.000], mean action: 0.571 [0.000, 2.000],  loss: 79.532724, mean_q: 52.735133, mean_eps: 0.100000\n","     336227/2000000000: episode: 9494, duration: 4.369s, episode steps:  29, steps per second:   7, episode reward: 176.300, mean reward:  6.079 [-20.000, 19.600], mean action: 0.897 [0.000, 2.000],  loss: 79.903555, mean_q: 52.054143, mean_eps: 0.100000\n","     336267/2000000000: episode: 9495, duration: 5.776s, episode steps:  40, steps per second:   7, episode reward: 94.200, mean reward:  2.355 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.513796, mean_q: 51.529255, mean_eps: 0.100000\n","     336296/2000000000: episode: 9496, duration: 4.014s, episode steps:  29, steps per second:   7, episode reward: 156.900, mean reward:  5.410 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.555247, mean_q: 52.439133, mean_eps: 0.100000\n","     336321/2000000000: episode: 9497, duration: 3.267s, episode steps:  25, steps per second:   8, episode reward: 138.900, mean reward:  5.556 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 88.568333, mean_q: 53.055314, mean_eps: 0.100000\n","     336345/2000000000: episode: 9498, duration: 3.064s, episode steps:  24, steps per second:   8, episode reward: 193.300, mean reward:  8.054 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 82.621625, mean_q: 51.235516, mean_eps: 0.100000\n","     336380/2000000000: episode: 9499, duration: 4.771s, episode steps:  35, steps per second:   7, episode reward: 230.000, mean reward:  6.571 [-5.400, 18.000], mean action: 1.200 [0.000, 2.000],  loss: 78.063191, mean_q: 52.431279, mean_eps: 0.100000\n","     336407/2000000000: episode: 9500, duration: 3.537s, episode steps:  27, steps per second:   8, episode reward: 200.800, mean reward:  7.437 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 89.084671, mean_q: 52.497439, mean_eps: 0.100000\n","     336436/2000000000: episode: 9501, duration: 3.795s, episode steps:  29, steps per second:   8, episode reward: 202.400, mean reward:  6.979 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.309915, mean_q: 51.604677, mean_eps: 0.100000\n","     336461/2000000000: episode: 9502, duration: 3.131s, episode steps:  25, steps per second:   8, episode reward: 170.000, mean reward:  6.800 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 91.515165, mean_q: 51.493065, mean_eps: 0.100000\n","     336486/2000000000: episode: 9503, duration: 3.085s, episode steps:  25, steps per second:   8, episode reward: 145.300, mean reward:  5.812 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 93.682340, mean_q: 51.667674, mean_eps: 0.100000\n","     336524/2000000000: episode: 9504, duration: 4.675s, episode steps:  38, steps per second:   8, episode reward: 215.000, mean reward:  5.658 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 89.452223, mean_q: 52.182220, mean_eps: 0.100000\n","     336547/2000000000: episode: 9505, duration: 2.860s, episode steps:  23, steps per second:   8, episode reward: 224.600, mean reward:  9.765 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 85.126960, mean_q: 51.782418, mean_eps: 0.100000\n","     336571/2000000000: episode: 9506, duration: 3.055s, episode steps:  24, steps per second:   8, episode reward: 54.300, mean reward:  2.263 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 84.216862, mean_q: 52.109287, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     336598/2000000000: episode: 9507, duration: 3.512s, episode steps:  27, steps per second:   8, episode reward: 155.800, mean reward:  5.770 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 94.552518, mean_q: 51.835522, mean_eps: 0.100000\n","     336631/2000000000: episode: 9508, duration: 4.278s, episode steps:  33, steps per second:   8, episode reward: -2.000, mean reward: -0.061 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.646035, mean_q: 51.567386, mean_eps: 0.100000\n","     336660/2000000000: episode: 9509, duration: 4.048s, episode steps:  29, steps per second:   7, episode reward: 73.700, mean reward:  2.541 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 81.218212, mean_q: 52.463626, mean_eps: 0.100000\n","     336683/2000000000: episode: 9510, duration: 2.985s, episode steps:  23, steps per second:   8, episode reward: 146.600, mean reward:  6.374 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 81.832487, mean_q: 52.033537, mean_eps: 0.100000\n","     336711/2000000000: episode: 9511, duration: 3.572s, episode steps:  28, steps per second:   8, episode reward: 202.300, mean reward:  7.225 [-20.000, 18.400], mean action: 0.929 [0.000, 2.000],  loss: 101.333509, mean_q: 51.300806, mean_eps: 0.100000\n","     336740/2000000000: episode: 9512, duration: 3.775s, episode steps:  29, steps per second:   8, episode reward: 167.400, mean reward:  5.772 [-20.000, 18.000], mean action: 1.034 [0.000, 2.000],  loss: 86.782735, mean_q: 51.175905, mean_eps: 0.100000\n","     336768/2000000000: episode: 9513, duration: 3.753s, episode steps:  28, steps per second:   7, episode reward: 161.400, mean reward:  5.764 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 92.396438, mean_q: 51.827242, mean_eps: 0.100000\n","     336793/2000000000: episode: 9514, duration: 3.426s, episode steps:  25, steps per second:   7, episode reward: 39.400, mean reward:  1.576 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 73.044910, mean_q: 51.745916, mean_eps: 0.100000\n","     336820/2000000000: episode: 9515, duration: 3.318s, episode steps:  27, steps per second:   8, episode reward: 85.300, mean reward:  3.159 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 77.721407, mean_q: 52.629851, mean_eps: 0.100000\n","     336850/2000000000: episode: 9516, duration: 3.743s, episode steps:  30, steps per second:   8, episode reward: 246.400, mean reward:  8.213 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 82.976434, mean_q: 52.847191, mean_eps: 0.100000\n","     336874/2000000000: episode: 9517, duration: 3.068s, episode steps:  24, steps per second:   8, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 88.039370, mean_q: 50.661690, mean_eps: 0.100000\n","     336906/2000000000: episode: 9518, duration: 4.088s, episode steps:  32, steps per second:   8, episode reward: 102.000, mean reward:  3.187 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 83.741093, mean_q: 51.803922, mean_eps: 0.100000\n","     336946/2000000000: episode: 9519, duration: 5.183s, episode steps:  40, steps per second:   8, episode reward: 98.500, mean reward:  2.463 [-20.000, 18.000], mean action: 1.400 [0.000, 2.000],  loss: 92.904863, mean_q: 51.243821, mean_eps: 0.100000\n","     336970/2000000000: episode: 9520, duration: 3.110s, episode steps:  24, steps per second:   8, episode reward: 71.800, mean reward:  2.992 [-20.000, 18.000], mean action: 0.708 [0.000, 2.000],  loss: 84.626320, mean_q: 51.540389, mean_eps: 0.100000\n","     336999/2000000000: episode: 9521, duration: 3.839s, episode steps:  29, steps per second:   8, episode reward: 18.000, mean reward:  0.621 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 86.197324, mean_q: 53.226754, mean_eps: 0.100000\n","     337028/2000000000: episode: 9522, duration: 3.784s, episode steps:  29, steps per second:   8, episode reward: 103.800, mean reward:  3.579 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.173626, mean_q: 51.264804, mean_eps: 0.100000\n","     337060/2000000000: episode: 9523, duration: 4.156s, episode steps:  32, steps per second:   8, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 85.880204, mean_q: 52.943627, mean_eps: 0.100000\n","     337092/2000000000: episode: 9524, duration: 4.076s, episode steps:  32, steps per second:   8, episode reward: 173.800, mean reward:  5.431 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 88.147847, mean_q: 51.428320, mean_eps: 0.100000\n","     337132/2000000000: episode: 9525, duration: 5.313s, episode steps:  40, steps per second:   8, episode reward: 117.200, mean reward:  2.930 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 87.316597, mean_q: 50.444901, mean_eps: 0.100000\n","     337164/2000000000: episode: 9526, duration: 4.099s, episode steps:  32, steps per second:   8, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 86.629076, mean_q: 52.218084, mean_eps: 0.100000\n","     337194/2000000000: episode: 9527, duration: 3.952s, episode steps:  30, steps per second:   8, episode reward: 209.700, mean reward:  6.990 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.982832, mean_q: 52.015193, mean_eps: 0.100000\n","     337218/2000000000: episode: 9528, duration: 3.214s, episode steps:  24, steps per second:   7, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 94.676624, mean_q: 53.033341, mean_eps: 0.100000\n","     337249/2000000000: episode: 9529, duration: 3.997s, episode steps:  31, steps per second:   8, episode reward: 22.100, mean reward:  0.713 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 86.017309, mean_q: 51.809795, mean_eps: 0.100000\n","     337279/2000000000: episode: 9530, duration: 3.947s, episode steps:  30, steps per second:   8, episode reward: 152.800, mean reward:  5.093 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 89.086100, mean_q: 50.997197, mean_eps: 0.100000\n","     337316/2000000000: episode: 9531, duration: 4.835s, episode steps:  37, steps per second:   8, episode reward: 170.000, mean reward:  4.595 [-20.000, 18.000], mean action: 1.297 [0.000, 2.000],  loss: 84.138408, mean_q: 52.211694, mean_eps: 0.100000\n","     337339/2000000000: episode: 9532, duration: 2.955s, episode steps:  23, steps per second:   8, episode reward: 132.000, mean reward:  5.739 [-20.000, 18.000], mean action: 0.739 [0.000, 2.000],  loss: 87.304973, mean_q: 52.052676, mean_eps: 0.100000\n","     337377/2000000000: episode: 9533, duration: 4.824s, episode steps:  38, steps per second:   8, episode reward: 170.000, mean reward:  4.474 [-20.000, 18.000], mean action: 1.263 [0.000, 2.000],  loss: 86.578382, mean_q: 52.988458, mean_eps: 0.100000\n","     337400/2000000000: episode: 9534, duration: 3.137s, episode steps:  23, steps per second:   7, episode reward: 34.700, mean reward:  1.509 [-20.000, 18.000], mean action: 0.696 [0.000, 2.000],  loss: 87.340878, mean_q: 50.630089, mean_eps: 0.100000\n","     337431/2000000000: episode: 9535, duration: 3.735s, episode steps:  31, steps per second:   8, episode reward:  7.400, mean reward:  0.239 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 86.003589, mean_q: 53.382484, mean_eps: 0.100000\n","     337458/2000000000: episode: 9536, duration: 3.303s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 77.687952, mean_q: 53.610788, mean_eps: 0.100000\n","     337495/2000000000: episode: 9537, duration: 4.754s, episode steps:  37, steps per second:   8, episode reward: 56.000, mean reward:  1.514 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 82.868449, mean_q: 52.190875, mean_eps: 0.100000\n","     337521/2000000000: episode: 9538, duration: 3.288s, episode steps:  26, steps per second:   8, episode reward: 120.100, mean reward:  4.619 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 81.417809, mean_q: 52.531676, mean_eps: 0.100000\n","     337555/2000000000: episode: 9539, duration: 4.287s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.429814, mean_q: 51.960899, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     337579/2000000000: episode: 9540, duration: 3.152s, episode steps:  24, steps per second:   8, episode reward: 94.000, mean reward:  3.917 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 77.229989, mean_q: 51.781627, mean_eps: 0.100000\n","     337607/2000000000: episode: 9541, duration: 3.654s, episode steps:  28, steps per second:   8, episode reward: 56.000, mean reward:  2.000 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 84.913556, mean_q: 51.839875, mean_eps: 0.100000\n","     337647/2000000000: episode: 9542, duration: 5.417s, episode steps:  40, steps per second:   7, episode reward: -16.000, mean reward: -0.400 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 95.360988, mean_q: 51.470463, mean_eps: 0.100000\n","     337671/2000000000: episode: 9543, duration: 3.318s, episode steps:  24, steps per second:   7, episode reward: 208.000, mean reward:  8.667 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 77.058829, mean_q: 52.226164, mean_eps: 0.100000\n","     337704/2000000000: episode: 9544, duration: 4.462s, episode steps:  33, steps per second:   7, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 84.966263, mean_q: 51.839977, mean_eps: 0.100000\n","     337742/2000000000: episode: 9545, duration: 5.017s, episode steps:  38, steps per second:   8, episode reward: 102.600, mean reward:  2.700 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 92.179502, mean_q: 50.580143, mean_eps: 0.100000\n","     337770/2000000000: episode: 9546, duration: 3.663s, episode steps:  28, steps per second:   8, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 89.289145, mean_q: 51.327827, mean_eps: 0.100000\n","     337799/2000000000: episode: 9547, duration: 3.712s, episode steps:  29, steps per second:   8, episode reward: 85.100, mean reward:  2.934 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 86.244105, mean_q: 52.486856, mean_eps: 0.100000\n","     337832/2000000000: episode: 9548, duration: 4.113s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.211515, mean_q: 51.639509, mean_eps: 0.100000\n","     337856/2000000000: episode: 9549, duration: 3.100s, episode steps:  24, steps per second:   8, episode reward: -58.000, mean reward: -2.417 [-20.000, 18.000], mean action: 0.958 [0.000, 2.000],  loss: 87.222142, mean_q: 51.325521, mean_eps: 0.100000\n","     337886/2000000000: episode: 9550, duration: 3.809s, episode steps:  30, steps per second:   8, episode reward: 120.700, mean reward:  4.023 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 79.096353, mean_q: 52.114452, mean_eps: 0.100000\n","     337910/2000000000: episode: 9551, duration: 3.042s, episode steps:  24, steps per second:   8, episode reward: 238.900, mean reward:  9.954 [-20.000, 18.000], mean action: 0.875 [0.000, 2.000],  loss: 81.184124, mean_q: 52.637079, mean_eps: 0.100000\n","     337950/2000000000: episode: 9552, duration: 5.005s, episode steps:  40, steps per second:   8, episode reward: 158.200, mean reward:  3.955 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 78.077063, mean_q: 51.877383, mean_eps: 0.100000\n","     337980/2000000000: episode: 9553, duration: 3.613s, episode steps:  30, steps per second:   8, episode reward: 219.100, mean reward:  7.303 [-20.000, 18.000], mean action: 0.867 [0.000, 2.000],  loss: 99.017825, mean_q: 51.505611, mean_eps: 0.100000\n","     338013/2000000000: episode: 9554, duration: 3.866s, episode steps:  33, steps per second:   9, episode reward: -96.000, mean reward: -2.909 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 90.610663, mean_q: 51.295442, mean_eps: 0.100000\n","     338039/2000000000: episode: 9555, duration: 3.249s, episode steps:  26, steps per second:   8, episode reward: -110.800, mean reward: -4.262 [-20.000, 18.000], mean action: 0.962 [0.000, 2.000],  loss: 89.299801, mean_q: 51.158966, mean_eps: 0.100000\n","     338072/2000000000: episode: 9556, duration: 4.143s, episode steps:  33, steps per second:   8, episode reward: 126.100, mean reward:  3.821 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 88.245765, mean_q: 51.227862, mean_eps: 0.100000\n","     338098/2000000000: episode: 9557, duration: 3.391s, episode steps:  26, steps per second:   8, episode reward: 56.000, mean reward:  2.154 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 80.405488, mean_q: 52.027476, mean_eps: 0.100000\n","     338130/2000000000: episode: 9558, duration: 4.198s, episode steps:  32, steps per second:   8, episode reward: 74.600, mean reward:  2.331 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 87.330520, mean_q: 52.301788, mean_eps: 0.100000\n","     338159/2000000000: episode: 9559, duration: 3.617s, episode steps:  29, steps per second:   8, episode reward: 208.000, mean reward:  7.172 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 87.389993, mean_q: 51.856497, mean_eps: 0.100000\n","     338188/2000000000: episode: 9560, duration: 3.814s, episode steps:  29, steps per second:   8, episode reward: 122.700, mean reward:  4.231 [-20.000, 18.000], mean action: 1.103 [0.000, 2.000],  loss: 84.337605, mean_q: 53.351694, mean_eps: 0.100000\n","     338216/2000000000: episode: 9561, duration: 3.507s, episode steps:  28, steps per second:   8, episode reward: 67.700, mean reward:  2.418 [-20.000, 18.000], mean action: 1.071 [0.000, 2.000],  loss: 85.937037, mean_q: 53.313756, mean_eps: 0.100000\n","     338250/2000000000: episode: 9562, duration: 4.239s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 82.599889, mean_q: 52.451430, mean_eps: 0.100000\n","     338278/2000000000: episode: 9563, duration: 3.480s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 89.224341, mean_q: 52.653466, mean_eps: 0.100000\n","     338307/2000000000: episode: 9564, duration: 3.626s, episode steps:  29, steps per second:   8, episode reward: 208.000, mean reward:  7.172 [-20.000, 18.000], mean action: 0.793 [0.000, 2.000],  loss: 89.089460, mean_q: 52.411771, mean_eps: 0.100000\n","     338338/2000000000: episode: 9565, duration: 3.922s, episode steps:  31, steps per second:   8, episode reward: 20.300, mean reward:  0.655 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 83.301756, mean_q: 52.706300, mean_eps: 0.100000\n","     338366/2000000000: episode: 9566, duration: 3.586s, episode steps:  28, steps per second:   8, episode reward: 201.700, mean reward:  7.204 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 79.120793, mean_q: 51.445373, mean_eps: 0.100000\n","     338393/2000000000: episode: 9567, duration: 3.413s, episode steps:  27, steps per second:   8, episode reward: 94.000, mean reward:  3.481 [-20.000, 18.000], mean action: 1.037 [0.000, 2.000],  loss: 79.351601, mean_q: 50.752885, mean_eps: 0.100000\n","     338429/2000000000: episode: 9568, duration: 4.627s, episode steps:  36, steps per second:   8, episode reward: 77.200, mean reward:  2.144 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 92.191298, mean_q: 51.717185, mean_eps: 0.100000\n","     338462/2000000000: episode: 9569, duration: 4.482s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 85.458146, mean_q: 52.619045, mean_eps: 0.100000\n","     338491/2000000000: episode: 9570, duration: 3.885s, episode steps:  29, steps per second:   7, episode reward: 206.100, mean reward:  7.107 [-20.000, 18.000], mean action: 0.897 [0.000, 2.000],  loss: 87.291392, mean_q: 51.974268, mean_eps: 0.100000\n","     338515/2000000000: episode: 9571, duration: 3.261s, episode steps:  24, steps per second:   7, episode reward: 229.500, mean reward:  9.563 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 80.229056, mean_q: 50.830463, mean_eps: 0.100000\n","     338545/2000000000: episode: 9572, duration: 3.774s, episode steps:  30, steps per second:   8, episode reward: 168.600, mean reward:  5.620 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 78.103301, mean_q: 53.152324, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     338585/2000000000: episode: 9573, duration: 5.259s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 83.021344, mean_q: 52.313453, mean_eps: 0.100000\n","     338619/2000000000: episode: 9574, duration: 4.465s, episode steps:  34, steps per second:   8, episode reward: 175.600, mean reward:  5.165 [-20.000, 18.000], mean action: 1.118 [0.000, 2.000],  loss: 80.973006, mean_q: 51.924474, mean_eps: 0.100000\n","     338651/2000000000: episode: 9575, duration: 4.141s, episode steps:  32, steps per second:   8, episode reward: 266.300, mean reward:  8.322 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 83.671353, mean_q: 52.297162, mean_eps: 0.100000\n","     338678/2000000000: episode: 9576, duration: 3.574s, episode steps:  27, steps per second:   8, episode reward: -7.100, mean reward: -0.263 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 84.456899, mean_q: 52.692703, mean_eps: 0.100000\n","     338702/2000000000: episode: 9577, duration: 3.123s, episode steps:  24, steps per second:   8, episode reward: 94.000, mean reward:  3.917 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 86.215738, mean_q: 52.807332, mean_eps: 0.100000\n","     338723/2000000000: episode: 9578, duration: 2.844s, episode steps:  21, steps per second:   7, episode reward: 87.900, mean reward:  4.186 [-20.000, 18.000], mean action: 0.429 [0.000, 2.000],  loss: 83.277517, mean_q: 52.107672, mean_eps: 0.100000\n","     338750/2000000000: episode: 9579, duration: 3.421s, episode steps:  27, steps per second:   8, episode reward: 178.300, mean reward:  6.604 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 90.336017, mean_q: 51.752176, mean_eps: 0.100000\n","     338790/2000000000: episode: 9580, duration: 5.063s, episode steps:  40, steps per second:   8, episode reward: 59.400, mean reward:  1.485 [-20.000, 18.000], mean action: 1.425 [0.000, 2.000],  loss: 80.355389, mean_q: 52.661181, mean_eps: 0.100000\n","     338821/2000000000: episode: 9581, duration: 4.115s, episode steps:  31, steps per second:   8, episode reward: 30.900, mean reward:  0.997 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 89.793401, mean_q: 51.049266, mean_eps: 0.100000\n","     338848/2000000000: episode: 9582, duration: 3.533s, episode steps:  27, steps per second:   8, episode reward: 29.500, mean reward:  1.093 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 77.906732, mean_q: 52.242024, mean_eps: 0.100000\n","     338883/2000000000: episode: 9583, duration: 4.817s, episode steps:  35, steps per second:   7, episode reward: 187.700, mean reward:  5.363 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 91.749717, mean_q: 52.218409, mean_eps: 0.100000\n","     338913/2000000000: episode: 9584, duration: 3.963s, episode steps:  30, steps per second:   8, episode reward: 41.300, mean reward:  1.377 [-20.000, 18.000], mean action: 1.033 [0.000, 2.000],  loss: 84.538032, mean_q: 52.013520, mean_eps: 0.100000\n","     338935/2000000000: episode: 9585, duration: 2.928s, episode steps:  22, steps per second:   8, episode reward: 72.500, mean reward:  3.295 [-20.000, 18.000], mean action: 0.818 [0.000, 2.000],  loss: 100.057142, mean_q: 51.942786, mean_eps: 0.100000\n","     338963/2000000000: episode: 9586, duration: 3.842s, episode steps:  28, steps per second:   7, episode reward: 208.000, mean reward:  7.429 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 87.165010, mean_q: 52.493577, mean_eps: 0.100000\n","     338999/2000000000: episode: 9587, duration: 4.676s, episode steps:  36, steps per second:   8, episode reward: 142.900, mean reward:  3.969 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 90.402382, mean_q: 51.384923, mean_eps: 0.100000\n","     339039/2000000000: episode: 9588, duration: 5.165s, episode steps:  40, steps per second:   8, episode reward: 154.000, mean reward:  3.850 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 85.708829, mean_q: 52.149189, mean_eps: 0.100000\n","     339065/2000000000: episode: 9589, duration: 3.397s, episode steps:  26, steps per second:   8, episode reward: -33.800, mean reward: -1.300 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 99.312644, mean_q: 51.915367, mean_eps: 0.100000\n","     339094/2000000000: episode: 9590, duration: 3.694s, episode steps:  29, steps per second:   8, episode reward: 18.700, mean reward:  0.645 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 89.632543, mean_q: 52.154868, mean_eps: 0.100000\n","     339119/2000000000: episode: 9591, duration: 3.208s, episode steps:  25, steps per second:   8, episode reward: 165.200, mean reward:  6.608 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 77.413514, mean_q: 51.299159, mean_eps: 0.100000\n","     339143/2000000000: episode: 9592, duration: 3.171s, episode steps:  24, steps per second:   8, episode reward: 85.800, mean reward:  3.575 [-20.000, 18.000], mean action: 0.792 [0.000, 2.000],  loss: 85.104963, mean_q: 51.187366, mean_eps: 0.100000\n","     339174/2000000000: episode: 9593, duration: 4.047s, episode steps:  31, steps per second:   8, episode reward: 152.100, mean reward:  4.906 [-20.000, 19.100], mean action: 1.065 [0.000, 2.000],  loss: 77.557145, mean_q: 51.742513, mean_eps: 0.100000\n","     339204/2000000000: episode: 9594, duration: 4.048s, episode steps:  30, steps per second:   7, episode reward: 254.400, mean reward:  8.480 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.306766, mean_q: 51.464900, mean_eps: 0.100000\n","     339226/2000000000: episode: 9595, duration: 2.863s, episode steps:  22, steps per second:   8, episode reward: 178.500, mean reward:  8.114 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 86.603586, mean_q: 53.534185, mean_eps: 0.100000\n","     339257/2000000000: episode: 9596, duration: 4.038s, episode steps:  31, steps per second:   8, episode reward: 148.500, mean reward:  4.790 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 87.084291, mean_q: 51.333930, mean_eps: 0.100000\n","     339284/2000000000: episode: 9597, duration: 3.689s, episode steps:  27, steps per second:   7, episode reward: 66.700, mean reward:  2.470 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 91.269235, mean_q: 52.181523, mean_eps: 0.100000\n","     339313/2000000000: episode: 9598, duration: 3.715s, episode steps:  29, steps per second:   8, episode reward: 142.000, mean reward:  4.897 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 80.432105, mean_q: 52.724963, mean_eps: 0.100000\n","     339350/2000000000: episode: 9599, duration: 4.716s, episode steps:  37, steps per second:   8, episode reward: 79.900, mean reward:  2.159 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 93.676015, mean_q: 52.743802, mean_eps: 0.100000\n","     339386/2000000000: episode: 9600, duration: 4.630s, episode steps:  36, steps per second:   8, episode reward: 172.300, mean reward:  4.786 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 84.962738, mean_q: 52.366818, mean_eps: 0.100000\n","     339412/2000000000: episode: 9601, duration: 3.496s, episode steps:  26, steps per second:   7, episode reward: 93.400, mean reward:  3.592 [-20.000, 18.000], mean action: 0.692 [0.000, 2.000],  loss: 79.767787, mean_q: 51.322426, mean_eps: 0.100000\n","     339435/2000000000: episode: 9602, duration: 3.198s, episode steps:  23, steps per second:   7, episode reward: 28.000, mean reward:  1.217 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 80.312582, mean_q: 52.826816, mean_eps: 0.100000\n","     339472/2000000000: episode: 9603, duration: 5.108s, episode steps:  37, steps per second:   7, episode reward: -31.000, mean reward: -0.838 [-20.000, 18.000], mean action: 1.216 [0.000, 2.000],  loss: 82.018868, mean_q: 51.938215, mean_eps: 0.100000\n","     339512/2000000000: episode: 9604, duration: 5.379s, episode steps:  40, steps per second:   7, episode reward: 100.000, mean reward:  2.500 [-20.000, 18.000], mean action: 1.525 [0.000, 2.000],  loss: 85.440922, mean_q: 51.677333, mean_eps: 0.100000\n","     339544/2000000000: episode: 9605, duration: 4.139s, episode steps:  32, steps per second:   8, episode reward: 110.500, mean reward:  3.453 [-20.000, 18.000], mean action: 0.969 [0.000, 2.000],  loss: 81.776984, mean_q: 52.541911, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     339574/2000000000: episode: 9606, duration: 3.879s, episode steps:  30, steps per second:   8, episode reward: 125.800, mean reward:  4.193 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 85.598692, mean_q: 51.485214, mean_eps: 0.100000\n","     339600/2000000000: episode: 9607, duration: 3.540s, episode steps:  26, steps per second:   7, episode reward: 168.600, mean reward:  6.485 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.543275, mean_q: 52.650864, mean_eps: 0.100000\n","     339632/2000000000: episode: 9608, duration: 4.132s, episode steps:  32, steps per second:   8, episode reward: 151.800, mean reward:  4.744 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 88.435423, mean_q: 52.363971, mean_eps: 0.100000\n","     339670/2000000000: episode: 9609, duration: 5.005s, episode steps:  38, steps per second:   8, episode reward: 83.800, mean reward:  2.205 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 90.276492, mean_q: 52.091326, mean_eps: 0.100000\n","     339697/2000000000: episode: 9610, duration: 3.567s, episode steps:  27, steps per second:   8, episode reward: 118.400, mean reward:  4.385 [-20.000, 18.400], mean action: 0.963 [0.000, 2.000],  loss: 86.947863, mean_q: 51.559984, mean_eps: 0.100000\n","     339723/2000000000: episode: 9611, duration: 3.464s, episode steps:  26, steps per second:   8, episode reward: 148.800, mean reward:  5.723 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 87.993038, mean_q: 51.573198, mean_eps: 0.100000\n","     339753/2000000000: episode: 9612, duration: 3.916s, episode steps:  30, steps per second:   8, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 74.729341, mean_q: 51.711091, mean_eps: 0.100000\n","     339777/2000000000: episode: 9613, duration: 3.207s, episode steps:  24, steps per second:   7, episode reward: 121.500, mean reward:  5.062 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 83.228319, mean_q: 51.346619, mean_eps: 0.100000\n","     339813/2000000000: episode: 9614, duration: 4.441s, episode steps:  36, steps per second:   8, episode reward: 187.000, mean reward:  5.194 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 91.574404, mean_q: 51.226801, mean_eps: 0.100000\n","     339839/2000000000: episode: 9615, duration: 3.389s, episode steps:  26, steps per second:   8, episode reward: 168.500, mean reward:  6.481 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 84.288451, mean_q: 51.287199, mean_eps: 0.100000\n","     339872/2000000000: episode: 9616, duration: 4.225s, episode steps:  33, steps per second:   8, episode reward: 149.500, mean reward:  4.530 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 92.236193, mean_q: 51.718487, mean_eps: 0.100000\n","     339900/2000000000: episode: 9617, duration: 3.633s, episode steps:  28, steps per second:   8, episode reward: 64.700, mean reward:  2.311 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.773738, mean_q: 51.415888, mean_eps: 0.100000\n","     339928/2000000000: episode: 9618, duration: 3.618s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 82.868118, mean_q: 52.744505, mean_eps: 0.100000\n","     339958/2000000000: episode: 9619, duration: 4.058s, episode steps:  30, steps per second:   7, episode reward: 56.000, mean reward:  1.867 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 79.434972, mean_q: 52.432156, mean_eps: 0.100000\n","     339995/2000000000: episode: 9620, duration: 5.019s, episode steps:  37, steps per second:   7, episode reward: -134.000, mean reward: -3.622 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 76.286260, mean_q: 51.419193, mean_eps: 0.100000\n","     340028/2000000000: episode: 9621, duration: 4.551s, episode steps:  33, steps per second:   7, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.456717, mean_q: 52.931090, mean_eps: 0.100000\n","     340061/2000000000: episode: 9622, duration: 4.246s, episode steps:  33, steps per second:   8, episode reward: 102.700, mean reward:  3.112 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 90.502656, mean_q: 52.790888, mean_eps: 0.100000\n","     340094/2000000000: episode: 9623, duration: 4.278s, episode steps:  33, steps per second:   8, episode reward: 39.700, mean reward:  1.203 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 92.455717, mean_q: 52.897068, mean_eps: 0.100000\n","     340123/2000000000: episode: 9624, duration: 3.710s, episode steps:  29, steps per second:   8, episode reward: 158.200, mean reward:  5.455 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 83.006652, mean_q: 54.009832, mean_eps: 0.100000\n","     340147/2000000000: episode: 9625, duration: 3.060s, episode steps:  24, steps per second:   8, episode reward: 135.800, mean reward:  5.658 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 88.631621, mean_q: 54.436148, mean_eps: 0.100000\n","     340172/2000000000: episode: 9626, duration: 3.084s, episode steps:  25, steps per second:   8, episode reward: 190.900, mean reward:  7.636 [-20.000, 18.000], mean action: 0.640 [0.000, 2.000],  loss: 97.901105, mean_q: 54.293522, mean_eps: 0.100000\n","     340194/2000000000: episode: 9627, duration: 2.799s, episode steps:  22, steps per second:   8, episode reward: 94.000, mean reward:  4.273 [-20.000, 18.000], mean action: 0.455 [0.000, 2.000],  loss: 88.872867, mean_q: 53.907634, mean_eps: 0.100000\n","     340219/2000000000: episode: 9628, duration: 3.232s, episode steps:  25, steps per second:   8, episode reward: 151.800, mean reward:  6.072 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 96.115894, mean_q: 53.309869, mean_eps: 0.100000\n","     340244/2000000000: episode: 9629, duration: 3.485s, episode steps:  25, steps per second:   7, episode reward: 26.200, mean reward:  1.048 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 95.885909, mean_q: 54.646641, mean_eps: 0.100000\n","     340271/2000000000: episode: 9630, duration: 3.674s, episode steps:  27, steps per second:   7, episode reward: 34.400, mean reward:  1.274 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 93.545683, mean_q: 52.779572, mean_eps: 0.100000\n","     340309/2000000000: episode: 9631, duration: 4.985s, episode steps:  38, steps per second:   8, episode reward: 132.000, mean reward:  3.474 [-20.000, 18.000], mean action: 1.184 [0.000, 2.000],  loss: 78.379667, mean_q: 53.826649, mean_eps: 0.100000\n","     340335/2000000000: episode: 9632, duration: 3.421s, episode steps:  26, steps per second:   8, episode reward: 132.000, mean reward:  5.077 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 86.236234, mean_q: 54.109189, mean_eps: 0.100000\n","     340368/2000000000: episode: 9633, duration: 4.280s, episode steps:  33, steps per second:   8, episode reward: 40.500, mean reward:  1.227 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 93.757507, mean_q: 53.375602, mean_eps: 0.100000\n","     340390/2000000000: episode: 9634, duration: 2.989s, episode steps:  22, steps per second:   7, episode reward: 44.800, mean reward:  2.036 [-20.000, 18.000], mean action: 0.545 [0.000, 2.000],  loss: 100.662234, mean_q: 53.942658, mean_eps: 0.100000\n","     340426/2000000000: episode: 9635, duration: 4.906s, episode steps:  36, steps per second:   7, episode reward: 140.500, mean reward:  3.903 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 97.222046, mean_q: 53.578016, mean_eps: 0.100000\n","     340455/2000000000: episode: 9636, duration: 3.880s, episode steps:  29, steps per second:   7, episode reward: 114.900, mean reward:  3.962 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 94.419202, mean_q: 53.392057, mean_eps: 0.100000\n","     340481/2000000000: episode: 9637, duration: 3.793s, episode steps:  26, steps per second:   7, episode reward: 170.000, mean reward:  6.538 [-20.000, 18.000], mean action: 0.885 [0.000, 2.000],  loss: 95.359237, mean_q: 53.418581, mean_eps: 0.100000\n","     340514/2000000000: episode: 9638, duration: 4.841s, episode steps:  33, steps per second:   7, episode reward: 173.500, mean reward:  5.258 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 92.013259, mean_q: 53.381364, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     340536/2000000000: episode: 9639, duration: 3.005s, episode steps:  22, steps per second:   7, episode reward: 170.000, mean reward:  7.727 [-20.000, 18.000], mean action: 0.682 [0.000, 2.000],  loss: 92.197818, mean_q: 55.037140, mean_eps: 0.100000\n","     340564/2000000000: episode: 9640, duration: 4.036s, episode steps:  28, steps per second:   7, episode reward: 133.700, mean reward:  4.775 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 87.233998, mean_q: 53.555409, mean_eps: 0.100000\n","     340588/2000000000: episode: 9641, duration: 3.223s, episode steps:  24, steps per second:   7, episode reward: 60.900, mean reward:  2.537 [-20.000, 18.000], mean action: 0.625 [0.000, 2.000],  loss: 81.716389, mean_q: 53.837327, mean_eps: 0.100000\n","     340620/2000000000: episode: 9642, duration: 4.457s, episode steps:  32, steps per second:   7, episode reward:  7.700, mean reward:  0.241 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 97.331851, mean_q: 53.743466, mean_eps: 0.100000\n","     340657/2000000000: episode: 9643, duration: 4.803s, episode steps:  37, steps per second:   8, episode reward: 113.600, mean reward:  3.070 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 88.436314, mean_q: 53.271103, mean_eps: 0.100000\n","     340681/2000000000: episode: 9644, duration: 3.300s, episode steps:  24, steps per second:   7, episode reward: 145.900, mean reward:  6.079 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 89.632299, mean_q: 53.999379, mean_eps: 0.100000\n","     340703/2000000000: episode: 9645, duration: 2.934s, episode steps:  22, steps per second:   7, episode reward: 170.400, mean reward:  7.745 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 92.635212, mean_q: 52.583826, mean_eps: 0.100000\n","     340734/2000000000: episode: 9646, duration: 4.034s, episode steps:  31, steps per second:   8, episode reward: 215.900, mean reward:  6.965 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 97.235520, mean_q: 53.784273, mean_eps: 0.100000\n","     340759/2000000000: episode: 9647, duration: 3.429s, episode steps:  25, steps per second:   7, episode reward: 176.000, mean reward:  7.040 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 88.388305, mean_q: 54.685679, mean_eps: 0.100000\n","     340794/2000000000: episode: 9648, duration: 4.763s, episode steps:  35, steps per second:   7, episode reward: 275.900, mean reward:  7.883 [-20.000, 18.000], mean action: 1.143 [0.000, 2.000],  loss: 86.845504, mean_q: 52.988396, mean_eps: 0.100000\n","     340821/2000000000: episode: 9649, duration: 4.088s, episode steps:  27, steps per second:   7, episode reward: 87.200, mean reward:  3.230 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 80.808805, mean_q: 54.687754, mean_eps: 0.100000\n","     340846/2000000000: episode: 9650, duration: 3.344s, episode steps:  25, steps per second:   7, episode reward: -18.900, mean reward: -0.756 [-20.000, 18.000], mean action: 0.680 [0.000, 2.000],  loss: 83.598088, mean_q: 54.815201, mean_eps: 0.100000\n","     340879/2000000000: episode: 9651, duration: 4.083s, episode steps:  33, steps per second:   8, episode reward: 69.100, mean reward:  2.094 [-20.000, 18.000], mean action: 1.121 [0.000, 2.000],  loss: 87.692793, mean_q: 53.818859, mean_eps: 0.100000\n","     340909/2000000000: episode: 9652, duration: 4.681s, episode steps:  30, steps per second:   6, episode reward: 38.600, mean reward:  1.287 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.371618, mean_q: 53.480438, mean_eps: 0.100000\n","     340936/2000000000: episode: 9653, duration: 3.773s, episode steps:  27, steps per second:   7, episode reward: 38.100, mean reward:  1.411 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 88.763899, mean_q: 54.263719, mean_eps: 0.100000\n","     340965/2000000000: episode: 9654, duration: 3.688s, episode steps:  29, steps per second:   8, episode reward: 56.100, mean reward:  1.934 [-20.000, 18.600], mean action: 1.069 [0.000, 2.000],  loss: 85.575401, mean_q: 53.023222, mean_eps: 0.100000\n","     340993/2000000000: episode: 9655, duration: 3.708s, episode steps:  28, steps per second:   8, episode reward: 124.700, mean reward:  4.454 [-20.000, 18.000], mean action: 1.036 [0.000, 2.000],  loss: 84.474894, mean_q: 53.843724, mean_eps: 0.100000\n","     341026/2000000000: episode: 9656, duration: 4.978s, episode steps:  33, steps per second:   7, episode reward: 144.600, mean reward:  4.382 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 89.242688, mean_q: 53.354149, mean_eps: 0.100000\n","     341055/2000000000: episode: 9657, duration: 3.799s, episode steps:  29, steps per second:   8, episode reward: 225.400, mean reward:  7.772 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 92.877943, mean_q: 54.641899, mean_eps: 0.100000\n","     341082/2000000000: episode: 9658, duration: 3.498s, episode steps:  27, steps per second:   8, episode reward: 207.200, mean reward:  7.674 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 89.488196, mean_q: 53.304934, mean_eps: 0.100000\n","     341113/2000000000: episode: 9659, duration: 4.102s, episode steps:  31, steps per second:   8, episode reward: 115.000, mean reward:  3.710 [-20.000, 18.000], mean action: 1.129 [0.000, 2.000],  loss: 96.572774, mean_q: 54.582401, mean_eps: 0.100000\n","     341139/2000000000: episode: 9660, duration: 3.897s, episode steps:  26, steps per second:   7, episode reward: 99.300, mean reward:  3.819 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 86.532561, mean_q: 53.348429, mean_eps: 0.100000\n","     341166/2000000000: episode: 9661, duration: 3.537s, episode steps:  27, steps per second:   8, episode reward: 62.000, mean reward:  2.296 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 94.799870, mean_q: 53.152390, mean_eps: 0.100000\n","     341198/2000000000: episode: 9662, duration: 4.310s, episode steps:  32, steps per second:   7, episode reward: 243.400, mean reward:  7.606 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 93.664591, mean_q: 52.667011, mean_eps: 0.100000\n","     341225/2000000000: episode: 9663, duration: 3.671s, episode steps:  27, steps per second:   7, episode reward: 114.300, mean reward:  4.233 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 84.510465, mean_q: 55.004362, mean_eps: 0.100000\n","     341248/2000000000: episode: 9664, duration: 3.412s, episode steps:  23, steps per second:   7, episode reward: 142.900, mean reward:  6.213 [-20.000, 18.000], mean action: 0.870 [0.000, 2.000],  loss: 82.870746, mean_q: 53.791758, mean_eps: 0.100000\n","     341274/2000000000: episode: 9665, duration: 3.876s, episode steps:  26, steps per second:   7, episode reward: 186.900, mean reward:  7.188 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 93.567754, mean_q: 55.375329, mean_eps: 0.100000\n","     341296/2000000000: episode: 9666, duration: 3.005s, episode steps:  22, steps per second:   7, episode reward: 142.300, mean reward:  6.468 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 102.151595, mean_q: 53.648070, mean_eps: 0.100000\n","     341319/2000000000: episode: 9667, duration: 2.928s, episode steps:  23, steps per second:   8, episode reward: 147.400, mean reward:  6.409 [-20.000, 18.000], mean action: 0.913 [0.000, 2.000],  loss: 89.210155, mean_q: 53.904339, mean_eps: 0.100000\n","     341347/2000000000: episode: 9668, duration: 3.704s, episode steps:  28, steps per second:   8, episode reward: 338.300, mean reward: 12.082 [ 0.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 81.576819, mean_q: 54.740070, mean_eps: 0.100000\n","     341369/2000000000: episode: 9669, duration: 2.851s, episode steps:  22, steps per second:   8, episode reward: 147.200, mean reward:  6.691 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 94.455467, mean_q: 53.185427, mean_eps: 0.100000\n","     341394/2000000000: episode: 9670, duration: 3.270s, episode steps:  25, steps per second:   8, episode reward: 188.100, mean reward:  7.524 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 88.260092, mean_q: 54.563162, mean_eps: 0.100000\n","     341426/2000000000: episode: 9671, duration: 4.499s, episode steps:  32, steps per second:   7, episode reward: 127.600, mean reward:  3.987 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 90.758950, mean_q: 53.155523, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     341464/2000000000: episode: 9672, duration: 4.869s, episode steps:  38, steps per second:   8, episode reward: 42.100, mean reward:  1.108 [-20.000, 18.000], mean action: 1.211 [0.000, 2.000],  loss: 80.777071, mean_q: 53.431156, mean_eps: 0.100000\n","     341486/2000000000: episode: 9673, duration: 2.879s, episode steps:  22, steps per second:   8, episode reward: 207.500, mean reward:  9.432 [-20.000, 18.000], mean action: 0.636 [0.000, 2.000],  loss: 93.101441, mean_q: 52.701277, mean_eps: 0.100000\n","     341526/2000000000: episode: 9674, duration: 5.725s, episode steps:  40, steps per second:   7, episode reward: 142.200, mean reward:  3.555 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 88.698920, mean_q: 54.908167, mean_eps: 0.100000\n","     341548/2000000000: episode: 9675, duration: 3.053s, episode steps:  22, steps per second:   7, episode reward: 210.100, mean reward:  9.550 [-19.400, 18.000], mean action: 0.545 [0.000, 2.000],  loss: 88.097950, mean_q: 53.493205, mean_eps: 0.100000\n","     341575/2000000000: episode: 9676, duration: 4.194s, episode steps:  27, steps per second:   6, episode reward: 48.300, mean reward:  1.789 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 81.325638, mean_q: 53.017302, mean_eps: 0.100000\n","     341608/2000000000: episode: 9677, duration: 5.194s, episode steps:  33, steps per second:   6, episode reward: 156.600, mean reward:  4.745 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 88.681202, mean_q: 54.085990, mean_eps: 0.100000\n","     341634/2000000000: episode: 9678, duration: 3.664s, episode steps:  26, steps per second:   7, episode reward: 206.900, mean reward:  7.958 [-20.000, 18.000], mean action: 0.731 [0.000, 2.000],  loss: 95.402964, mean_q: 54.065932, mean_eps: 0.100000\n","     341662/2000000000: episode: 9679, duration: 3.972s, episode steps:  28, steps per second:   7, episode reward: 164.900, mean reward:  5.889 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 91.749879, mean_q: 53.479029, mean_eps: 0.100000\n","     341701/2000000000: episode: 9680, duration: 5.608s, episode steps:  39, steps per second:   7, episode reward: 160.900, mean reward:  4.126 [-20.000, 18.100], mean action: 1.205 [0.000, 2.000],  loss: 83.921315, mean_q: 54.172499, mean_eps: 0.100000\n","     341738/2000000000: episode: 9681, duration: 5.149s, episode steps:  37, steps per second:   7, episode reward: 121.700, mean reward:  3.289 [-20.000, 18.000], mean action: 1.108 [0.000, 2.000],  loss: 90.610321, mean_q: 53.955033, mean_eps: 0.100000\n","     341766/2000000000: episode: 9682, duration: 3.646s, episode steps:  28, steps per second:   8, episode reward: 132.000, mean reward:  4.714 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 84.452967, mean_q: 54.103677, mean_eps: 0.100000\n","     341794/2000000000: episode: 9683, duration: 4.056s, episode steps:  28, steps per second:   7, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.964 [0.000, 2.000],  loss: 88.362007, mean_q: 54.395502, mean_eps: 0.100000\n","     341831/2000000000: episode: 9684, duration: 5.062s, episode steps:  37, steps per second:   7, episode reward: 154.800, mean reward:  4.184 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 93.714282, mean_q: 53.977895, mean_eps: 0.100000\n","     341857/2000000000: episode: 9685, duration: 3.466s, episode steps:  26, steps per second:   8, episode reward: 119.700, mean reward:  4.604 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 103.621629, mean_q: 54.542251, mean_eps: 0.100000\n","     341886/2000000000: episode: 9686, duration: 3.726s, episode steps:  29, steps per second:   8, episode reward: 65.900, mean reward:  2.272 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 93.662995, mean_q: 52.889237, mean_eps: 0.100000\n","     341912/2000000000: episode: 9687, duration: 3.344s, episode steps:  26, steps per second:   8, episode reward: 117.000, mean reward:  4.500 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 92.315720, mean_q: 55.478121, mean_eps: 0.100000\n","     341949/2000000000: episode: 9688, duration: 4.578s, episode steps:  37, steps per second:   8, episode reward: 170.000, mean reward:  4.595 [-20.000, 18.000], mean action: 1.270 [0.000, 2.000],  loss: 93.221180, mean_q: 53.468061, mean_eps: 0.100000\n","     341976/2000000000: episode: 9689, duration: 3.403s, episode steps:  27, steps per second:   8, episode reward: 175.400, mean reward:  6.496 [-20.000, 18.000], mean action: 0.889 [0.000, 2.000],  loss: 89.548533, mean_q: 53.870305, mean_eps: 0.100000\n","     342002/2000000000: episode: 9690, duration: 3.244s, episode steps:  26, steps per second:   8, episode reward: 246.000, mean reward:  9.462 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 96.801726, mean_q: 53.813728, mean_eps: 0.100000\n","     342039/2000000000: episode: 9691, duration: 4.474s, episode steps:  37, steps per second:   8, episode reward: -58.000, mean reward: -1.568 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 83.283266, mean_q: 54.208802, mean_eps: 0.100000\n","     342079/2000000000: episode: 9692, duration: 4.936s, episode steps:  40, steps per second:   8, episode reward: 171.000, mean reward:  4.275 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 85.794495, mean_q: 54.416111, mean_eps: 0.100000\n","     342113/2000000000: episode: 9693, duration: 5.078s, episode steps:  34, steps per second:   7, episode reward: 226.800, mean reward:  6.671 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 87.128835, mean_q: 54.809970, mean_eps: 0.100000\n","     342143/2000000000: episode: 9694, duration: 4.120s, episode steps:  30, steps per second:   7, episode reward: 208.000, mean reward:  6.933 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 84.670967, mean_q: 53.214650, mean_eps: 0.100000\n","     342176/2000000000: episode: 9695, duration: 5.497s, episode steps:  33, steps per second:   6, episode reward: 132.000, mean reward:  4.000 [-20.000, 18.000], mean action: 1.212 [0.000, 2.000],  loss: 95.241395, mean_q: 53.773981, mean_eps: 0.100000\n","     342208/2000000000: episode: 9696, duration: 4.683s, episode steps:  32, steps per second:   7, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.296989, mean_q: 54.110081, mean_eps: 0.100000\n","     342237/2000000000: episode: 9697, duration: 3.730s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 84.689468, mean_q: 53.303732, mean_eps: 0.100000\n","     342265/2000000000: episode: 9698, duration: 3.426s, episode steps:  28, steps per second:   8, episode reward: 170.000, mean reward:  6.071 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 92.908236, mean_q: 53.488128, mean_eps: 0.100000\n","     342289/2000000000: episode: 9699, duration: 2.992s, episode steps:  24, steps per second:   8, episode reward: 105.500, mean reward:  4.396 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 85.426771, mean_q: 52.075849, mean_eps: 0.100000\n","     342329/2000000000: episode: 9700, duration: 4.949s, episode steps:  40, steps per second:   8, episode reward: 98.000, mean reward:  2.450 [-20.000, 18.000], mean action: 1.500 [0.000, 2.000],  loss: 84.267364, mean_q: 53.393963, mean_eps: 0.100000\n","     342351/2000000000: episode: 9701, duration: 2.698s, episode steps:  22, steps per second:   8, episode reward: 132.000, mean reward:  6.000 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 88.951775, mean_q: 53.855504, mean_eps: 0.100000\n","     342376/2000000000: episode: 9702, duration: 3.060s, episode steps:  25, steps per second:   8, episode reward: -78.200, mean reward: -3.128 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 87.044285, mean_q: 53.036043, mean_eps: 0.100000\n","     342407/2000000000: episode: 9703, duration: 3.894s, episode steps:  31, steps per second:   8, episode reward: 170.000, mean reward:  5.484 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 90.758103, mean_q: 53.033571, mean_eps: 0.100000\n","     342447/2000000000: episode: 9704, duration: 4.827s, episode steps:  40, steps per second:   8, episode reward: 94.000, mean reward:  2.350 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 88.735699, mean_q: 53.711194, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     342487/2000000000: episode: 9705, duration: 4.995s, episode steps:  40, steps per second:   8, episode reward: 80.000, mean reward:  2.000 [-20.000, 18.000], mean action: 1.475 [0.000, 2.000],  loss: 92.458508, mean_q: 53.321456, mean_eps: 0.100000\n","     342509/2000000000: episode: 9706, duration: 2.800s, episode steps:  22, steps per second:   8, episode reward: 94.000, mean reward:  4.273 [-20.000, 18.000], mean action: 0.545 [0.000, 2.000],  loss: 83.192879, mean_q: 52.763235, mean_eps: 0.100000\n","     342536/2000000000: episode: 9707, duration: 3.396s, episode steps:  27, steps per second:   8, episode reward: 208.000, mean reward:  7.704 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 87.236798, mean_q: 54.845194, mean_eps: 0.100000\n","     342569/2000000000: episode: 9708, duration: 4.165s, episode steps:  33, steps per second:   8, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.868589, mean_q: 54.334919, mean_eps: 0.100000\n","     342604/2000000000: episode: 9709, duration: 4.326s, episode steps:  35, steps per second:   8, episode reward: -96.000, mean reward: -2.743 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 83.420337, mean_q: 53.505497, mean_eps: 0.100000\n","     342633/2000000000: episode: 9710, duration: 3.648s, episode steps:  29, steps per second:   8, episode reward: 56.000, mean reward:  1.931 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.597967, mean_q: 53.442624, mean_eps: 0.100000\n","     342667/2000000000: episode: 9711, duration: 4.165s, episode steps:  34, steps per second:   8, episode reward: 132.000, mean reward:  3.882 [-20.000, 18.000], mean action: 1.147 [0.000, 2.000],  loss: 78.360507, mean_q: 53.927390, mean_eps: 0.100000\n","     342692/2000000000: episode: 9712, duration: 3.555s, episode steps:  25, steps per second:   7, episode reward: 195.700, mean reward:  7.828 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 86.160973, mean_q: 54.349735, mean_eps: 0.100000\n","     342720/2000000000: episode: 9713, duration: 3.692s, episode steps:  28, steps per second:   8, episode reward: 150.100, mean reward:  5.361 [-20.000, 18.000], mean action: 0.857 [0.000, 2.000],  loss: 86.560921, mean_q: 53.616191, mean_eps: 0.100000\n","     342755/2000000000: episode: 9714, duration: 4.587s, episode steps:  35, steps per second:   8, episode reward: 195.300, mean reward:  5.580 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 89.433314, mean_q: 52.550314, mean_eps: 0.100000\n","     342788/2000000000: episode: 9715, duration: 4.805s, episode steps:  33, steps per second:   7, episode reward: 82.700, mean reward:  2.506 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 85.223532, mean_q: 53.797331, mean_eps: 0.100000\n","     342819/2000000000: episode: 9716, duration: 4.203s, episode steps:  31, steps per second:   7, episode reward:  6.500, mean reward:  0.210 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 81.188196, mean_q: 54.910860, mean_eps: 0.100000\n","     342841/2000000000: episode: 9717, duration: 3.177s, episode steps:  22, steps per second:   7, episode reward: 162.300, mean reward:  7.377 [-20.000, 18.000], mean action: 0.591 [0.000, 2.000],  loss: 87.265156, mean_q: 54.686188, mean_eps: 0.100000\n","     342878/2000000000: episode: 9718, duration: 4.953s, episode steps:  37, steps per second:   7, episode reward: -15.900, mean reward: -0.430 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 93.741643, mean_q: 53.558763, mean_eps: 0.100000\n","     342909/2000000000: episode: 9719, duration: 3.920s, episode steps:  31, steps per second:   8, episode reward: 162.100, mean reward:  5.229 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.850521, mean_q: 53.325635, mean_eps: 0.100000\n","     342934/2000000000: episode: 9720, duration: 3.562s, episode steps:  25, steps per second:   7, episode reward: 18.000, mean reward:  0.720 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 94.574613, mean_q: 54.412749, mean_eps: 0.100000\n","     342960/2000000000: episode: 9721, duration: 4.223s, episode steps:  26, steps per second:   6, episode reward: 115.500, mean reward:  4.442 [-20.000, 18.000], mean action: 0.923 [0.000, 2.000],  loss: 90.571912, mean_q: 54.702736, mean_eps: 0.100000\n","     342990/2000000000: episode: 9722, duration: 4.376s, episode steps:  30, steps per second:   7, episode reward: 75.300, mean reward:  2.510 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 86.682543, mean_q: 54.281741, mean_eps: 0.100000\n","     343028/2000000000: episode: 9723, duration: 5.692s, episode steps:  38, steps per second:   7, episode reward: 56.000, mean reward:  1.474 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 88.599435, mean_q: 54.471770, mean_eps: 0.100000\n","     343061/2000000000: episode: 9724, duration: 4.823s, episode steps:  33, steps per second:   7, episode reward: 87.100, mean reward:  2.639 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 82.216410, mean_q: 54.141569, mean_eps: 0.100000\n","     343096/2000000000: episode: 9725, duration: 4.953s, episode steps:  35, steps per second:   7, episode reward: 170.000, mean reward:  4.857 [-20.000, 18.000], mean action: 1.229 [0.000, 2.000],  loss: 95.826740, mean_q: 53.327235, mean_eps: 0.100000\n","     343136/2000000000: episode: 9726, duration: 5.333s, episode steps:  40, steps per second:   8, episode reward: 74.600, mean reward:  1.865 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 85.807260, mean_q: 53.528017, mean_eps: 0.100000\n","     343168/2000000000: episode: 9727, duration: 4.288s, episode steps:  32, steps per second:   7, episode reward: 208.000, mean reward:  6.500 [-20.000, 18.000], mean action: 1.031 [0.000, 2.000],  loss: 92.443485, mean_q: 53.958209, mean_eps: 0.100000\n","     343207/2000000000: episode: 9728, duration: 5.438s, episode steps:  39, steps per second:   7, episode reward: -34.400, mean reward: -0.882 [-20.000, 18.000], mean action: 1.179 [0.000, 2.000],  loss: 89.663420, mean_q: 53.572189, mean_eps: 0.100000\n","     343232/2000000000: episode: 9729, duration: 3.330s, episode steps:  25, steps per second:   8, episode reward: 56.000, mean reward:  2.240 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 87.688698, mean_q: 54.611917, mean_eps: 0.100000\n","     343257/2000000000: episode: 9730, duration: 3.405s, episode steps:  25, steps per second:   7, episode reward: 176.900, mean reward:  7.076 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 89.407805, mean_q: 52.180823, mean_eps: 0.100000\n","     343288/2000000000: episode: 9731, duration: 4.292s, episode steps:  31, steps per second:   7, episode reward: 115.900, mean reward:  3.739 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 81.254646, mean_q: 53.265612, mean_eps: 0.100000\n","     343312/2000000000: episode: 9732, duration: 3.302s, episode steps:  24, steps per second:   7, episode reward: 125.600, mean reward:  5.233 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 88.652037, mean_q: 52.895228, mean_eps: 0.100000\n","     343341/2000000000: episode: 9733, duration: 4.046s, episode steps:  29, steps per second:   7, episode reward: 227.300, mean reward:  7.838 [-20.000, 18.000], mean action: 0.862 [0.000, 2.000],  loss: 87.004217, mean_q: 53.774221, mean_eps: 0.100000\n","     343381/2000000000: episode: 9734, duration: 5.884s, episode steps:  40, steps per second:   7, episode reward: 153.600, mean reward:  3.840 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 91.175448, mean_q: 54.002599, mean_eps: 0.100000\n","     343408/2000000000: episode: 9735, duration: 4.106s, episode steps:  27, steps per second:   7, episode reward: 56.000, mean reward:  2.074 [-20.000, 18.000], mean action: 0.926 [0.000, 2.000],  loss: 91.965723, mean_q: 53.261582, mean_eps: 0.100000\n","     343436/2000000000: episode: 9736, duration: 4.100s, episode steps:  28, steps per second:   7, episode reward: -20.000, mean reward: -0.714 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 87.834538, mean_q: 54.119638, mean_eps: 0.100000\n","     343469/2000000000: episode: 9737, duration: 4.415s, episode steps:  33, steps per second:   7, episode reward: 94.000, mean reward:  2.848 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 86.213985, mean_q: 53.911624, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     343493/2000000000: episode: 9738, duration: 3.448s, episode steps:  24, steps per second:   7, episode reward: 94.000, mean reward:  3.917 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 83.076131, mean_q: 54.717180, mean_eps: 0.100000\n","     343530/2000000000: episode: 9739, duration: 5.219s, episode steps:  37, steps per second:   7, episode reward: 193.700, mean reward:  5.235 [-20.000, 18.000], mean action: 1.135 [0.000, 2.000],  loss: 88.766285, mean_q: 54.665199, mean_eps: 0.100000\n","     343557/2000000000: episode: 9740, duration: 4.465s, episode steps:  27, steps per second:   6, episode reward: -17.900, mean reward: -0.663 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 95.698935, mean_q: 52.916392, mean_eps: 0.100000\n","     343588/2000000000: episode: 9741, duration: 4.367s, episode steps:  31, steps per second:   7, episode reward: 206.900, mean reward:  6.674 [-20.000, 18.000], mean action: 0.903 [0.000, 2.000],  loss: 93.269012, mean_q: 54.426533, mean_eps: 0.100000\n","     343615/2000000000: episode: 9742, duration: 3.749s, episode steps:  27, steps per second:   7, episode reward: 30.800, mean reward:  1.141 [-20.000, 18.000], mean action: 0.815 [0.000, 2.000],  loss: 86.423610, mean_q: 54.315762, mean_eps: 0.100000\n","     343647/2000000000: episode: 9743, duration: 4.616s, episode steps:  32, steps per second:   7, episode reward: 232.400, mean reward:  7.262 [-20.000, 19.500], mean action: 1.125 [0.000, 2.000],  loss: 94.127845, mean_q: 53.676500, mean_eps: 0.100000\n","     343674/2000000000: episode: 9744, duration: 3.781s, episode steps:  27, steps per second:   7, episode reward: 185.100, mean reward:  6.856 [-20.000, 18.000], mean action: 0.778 [0.000, 2.000],  loss: 97.498196, mean_q: 54.200747, mean_eps: 0.100000\n","     343707/2000000000: episode: 9745, duration: 5.183s, episode steps:  33, steps per second:   6, episode reward: 61.900, mean reward:  1.876 [-20.000, 18.000], mean action: 0.939 [0.000, 2.000],  loss: 91.269384, mean_q: 54.094655, mean_eps: 0.100000\n","     343747/2000000000: episode: 9746, duration: 6.071s, episode steps:  40, steps per second:   7, episode reward: 158.500, mean reward:  3.963 [-20.000, 18.000], mean action: 1.225 [0.000, 2.000],  loss: 93.599559, mean_q: 53.110282, mean_eps: 0.100000\n","     343772/2000000000: episode: 9747, duration: 3.809s, episode steps:  25, steps per second:   7, episode reward: 99.100, mean reward:  3.964 [-20.000, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 93.182188, mean_q: 54.367850, mean_eps: 0.100000\n","     343802/2000000000: episode: 9748, duration: 4.737s, episode steps:  30, steps per second:   6, episode reward: 73.800, mean reward:  2.460 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 87.016614, mean_q: 52.511143, mean_eps: 0.100000\n","     343842/2000000000: episode: 9749, duration: 5.786s, episode steps:  40, steps per second:   7, episode reward: 44.600, mean reward:  1.115 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 94.194557, mean_q: 53.146654, mean_eps: 0.100000\n","     343870/2000000000: episode: 9750, duration: 3.806s, episode steps:  28, steps per second:   7, episode reward: 284.900, mean reward: 10.175 [-20.000, 18.900], mean action: 1.000 [0.000, 2.000],  loss: 89.439777, mean_q: 54.112791, mean_eps: 0.100000\n","     343902/2000000000: episode: 9751, duration: 4.911s, episode steps:  32, steps per second:   7, episode reward: 132.000, mean reward:  4.125 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 91.323017, mean_q: 53.228682, mean_eps: 0.100000\n","     343935/2000000000: episode: 9752, duration: 4.374s, episode steps:  33, steps per second:   8, episode reward: 18.000, mean reward:  0.545 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 81.210700, mean_q: 54.062125, mean_eps: 0.100000\n","     343968/2000000000: episode: 9753, duration: 4.518s, episode steps:  33, steps per second:   7, episode reward: -20.000, mean reward: -0.606 [-20.000, 18.000], mean action: 1.152 [0.000, 2.000],  loss: 90.885400, mean_q: 53.331126, mean_eps: 0.100000\n","     343998/2000000000: episode: 9754, duration: 4.467s, episode steps:  30, steps per second:   7, episode reward: 102.300, mean reward:  3.410 [-20.000, 18.000], mean action: 0.933 [0.000, 2.000],  loss: 93.947032, mean_q: 52.487889, mean_eps: 0.100000\n","     344032/2000000000: episode: 9755, duration: 4.864s, episode steps:  34, steps per second:   7, episode reward: 171.400, mean reward:  5.041 [-20.000, 19.800], mean action: 1.176 [0.000, 2.000],  loss: 93.763398, mean_q: 53.794060, mean_eps: 0.100000\n","     344058/2000000000: episode: 9756, duration: 3.638s, episode steps:  26, steps per second:   7, episode reward: 82.000, mean reward:  3.154 [-20.000, 18.000], mean action: 0.769 [0.000, 2.000],  loss: 85.596546, mean_q: 54.087000, mean_eps: 0.100000\n","     344083/2000000000: episode: 9757, duration: 3.614s, episode steps:  25, steps per second:   7, episode reward: 201.900, mean reward:  8.076 [-20.000, 18.000], mean action: 0.720 [0.000, 2.000],  loss: 79.403816, mean_q: 53.157242, mean_eps: 0.100000\n","     344116/2000000000: episode: 9758, duration: 4.264s, episode steps:  33, steps per second:   8, episode reward: 76.300, mean reward:  2.312 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 85.951579, mean_q: 53.149811, mean_eps: 0.100000\n","     344149/2000000000: episode: 9759, duration: 4.476s, episode steps:  33, steps per second:   7, episode reward: 24.100, mean reward:  0.730 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 96.972074, mean_q: 52.824413, mean_eps: 0.100000\n","     344177/2000000000: episode: 9760, duration: 3.905s, episode steps:  28, steps per second:   7, episode reward: 59.000, mean reward:  2.107 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 83.330558, mean_q: 55.013529, mean_eps: 0.100000\n","     344209/2000000000: episode: 9761, duration: 4.509s, episode steps:  32, steps per second:   7, episode reward: 72.300, mean reward:  2.259 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 92.807798, mean_q: 53.174587, mean_eps: 0.100000\n","     344240/2000000000: episode: 9762, duration: 4.468s, episode steps:  31, steps per second:   7, episode reward: 110.700, mean reward:  3.571 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 89.286837, mean_q: 53.749341, mean_eps: 0.100000\n","     344277/2000000000: episode: 9763, duration: 5.103s, episode steps:  37, steps per second:   7, episode reward: 208.100, mean reward:  5.624 [-20.000, 18.000], mean action: 1.189 [0.000, 2.000],  loss: 90.282113, mean_q: 54.919295, mean_eps: 0.100000\n","     344317/2000000000: episode: 9764, duration: 5.154s, episode steps:  40, steps per second:   8, episode reward: 100.600, mean reward:  2.515 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 90.290141, mean_q: 52.475249, mean_eps: 0.100000\n","     344347/2000000000: episode: 9765, duration: 4.465s, episode steps:  30, steps per second:   7, episode reward: 92.800, mean reward:  3.093 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 81.822028, mean_q: 54.503553, mean_eps: 0.100000\n","     344372/2000000000: episode: 9766, duration: 3.637s, episode steps:  25, steps per second:   7, episode reward: 95.900, mean reward:  3.836 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 90.076842, mean_q: 52.734495, mean_eps: 0.100000\n","     344406/2000000000: episode: 9767, duration: 5.208s, episode steps:  34, steps per second:   7, episode reward: 90.800, mean reward:  2.671 [-20.000, 18.000], mean action: 1.059 [0.000, 2.000],  loss: 80.800241, mean_q: 55.005395, mean_eps: 0.100000\n","     344436/2000000000: episode: 9768, duration: 4.312s, episode steps:  30, steps per second:   7, episode reward: -22.100, mean reward: -0.737 [-20.000, 18.000], mean action: 1.100 [0.000, 2.000],  loss: 83.866563, mean_q: 54.318670, mean_eps: 0.100000\n","     344460/2000000000: episode: 9769, duration: 3.280s, episode steps:  24, steps per second:   7, episode reward: 186.100, mean reward:  7.754 [-20.000, 18.000], mean action: 0.500 [0.000, 2.000],  loss: 86.423396, mean_q: 54.681052, mean_eps: 0.100000\n","     344490/2000000000: episode: 9770, duration: 4.072s, episode steps:  30, steps per second:   7, episode reward: 181.300, mean reward:  6.043 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 83.922902, mean_q: 53.364101, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     344520/2000000000: episode: 9771, duration: 4.404s, episode steps:  30, steps per second:   7, episode reward: 173.700, mean reward:  5.790 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 76.650909, mean_q: 54.884704, mean_eps: 0.100000\n","     344550/2000000000: episode: 9772, duration: 4.043s, episode steps:  30, steps per second:   7, episode reward: 160.500, mean reward:  5.350 [-20.000, 18.000], mean action: 1.167 [0.000, 2.000],  loss: 85.029733, mean_q: 54.094087, mean_eps: 0.100000\n","     344577/2000000000: episode: 9773, duration: 3.503s, episode steps:  27, steps per second:   8, episode reward: 116.600, mean reward:  4.319 [-20.000, 18.000], mean action: 0.741 [0.000, 2.000],  loss: 80.946018, mean_q: 54.397962, mean_eps: 0.100000\n","     344606/2000000000: episode: 9774, duration: 3.746s, episode steps:  29, steps per second:   8, episode reward: 112.500, mean reward:  3.879 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 76.818114, mean_q: 54.693397, mean_eps: 0.100000\n","     344634/2000000000: episode: 9775, duration: 3.530s, episode steps:  28, steps per second:   8, episode reward: 153.800, mean reward:  5.493 [-20.000, 18.000], mean action: 0.929 [0.000, 2.000],  loss: 84.941170, mean_q: 54.562039, mean_eps: 0.100000\n","     344663/2000000000: episode: 9776, duration: 3.805s, episode steps:  29, steps per second:   8, episode reward: 94.000, mean reward:  3.241 [-20.000, 18.000], mean action: 0.931 [0.000, 2.000],  loss: 97.260143, mean_q: 54.495023, mean_eps: 0.100000\n","     344695/2000000000: episode: 9777, duration: 4.115s, episode steps:  32, steps per second:   8, episode reward: 121.900, mean reward:  3.809 [-20.000, 18.000], mean action: 1.094 [0.000, 2.000],  loss: 89.475779, mean_q: 53.846428, mean_eps: 0.100000\n","     344732/2000000000: episode: 9778, duration: 4.628s, episode steps:  37, steps per second:   8, episode reward: 96.000, mean reward:  2.595 [-20.000, 18.000], mean action: 1.243 [0.000, 2.000],  loss: 86.124417, mean_q: 53.845958, mean_eps: 0.100000\n","     344764/2000000000: episode: 9779, duration: 3.960s, episode steps:  32, steps per second:   8, episode reward: 147.000, mean reward:  4.594 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 86.974802, mean_q: 54.274015, mean_eps: 0.100000\n","     344794/2000000000: episode: 9780, duration: 3.875s, episode steps:  30, steps per second:   8, episode reward: 119.700, mean reward:  3.990 [-20.000, 18.000], mean action: 0.900 [0.000, 2.000],  loss: 90.337200, mean_q: 54.222305, mean_eps: 0.100000\n","     344820/2000000000: episode: 9781, duration: 3.326s, episode steps:  26, steps per second:   8, episode reward: -134.000, mean reward: -5.154 [-20.000, 18.000], mean action: 0.846 [0.000, 2.000],  loss: 85.959672, mean_q: 54.246227, mean_eps: 0.100000\n","     344849/2000000000: episode: 9782, duration: 3.721s, episode steps:  29, steps per second:   8, episode reward: 41.400, mean reward:  1.428 [-20.000, 18.000], mean action: 0.966 [0.000, 2.000],  loss: 81.615677, mean_q: 54.426914, mean_eps: 0.100000\n","     344889/2000000000: episode: 9783, duration: 5.128s, episode steps:  40, steps per second:   8, episode reward: 108.100, mean reward:  2.702 [-20.000, 18.000], mean action: 1.325 [0.000, 2.000],  loss: 82.110820, mean_q: 53.292269, mean_eps: 0.100000\n","     344929/2000000000: episode: 9784, duration: 5.043s, episode steps:  40, steps per second:   8, episode reward: 123.600, mean reward:  3.090 [-20.000, 18.000], mean action: 1.375 [0.000, 2.000],  loss: 86.623484, mean_q: 53.608950, mean_eps: 0.100000\n","     344964/2000000000: episode: 9785, duration: 4.809s, episode steps:  35, steps per second:   7, episode reward: 96.500, mean reward:  2.757 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 91.469765, mean_q: 53.751280, mean_eps: 0.100000\n","     344999/2000000000: episode: 9786, duration: 4.562s, episode steps:  35, steps per second:   8, episode reward: 198.000, mean reward:  5.657 [-20.000, 18.000], mean action: 1.114 [0.000, 2.000],  loss: 84.285345, mean_q: 53.390794, mean_eps: 0.100000\n","     345024/2000000000: episode: 9787, duration: 3.074s, episode steps:  25, steps per second:   8, episode reward: 161.100, mean reward:  6.444 [-20.000, 18.000], mean action: 0.560 [0.000, 2.000],  loss: 83.061725, mean_q: 54.331667, mean_eps: 0.100000\n","     345058/2000000000: episode: 9788, duration: 4.136s, episode steps:  34, steps per second:   8, episode reward: 56.000, mean reward:  1.647 [-20.000, 18.000], mean action: 1.265 [0.000, 2.000],  loss: 89.669173, mean_q: 52.957269, mean_eps: 0.100000\n","     345094/2000000000: episode: 9789, duration: 4.489s, episode steps:  36, steps per second:   8, episode reward: 132.000, mean reward:  3.667 [-20.000, 18.000], mean action: 1.278 [0.000, 2.000],  loss: 85.799017, mean_q: 52.654208, mean_eps: 0.100000\n","     345120/2000000000: episode: 9790, duration: 3.212s, episode steps:  26, steps per second:   8, episode reward: 155.000, mean reward:  5.962 [-20.000, 18.000], mean action: 0.808 [0.000, 2.000],  loss: 83.128908, mean_q: 53.705359, mean_eps: 0.100000\n","     345153/2000000000: episode: 9791, duration: 4.232s, episode steps:  33, steps per second:   8, episode reward: -24.100, mean reward: -0.730 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 77.373646, mean_q: 54.640163, mean_eps: 0.100000\n","     345193/2000000000: episode: 9792, duration: 5.138s, episode steps:  40, steps per second:   8, episode reward: 32.900, mean reward:  0.822 [-20.000, 18.000], mean action: 1.300 [0.000, 2.000],  loss: 90.151272, mean_q: 54.566738, mean_eps: 0.100000\n","     345223/2000000000: episode: 9793, duration: 4.027s, episode steps:  30, steps per second:   7, episode reward: 148.300, mean reward:  4.943 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 91.913483, mean_q: 54.014604, mean_eps: 0.100000\n","     345253/2000000000: episode: 9794, duration: 3.805s, episode steps:  30, steps per second:   8, episode reward: 264.200, mean reward:  8.807 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 86.748929, mean_q: 52.769413, mean_eps: 0.100000\n","     345277/2000000000: episode: 9795, duration: 3.086s, episode steps:  24, steps per second:   8, episode reward: 56.000, mean reward:  2.333 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 96.317665, mean_q: 53.735817, mean_eps: 0.100000\n","     345308/2000000000: episode: 9796, duration: 4.057s, episode steps:  31, steps per second:   8, episode reward: 121.300, mean reward:  3.913 [-20.000, 18.000], mean action: 1.194 [0.000, 2.000],  loss: 82.935260, mean_q: 54.798238, mean_eps: 0.100000\n","     345341/2000000000: episode: 9797, duration: 4.427s, episode steps:  33, steps per second:   7, episode reward: 74.100, mean reward:  2.245 [-20.000, 18.000], mean action: 1.182 [0.000, 2.000],  loss: 87.272859, mean_q: 53.097311, mean_eps: 0.100000\n","     345381/2000000000: episode: 9798, duration: 4.945s, episode steps:  40, steps per second:   8, episode reward: -64.500, mean reward: -1.612 [-20.000, 18.000], mean action: 1.450 [0.000, 2.000],  loss: 80.233518, mean_q: 54.170993, mean_eps: 0.100000\n","     345417/2000000000: episode: 9799, duration: 4.728s, episode steps:  36, steps per second:   8, episode reward: 183.500, mean reward:  5.097 [-20.000, 18.000], mean action: 1.111 [0.000, 2.000],  loss: 94.569914, mean_q: 55.125278, mean_eps: 0.100000\n","     345450/2000000000: episode: 9800, duration: 4.288s, episode steps:  33, steps per second:   8, episode reward: 208.000, mean reward:  6.303 [-20.000, 18.000], mean action: 1.030 [0.000, 2.000],  loss: 85.729795, mean_q: 54.740642, mean_eps: 0.100000\n","     345482/2000000000: episode: 9801, duration: 4.015s, episode steps:  32, steps per second:   8, episode reward: 194.300, mean reward:  6.072 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 97.659144, mean_q: 54.434354, mean_eps: 0.100000\n","     345512/2000000000: episode: 9802, duration: 3.695s, episode steps:  30, steps per second:   8, episode reward: 70.100, mean reward:  2.337 [-20.000, 18.000], mean action: 0.967 [0.000, 2.000],  loss: 98.548723, mean_q: 54.962791, mean_eps: 0.100000\n","     345543/2000000000: episode: 9803, duration: 3.965s, episode steps:  31, steps per second:   8, episode reward: 31.500, mean reward:  1.016 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 88.471146, mean_q: 53.382830, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     345578/2000000000: episode: 9804, duration: 4.681s, episode steps:  35, steps per second:   7, episode reward: 268.400, mean reward:  7.669 [-20.000, 18.000], mean action: 1.171 [0.000, 2.000],  loss: 86.922882, mean_q: 54.916237, mean_eps: 0.100000\n","     345616/2000000000: episode: 9805, duration: 4.972s, episode steps:  38, steps per second:   8, episode reward: 150.900, mean reward:  3.971 [-20.000, 18.000], mean action: 1.132 [0.000, 2.000],  loss: 83.481839, mean_q: 54.079749, mean_eps: 0.100000\n","     345652/2000000000: episode: 9806, duration: 5.112s, episode steps:  36, steps per second:   7, episode reward: 147.600, mean reward:  4.100 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 83.450551, mean_q: 54.133496, mean_eps: 0.100000\n","     345680/2000000000: episode: 9807, duration: 4.230s, episode steps:  28, steps per second:   7, episode reward: 75.300, mean reward:  2.689 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 80.009140, mean_q: 54.162975, mean_eps: 0.100000\n","     345711/2000000000: episode: 9808, duration: 4.484s, episode steps:  31, steps per second:   7, episode reward: 88.200, mean reward:  2.845 [-20.000, 18.000], mean action: 0.968 [0.000, 2.000],  loss: 84.785390, mean_q: 52.514480, mean_eps: 0.100000\n","     345736/2000000000: episode: 9809, duration: 3.474s, episode steps:  25, steps per second:   7, episode reward: 263.800, mean reward: 10.552 [-2.600, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 79.416270, mean_q: 53.948333, mean_eps: 0.100000\n","     345776/2000000000: episode: 9810, duration: 5.480s, episode steps:  40, steps per second:   7, episode reward: 139.000, mean reward:  3.475 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.274114, mean_q: 53.761499, mean_eps: 0.100000\n","     345800/2000000000: episode: 9811, duration: 3.288s, episode steps:  24, steps per second:   7, episode reward: 118.400, mean reward:  4.933 [-20.000, 18.000], mean action: 0.667 [0.000, 2.000],  loss: 86.898105, mean_q: 53.403474, mean_eps: 0.100000\n","     345831/2000000000: episode: 9812, duration: 4.680s, episode steps:  31, steps per second:   7, episode reward: 131.700, mean reward:  4.248 [-20.000, 18.000], mean action: 1.097 [0.000, 2.000],  loss: 94.053000, mean_q: 52.605052, mean_eps: 0.100000\n","     345871/2000000000: episode: 9813, duration: 6.093s, episode steps:  40, steps per second:   7, episode reward: 28.300, mean reward:  0.707 [-20.000, 18.000], mean action: 1.350 [0.000, 2.000],  loss: 84.967900, mean_q: 53.493447, mean_eps: 0.100000\n","     345895/2000000000: episode: 9814, duration: 3.144s, episode steps:  24, steps per second:   8, episode reward: 231.900, mean reward:  9.662 [-20.000, 18.000], mean action: 0.750 [0.000, 2.000],  loss: 80.732125, mean_q: 54.790045, mean_eps: 0.100000\n","     345926/2000000000: episode: 9815, duration: 4.471s, episode steps:  31, steps per second:   7, episode reward: 242.300, mean reward:  7.816 [-20.000, 18.000], mean action: 1.161 [0.000, 2.000],  loss: 83.696068, mean_q: 52.692149, mean_eps: 0.100000\n","     345951/2000000000: episode: 9816, duration: 3.206s, episode steps:  25, steps per second:   8, episode reward: 112.100, mean reward:  4.484 [-20.000, 18.000], mean action: 0.920 [0.000, 2.000],  loss: 92.932476, mean_q: 53.609931, mean_eps: 0.100000\n","     345976/2000000000: episode: 9817, duration: 3.685s, episode steps:  25, steps per second:   7, episode reward: 316.800, mean reward: 12.672 [-8.900, 18.000], mean action: 0.800 [0.000, 2.000],  loss: 82.298337, mean_q: 53.525326, mean_eps: 0.100000\n","     346005/2000000000: episode: 9818, duration: 4.315s, episode steps:  29, steps per second:   7, episode reward: 193.600, mean reward:  6.676 [-20.000, 18.000], mean action: 1.069 [0.000, 2.000],  loss: 81.614164, mean_q: 53.973448, mean_eps: 0.100000\n","     346030/2000000000: episode: 9819, duration: 3.571s, episode steps:  25, steps per second:   7, episode reward: 106.200, mean reward:  4.248 [-20.000, 18.000], mean action: 0.760 [0.000, 2.000],  loss: 84.833312, mean_q: 51.590895, mean_eps: 0.100000\n","     346063/2000000000: episode: 9820, duration: 4.314s, episode steps:  33, steps per second:   8, episode reward: 39.400, mean reward:  1.194 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 86.286555, mean_q: 54.235358, mean_eps: 0.100000\n","     346096/2000000000: episode: 9821, duration: 4.032s, episode steps:  33, steps per second:   8, episode reward: 179.200, mean reward:  5.430 [-20.000, 18.000], mean action: 1.273 [0.000, 2.000],  loss: 81.936558, mean_q: 54.284511, mean_eps: 0.100000\n","     346124/2000000000: episode: 9822, duration: 3.604s, episode steps:  28, steps per second:   8, episode reward: 147.200, mean reward:  5.257 [-20.000, 18.000], mean action: 0.893 [0.000, 2.000],  loss: 80.346783, mean_q: 53.182340, mean_eps: 0.100000\n","     346147/2000000000: episode: 9823, duration: 3.202s, episode steps:  23, steps per second:   7, episode reward: 40.000, mean reward:  1.739 [-20.000, 18.000], mean action: 0.565 [0.000, 2.000],  loss: 96.828331, mean_q: 53.229614, mean_eps: 0.100000\n","     346180/2000000000: episode: 9824, duration: 4.272s, episode steps:  33, steps per second:   8, episode reward: 41.200, mean reward:  1.248 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 83.498982, mean_q: 53.685702, mean_eps: 0.100000\n","     346209/2000000000: episode: 9825, duration: 3.919s, episode steps:  29, steps per second:   7, episode reward: 120.700, mean reward:  4.162 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 88.020557, mean_q: 53.779640, mean_eps: 0.100000\n","     346239/2000000000: episode: 9826, duration: 4.209s, episode steps:  30, steps per second:   7, episode reward: 196.000, mean reward:  6.533 [-20.000, 18.000], mean action: 1.000 [0.000, 2.000],  loss: 87.776946, mean_q: 53.912394, mean_eps: 0.100000\n","     346263/2000000000: episode: 9827, duration: 3.052s, episode steps:  24, steps per second:   8, episode reward: -67.400, mean reward: -2.808 [-20.000, 18.000], mean action: 0.833 [0.000, 2.000],  loss: 86.823633, mean_q: 53.666931, mean_eps: 0.100000\n","     346288/2000000000: episode: 9828, duration: 3.225s, episode steps:  25, steps per second:   8, episode reward: 177.900, mean reward:  7.116 [-20.000, 18.000], mean action: 0.840 [0.000, 2.000],  loss: 99.524302, mean_q: 54.359121, mean_eps: 0.100000\n","     346314/2000000000: episode: 9829, duration: 3.314s, episode steps:  26, steps per second:   8, episode reward: 152.200, mean reward:  5.854 [-20.000, 18.000], mean action: 0.654 [0.000, 2.000],  loss: 88.738876, mean_q: 53.840201, mean_eps: 0.100000\n","     346350/2000000000: episode: 9830, duration: 4.537s, episode steps:  36, steps per second:   8, episode reward: 62.900, mean reward:  1.747 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 94.736083, mean_q: 53.137955, mean_eps: 0.100000\n","     346377/2000000000: episode: 9831, duration: 3.588s, episode steps:  27, steps per second:   8, episode reward: 163.300, mean reward:  6.048 [-20.000, 18.000], mean action: 0.852 [0.000, 2.000],  loss: 87.430055, mean_q: 52.903368, mean_eps: 0.100000\n","     346413/2000000000: episode: 9832, duration: 4.480s, episode steps:  36, steps per second:   8, episode reward: 68.600, mean reward:  1.906 [-20.000, 18.000], mean action: 1.222 [0.000, 2.000],  loss: 86.170857, mean_q: 53.702349, mean_eps: 0.100000\n","     346445/2000000000: episode: 9833, duration: 3.873s, episode steps:  32, steps per second:   8, episode reward:  6.900, mean reward:  0.216 [-20.000, 18.000], mean action: 1.125 [0.000, 2.000],  loss: 88.188504, mean_q: 54.966110, mean_eps: 0.100000\n","     346484/2000000000: episode: 9834, duration: 4.702s, episode steps:  39, steps per second:   8, episode reward: 66.400, mean reward:  1.703 [-20.000, 18.000], mean action: 1.282 [0.000, 2.000],  loss: 91.390303, mean_q: 53.654581, mean_eps: 0.100000\n","     346516/2000000000: episode: 9835, duration: 4.109s, episode steps:  32, steps per second:   8, episode reward: 18.000, mean reward:  0.562 [-20.000, 18.000], mean action: 1.156 [0.000, 2.000],  loss: 92.496827, mean_q: 54.580016, mean_eps: 0.100000\n","     346556/2000000000: episode: 9836, duration: 5.021s, episode steps:  40, steps per second:   8, episode reward: 185.300, mean reward:  4.633 [-20.000, 18.000], mean action: 1.175 [0.000, 2.000],  loss: 90.011127, mean_q: 53.577271, mean_eps: 0.100000\n"],"name":"stdout"},{"output_type":"stream","text":["     346585/2000000000: episode: 9837, duration: 3.682s, episode steps:  29, steps per second:   8, episode reward: 170.200, mean reward:  5.869 [-20.000, 18.200], mean action: 0.759 [0.000, 2.000],  loss: 82.724487, mean_q: 52.526863, mean_eps: 0.100000\n","     346625/2000000000: episode: 9838, duration: 5.270s, episode steps:  40, steps per second:   8, episode reward: 132.000, mean reward:  3.300 [-20.000, 18.000], mean action: 1.250 [0.000, 2.000],  loss: 83.809702, mean_q: 53.589493, mean_eps: 0.100000\n","     346662/2000000000: episode: 9839, duration: 4.980s, episode steps:  37, steps per second:   7, episode reward: 18.000, mean reward:  0.486 [-20.000, 18.000], mean action: 1.081 [0.000, 2.000],  loss: 95.149683, mean_q: 55.103849, mean_eps: 0.100000\n","     346690/2000000000: episode: 9840, duration: 3.706s, episode steps:  28, steps per second:   8, episode reward: 194.800, mean reward:  6.957 [-20.000, 18.000], mean action: 0.786 [0.000, 2.000],  loss: 84.272505, mean_q: 53.944693, mean_eps: 0.100000\n","     346723/2000000000: episode: 9841, duration: 4.135s, episode steps:  33, steps per second:   8, episode reward: 56.000, mean reward:  1.697 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 88.554525, mean_q: 54.079519, mean_eps: 0.100000\n","     346754/2000000000: episode: 9842, duration: 4.105s, episode steps:  31, steps per second:   8, episode reward: 132.000, mean reward:  4.258 [-20.000, 18.000], mean action: 0.935 [0.000, 2.000],  loss: 92.396033, mean_q: 54.443284, mean_eps: 0.100000\n","     346785/2000000000: episode: 9843, duration: 4.007s, episode steps:  31, steps per second:   8, episode reward: 110.200, mean reward:  3.555 [-20.000, 18.000], mean action: 1.065 [0.000, 2.000],  loss: 91.774916, mean_q: 54.453602, mean_eps: 0.100000\n","     346815/2000000000: episode: 9844, duration: 3.812s, episode steps:  30, steps per second:   8, episode reward: 160.600, mean reward:  5.353 [-20.000, 18.000], mean action: 1.133 [0.000, 2.000],  loss: 94.479731, mean_q: 54.251301, mean_eps: 0.100000\n","     346843/2000000000: episode: 9845, duration: 3.747s, episode steps:  28, steps per second:   7, episode reward: 189.800, mean reward:  6.779 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 83.276442, mean_q: 53.917175, mean_eps: 0.100000\n","     346866/2000000000: episode: 9846, duration: 3.405s, episode steps:  23, steps per second:   7, episode reward: 170.000, mean reward:  7.391 [-20.000, 18.000], mean action: 0.652 [0.000, 2.000],  loss: 83.892907, mean_q: 54.318822, mean_eps: 0.100000\n","     346901/2000000000: episode: 9847, duration: 4.820s, episode steps:  35, steps per second:   7, episode reward: 153.300, mean reward:  4.380 [-20.000, 18.000], mean action: 1.314 [0.000, 2.000],  loss: 82.309258, mean_q: 53.838313, mean_eps: 0.100000\n","     346934/2000000000: episode: 9848, duration: 4.628s, episode steps:  33, steps per second:   7, episode reward: 208.000, mean reward:  6.303 [-20.000, 18.000], mean action: 1.091 [0.000, 2.000],  loss: 83.458812, mean_q: 54.150256, mean_eps: 0.100000\n","     346962/2000000000: episode: 9849, duration: 3.940s, episode steps:  28, steps per second:   7, episode reward: -58.000, mean reward: -2.071 [-20.000, 18.000], mean action: 0.714 [0.000, 2.000],  loss: 89.819013, mean_q: 53.432632, mean_eps: 0.100000\n","     346994/2000000000: episode: 9850, duration: 4.348s, episode steps:  32, steps per second:   7, episode reward: 94.000, mean reward:  2.938 [-20.000, 18.000], mean action: 1.062 [0.000, 2.000],  loss: 94.189166, mean_q: 53.778348, mean_eps: 0.100000\n","     347029/2000000000: episode: 9851, duration: 4.667s, episode steps:  35, steps per second:   7, episode reward: 132.000, mean reward:  3.771 [-20.000, 18.000], mean action: 1.057 [0.000, 2.000],  loss: 82.078930, mean_q: 54.191221, mean_eps: 0.100000\n","     347057/2000000000: episode: 9852, duration: 3.750s, episode steps:  28, steps per second:   7, episode reward: 94.000, mean reward:  3.357 [-20.000, 18.000], mean action: 0.821 [0.000, 2.000],  loss: 77.829640, mean_q: 53.712242, mean_eps: 0.100000\n","     347088/2000000000: episode: 9853, duration: 4.287s, episode steps:  31, steps per second:   7, episode reward: 18.000, mean reward:  0.581 [-20.000, 18.000], mean action: 1.032 [0.000, 2.000],  loss: 76.712052, mean_q: 55.179634, mean_eps: 0.100000\n","     347128/2000000000: episode: 9854, duration: 5.428s, episode steps:  40, steps per second:   7, episode reward: -38.000, mean reward: -0.950 [-20.000, 18.000], mean action: 1.275 [0.000, 2.000],  loss: 89.106441, mean_q: 54.020417, mean_eps: 0.100000\n","     347164/2000000000: episode: 9855, duration: 5.033s, episode steps:  36, steps per second:   7, episode reward: 284.000, mean reward:  7.889 [-20.000, 18.000], mean action: 1.139 [0.000, 2.000],  loss: 85.347845, mean_q: 52.928229, mean_eps: 0.100000\n","     347197/2000000000: episode: 9856, duration: 5.029s, episode steps:  33, steps per second:   7, episode reward: 170.000, mean reward:  5.152 [-20.000, 18.000], mean action: 1.061 [0.000, 2.000],  loss: 83.006559, mean_q: 54.236214, mean_eps: 0.100000\n","     347220/2000000000: episode: 9857, duration: 4.142s, episode steps:  23, steps per second:   6, episode reward: 170.000, mean reward:  7.391 [-20.000, 18.000], mean action: 0.913 [0.000, 2.000],  loss: 82.704662, mean_q: 53.640946, mean_eps: 0.100000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_SGClsvLVFzE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbVmj1n6f03m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcF0hSHn4LFm"},"source":["Reloud Model\n"]},{"cell_type":"code","metadata":{"id":"Tnw-3p9Rf0-T"},"source":["#name = 'WSaved/n1/13-03-03/WSaveddqn_13-03-03.h5f'\n","name = 'WSaved/n2/15-10-14/WSaveddqn_15-10-14.h5f'\n","dqnlive.load_weights(name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"axfauThlf1Dr","outputId":"9b4fab1a-1922-4ddc-e278-ab77ba12111a"},"source":["  AllReward = []\n","  jTime = 1\n","  dqnlive.test(env_8M,nb_episodes=30,visualize=False)\n","  ShowV3()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing for 30 episodes ...\n"," jTime :1 \n","Episode 1: reward: 44.000, steps: 25\n"," jTime :11336 \n","Episode 2: reward: 26.000, steps: 51\n"," jTime :40548 \n","Episode 3: reward: 26.000, steps: 23\n"," jTime :50260 \n","Episode 4: reward: -10.000, steps: 28\n"," jTime :63133 \n","Episode 5: reward: -10.000, steps: 20\n"," jTime :70783 \n","Episode 6: reward: 26.000, steps: 27\n"," jTime :83155 \n","Episode 7: reward: 8.000, steps: 22\n"," jTime :91845 \n","Episode 8: reward: 8.000, steps: 23\n"," jTime :101880 \n","Episode 9: reward: -28.000, steps: 38\n"," jTime :121887 \n","Episode 10: reward: 26.000, steps: 47\n"," jTime :148187 \n","Episode 11: reward: 44.000, steps: 52\n"," jTime :178408 \n","Episode 12: reward: -10.000, steps: 38\n"," jTime :198902 \n","Episode 13: reward: -28.000, steps: 28\n"," jTime :212530 \n","Episode 14: reward: -64.000, steps: 44\n"," jTime :237035 \n","Episode 15: reward: 44.000, steps: 21\n"," jTime :245084 \n","Episode 16: reward: -10.000, steps: 21\n"," jTime :253459 \n","Episode 17: reward: -10.000, steps: 21\n"," jTime :261821 \n","Episode 18: reward: -64.000, steps: 21\n"," jTime :269888 \n","Episode 19: reward: 8.000, steps: 19\n"," jTime :277069 \n","Episode 20: reward: -28.000, steps: 30\n"," jTime :291905 \n","Episode 21: reward: -28.000, steps: 20\n"," jTime :299366 \n","Episode 22: reward: 26.000, steps: 24\n"," jTime :309921 \n","Episode 23: reward: -28.000, steps: 22\n"," jTime :318941 \n","Episode 24: reward: 44.000, steps: 26\n"," jTime :330629 \n","Episode 25: reward: -28.000, steps: 24\n"," jTime :340824 \n","Episode 26: reward: 8.000, steps: 39\n"," jTime :361579 \n","Episode 27: reward: -10.000, steps: 23\n"," jTime :371694 \n","Episode 28: reward: 26.000, steps: 24\n"," jTime :382048 \n","Episode 29: reward: 8.000, steps: 39\n"," jTime :402849 \n","Episode 30: reward: 44.000, steps: 30\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'ShowV3' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-01b335ef6054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdqnlive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_8M\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mShowV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ShowV3' is not defined"]}]},{"cell_type":"code","metadata":{"id":"oExGy3kEf1Kr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"ehfJBZ6YRE9o","outputId":"d6458d9c-c536-4f28-80e2-dec4a00d7092"},"source":["def ShowV3():\n","  R = AllReward\n","  x = []\n","  s = 0\n","  for i in range(0,len(R)):\n","    s = s + R[i]\n","    x.append(s)\n","  y = np.arange(0,len(R) ) \n","  plt.figure(figsize=(13, 6))\n","  plt.plot( y, x, label=\"US30 Line\") \n","  plt.xlabel(\"x axis\")\n","  plt.ylabel(\"y axis\")\n","  plt.title(\"Line Graph Example\")\n","  plt.show()  # 200\n","ShowV3()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 936x432 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"IYxO0LcSRFGo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyliALuZRFNr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaKZA2okRFVS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tehATExIRFcy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-8aBSqyRFj1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"2W1mRDmrf1Px","outputId":"50ac1c76-39d3-4b6a-b62f-47057c036130"},"source":["b = 214022 +5\n","def showV6(b):  # data is np\n","  plt.figure(figsize=(13, 6))\n","  x = data[b:(b+700)]\n","  y  = np.arange(start=0, stop=len(x) , step=1) \n","  price = data[b]\n","  plt.plot( 0  , price ,  marker='.', markersize=16, color=\"darkgreen\") \n","  plt.plot(  0 , price + 0.001 ,  marker='*', markersize=16, color=\"green\") \n","  plt.plot(  0 , price - 0.001 ,  marker='*', markersize=16, color=\"green\") \n","  plt.plot(  y ,  x    , label=\"EURUSD Line\" )\n","  plt.show()\n","\n","showV6(b)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 936x432 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}